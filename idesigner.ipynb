{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# self_defined_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFilter\n",
    "from torchvision.transforms import *\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter, ImageOps\n",
    "import cv2\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# import visdom\n",
    "\n",
    "import argparse, time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.parallel.data_parallel import data_parallel\n",
    "import torchvision\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "# import visdom\n",
    "# import pretrainedmodels\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.nn.parallel.data_parallel import data_parallel\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdesigned_image_test_v2\u001b[0m/  \u001b[01;34mdesigner_image_train_v2_cropped\u001b[0m/  sample.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls ../input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultConfigs(object):\n",
    "    data_root = '../input'\n",
    "    train_root = '../input/designer_image_train_v2_cropped/designer_image_train_v2_cropped' \n",
    "    test_root = '../input/designed_image_test_v2/test'\n",
    "    \n",
    "    model = 'ResNet18' # ResNet152 使用的模型\n",
    "    freeze = True # 是否冻结卷基层\n",
    "\n",
    "    seed = 1000 # 固定随机种子\n",
    "    num_workers = 2 # DataLoader 中的多线程数量\n",
    "    num_classes = 50 # 分类类别数\n",
    "    num_epochs = 60\n",
    "    batch_size = 64 # 128 48\n",
    "    lr = 0.01 # 初始lr\n",
    "    width = 256 # 输入图像的宽\n",
    "    height = 256 # 输入图像的高\n",
    "    iter_smooth = 1 # 打印&记录log的频率\n",
    "\n",
    "    # OHEM\n",
    "    OHEM = False\n",
    "    OHEM_ratio = 0.1\n",
    "\n",
    "    # resume checkpoint\n",
    "    resume = False #\n",
    "    checkpoint = 'ResNet18.pth' # 训练完成的模型名\n",
    "    # checkpoint = 'ResNet18.pth' # 训练完成的模型名\n",
    "    # smooth label\n",
    "    smooth_label = False\n",
    "\n",
    "config = DefaultConfigs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creat img list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5325\n"
     ]
    }
   ],
   "source": [
    "random.seed(config.seed)\n",
    "\n",
    "if not os.path.exists('./data'):\n",
    "    os.makedirs('./data')\n",
    "\n",
    "train_txt = open('./data/train.txt', 'w')\n",
    "val_txt = open('./data/valid.txt', 'w')\n",
    "test_txt = open('./data/test.txt', 'w')\n",
    "label_txt = open('./data/label_list.txt', 'w')\n",
    "\n",
    "# creat label list\n",
    "label_list = []\n",
    "for dir in os.listdir(config.train_root):\n",
    "    if dir not in label_list:\n",
    "        label_list.append(dir)\n",
    "        label_txt.write('{}  {}\\n'.format(dir,len(label_list)))\n",
    "\n",
    "def creat_train_txt(root = config.train_root):\n",
    "    i = 0\n",
    "    for i in range(len(label_list)):\n",
    "        im_path = os.path.join(root,label_list[i])\n",
    "        train_list = random.sample(os.listdir(im_path), int(len(os.listdir(im_path))*0.9))\n",
    "        for im in train_list:\n",
    "            train_txt.write('{}/{}  {}\\n'.format(label_list[i], im, i))\n",
    "        for im in os.listdir(im_path):\n",
    "            if im not in train_list:\n",
    "                val_txt.write('{}/{}  {}\\n'.format(label_list[i], im, i))\n",
    "        i += 1\n",
    "            \n",
    "def creat_test_txt(root = config.test_root):\n",
    "    test_list = os.listdir(root)\n",
    "    print(len(test_list))\n",
    "    for im in test_list:\n",
    "        test_txt.write('{}  {}\\n'.format(im, 0))\n",
    "        \n",
    "creat_train_txt()\n",
    "creat_test_txt()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5325\n"
     ]
    }
   ],
   "source": [
    "random.seed(config.seed)\n",
    "\n",
    "if not os.path.exists('./data'):\n",
    "    os.makedirs('./data')\n",
    "\n",
    "train_txt = open('./data/train.txt', 'w')\n",
    "val_txt = open('./data/valid.txt', 'w')\n",
    "test_txt = open('./data/test.txt', 'w')\n",
    "label_txt = open('./data/label_list.txt', 'w')\n",
    "\n",
    "# creat label list\n",
    "label_list = []\n",
    "for dir in os.listdir(config.train_root):\n",
    "    if dir not in label_list:\n",
    "        label_list.append(dir)\n",
    "        label_txt.write('{}  {}\\n'.format(dir,len(label_list)))\n",
    "\n",
    "def creat_train_txt(root = config.train_root):\n",
    "    i = 0\n",
    "    for i in range(len(label_list)):\n",
    "        im_path = os.path.join(root,label_list[i])\n",
    "        train_list = random.sample(os.listdir(im_path), int(len(os.listdir(im_path))*0.9))\n",
    "        for im in train_list:\n",
    "            train_txt.write('{}/{}  {}\\n'.format(label_list[i], im, i))\n",
    "        for im in os.listdir(im_path):\n",
    "            if im not in train_list:\n",
    "                val_txt.write('{}/{}  {}\\n'.format(label_list[i], im, i))\n",
    "        i += 1\n",
    "            \n",
    "def creat_test_txt(root = config.test_root):\n",
    "    test_list = os.listdir(root)\n",
    "    print(len(test_list))\n",
    "    for im in test_list:\n",
    "        test_txt.write('{}  {}\\n'.format(im, 0))\n",
    "        \n",
    "creat_train_txt()\n",
    "creat_test_txt()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# see txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 40574\n",
      "valid: 4533\n",
      "test: 5325\n",
      "label list: 50\n"
     ]
    }
   ],
   "source": [
    "with open('./data/train.txt') as f1:\n",
    "    lines1 = f1.readlines()\n",
    "    print('train:',len(lines1))\n",
    "\n",
    "with open('./data/valid.txt') as f2:\n",
    "    lines2 = f2.readlines()\n",
    "    print('valid:',len(lines2))\n",
    "\n",
    "with open('./data/test.txt') as f3:\n",
    "    lines3 = f3.readlines()\n",
    "    print('test:',len(lines3))\n",
    "    \n",
    "    \n",
    "with open('./data/label_list.txt') as f4:\n",
    "    lines4 = f4.readlines()\n",
    "    print('label list:',len(lines4))\n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomErasing(im, probability = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[128, 128, 128]):\n",
    "    '''\n",
    "    performs Random Erasing in Random Erasing Data Augmentation by Zhong et al. \n",
    "    -------------------------------------------------------------------------------------\n",
    "    img: PIL img\n",
    "    probability: The probability that the operation will be performed.\n",
    "    sl: min erasing area\n",
    "    sh: max erasing area\n",
    "    r1: min aspect ratio\n",
    "    mean: erasing value\n",
    "    -------------------------------------------------------------------------------------\n",
    "    '''\n",
    "    if random.uniform(0, 1) > probability:\n",
    "        return im\n",
    "\n",
    "    else:\n",
    "        img = np.array(im)\n",
    "        area = img.shape[0] * img.shape[1]\n",
    "       \n",
    "        while True:\n",
    "            target_area = random.uniform(sl, sh) * area\n",
    "            aspect_ratio = random.uniform(r1, 1/r1)\n",
    "            h = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "            w = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "            if img.shape[0] > w and img.shape[1] > h:\n",
    "                break\n",
    "\n",
    "        if w < img.shape[0] and h < img.shape[1]:\n",
    "            x1 = random.randint(0, img.shape[0] - w)\n",
    "            y1 = random.randint(0, img.shape[1] - h)\n",
    "            # if img.size()[0] == 3:\n",
    "            if im.mode == 'RGB':\n",
    "                img[x1:x1+h, y1:y1+w, 0] = mean[0]\n",
    "                img[x1:x1+h, y1:y1+w, 1] = mean[1]\n",
    "                img[x1:x1+h, y1:y1+w, 2] = mean[2]\n",
    "            elif im.mode == 'L':\n",
    "                img[x1:x1+h, y1:y1+w] = mean[0]\n",
    "        img = Image.fromarray(np.uint8(img))\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "class MyGaussianBlur(ImageFilter.Filter):\n",
    "    name = \"GaussianBlur\"\n",
    "\n",
    "    def __init__(self, radius=2):\n",
    "        self.radius = radius\n",
    "\n",
    "    def filter(self, image):\n",
    "        return image.gaussian_blur(self.radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(path):\n",
    "    ims, labels = [], []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            im, label = line.strip().split('  ')\n",
    "            ims.append(im)\n",
    "            labels.append(int(label))\n",
    "    return ims, labels\n",
    "\n",
    "class iDesignerDataset(Dataset):\n",
    "    def __init__(self, txt_path, transform=None):\n",
    "        self.ims, self.labels = read_txt(txt_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        im_path = self.ims[index]\n",
    "        label = self.labels[index]\n",
    "        im_path = os.path.join(config.train_root, im_path)\n",
    "        im = Image.open(im_path)\n",
    "        #im = im.resize((self.width, self.height))\n",
    "        if self.transform is not None:\n",
    "            im = RandomErasing(im, probability=0.5, sl=0.1, sh=0.4, r1=0.3, mean=[128, 128, 128])\n",
    "            if random.random() < 0.5:\n",
    "                im = im.filter(MyGaussianBlur(radius=5))\n",
    "            im = self.transform(im)\n",
    "\n",
    "        return im, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ims)\n",
    "\n",
    "def padding_image(im):\n",
    "    w, h = im.size\n",
    "    pad_im = Image.new('RGB', (w, h), (128, 128, 128))\n",
    "    pad_im.paste(im, (0, 0))\n",
    "\n",
    "    return pad_im\n",
    "\n",
    "class iDesignerTestDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None, augment=None):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.ims = df['Id']\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name = self.ims[index]\n",
    "        # im_path = os.path.join(config.data_root, 'test', name)\n",
    "        im_path = os.path.join(config.data_root, 'designed_image_test_v2/test', name)\n",
    "        # print(im_path)\n",
    "        im = Image.open(im_path)\n",
    "        im = padding_image(im)\n",
    "        #im = im.resize((self.width, self.height))\n",
    "        if self.augment == 2:\n",
    "            im = im\n",
    "        # top-left crop\n",
    "        if self.augment == 1:\n",
    "            w, h = im.size\n",
    "            im = im.crop((0, 0, w-10, h-10))\n",
    "        # center crop\n",
    "        if self.augment == 0:\n",
    "            w, h = im.size\n",
    "            im = im.crop((10, 10, w-10, h-10))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)\n",
    "\n",
    "        return im, name\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# self-defined softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def self_defined_softmax(outputs):\n",
    "#     # outputs = Variable(outputs,requires_grad=True)\n",
    "#     batch_size = outputs.size(0)\n",
    "#     num_classes = outputs.size(1)\n",
    "    \n",
    "#     # step 1 : exp\n",
    "#     for i in range(batch_size):\n",
    "#         outputs[i] = torch.exp(outputs[i])\n",
    "\n",
    "#     # step 2 : normalize\n",
    "#     for i in range(batch_size):\n",
    "#         sum = torch.ones(batch_size)\n",
    "#         sum[i] = torch.FloatTensor([0])\n",
    "#         for j in range(num_classes):\n",
    "#             sum[i] = torch.add(sum[i], outputs[i][j]).cuda()\n",
    "#         outputs[i] = torch.div(outputs[i], sum[i])\n",
    "    \n",
    "#     return outputs\n",
    "\n",
    "def self_defined_softmax(outputs):\n",
    "    batch_size = outputs.size(0)\n",
    "    for i in range(batch_size):\n",
    "        outputs[i]=torch.exp(outputs[i])/torch.sum(torch.exp(outputs[i]))\n",
    "    return  outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    r\"\"\"\n",
    "        This criterion is a implemenation of Focal Loss, which is proposed in \n",
    "        Focal Loss for Dense Object Detection.\n",
    "\n",
    "            Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n",
    "\n",
    "        The losses are averaged across observations for each minibatch.\n",
    "\n",
    "        Args:\n",
    "            alpha(1D Tensor, Variable) : the scalar factor for this criterion\n",
    "            gamma(float, double) : gamma > 0; reduces the relative loss for well-classiﬁed examples (p > .5), \n",
    "                                   putting more focus on hard, misclassiﬁed examples\n",
    "            size_average(bool): By default, the losses are averaged over observations for each minibatch.\n",
    "                                However, if the field size_average is set to False, the losses are\n",
    "                                instead summed for each minibatch.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        if alpha is None:\n",
    "            self.alpha = Variable(torch.ones(class_num, 1))\n",
    "        else:\n",
    "            if isinstance(alpha, Variable):\n",
    "                self.alpha = alpha\n",
    "            else:\n",
    "                self.alpha = Variable(alpha).cuda()\n",
    "        self.gamma = gamma\n",
    "        self.class_num = class_num\n",
    "        self.size_average = size_average\n",
    "\n",
    "    # inputs is the socres of output, and has 2 dims,one for batchsize, the other for num_classes\n",
    "    # targets is groundtruth labels\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        N = inputs.size(0)    # number of batch size\n",
    "        C = inputs.size(1)    # number of classes\n",
    "        # P = F.softmax(inputs)\n",
    "        P = self_defined_softmax(inputs)\n",
    "        # print('after softmax: ',P)\n",
    "        \n",
    "        class_mask = inputs.data.new(N, C).fill_(0)\n",
    "        class_mask = Variable(class_mask)\n",
    "        ids = targets.view(-1, 1)\n",
    "        class_mask.scatter_(1, ids.data, 1.)\n",
    "\n",
    "        if inputs.is_cuda and not self.alpha.is_cuda:\n",
    "            self.alpha = self.alpha.cuda()\n",
    "        alpha = self.alpha[ids.data.view(-1)]\n",
    "\n",
    "        probs = (P*class_mask).sum(1).view(-1,1)\n",
    "\n",
    "        log_p = probs.log()\n",
    "\n",
    "        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p \n",
    "\n",
    "        if self.size_average:\n",
    "            loss = batch_loss.mean()\n",
    "        else:\n",
    "            loss = batch_loss.sum()\n",
    "        return loss\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        if torch.cuda.is_available():\n",
    "            self.criterion = nn.KLDivLoss(size_average=False).cuda()\n",
    "        else:\n",
    "            self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            loss = self.criterion(x.cuda(), Variable(true_dist, requires_grad=False).cuda())\n",
    "        else:\n",
    "            loss = self.criterion(x, Variable(true_dist, requires_grad=False))\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1, 5)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def whitening(im):\n",
    "    batch_size, channel, h, w = im.shape\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    im = torch.cat([(im[:,[0]]-mean[0])/std[0],\n",
    "                    (im[:,[1]]-mean[1])/std[1],\n",
    "                    (im[:,[2]]-mean[2])/std[2]], 1)\n",
    "    return im\n",
    "\n",
    "def l2_norm(x):\n",
    "    norm = torch.norm(x, p=2, dim=1, keepdim=True)\n",
    "    x = torch.div(x, norm)\n",
    "    return x\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, model, num_classes=1000):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.backbone = model\n",
    "\n",
    "        # 3 3*3 convs replace 1 7*7\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
    "\n",
    "        # conv\n",
    "        # conv replace FC\n",
    "        self.conv_final = nn.Conv2d(512, num_classes, 1, stride=1)\n",
    "        self.ada_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # FC\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = whitening(x)\n",
    "        '''\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        '''\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        # FC\n",
    "        x = self.backbone.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = l2_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = l2_norm(x)\n",
    "\n",
    "        '''\n",
    "        # fully conv\n",
    "        x = self.conv_final(x)\n",
    "        x = self.ada_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print x.size()\n",
    "        '''\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_lr(init_lr, ep):\n",
    "    lr = init_lr / 2**ep\n",
    "\n",
    "    return lr\n",
    "\n",
    "def step_lr(ep):\n",
    "    if ep < 20:\n",
    "        lr = 0.005\n",
    "    elif ep < 40:\n",
    "        lr = 0.001\n",
    "    elif epo < 50:\n",
    "        lr = 0.0001\n",
    "    else:\n",
    "        lr = 0.00001\n",
    "    return lr\n",
    "\n",
    "\n",
    "def warmup_lr(init_lr, warmup_epoch, epoch):\n",
    "    if epoch < warmup_epoch:\n",
    "        lr = init_lr / warmup_epoch * (epoch+1)\n",
    "    elif epoch < warmup_epcoh + 80:\n",
    "        lr = init_lr\n",
    "    elif epoch < warmup_epoch + 120:\n",
    "        lr = init_lr / 10\n",
    "    elif epoch < warmup_epoch + 150:\n",
    "        lr = init_lr / 100\n",
    "    elif epoch < warmup_epoch + 200:\n",
    "        lr = init_lr / 1000\n",
    "    else:\n",
    "        lr = 1e-5\n",
    "\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /tmp/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
      "100%|██████████| 46827520/46827520 [00:00<00:00, 133403193.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18(\n",
      "  (backbone): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv_final): Conv2d(512, 50, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (ada_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc2): Linear(in_features=256, out_features=50, bias=True)\n",
      ")\n",
      "Epoch [1/60], Iter [1/633], LR: 0.005000, Loss: 3.7405, top1: 4.6875\n",
      "Epoch [1/60], Iter [2/633], LR: 0.005000, Loss: 3.7811, top1: 0.0000\n",
      "Epoch [1/60], Iter [3/633], LR: 0.005000, Loss: 3.7656, top1: 1.5625\n",
      "Epoch [1/60], Iter [4/633], LR: 0.005000, Loss: 3.7665, top1: 1.5625\n",
      "Epoch [1/60], Iter [5/633], LR: 0.005000, Loss: 3.7512, top1: 1.5625\n",
      "Epoch [1/60], Iter [6/633], LR: 0.005000, Loss: 3.7405, top1: 3.1250\n",
      "Epoch [1/60], Iter [7/633], LR: 0.005000, Loss: 3.7416, top1: 3.1250\n",
      "Epoch [1/60], Iter [8/633], LR: 0.005000, Loss: 3.7338, top1: 3.1250\n",
      "Epoch [1/60], Iter [9/633], LR: 0.005000, Loss: 3.7689, top1: 1.5625\n",
      "Epoch [1/60], Iter [10/633], LR: 0.005000, Loss: 3.7232, top1: 3.1250\n",
      "Epoch [1/60], Iter [11/633], LR: 0.005000, Loss: 3.7536, top1: 3.1250\n",
      "Epoch [1/60], Iter [12/633], LR: 0.005000, Loss: 3.7419, top1: 1.5625\n",
      "Epoch [1/60], Iter [13/633], LR: 0.005000, Loss: 3.7222, top1: 1.5625\n",
      "Epoch [1/60], Iter [14/633], LR: 0.005000, Loss: 3.7413, top1: 1.5625\n",
      "Epoch [1/60], Iter [15/633], LR: 0.005000, Loss: 3.7479, top1: 6.2500\n",
      "Epoch [1/60], Iter [16/633], LR: 0.005000, Loss: 3.7028, top1: 4.6875\n",
      "Epoch [1/60], Iter [17/633], LR: 0.005000, Loss: 3.7447, top1: 1.5625\n",
      "Epoch [1/60], Iter [18/633], LR: 0.005000, Loss: 3.7421, top1: 3.1250\n",
      "Epoch [1/60], Iter [19/633], LR: 0.005000, Loss: 3.7431, top1: 6.2500\n",
      "Epoch [1/60], Iter [20/633], LR: 0.005000, Loss: 3.7174, top1: 0.0000\n",
      "Epoch [1/60], Iter [21/633], LR: 0.005000, Loss: 3.7446, top1: 3.1250\n",
      "Epoch [1/60], Iter [22/633], LR: 0.005000, Loss: 3.7246, top1: 3.1250\n",
      "Epoch [1/60], Iter [23/633], LR: 0.005000, Loss: 3.6934, top1: 4.6875\n",
      "Epoch [1/60], Iter [24/633], LR: 0.005000, Loss: 3.6956, top1: 3.1250\n",
      "Epoch [1/60], Iter [25/633], LR: 0.005000, Loss: 3.7340, top1: 6.2500\n",
      "Epoch [1/60], Iter [26/633], LR: 0.005000, Loss: 3.7308, top1: 4.6875\n",
      "Epoch [1/60], Iter [27/633], LR: 0.005000, Loss: 3.7182, top1: 1.5625\n",
      "Epoch [1/60], Iter [28/633], LR: 0.005000, Loss: 3.7656, top1: 1.5625\n",
      "Epoch [1/60], Iter [29/633], LR: 0.005000, Loss: 3.7103, top1: 3.1250\n",
      "Epoch [1/60], Iter [30/633], LR: 0.005000, Loss: 3.7036, top1: 3.1250\n",
      "Epoch [1/60], Iter [31/633], LR: 0.005000, Loss: 3.7544, top1: 0.0000\n",
      "Epoch [1/60], Iter [32/633], LR: 0.005000, Loss: 3.7392, top1: 0.0000\n",
      "Epoch [1/60], Iter [33/633], LR: 0.005000, Loss: 3.7660, top1: 1.5625\n",
      "Epoch [1/60], Iter [34/633], LR: 0.005000, Loss: 3.7849, top1: 4.6875\n",
      "Epoch [1/60], Iter [35/633], LR: 0.005000, Loss: 3.7513, top1: 1.5625\n",
      "Epoch [1/60], Iter [36/633], LR: 0.005000, Loss: 3.7138, top1: 1.5625\n",
      "Epoch [1/60], Iter [37/633], LR: 0.005000, Loss: 3.7343, top1: 4.6875\n",
      "Epoch [1/60], Iter [38/633], LR: 0.005000, Loss: 3.7461, top1: 0.0000\n",
      "Epoch [1/60], Iter [39/633], LR: 0.005000, Loss: 3.7613, top1: 1.5625\n",
      "Epoch [1/60], Iter [40/633], LR: 0.005000, Loss: 3.7299, top1: 4.6875\n",
      "Epoch [1/60], Iter [41/633], LR: 0.005000, Loss: 3.7232, top1: 3.1250\n",
      "Epoch [1/60], Iter [42/633], LR: 0.005000, Loss: 3.7375, top1: 1.5625\n",
      "Epoch [1/60], Iter [43/633], LR: 0.005000, Loss: 3.7491, top1: 0.0000\n",
      "Epoch [1/60], Iter [44/633], LR: 0.005000, Loss: 3.7571, top1: 0.0000\n",
      "Epoch [1/60], Iter [45/633], LR: 0.005000, Loss: 3.7306, top1: 7.8125\n",
      "Epoch [1/60], Iter [46/633], LR: 0.005000, Loss: 3.7583, top1: 3.1250\n",
      "Epoch [1/60], Iter [47/633], LR: 0.005000, Loss: 3.7170, top1: 1.5625\n",
      "Epoch [1/60], Iter [48/633], LR: 0.005000, Loss: 3.7360, top1: 3.1250\n",
      "Epoch [1/60], Iter [49/633], LR: 0.005000, Loss: 3.7607, top1: 0.0000\n",
      "Epoch [1/60], Iter [50/633], LR: 0.005000, Loss: 3.7326, top1: 0.0000\n",
      "Epoch [1/60], Iter [51/633], LR: 0.005000, Loss: 3.7045, top1: 6.2500\n",
      "Epoch [1/60], Iter [52/633], LR: 0.005000, Loss: 3.7576, top1: 1.5625\n",
      "Epoch [1/60], Iter [53/633], LR: 0.005000, Loss: 3.7226, top1: 1.5625\n",
      "Epoch [1/60], Iter [54/633], LR: 0.005000, Loss: 3.7341, top1: 1.5625\n",
      "Epoch [1/60], Iter [55/633], LR: 0.005000, Loss: 3.7635, top1: 0.0000\n",
      "Epoch [1/60], Iter [56/633], LR: 0.005000, Loss: 3.7160, top1: 3.1250\n",
      "Epoch [1/60], Iter [57/633], LR: 0.005000, Loss: 3.7359, top1: 4.6875\n",
      "Epoch [1/60], Iter [58/633], LR: 0.005000, Loss: 3.7249, top1: 6.2500\n",
      "Epoch [1/60], Iter [59/633], LR: 0.005000, Loss: 3.7321, top1: 3.1250\n",
      "Epoch [1/60], Iter [60/633], LR: 0.005000, Loss: 3.7452, top1: 7.8125\n",
      "Epoch [1/60], Iter [61/633], LR: 0.005000, Loss: 3.7341, top1: 3.1250\n",
      "Epoch [1/60], Iter [62/633], LR: 0.005000, Loss: 3.7192, top1: 4.6875\n",
      "Epoch [1/60], Iter [63/633], LR: 0.005000, Loss: 3.7066, top1: 1.5625\n",
      "Epoch [1/60], Iter [64/633], LR: 0.005000, Loss: 3.7226, top1: 4.6875\n",
      "Epoch [1/60], Iter [65/633], LR: 0.005000, Loss: 3.7448, top1: 1.5625\n",
      "Epoch [1/60], Iter [66/633], LR: 0.005000, Loss: 3.7451, top1: 4.6875\n",
      "Epoch [1/60], Iter [67/633], LR: 0.005000, Loss: 3.7356, top1: 3.1250\n",
      "Epoch [1/60], Iter [68/633], LR: 0.005000, Loss: 3.7261, top1: 1.5625\n",
      "Epoch [1/60], Iter [69/633], LR: 0.005000, Loss: 3.7593, top1: 3.1250\n",
      "Epoch [1/60], Iter [70/633], LR: 0.005000, Loss: 3.6929, top1: 6.2500\n",
      "Epoch [1/60], Iter [71/633], LR: 0.005000, Loss: 3.6963, top1: 7.8125\n",
      "Epoch [1/60], Iter [72/633], LR: 0.005000, Loss: 3.7472, top1: 1.5625\n",
      "Epoch [1/60], Iter [73/633], LR: 0.005000, Loss: 3.7137, top1: 4.6875\n",
      "Epoch [1/60], Iter [74/633], LR: 0.005000, Loss: 3.7268, top1: 6.2500\n",
      "Epoch [1/60], Iter [75/633], LR: 0.005000, Loss: 3.7283, top1: 3.1250\n",
      "Epoch [1/60], Iter [76/633], LR: 0.005000, Loss: 3.7293, top1: 3.1250\n",
      "Epoch [1/60], Iter [77/633], LR: 0.005000, Loss: 3.7032, top1: 1.5625\n",
      "Epoch [1/60], Iter [78/633], LR: 0.005000, Loss: 3.6801, top1: 7.8125\n",
      "Epoch [1/60], Iter [79/633], LR: 0.005000, Loss: 3.7149, top1: 3.1250\n",
      "Epoch [1/60], Iter [80/633], LR: 0.005000, Loss: 3.7046, top1: 3.1250\n",
      "Epoch [1/60], Iter [81/633], LR: 0.005000, Loss: 3.7038, top1: 4.6875\n",
      "Epoch [1/60], Iter [82/633], LR: 0.005000, Loss: 3.6787, top1: 9.3750\n",
      "Epoch [1/60], Iter [83/633], LR: 0.005000, Loss: 3.6788, top1: 6.2500\n",
      "Epoch [1/60], Iter [84/633], LR: 0.005000, Loss: 3.6985, top1: 0.0000\n",
      "Epoch [1/60], Iter [85/633], LR: 0.005000, Loss: 3.7010, top1: 1.5625\n",
      "Epoch [1/60], Iter [86/633], LR: 0.005000, Loss: 3.6755, top1: 9.3750\n",
      "Epoch [1/60], Iter [87/633], LR: 0.005000, Loss: 3.6963, top1: 7.8125\n",
      "Epoch [1/60], Iter [88/633], LR: 0.005000, Loss: 3.6919, top1: 6.2500\n",
      "Epoch [1/60], Iter [89/633], LR: 0.005000, Loss: 3.7135, top1: 1.5625\n",
      "Epoch [1/60], Iter [90/633], LR: 0.005000, Loss: 3.7196, top1: 1.5625\n",
      "Epoch [1/60], Iter [91/633], LR: 0.005000, Loss: 3.7360, top1: 3.1250\n",
      "Epoch [1/60], Iter [92/633], LR: 0.005000, Loss: 3.7315, top1: 9.3750\n",
      "Epoch [1/60], Iter [93/633], LR: 0.005000, Loss: 3.7568, top1: 3.1250\n",
      "Epoch [1/60], Iter [94/633], LR: 0.005000, Loss: 3.7507, top1: 1.5625\n",
      "Epoch [1/60], Iter [95/633], LR: 0.005000, Loss: 3.7213, top1: 1.5625\n",
      "Epoch [1/60], Iter [96/633], LR: 0.005000, Loss: 3.7031, top1: 4.6875\n",
      "Epoch [1/60], Iter [97/633], LR: 0.005000, Loss: 3.7129, top1: 3.1250\n",
      "Epoch [1/60], Iter [98/633], LR: 0.005000, Loss: 3.7051, top1: 7.8125\n",
      "Epoch [1/60], Iter [99/633], LR: 0.005000, Loss: 3.7309, top1: 3.1250\n",
      "Epoch [1/60], Iter [100/633], LR: 0.005000, Loss: 3.7160, top1: 3.1250\n",
      "Epoch [1/60], Iter [101/633], LR: 0.005000, Loss: 3.7583, top1: 1.5625\n",
      "Epoch [1/60], Iter [102/633], LR: 0.005000, Loss: 3.7544, top1: 1.5625\n",
      "Epoch [1/60], Iter [103/633], LR: 0.005000, Loss: 3.7364, top1: 4.6875\n",
      "Epoch [1/60], Iter [104/633], LR: 0.005000, Loss: 3.7026, top1: 3.1250\n",
      "Epoch [1/60], Iter [105/633], LR: 0.005000, Loss: 3.6905, top1: 4.6875\n",
      "Epoch [1/60], Iter [106/633], LR: 0.005000, Loss: 3.6824, top1: 6.2500\n",
      "Epoch [1/60], Iter [107/633], LR: 0.005000, Loss: 3.6767, top1: 3.1250\n",
      "Epoch [1/60], Iter [108/633], LR: 0.005000, Loss: 3.6874, top1: 6.2500\n",
      "Epoch [1/60], Iter [109/633], LR: 0.005000, Loss: 3.7469, top1: 1.5625\n",
      "Epoch [1/60], Iter [110/633], LR: 0.005000, Loss: 3.6999, top1: 1.5625\n",
      "Epoch [1/60], Iter [111/633], LR: 0.005000, Loss: 3.6925, top1: 7.8125\n",
      "Epoch [1/60], Iter [112/633], LR: 0.005000, Loss: 3.6847, top1: 3.1250\n",
      "Epoch [1/60], Iter [113/633], LR: 0.005000, Loss: 3.7047, top1: 4.6875\n",
      "Epoch [1/60], Iter [114/633], LR: 0.005000, Loss: 3.7162, top1: 4.6875\n",
      "Epoch [1/60], Iter [115/633], LR: 0.005000, Loss: 3.7093, top1: 3.1250\n",
      "Epoch [1/60], Iter [116/633], LR: 0.005000, Loss: 3.6979, top1: 6.2500\n",
      "Epoch [1/60], Iter [117/633], LR: 0.005000, Loss: 3.6713, top1: 1.5625\n",
      "Epoch [1/60], Iter [118/633], LR: 0.005000, Loss: 3.6923, top1: 9.3750\n",
      "Epoch [1/60], Iter [119/633], LR: 0.005000, Loss: 3.7012, top1: 6.2500\n",
      "Epoch [1/60], Iter [120/633], LR: 0.005000, Loss: 3.7351, top1: 4.6875\n",
      "Epoch [1/60], Iter [121/633], LR: 0.005000, Loss: 3.6901, top1: 3.1250\n",
      "Epoch [1/60], Iter [122/633], LR: 0.005000, Loss: 3.7031, top1: 6.2500\n",
      "Epoch [1/60], Iter [123/633], LR: 0.005000, Loss: 3.6915, top1: 6.2500\n",
      "Epoch [1/60], Iter [124/633], LR: 0.005000, Loss: 3.6995, top1: 3.1250\n",
      "Epoch [1/60], Iter [125/633], LR: 0.005000, Loss: 3.6949, top1: 3.1250\n",
      "Epoch [1/60], Iter [126/633], LR: 0.005000, Loss: 3.6961, top1: 6.2500\n",
      "Epoch [1/60], Iter [127/633], LR: 0.005000, Loss: 3.7011, top1: 3.1250\n",
      "Epoch [1/60], Iter [128/633], LR: 0.005000, Loss: 3.6484, top1: 3.1250\n",
      "Epoch [1/60], Iter [129/633], LR: 0.005000, Loss: 3.7100, top1: 3.1250\n",
      "Epoch [1/60], Iter [130/633], LR: 0.005000, Loss: 3.6905, top1: 4.6875\n",
      "Epoch [1/60], Iter [131/633], LR: 0.005000, Loss: 3.7333, top1: 0.0000\n",
      "Epoch [1/60], Iter [132/633], LR: 0.005000, Loss: 3.7091, top1: 3.1250\n",
      "Epoch [1/60], Iter [133/633], LR: 0.005000, Loss: 3.7171, top1: 4.6875\n",
      "Epoch [1/60], Iter [134/633], LR: 0.005000, Loss: 3.7541, top1: 0.0000\n",
      "Epoch [1/60], Iter [135/633], LR: 0.005000, Loss: 3.6936, top1: 9.3750\n",
      "Epoch [1/60], Iter [136/633], LR: 0.005000, Loss: 3.7078, top1: 4.6875\n",
      "Epoch [1/60], Iter [137/633], LR: 0.005000, Loss: 3.7569, top1: 1.5625\n",
      "Epoch [1/60], Iter [138/633], LR: 0.005000, Loss: 3.7192, top1: 6.2500\n",
      "Epoch [1/60], Iter [139/633], LR: 0.005000, Loss: 3.7107, top1: 4.6875\n",
      "Epoch [1/60], Iter [140/633], LR: 0.005000, Loss: 3.7274, top1: 3.1250\n",
      "Epoch [1/60], Iter [141/633], LR: 0.005000, Loss: 3.7085, top1: 4.6875\n",
      "Epoch [1/60], Iter [142/633], LR: 0.005000, Loss: 3.7226, top1: 3.1250\n",
      "Epoch [1/60], Iter [143/633], LR: 0.005000, Loss: 3.7105, top1: 6.2500\n",
      "Epoch [1/60], Iter [144/633], LR: 0.005000, Loss: 3.7536, top1: 4.6875\n",
      "Epoch [1/60], Iter [145/633], LR: 0.005000, Loss: 3.6690, top1: 7.8125\n",
      "Epoch [1/60], Iter [146/633], LR: 0.005000, Loss: 3.7092, top1: 1.5625\n",
      "Epoch [1/60], Iter [147/633], LR: 0.005000, Loss: 3.7027, top1: 6.2500\n",
      "Epoch [1/60], Iter [148/633], LR: 0.005000, Loss: 3.7172, top1: 3.1250\n",
      "Epoch [1/60], Iter [149/633], LR: 0.005000, Loss: 3.7140, top1: 1.5625\n",
      "Epoch [1/60], Iter [150/633], LR: 0.005000, Loss: 3.7262, top1: 3.1250\n",
      "Epoch [1/60], Iter [151/633], LR: 0.005000, Loss: 3.7000, top1: 3.1250\n",
      "Epoch [1/60], Iter [152/633], LR: 0.005000, Loss: 3.6819, top1: 6.2500\n",
      "Epoch [1/60], Iter [153/633], LR: 0.005000, Loss: 3.7407, top1: 3.1250\n",
      "Epoch [1/60], Iter [154/633], LR: 0.005000, Loss: 3.6978, top1: 4.6875\n",
      "Epoch [1/60], Iter [155/633], LR: 0.005000, Loss: 3.7012, top1: 3.1250\n",
      "Epoch [1/60], Iter [156/633], LR: 0.005000, Loss: 3.6868, top1: 1.5625\n",
      "Epoch [1/60], Iter [157/633], LR: 0.005000, Loss: 3.7050, top1: 4.6875\n",
      "Epoch [1/60], Iter [158/633], LR: 0.005000, Loss: 3.6936, top1: 7.8125\n",
      "Epoch [1/60], Iter [159/633], LR: 0.005000, Loss: 3.7072, top1: 1.5625\n",
      "Epoch [1/60], Iter [160/633], LR: 0.005000, Loss: 3.6970, top1: 6.2500\n",
      "Epoch [1/60], Iter [161/633], LR: 0.005000, Loss: 3.7184, top1: 1.5625\n",
      "Epoch [1/60], Iter [162/633], LR: 0.005000, Loss: 3.6871, top1: 6.2500\n",
      "Epoch [1/60], Iter [163/633], LR: 0.005000, Loss: 3.7059, top1: 1.5625\n",
      "Epoch [1/60], Iter [164/633], LR: 0.005000, Loss: 3.7144, top1: 4.6875\n",
      "Epoch [1/60], Iter [165/633], LR: 0.005000, Loss: 3.6658, top1: 4.6875\n",
      "Epoch [1/60], Iter [166/633], LR: 0.005000, Loss: 3.6985, top1: 3.1250\n",
      "Epoch [1/60], Iter [167/633], LR: 0.005000, Loss: 3.6797, top1: 3.1250\n",
      "Epoch [1/60], Iter [168/633], LR: 0.005000, Loss: 3.6643, top1: 10.9375\n",
      "Epoch [1/60], Iter [169/633], LR: 0.005000, Loss: 3.6900, top1: 6.2500\n",
      "Epoch [1/60], Iter [170/633], LR: 0.005000, Loss: 3.7090, top1: 1.5625\n",
      "Epoch [1/60], Iter [171/633], LR: 0.005000, Loss: 3.7013, top1: 1.5625\n",
      "Epoch [1/60], Iter [172/633], LR: 0.005000, Loss: 3.6640, top1: 3.1250\n",
      "Epoch [1/60], Iter [173/633], LR: 0.005000, Loss: 3.7057, top1: 4.6875\n",
      "Epoch [1/60], Iter [174/633], LR: 0.005000, Loss: 3.6884, top1: 3.1250\n",
      "Epoch [1/60], Iter [175/633], LR: 0.005000, Loss: 3.6943, top1: 4.6875\n",
      "Epoch [1/60], Iter [176/633], LR: 0.005000, Loss: 3.6879, top1: 4.6875\n",
      "Epoch [1/60], Iter [177/633], LR: 0.005000, Loss: 3.6796, top1: 6.2500\n",
      "Epoch [1/60], Iter [178/633], LR: 0.005000, Loss: 3.6935, top1: 6.2500\n",
      "Epoch [1/60], Iter [179/633], LR: 0.005000, Loss: 3.7138, top1: 3.1250\n",
      "Epoch [1/60], Iter [180/633], LR: 0.005000, Loss: 3.6806, top1: 4.6875\n",
      "Epoch [1/60], Iter [181/633], LR: 0.005000, Loss: 3.7004, top1: 6.2500\n",
      "Epoch [1/60], Iter [182/633], LR: 0.005000, Loss: 3.7285, top1: 3.1250\n",
      "Epoch [1/60], Iter [183/633], LR: 0.005000, Loss: 3.7086, top1: 4.6875\n",
      "Epoch [1/60], Iter [184/633], LR: 0.005000, Loss: 3.6968, top1: 3.1250\n",
      "Epoch [1/60], Iter [185/633], LR: 0.005000, Loss: 3.6960, top1: 7.8125\n",
      "Epoch [1/60], Iter [186/633], LR: 0.005000, Loss: 3.6967, top1: 7.8125\n",
      "Epoch [1/60], Iter [187/633], LR: 0.005000, Loss: 3.6940, top1: 6.2500\n",
      "Epoch [1/60], Iter [188/633], LR: 0.005000, Loss: 3.7102, top1: 6.2500\n",
      "Epoch [1/60], Iter [189/633], LR: 0.005000, Loss: 3.6873, top1: 4.6875\n",
      "Epoch [1/60], Iter [190/633], LR: 0.005000, Loss: 3.7083, top1: 9.3750\n",
      "Epoch [1/60], Iter [191/633], LR: 0.005000, Loss: 3.6772, top1: 6.2500\n",
      "Epoch [1/60], Iter [192/633], LR: 0.005000, Loss: 3.6731, top1: 6.2500\n",
      "Epoch [1/60], Iter [193/633], LR: 0.005000, Loss: 3.7130, top1: 1.5625\n",
      "Epoch [1/60], Iter [194/633], LR: 0.005000, Loss: 3.6819, top1: 3.1250\n",
      "Epoch [1/60], Iter [195/633], LR: 0.005000, Loss: 3.6933, top1: 1.5625\n",
      "Epoch [1/60], Iter [196/633], LR: 0.005000, Loss: 3.6743, top1: 3.1250\n",
      "Epoch [1/60], Iter [197/633], LR: 0.005000, Loss: 3.6827, top1: 6.2500\n",
      "Epoch [1/60], Iter [198/633], LR: 0.005000, Loss: 3.6962, top1: 4.6875\n",
      "Epoch [1/60], Iter [199/633], LR: 0.005000, Loss: 3.6834, top1: 7.8125\n",
      "Epoch [1/60], Iter [200/633], LR: 0.005000, Loss: 3.6917, top1: 4.6875\n",
      "Epoch [1/60], Iter [201/633], LR: 0.005000, Loss: 3.6684, top1: 7.8125\n",
      "Epoch [1/60], Iter [202/633], LR: 0.005000, Loss: 3.7066, top1: 6.2500\n",
      "Epoch [1/60], Iter [203/633], LR: 0.005000, Loss: 3.6992, top1: 3.1250\n",
      "Epoch [1/60], Iter [204/633], LR: 0.005000, Loss: 3.7105, top1: 0.0000\n",
      "Epoch [1/60], Iter [205/633], LR: 0.005000, Loss: 3.7042, top1: 1.5625\n",
      "Epoch [1/60], Iter [206/633], LR: 0.005000, Loss: 3.6932, top1: 3.1250\n",
      "Epoch [1/60], Iter [207/633], LR: 0.005000, Loss: 3.6991, top1: 4.6875\n",
      "Epoch [1/60], Iter [208/633], LR: 0.005000, Loss: 3.6539, top1: 6.2500\n",
      "Epoch [1/60], Iter [209/633], LR: 0.005000, Loss: 3.6819, top1: 6.2500\n",
      "Epoch [1/60], Iter [210/633], LR: 0.005000, Loss: 3.6753, top1: 3.1250\n",
      "Epoch [1/60], Iter [211/633], LR: 0.005000, Loss: 3.6718, top1: 3.1250\n",
      "Epoch [1/60], Iter [212/633], LR: 0.005000, Loss: 3.7049, top1: 6.2500\n",
      "Epoch [1/60], Iter [213/633], LR: 0.005000, Loss: 3.6636, top1: 7.8125\n",
      "Epoch [1/60], Iter [214/633], LR: 0.005000, Loss: 3.6710, top1: 4.6875\n",
      "Epoch [1/60], Iter [215/633], LR: 0.005000, Loss: 3.6848, top1: 4.6875\n",
      "Epoch [1/60], Iter [216/633], LR: 0.005000, Loss: 3.6751, top1: 4.6875\n",
      "Epoch [1/60], Iter [217/633], LR: 0.005000, Loss: 3.7005, top1: 4.6875\n",
      "Epoch [1/60], Iter [218/633], LR: 0.005000, Loss: 3.7187, top1: 6.2500\n",
      "Epoch [1/60], Iter [219/633], LR: 0.005000, Loss: 3.6883, top1: 4.6875\n",
      "Epoch [1/60], Iter [220/633], LR: 0.005000, Loss: 3.7077, top1: 7.8125\n",
      "Epoch [1/60], Iter [221/633], LR: 0.005000, Loss: 3.6824, top1: 6.2500\n",
      "Epoch [1/60], Iter [222/633], LR: 0.005000, Loss: 3.6922, top1: 4.6875\n",
      "Epoch [1/60], Iter [223/633], LR: 0.005000, Loss: 3.6844, top1: 4.6875\n",
      "Epoch [1/60], Iter [224/633], LR: 0.005000, Loss: 3.7171, top1: 3.1250\n",
      "Epoch [1/60], Iter [225/633], LR: 0.005000, Loss: 3.7056, top1: 4.6875\n",
      "Epoch [1/60], Iter [226/633], LR: 0.005000, Loss: 3.7072, top1: 3.1250\n",
      "Epoch [1/60], Iter [227/633], LR: 0.005000, Loss: 3.6893, top1: 4.6875\n",
      "Epoch [1/60], Iter [228/633], LR: 0.005000, Loss: 3.6498, top1: 7.8125\n",
      "Epoch [1/60], Iter [229/633], LR: 0.005000, Loss: 3.6991, top1: 9.3750\n",
      "Epoch [1/60], Iter [230/633], LR: 0.005000, Loss: 3.6918, top1: 3.1250\n",
      "Epoch [1/60], Iter [231/633], LR: 0.005000, Loss: 3.7084, top1: 4.6875\n",
      "Epoch [1/60], Iter [232/633], LR: 0.005000, Loss: 3.6535, top1: 9.3750\n",
      "Epoch [1/60], Iter [233/633], LR: 0.005000, Loss: 3.6740, top1: 1.5625\n",
      "Epoch [1/60], Iter [234/633], LR: 0.005000, Loss: 3.6651, top1: 3.1250\n",
      "Epoch [1/60], Iter [235/633], LR: 0.005000, Loss: 3.7347, top1: 1.5625\n",
      "Epoch [1/60], Iter [236/633], LR: 0.005000, Loss: 3.6889, top1: 1.5625\n",
      "Epoch [1/60], Iter [237/633], LR: 0.005000, Loss: 3.7012, top1: 3.1250\n",
      "Epoch [1/60], Iter [238/633], LR: 0.005000, Loss: 3.6891, top1: 3.1250\n",
      "Epoch [1/60], Iter [239/633], LR: 0.005000, Loss: 3.7077, top1: 9.3750\n",
      "Epoch [1/60], Iter [240/633], LR: 0.005000, Loss: 3.7047, top1: 1.5625\n",
      "Epoch [1/60], Iter [241/633], LR: 0.005000, Loss: 3.7042, top1: 1.5625\n",
      "Epoch [1/60], Iter [242/633], LR: 0.005000, Loss: 3.7002, top1: 4.6875\n",
      "Epoch [1/60], Iter [243/633], LR: 0.005000, Loss: 3.7031, top1: 3.1250\n",
      "Epoch [1/60], Iter [244/633], LR: 0.005000, Loss: 3.7099, top1: 4.6875\n",
      "Epoch [1/60], Iter [245/633], LR: 0.005000, Loss: 3.6687, top1: 4.6875\n",
      "Epoch [1/60], Iter [246/633], LR: 0.005000, Loss: 3.6907, top1: 9.3750\n",
      "Epoch [1/60], Iter [247/633], LR: 0.005000, Loss: 3.6986, top1: 3.1250\n",
      "Epoch [1/60], Iter [248/633], LR: 0.005000, Loss: 3.6893, top1: 4.6875\n",
      "Epoch [1/60], Iter [249/633], LR: 0.005000, Loss: 3.6806, top1: 3.1250\n",
      "Epoch [1/60], Iter [250/633], LR: 0.005000, Loss: 3.6938, top1: 0.0000\n",
      "Epoch [1/60], Iter [251/633], LR: 0.005000, Loss: 3.7123, top1: 1.5625\n",
      "Epoch [1/60], Iter [252/633], LR: 0.005000, Loss: 3.6868, top1: 9.3750\n",
      "Epoch [1/60], Iter [253/633], LR: 0.005000, Loss: 3.6867, top1: 4.6875\n",
      "Epoch [1/60], Iter [254/633], LR: 0.005000, Loss: 3.6576, top1: 10.9375\n",
      "Epoch [1/60], Iter [255/633], LR: 0.005000, Loss: 3.7229, top1: 0.0000\n",
      "Epoch [1/60], Iter [256/633], LR: 0.005000, Loss: 3.7228, top1: 0.0000\n",
      "Epoch [1/60], Iter [257/633], LR: 0.005000, Loss: 3.7155, top1: 1.5625\n",
      "Epoch [1/60], Iter [258/633], LR: 0.005000, Loss: 3.6695, top1: 6.2500\n",
      "Epoch [1/60], Iter [259/633], LR: 0.005000, Loss: 3.6743, top1: 4.6875\n",
      "Epoch [1/60], Iter [260/633], LR: 0.005000, Loss: 3.7289, top1: 10.9375\n",
      "Epoch [1/60], Iter [261/633], LR: 0.005000, Loss: 3.7135, top1: 6.2500\n",
      "Epoch [1/60], Iter [262/633], LR: 0.005000, Loss: 3.7009, top1: 4.6875\n",
      "Epoch [1/60], Iter [263/633], LR: 0.005000, Loss: 3.6797, top1: 1.5625\n",
      "Epoch [1/60], Iter [264/633], LR: 0.005000, Loss: 3.6835, top1: 4.6875\n",
      "Epoch [1/60], Iter [265/633], LR: 0.005000, Loss: 3.6607, top1: 7.8125\n",
      "Epoch [1/60], Iter [266/633], LR: 0.005000, Loss: 3.6765, top1: 6.2500\n",
      "Epoch [1/60], Iter [267/633], LR: 0.005000, Loss: 3.7084, top1: 3.1250\n",
      "Epoch [1/60], Iter [268/633], LR: 0.005000, Loss: 3.6769, top1: 10.9375\n",
      "Epoch [1/60], Iter [269/633], LR: 0.005000, Loss: 3.6862, top1: 3.1250\n",
      "Epoch [1/60], Iter [270/633], LR: 0.005000, Loss: 3.7030, top1: 3.1250\n",
      "Epoch [1/60], Iter [271/633], LR: 0.005000, Loss: 3.6920, top1: 6.2500\n",
      "Epoch [1/60], Iter [272/633], LR: 0.005000, Loss: 3.7049, top1: 6.2500\n",
      "Epoch [1/60], Iter [273/633], LR: 0.005000, Loss: 3.6927, top1: 9.3750\n",
      "Epoch [1/60], Iter [274/633], LR: 0.005000, Loss: 3.6572, top1: 7.8125\n",
      "Epoch [1/60], Iter [275/633], LR: 0.005000, Loss: 3.6705, top1: 1.5625\n",
      "Epoch [1/60], Iter [276/633], LR: 0.005000, Loss: 3.6863, top1: 1.5625\n",
      "Epoch [1/60], Iter [277/633], LR: 0.005000, Loss: 3.6492, top1: 10.9375\n",
      "Epoch [1/60], Iter [278/633], LR: 0.005000, Loss: 3.7016, top1: 3.1250\n",
      "Epoch [1/60], Iter [279/633], LR: 0.005000, Loss: 3.6545, top1: 3.1250\n",
      "Epoch [1/60], Iter [280/633], LR: 0.005000, Loss: 3.6721, top1: 6.2500\n",
      "Epoch [1/60], Iter [281/633], LR: 0.005000, Loss: 3.7102, top1: 3.1250\n",
      "Epoch [1/60], Iter [282/633], LR: 0.005000, Loss: 3.7164, top1: 6.2500\n",
      "Epoch [1/60], Iter [283/633], LR: 0.005000, Loss: 3.7230, top1: 1.5625\n",
      "Epoch [1/60], Iter [284/633], LR: 0.005000, Loss: 3.6760, top1: 7.8125\n",
      "Epoch [1/60], Iter [285/633], LR: 0.005000, Loss: 3.7168, top1: 7.8125\n",
      "Epoch [1/60], Iter [286/633], LR: 0.005000, Loss: 3.6808, top1: 6.2500\n",
      "Epoch [1/60], Iter [287/633], LR: 0.005000, Loss: 3.6871, top1: 7.8125\n",
      "Epoch [1/60], Iter [288/633], LR: 0.005000, Loss: 3.7037, top1: 6.2500\n",
      "Epoch [1/60], Iter [289/633], LR: 0.005000, Loss: 3.6659, top1: 6.2500\n",
      "Epoch [1/60], Iter [290/633], LR: 0.005000, Loss: 3.6839, top1: 4.6875\n",
      "Epoch [1/60], Iter [291/633], LR: 0.005000, Loss: 3.6827, top1: 4.6875\n",
      "Epoch [1/60], Iter [292/633], LR: 0.005000, Loss: 3.6852, top1: 6.2500\n",
      "Epoch [1/60], Iter [293/633], LR: 0.005000, Loss: 3.6856, top1: 3.1250\n",
      "Epoch [1/60], Iter [294/633], LR: 0.005000, Loss: 3.6857, top1: 9.3750\n",
      "Epoch [1/60], Iter [295/633], LR: 0.005000, Loss: 3.7087, top1: 4.6875\n",
      "Epoch [1/60], Iter [296/633], LR: 0.005000, Loss: 3.6728, top1: 4.6875\n",
      "Epoch [1/60], Iter [297/633], LR: 0.005000, Loss: 3.6759, top1: 3.1250\n",
      "Epoch [1/60], Iter [298/633], LR: 0.005000, Loss: 3.6768, top1: 3.1250\n",
      "Epoch [1/60], Iter [299/633], LR: 0.005000, Loss: 3.6798, top1: 4.6875\n",
      "Epoch [1/60], Iter [300/633], LR: 0.005000, Loss: 3.6983, top1: 0.0000\n",
      "Epoch [1/60], Iter [301/633], LR: 0.005000, Loss: 3.6811, top1: 3.1250\n",
      "Epoch [1/60], Iter [302/633], LR: 0.005000, Loss: 3.7040, top1: 3.1250\n",
      "Epoch [1/60], Iter [303/633], LR: 0.005000, Loss: 3.7050, top1: 4.6875\n",
      "Epoch [1/60], Iter [304/633], LR: 0.005000, Loss: 3.6809, top1: 6.2500\n",
      "Epoch [1/60], Iter [305/633], LR: 0.005000, Loss: 3.6671, top1: 6.2500\n",
      "Epoch [1/60], Iter [306/633], LR: 0.005000, Loss: 3.6926, top1: 9.3750\n",
      "Epoch [1/60], Iter [307/633], LR: 0.005000, Loss: 3.6959, top1: 4.6875\n",
      "Epoch [1/60], Iter [308/633], LR: 0.005000, Loss: 3.6250, top1: 6.2500\n",
      "Epoch [1/60], Iter [309/633], LR: 0.005000, Loss: 3.6638, top1: 9.3750\n",
      "Epoch [1/60], Iter [310/633], LR: 0.005000, Loss: 3.6825, top1: 7.8125\n",
      "Epoch [1/60], Iter [311/633], LR: 0.005000, Loss: 3.7008, top1: 1.5625\n",
      "Epoch [1/60], Iter [312/633], LR: 0.005000, Loss: 3.7019, top1: 6.2500\n",
      "Epoch [1/60], Iter [313/633], LR: 0.005000, Loss: 3.6751, top1: 7.8125\n",
      "Epoch [1/60], Iter [314/633], LR: 0.005000, Loss: 3.7263, top1: 1.5625\n",
      "Epoch [1/60], Iter [315/633], LR: 0.005000, Loss: 3.6951, top1: 1.5625\n",
      "Epoch [1/60], Iter [316/633], LR: 0.005000, Loss: 3.6891, top1: 1.5625\n",
      "Epoch [1/60], Iter [317/633], LR: 0.005000, Loss: 3.6830, top1: 3.1250\n",
      "Epoch [1/60], Iter [318/633], LR: 0.005000, Loss: 3.7001, top1: 3.1250\n",
      "Epoch [1/60], Iter [319/633], LR: 0.005000, Loss: 3.6746, top1: 9.3750\n",
      "Epoch [1/60], Iter [320/633], LR: 0.005000, Loss: 3.6540, top1: 6.2500\n",
      "Epoch [1/60], Iter [321/633], LR: 0.005000, Loss: 3.6777, top1: 6.2500\n",
      "Epoch [1/60], Iter [322/633], LR: 0.005000, Loss: 3.6896, top1: 3.1250\n",
      "Epoch [1/60], Iter [323/633], LR: 0.005000, Loss: 3.6890, top1: 1.5625\n",
      "Epoch [1/60], Iter [324/633], LR: 0.005000, Loss: 3.6742, top1: 3.1250\n",
      "Epoch [1/60], Iter [325/633], LR: 0.005000, Loss: 3.6930, top1: 1.5625\n",
      "Epoch [1/60], Iter [326/633], LR: 0.005000, Loss: 3.6991, top1: 7.8125\n",
      "Epoch [1/60], Iter [327/633], LR: 0.005000, Loss: 3.6932, top1: 4.6875\n",
      "Epoch [1/60], Iter [328/633], LR: 0.005000, Loss: 3.6662, top1: 4.6875\n",
      "Epoch [1/60], Iter [329/633], LR: 0.005000, Loss: 3.6807, top1: 6.2500\n",
      "Epoch [1/60], Iter [330/633], LR: 0.005000, Loss: 3.6726, top1: 6.2500\n",
      "Epoch [1/60], Iter [331/633], LR: 0.005000, Loss: 3.6712, top1: 3.1250\n",
      "Epoch [1/60], Iter [332/633], LR: 0.005000, Loss: 3.6952, top1: 4.6875\n",
      "Epoch [1/60], Iter [333/633], LR: 0.005000, Loss: 3.7017, top1: 6.2500\n",
      "Epoch [1/60], Iter [334/633], LR: 0.005000, Loss: 3.6893, top1: 6.2500\n",
      "Epoch [1/60], Iter [335/633], LR: 0.005000, Loss: 3.6662, top1: 6.2500\n",
      "Epoch [1/60], Iter [336/633], LR: 0.005000, Loss: 3.6837, top1: 9.3750\n",
      "Epoch [1/60], Iter [337/633], LR: 0.005000, Loss: 3.6713, top1: 4.6875\n",
      "Epoch [1/60], Iter [338/633], LR: 0.005000, Loss: 3.6740, top1: 7.8125\n",
      "Epoch [1/60], Iter [339/633], LR: 0.005000, Loss: 3.6797, top1: 6.2500\n",
      "Epoch [1/60], Iter [340/633], LR: 0.005000, Loss: 3.6994, top1: 6.2500\n",
      "Epoch [1/60], Iter [341/633], LR: 0.005000, Loss: 3.6981, top1: 4.6875\n",
      "Epoch [1/60], Iter [342/633], LR: 0.005000, Loss: 3.6924, top1: 1.5625\n",
      "Epoch [1/60], Iter [343/633], LR: 0.005000, Loss: 3.6759, top1: 3.1250\n",
      "Epoch [1/60], Iter [344/633], LR: 0.005000, Loss: 3.6719, top1: 9.3750\n",
      "Epoch [1/60], Iter [345/633], LR: 0.005000, Loss: 3.7047, top1: 4.6875\n",
      "Epoch [1/60], Iter [346/633], LR: 0.005000, Loss: 3.6721, top1: 9.3750\n",
      "Epoch [1/60], Iter [347/633], LR: 0.005000, Loss: 3.6655, top1: 6.2500\n",
      "Epoch [1/60], Iter [348/633], LR: 0.005000, Loss: 3.6904, top1: 6.2500\n",
      "Epoch [1/60], Iter [349/633], LR: 0.005000, Loss: 3.6938, top1: 3.1250\n",
      "Epoch [1/60], Iter [350/633], LR: 0.005000, Loss: 3.6985, top1: 9.3750\n",
      "Epoch [1/60], Iter [351/633], LR: 0.005000, Loss: 3.6568, top1: 12.5000\n",
      "Epoch [1/60], Iter [352/633], LR: 0.005000, Loss: 3.7311, top1: 3.1250\n",
      "Epoch [1/60], Iter [353/633], LR: 0.005000, Loss: 3.6791, top1: 7.8125\n",
      "Epoch [1/60], Iter [354/633], LR: 0.005000, Loss: 3.6787, top1: 7.8125\n",
      "Epoch [1/60], Iter [355/633], LR: 0.005000, Loss: 3.6761, top1: 4.6875\n",
      "Epoch [1/60], Iter [356/633], LR: 0.005000, Loss: 3.7017, top1: 0.0000\n",
      "Epoch [1/60], Iter [357/633], LR: 0.005000, Loss: 3.6602, top1: 6.2500\n",
      "Epoch [1/60], Iter [358/633], LR: 0.005000, Loss: 3.6903, top1: 7.8125\n",
      "Epoch [1/60], Iter [359/633], LR: 0.005000, Loss: 3.6648, top1: 7.8125\n",
      "Epoch [1/60], Iter [360/633], LR: 0.005000, Loss: 3.7051, top1: 4.6875\n",
      "Epoch [1/60], Iter [361/633], LR: 0.005000, Loss: 3.6571, top1: 4.6875\n",
      "Epoch [1/60], Iter [362/633], LR: 0.005000, Loss: 3.6651, top1: 4.6875\n",
      "Epoch [1/60], Iter [363/633], LR: 0.005000, Loss: 3.6993, top1: 4.6875\n",
      "Epoch [1/60], Iter [364/633], LR: 0.005000, Loss: 3.7260, top1: 1.5625\n",
      "Epoch [1/60], Iter [365/633], LR: 0.005000, Loss: 3.6671, top1: 6.2500\n",
      "Epoch [1/60], Iter [366/633], LR: 0.005000, Loss: 3.7089, top1: 6.2500\n",
      "Epoch [1/60], Iter [367/633], LR: 0.005000, Loss: 3.6910, top1: 7.8125\n",
      "Epoch [1/60], Iter [368/633], LR: 0.005000, Loss: 3.6647, top1: 4.6875\n",
      "Epoch [1/60], Iter [369/633], LR: 0.005000, Loss: 3.6647, top1: 9.3750\n",
      "Epoch [1/60], Iter [370/633], LR: 0.005000, Loss: 3.7274, top1: 3.1250\n",
      "Epoch [1/60], Iter [371/633], LR: 0.005000, Loss: 3.6730, top1: 3.1250\n",
      "Epoch [1/60], Iter [372/633], LR: 0.005000, Loss: 3.6888, top1: 3.1250\n",
      "Epoch [1/60], Iter [373/633], LR: 0.005000, Loss: 3.6936, top1: 7.8125\n",
      "Epoch [1/60], Iter [374/633], LR: 0.005000, Loss: 3.7231, top1: 3.1250\n",
      "Epoch [1/60], Iter [375/633], LR: 0.005000, Loss: 3.7164, top1: 3.1250\n",
      "Epoch [1/60], Iter [376/633], LR: 0.005000, Loss: 3.7101, top1: 3.1250\n",
      "Epoch [1/60], Iter [377/633], LR: 0.005000, Loss: 3.6840, top1: 4.6875\n",
      "Epoch [1/60], Iter [378/633], LR: 0.005000, Loss: 3.6901, top1: 6.2500\n",
      "Epoch [1/60], Iter [379/633], LR: 0.005000, Loss: 3.6809, top1: 6.2500\n",
      "Epoch [1/60], Iter [380/633], LR: 0.005000, Loss: 3.6760, top1: 1.5625\n",
      "Epoch [1/60], Iter [381/633], LR: 0.005000, Loss: 3.7003, top1: 0.0000\n",
      "Epoch [1/60], Iter [382/633], LR: 0.005000, Loss: 3.6967, top1: 1.5625\n",
      "Epoch [1/60], Iter [383/633], LR: 0.005000, Loss: 3.6969, top1: 3.1250\n",
      "Epoch [1/60], Iter [384/633], LR: 0.005000, Loss: 3.6833, top1: 4.6875\n",
      "Epoch [1/60], Iter [385/633], LR: 0.005000, Loss: 3.6849, top1: 3.1250\n",
      "Epoch [1/60], Iter [386/633], LR: 0.005000, Loss: 3.6431, top1: 12.5000\n",
      "Epoch [1/60], Iter [387/633], LR: 0.005000, Loss: 3.6697, top1: 4.6875\n",
      "Epoch [1/60], Iter [388/633], LR: 0.005000, Loss: 3.6687, top1: 3.1250\n",
      "Epoch [1/60], Iter [389/633], LR: 0.005000, Loss: 3.6545, top1: 10.9375\n",
      "Epoch [1/60], Iter [390/633], LR: 0.005000, Loss: 3.6925, top1: 1.5625\n",
      "Epoch [1/60], Iter [391/633], LR: 0.005000, Loss: 3.6507, top1: 9.3750\n",
      "Epoch [1/60], Iter [392/633], LR: 0.005000, Loss: 3.6817, top1: 9.3750\n",
      "Epoch [1/60], Iter [393/633], LR: 0.005000, Loss: 3.6895, top1: 4.6875\n",
      "Epoch [1/60], Iter [394/633], LR: 0.005000, Loss: 3.7236, top1: 3.1250\n",
      "Epoch [1/60], Iter [395/633], LR: 0.005000, Loss: 3.7095, top1: 3.1250\n",
      "Epoch [1/60], Iter [396/633], LR: 0.005000, Loss: 3.6698, top1: 9.3750\n",
      "Epoch [1/60], Iter [397/633], LR: 0.005000, Loss: 3.6814, top1: 6.2500\n",
      "Epoch [1/60], Iter [398/633], LR: 0.005000, Loss: 3.6710, top1: 9.3750\n",
      "Epoch [1/60], Iter [399/633], LR: 0.005000, Loss: 3.6704, top1: 6.2500\n",
      "Epoch [1/60], Iter [400/633], LR: 0.005000, Loss: 3.6498, top1: 4.6875\n",
      "Epoch [1/60], Iter [401/633], LR: 0.005000, Loss: 3.6686, top1: 6.2500\n",
      "Epoch [1/60], Iter [402/633], LR: 0.005000, Loss: 3.6989, top1: 3.1250\n",
      "Epoch [1/60], Iter [403/633], LR: 0.005000, Loss: 3.6782, top1: 7.8125\n",
      "Epoch [1/60], Iter [404/633], LR: 0.005000, Loss: 3.6824, top1: 4.6875\n",
      "Epoch [1/60], Iter [405/633], LR: 0.005000, Loss: 3.6711, top1: 4.6875\n",
      "Epoch [1/60], Iter [406/633], LR: 0.005000, Loss: 3.7184, top1: 3.1250\n",
      "Epoch [1/60], Iter [407/633], LR: 0.005000, Loss: 3.6420, top1: 6.2500\n",
      "Epoch [1/60], Iter [408/633], LR: 0.005000, Loss: 3.6784, top1: 3.1250\n",
      "Epoch [1/60], Iter [409/633], LR: 0.005000, Loss: 3.7060, top1: 3.1250\n",
      "Epoch [1/60], Iter [410/633], LR: 0.005000, Loss: 3.6578, top1: 9.3750\n",
      "Epoch [1/60], Iter [411/633], LR: 0.005000, Loss: 3.6630, top1: 4.6875\n",
      "Epoch [1/60], Iter [412/633], LR: 0.005000, Loss: 3.6962, top1: 3.1250\n",
      "Epoch [1/60], Iter [413/633], LR: 0.005000, Loss: 3.6274, top1: 10.9375\n",
      "Epoch [1/60], Iter [414/633], LR: 0.005000, Loss: 3.6751, top1: 0.0000\n",
      "Epoch [1/60], Iter [415/633], LR: 0.005000, Loss: 3.7064, top1: 4.6875\n",
      "Epoch [1/60], Iter [416/633], LR: 0.005000, Loss: 3.6842, top1: 3.1250\n",
      "Epoch [1/60], Iter [417/633], LR: 0.005000, Loss: 3.6818, top1: 7.8125\n",
      "Epoch [1/60], Iter [418/633], LR: 0.005000, Loss: 3.6993, top1: 3.1250\n",
      "Epoch [1/60], Iter [419/633], LR: 0.005000, Loss: 3.6518, top1: 9.3750\n",
      "Epoch [1/60], Iter [420/633], LR: 0.005000, Loss: 3.6763, top1: 7.8125\n",
      "Epoch [1/60], Iter [421/633], LR: 0.005000, Loss: 3.6528, top1: 4.6875\n",
      "Epoch [1/60], Iter [422/633], LR: 0.005000, Loss: 3.6520, top1: 9.3750\n",
      "Epoch [1/60], Iter [423/633], LR: 0.005000, Loss: 3.6771, top1: 6.2500\n",
      "Epoch [1/60], Iter [424/633], LR: 0.005000, Loss: 3.6999, top1: 1.5625\n",
      "Epoch [1/60], Iter [425/633], LR: 0.005000, Loss: 3.6874, top1: 6.2500\n",
      "Epoch [1/60], Iter [426/633], LR: 0.005000, Loss: 3.7045, top1: 1.5625\n",
      "Epoch [1/60], Iter [427/633], LR: 0.005000, Loss: 3.7310, top1: 0.0000\n",
      "Epoch [1/60], Iter [428/633], LR: 0.005000, Loss: 3.6453, top1: 7.8125\n",
      "Epoch [1/60], Iter [429/633], LR: 0.005000, Loss: 3.6650, top1: 7.8125\n",
      "Epoch [1/60], Iter [430/633], LR: 0.005000, Loss: 3.7114, top1: 4.6875\n",
      "Epoch [1/60], Iter [431/633], LR: 0.005000, Loss: 3.6948, top1: 3.1250\n",
      "Epoch [1/60], Iter [432/633], LR: 0.005000, Loss: 3.6811, top1: 7.8125\n",
      "Epoch [1/60], Iter [433/633], LR: 0.005000, Loss: 3.6877, top1: 6.2500\n",
      "Epoch [1/60], Iter [434/633], LR: 0.005000, Loss: 3.6361, top1: 6.2500\n",
      "Epoch [1/60], Iter [435/633], LR: 0.005000, Loss: 3.6719, top1: 10.9375\n",
      "Epoch [1/60], Iter [436/633], LR: 0.005000, Loss: 3.6603, top1: 3.1250\n",
      "Epoch [1/60], Iter [437/633], LR: 0.005000, Loss: 3.6678, top1: 9.3750\n",
      "Epoch [1/60], Iter [438/633], LR: 0.005000, Loss: 3.6794, top1: 7.8125\n",
      "Epoch [1/60], Iter [439/633], LR: 0.005000, Loss: 3.6840, top1: 6.2500\n",
      "Epoch [1/60], Iter [440/633], LR: 0.005000, Loss: 3.6615, top1: 9.3750\n",
      "Epoch [1/60], Iter [441/633], LR: 0.005000, Loss: 3.6512, top1: 7.8125\n",
      "Epoch [1/60], Iter [442/633], LR: 0.005000, Loss: 3.6882, top1: 9.3750\n",
      "Epoch [1/60], Iter [443/633], LR: 0.005000, Loss: 3.6536, top1: 6.2500\n",
      "Epoch [1/60], Iter [444/633], LR: 0.005000, Loss: 3.6922, top1: 4.6875\n",
      "Epoch [1/60], Iter [445/633], LR: 0.005000, Loss: 3.6922, top1: 4.6875\n",
      "Epoch [1/60], Iter [446/633], LR: 0.005000, Loss: 3.7219, top1: 4.6875\n",
      "Epoch [1/60], Iter [447/633], LR: 0.005000, Loss: 3.6828, top1: 7.8125\n",
      "Epoch [1/60], Iter [448/633], LR: 0.005000, Loss: 3.6536, top1: 7.8125\n",
      "Epoch [1/60], Iter [449/633], LR: 0.005000, Loss: 3.6800, top1: 4.6875\n",
      "Epoch [1/60], Iter [450/633], LR: 0.005000, Loss: 3.6702, top1: 9.3750\n",
      "Epoch [1/60], Iter [451/633], LR: 0.005000, Loss: 3.6834, top1: 12.5000\n",
      "Epoch [1/60], Iter [452/633], LR: 0.005000, Loss: 3.6781, top1: 4.6875\n",
      "Epoch [1/60], Iter [453/633], LR: 0.005000, Loss: 3.6884, top1: 3.1250\n",
      "Epoch [1/60], Iter [454/633], LR: 0.005000, Loss: 3.7138, top1: 3.1250\n",
      "Epoch [1/60], Iter [455/633], LR: 0.005000, Loss: 3.6634, top1: 4.6875\n",
      "Epoch [1/60], Iter [456/633], LR: 0.005000, Loss: 3.6963, top1: 3.1250\n",
      "Epoch [1/60], Iter [457/633], LR: 0.005000, Loss: 3.6455, top1: 9.3750\n",
      "Epoch [1/60], Iter [458/633], LR: 0.005000, Loss: 3.6859, top1: 1.5625\n",
      "Epoch [1/60], Iter [459/633], LR: 0.005000, Loss: 3.7082, top1: 3.1250\n",
      "Epoch [1/60], Iter [460/633], LR: 0.005000, Loss: 3.6443, top1: 4.6875\n",
      "Epoch [1/60], Iter [461/633], LR: 0.005000, Loss: 3.6816, top1: 1.5625\n",
      "Epoch [1/60], Iter [462/633], LR: 0.005000, Loss: 3.7022, top1: 6.2500\n",
      "Epoch [1/60], Iter [463/633], LR: 0.005000, Loss: 3.7000, top1: 9.3750\n",
      "Epoch [1/60], Iter [464/633], LR: 0.005000, Loss: 3.6765, top1: 7.8125\n",
      "Epoch [1/60], Iter [465/633], LR: 0.005000, Loss: 3.6955, top1: 3.1250\n",
      "Epoch [1/60], Iter [466/633], LR: 0.005000, Loss: 3.6828, top1: 6.2500\n",
      "Epoch [1/60], Iter [467/633], LR: 0.005000, Loss: 3.6704, top1: 7.8125\n",
      "Epoch [1/60], Iter [468/633], LR: 0.005000, Loss: 3.6595, top1: 1.5625\n",
      "Epoch [1/60], Iter [469/633], LR: 0.005000, Loss: 3.7016, top1: 1.5625\n",
      "Epoch [1/60], Iter [470/633], LR: 0.005000, Loss: 3.6922, top1: 6.2500\n",
      "Epoch [1/60], Iter [471/633], LR: 0.005000, Loss: 3.7196, top1: 3.1250\n",
      "Epoch [1/60], Iter [472/633], LR: 0.005000, Loss: 3.6804, top1: 3.1250\n",
      "Epoch [1/60], Iter [473/633], LR: 0.005000, Loss: 3.6375, top1: 10.9375\n",
      "Epoch [1/60], Iter [474/633], LR: 0.005000, Loss: 3.6990, top1: 1.5625\n",
      "Epoch [1/60], Iter [475/633], LR: 0.005000, Loss: 3.6986, top1: 3.1250\n",
      "Epoch [1/60], Iter [476/633], LR: 0.005000, Loss: 3.7050, top1: 1.5625\n",
      "Epoch [1/60], Iter [477/633], LR: 0.005000, Loss: 3.6749, top1: 3.1250\n",
      "Epoch [1/60], Iter [478/633], LR: 0.005000, Loss: 3.6881, top1: 6.2500\n",
      "Epoch [1/60], Iter [479/633], LR: 0.005000, Loss: 3.6451, top1: 10.9375\n",
      "Epoch [1/60], Iter [480/633], LR: 0.005000, Loss: 3.6818, top1: 4.6875\n",
      "Epoch [1/60], Iter [481/633], LR: 0.005000, Loss: 3.6946, top1: 3.1250\n",
      "Epoch [1/60], Iter [482/633], LR: 0.005000, Loss: 3.6936, top1: 3.1250\n",
      "Epoch [1/60], Iter [483/633], LR: 0.005000, Loss: 3.6832, top1: 4.6875\n",
      "Epoch [1/60], Iter [484/633], LR: 0.005000, Loss: 3.6838, top1: 6.2500\n",
      "Epoch [1/60], Iter [485/633], LR: 0.005000, Loss: 3.6939, top1: 1.5625\n",
      "Epoch [1/60], Iter [486/633], LR: 0.005000, Loss: 3.6704, top1: 0.0000\n",
      "Epoch [1/60], Iter [487/633], LR: 0.005000, Loss: 3.6505, top1: 7.8125\n",
      "Epoch [1/60], Iter [488/633], LR: 0.005000, Loss: 3.6782, top1: 9.3750\n",
      "Epoch [1/60], Iter [489/633], LR: 0.005000, Loss: 3.6508, top1: 4.6875\n",
      "Epoch [1/60], Iter [490/633], LR: 0.005000, Loss: 3.6667, top1: 3.1250\n",
      "Epoch [1/60], Iter [491/633], LR: 0.005000, Loss: 3.6814, top1: 7.8125\n",
      "Epoch [1/60], Iter [492/633], LR: 0.005000, Loss: 3.6624, top1: 7.8125\n",
      "Epoch [1/60], Iter [493/633], LR: 0.005000, Loss: 3.6823, top1: 9.3750\n",
      "Epoch [1/60], Iter [494/633], LR: 0.005000, Loss: 3.6781, top1: 1.5625\n",
      "Epoch [1/60], Iter [495/633], LR: 0.005000, Loss: 3.7086, top1: 1.5625\n",
      "Epoch [1/60], Iter [496/633], LR: 0.005000, Loss: 3.7034, top1: 3.1250\n",
      "Epoch [1/60], Iter [497/633], LR: 0.005000, Loss: 3.6709, top1: 4.6875\n",
      "Epoch [1/60], Iter [498/633], LR: 0.005000, Loss: 3.6581, top1: 3.1250\n",
      "Epoch [1/60], Iter [499/633], LR: 0.005000, Loss: 3.6809, top1: 4.6875\n",
      "Epoch [1/60], Iter [500/633], LR: 0.005000, Loss: 3.6682, top1: 3.1250\n",
      "Epoch [1/60], Iter [501/633], LR: 0.005000, Loss: 3.6637, top1: 7.8125\n",
      "Epoch [1/60], Iter [502/633], LR: 0.005000, Loss: 3.6748, top1: 3.1250\n",
      "Epoch [1/60], Iter [503/633], LR: 0.005000, Loss: 3.7084, top1: 0.0000\n",
      "Epoch [1/60], Iter [504/633], LR: 0.005000, Loss: 3.7069, top1: 6.2500\n",
      "Epoch [1/60], Iter [505/633], LR: 0.005000, Loss: 3.6735, top1: 1.5625\n",
      "Epoch [1/60], Iter [506/633], LR: 0.005000, Loss: 3.6519, top1: 3.1250\n",
      "Epoch [1/60], Iter [507/633], LR: 0.005000, Loss: 3.6752, top1: 7.8125\n",
      "Epoch [1/60], Iter [508/633], LR: 0.005000, Loss: 3.6696, top1: 6.2500\n",
      "Epoch [1/60], Iter [509/633], LR: 0.005000, Loss: 3.6716, top1: 6.2500\n",
      "Epoch [1/60], Iter [510/633], LR: 0.005000, Loss: 3.6885, top1: 6.2500\n",
      "Epoch [1/60], Iter [511/633], LR: 0.005000, Loss: 3.7000, top1: 1.5625\n",
      "Epoch [1/60], Iter [512/633], LR: 0.005000, Loss: 3.6984, top1: 1.5625\n",
      "Epoch [1/60], Iter [513/633], LR: 0.005000, Loss: 3.6869, top1: 3.1250\n",
      "Epoch [1/60], Iter [514/633], LR: 0.005000, Loss: 3.6785, top1: 6.2500\n",
      "Epoch [1/60], Iter [515/633], LR: 0.005000, Loss: 3.6819, top1: 6.2500\n",
      "Epoch [1/60], Iter [516/633], LR: 0.005000, Loss: 3.6642, top1: 3.1250\n",
      "Epoch [1/60], Iter [517/633], LR: 0.005000, Loss: 3.6802, top1: 4.6875\n",
      "Epoch [1/60], Iter [518/633], LR: 0.005000, Loss: 3.6785, top1: 4.6875\n",
      "Epoch [1/60], Iter [519/633], LR: 0.005000, Loss: 3.6833, top1: 7.8125\n",
      "Epoch [1/60], Iter [520/633], LR: 0.005000, Loss: 3.6615, top1: 9.3750\n",
      "Epoch [1/60], Iter [521/633], LR: 0.005000, Loss: 3.6811, top1: 9.3750\n",
      "Epoch [1/60], Iter [522/633], LR: 0.005000, Loss: 3.6846, top1: 3.1250\n",
      "Epoch [1/60], Iter [523/633], LR: 0.005000, Loss: 3.6827, top1: 4.6875\n",
      "Epoch [1/60], Iter [524/633], LR: 0.005000, Loss: 3.6641, top1: 4.6875\n",
      "Epoch [1/60], Iter [525/633], LR: 0.005000, Loss: 3.6658, top1: 7.8125\n",
      "Epoch [1/60], Iter [526/633], LR: 0.005000, Loss: 3.6560, top1: 3.1250\n",
      "Epoch [1/60], Iter [527/633], LR: 0.005000, Loss: 3.6638, top1: 4.6875\n",
      "Epoch [1/60], Iter [528/633], LR: 0.005000, Loss: 3.6729, top1: 9.3750\n",
      "Epoch [1/60], Iter [529/633], LR: 0.005000, Loss: 3.6558, top1: 6.2500\n",
      "Epoch [1/60], Iter [530/633], LR: 0.005000, Loss: 3.6287, top1: 9.3750\n",
      "Epoch [1/60], Iter [531/633], LR: 0.005000, Loss: 3.6739, top1: 4.6875\n",
      "Epoch [1/60], Iter [532/633], LR: 0.005000, Loss: 3.7136, top1: 6.2500\n",
      "Epoch [1/60], Iter [533/633], LR: 0.005000, Loss: 3.6533, top1: 6.2500\n",
      "Epoch [1/60], Iter [534/633], LR: 0.005000, Loss: 3.6830, top1: 6.2500\n",
      "Epoch [1/60], Iter [535/633], LR: 0.005000, Loss: 3.6807, top1: 6.2500\n",
      "Epoch [1/60], Iter [536/633], LR: 0.005000, Loss: 3.6951, top1: 3.1250\n",
      "Epoch [1/60], Iter [537/633], LR: 0.005000, Loss: 3.7228, top1: 3.1250\n",
      "Epoch [1/60], Iter [538/633], LR: 0.005000, Loss: 3.6508, top1: 7.8125\n",
      "Epoch [1/60], Iter [539/633], LR: 0.005000, Loss: 3.6710, top1: 7.8125\n",
      "Epoch [1/60], Iter [540/633], LR: 0.005000, Loss: 3.6885, top1: 3.1250\n",
      "Epoch [1/60], Iter [541/633], LR: 0.005000, Loss: 3.6877, top1: 4.6875\n",
      "Epoch [1/60], Iter [542/633], LR: 0.005000, Loss: 3.6568, top1: 4.6875\n",
      "Epoch [1/60], Iter [543/633], LR: 0.005000, Loss: 3.6856, top1: 6.2500\n",
      "Epoch [1/60], Iter [544/633], LR: 0.005000, Loss: 3.6614, top1: 4.6875\n",
      "Epoch [1/60], Iter [545/633], LR: 0.005000, Loss: 3.7030, top1: 6.2500\n",
      "Epoch [1/60], Iter [546/633], LR: 0.005000, Loss: 3.6994, top1: 9.3750\n",
      "Epoch [1/60], Iter [547/633], LR: 0.005000, Loss: 3.6889, top1: 4.6875\n",
      "Epoch [1/60], Iter [548/633], LR: 0.005000, Loss: 3.6510, top1: 6.2500\n",
      "Epoch [1/60], Iter [549/633], LR: 0.005000, Loss: 3.6843, top1: 4.6875\n",
      "Epoch [1/60], Iter [550/633], LR: 0.005000, Loss: 3.7045, top1: 10.9375\n",
      "Epoch [1/60], Iter [551/633], LR: 0.005000, Loss: 3.6407, top1: 4.6875\n",
      "Epoch [1/60], Iter [552/633], LR: 0.005000, Loss: 3.6685, top1: 9.3750\n",
      "Epoch [1/60], Iter [553/633], LR: 0.005000, Loss: 3.6681, top1: 7.8125\n",
      "Epoch [1/60], Iter [554/633], LR: 0.005000, Loss: 3.6536, top1: 4.6875\n",
      "Epoch [1/60], Iter [555/633], LR: 0.005000, Loss: 3.6772, top1: 6.2500\n",
      "Epoch [1/60], Iter [556/633], LR: 0.005000, Loss: 3.6599, top1: 6.2500\n",
      "Epoch [1/60], Iter [557/633], LR: 0.005000, Loss: 3.7063, top1: 4.6875\n",
      "Epoch [1/60], Iter [558/633], LR: 0.005000, Loss: 3.6447, top1: 7.8125\n",
      "Epoch [1/60], Iter [559/633], LR: 0.005000, Loss: 3.6895, top1: 4.6875\n",
      "Epoch [1/60], Iter [560/633], LR: 0.005000, Loss: 3.6524, top1: 10.9375\n",
      "Epoch [1/60], Iter [561/633], LR: 0.005000, Loss: 3.6966, top1: 6.2500\n",
      "Epoch [1/60], Iter [562/633], LR: 0.005000, Loss: 3.6866, top1: 10.9375\n",
      "Epoch [1/60], Iter [563/633], LR: 0.005000, Loss: 3.6996, top1: 4.6875\n",
      "Epoch [1/60], Iter [564/633], LR: 0.005000, Loss: 3.6804, top1: 3.1250\n",
      "Epoch [1/60], Iter [565/633], LR: 0.005000, Loss: 3.6916, top1: 4.6875\n",
      "Epoch [1/60], Iter [566/633], LR: 0.005000, Loss: 3.6826, top1: 4.6875\n",
      "Epoch [1/60], Iter [567/633], LR: 0.005000, Loss: 3.6981, top1: 6.2500\n",
      "Epoch [1/60], Iter [568/633], LR: 0.005000, Loss: 3.6641, top1: 1.5625\n",
      "Epoch [1/60], Iter [569/633], LR: 0.005000, Loss: 3.6682, top1: 7.8125\n",
      "Epoch [1/60], Iter [570/633], LR: 0.005000, Loss: 3.6852, top1: 3.1250\n",
      "Epoch [1/60], Iter [571/633], LR: 0.005000, Loss: 3.7016, top1: 3.1250\n",
      "Epoch [1/60], Iter [572/633], LR: 0.005000, Loss: 3.6544, top1: 12.5000\n",
      "Epoch [1/60], Iter [573/633], LR: 0.005000, Loss: 3.6368, top1: 9.3750\n",
      "Epoch [1/60], Iter [574/633], LR: 0.005000, Loss: 3.6864, top1: 4.6875\n",
      "Epoch [1/60], Iter [575/633], LR: 0.005000, Loss: 3.6570, top1: 9.3750\n",
      "Epoch [1/60], Iter [576/633], LR: 0.005000, Loss: 3.6928, top1: 4.6875\n",
      "Epoch [1/60], Iter [577/633], LR: 0.005000, Loss: 3.6983, top1: 4.6875\n",
      "Epoch [1/60], Iter [578/633], LR: 0.005000, Loss: 3.6168, top1: 9.3750\n",
      "Epoch [1/60], Iter [579/633], LR: 0.005000, Loss: 3.6601, top1: 6.2500\n",
      "Epoch [1/60], Iter [580/633], LR: 0.005000, Loss: 3.6770, top1: 7.8125\n",
      "Epoch [1/60], Iter [581/633], LR: 0.005000, Loss: 3.6789, top1: 6.2500\n",
      "Epoch [1/60], Iter [582/633], LR: 0.005000, Loss: 3.6650, top1: 6.2500\n",
      "Epoch [1/60], Iter [583/633], LR: 0.005000, Loss: 3.6828, top1: 7.8125\n",
      "Epoch [1/60], Iter [584/633], LR: 0.005000, Loss: 3.6595, top1: 7.8125\n",
      "Epoch [1/60], Iter [585/633], LR: 0.005000, Loss: 3.6798, top1: 3.1250\n",
      "Epoch [1/60], Iter [586/633], LR: 0.005000, Loss: 3.6720, top1: 9.3750\n",
      "Epoch [1/60], Iter [587/633], LR: 0.005000, Loss: 3.6686, top1: 1.5625\n",
      "Epoch [1/60], Iter [588/633], LR: 0.005000, Loss: 3.6701, top1: 3.1250\n",
      "Epoch [1/60], Iter [589/633], LR: 0.005000, Loss: 3.6519, top1: 6.2500\n",
      "Epoch [1/60], Iter [590/633], LR: 0.005000, Loss: 3.6813, top1: 6.2500\n",
      "Epoch [1/60], Iter [591/633], LR: 0.005000, Loss: 3.6756, top1: 3.1250\n",
      "Epoch [1/60], Iter [592/633], LR: 0.005000, Loss: 3.6842, top1: 3.1250\n",
      "Epoch [1/60], Iter [593/633], LR: 0.005000, Loss: 3.6628, top1: 6.2500\n",
      "Epoch [1/60], Iter [594/633], LR: 0.005000, Loss: 3.6561, top1: 7.8125\n",
      "Epoch [1/60], Iter [595/633], LR: 0.005000, Loss: 3.6782, top1: 7.8125\n",
      "Epoch [1/60], Iter [596/633], LR: 0.005000, Loss: 3.6546, top1: 6.2500\n",
      "Epoch [1/60], Iter [597/633], LR: 0.005000, Loss: 3.6786, top1: 7.8125\n",
      "Epoch [1/60], Iter [598/633], LR: 0.005000, Loss: 3.6632, top1: 7.8125\n",
      "Epoch [1/60], Iter [599/633], LR: 0.005000, Loss: 3.6868, top1: 3.1250\n",
      "Epoch [1/60], Iter [600/633], LR: 0.005000, Loss: 3.6659, top1: 4.6875\n",
      "Epoch [1/60], Iter [601/633], LR: 0.005000, Loss: 3.6919, top1: 6.2500\n",
      "Epoch [1/60], Iter [602/633], LR: 0.005000, Loss: 3.7034, top1: 0.0000\n",
      "Epoch [1/60], Iter [603/633], LR: 0.005000, Loss: 3.6767, top1: 1.5625\n",
      "Epoch [1/60], Iter [604/633], LR: 0.005000, Loss: 3.6631, top1: 3.1250\n",
      "Epoch [1/60], Iter [605/633], LR: 0.005000, Loss: 3.7064, top1: 6.2500\n",
      "Epoch [1/60], Iter [606/633], LR: 0.005000, Loss: 3.7085, top1: 6.2500\n",
      "Epoch [1/60], Iter [607/633], LR: 0.005000, Loss: 3.6790, top1: 0.0000\n",
      "Epoch [1/60], Iter [608/633], LR: 0.005000, Loss: 3.6994, top1: 7.8125\n",
      "Epoch [1/60], Iter [609/633], LR: 0.005000, Loss: 3.6756, top1: 4.6875\n",
      "Epoch [1/60], Iter [610/633], LR: 0.005000, Loss: 3.6856, top1: 10.9375\n",
      "Epoch [1/60], Iter [611/633], LR: 0.005000, Loss: 3.6789, top1: 9.3750\n",
      "Epoch [1/60], Iter [612/633], LR: 0.005000, Loss: 3.6992, top1: 4.6875\n",
      "Epoch [1/60], Iter [613/633], LR: 0.005000, Loss: 3.6971, top1: 1.5625\n",
      "Epoch [1/60], Iter [614/633], LR: 0.005000, Loss: 3.6613, top1: 9.3750\n",
      "Epoch [1/60], Iter [615/633], LR: 0.005000, Loss: 3.6756, top1: 6.2500\n",
      "Epoch [1/60], Iter [616/633], LR: 0.005000, Loss: 3.6755, top1: 4.6875\n",
      "Epoch [1/60], Iter [617/633], LR: 0.005000, Loss: 3.6591, top1: 7.8125\n",
      "Epoch [1/60], Iter [618/633], LR: 0.005000, Loss: 3.6817, top1: 3.1250\n",
      "Epoch [1/60], Iter [619/633], LR: 0.005000, Loss: 3.6859, top1: 7.8125\n",
      "Epoch [1/60], Iter [620/633], LR: 0.005000, Loss: 3.6895, top1: 6.2500\n",
      "Epoch [1/60], Iter [621/633], LR: 0.005000, Loss: 3.7173, top1: 0.0000\n",
      "Epoch [1/60], Iter [622/633], LR: 0.005000, Loss: 3.6635, top1: 6.2500\n",
      "Epoch [1/60], Iter [623/633], LR: 0.005000, Loss: 3.6732, top1: 0.0000\n",
      "Epoch [1/60], Iter [624/633], LR: 0.005000, Loss: 3.6642, top1: 10.9375\n",
      "Epoch [1/60], Iter [625/633], LR: 0.005000, Loss: 3.6995, top1: 3.1250\n",
      "Epoch [1/60], Iter [626/633], LR: 0.005000, Loss: 3.6786, top1: 1.5625\n",
      "Epoch [1/60], Iter [627/633], LR: 0.005000, Loss: 3.6509, top1: 4.6875\n",
      "Epoch [1/60], Iter [628/633], LR: 0.005000, Loss: 3.6813, top1: 4.6875\n",
      "Epoch [1/60], Iter [629/633], LR: 0.005000, Loss: 3.6723, top1: 4.6875\n",
      "Epoch [1/60], Iter [630/633], LR: 0.005000, Loss: 3.6904, top1: 1.5625\n",
      "Epoch [1/60], Iter [631/633], LR: 0.005000, Loss: 3.6957, top1: 4.6875\n",
      "Epoch [1/60], Iter [632/633], LR: 0.005000, Loss: 3.6505, top1: 6.2500\n",
      "Epoch [1/60], Iter [633/633], LR: 0.005000, Loss: 3.6492, top1: 7.8125\n",
      "Epoch [1/60], Iter [634/633], LR: 0.005000, Loss: 3.6721, top1: 4.8387\n",
      "Epoch [1/60], Val_Loss: 3.6761, Val_top1: 6.3600, best_top1: 0.0000\n",
      "epoch time: 5.242720373471578 min\n",
      "Taking top1 snapshot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ResNet18. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/60], Iter [1/633], LR: 0.005000, Loss: 3.6711, top1: 9.3750\n",
      "Epoch [2/60], Iter [2/633], LR: 0.005000, Loss: 3.6859, top1: 6.2500\n",
      "Epoch [2/60], Iter [3/633], LR: 0.005000, Loss: 3.6757, top1: 7.8125\n",
      "Epoch [2/60], Iter [4/633], LR: 0.005000, Loss: 3.6838, top1: 6.2500\n",
      "Epoch [2/60], Iter [5/633], LR: 0.005000, Loss: 3.7131, top1: 4.6875\n",
      "Epoch [2/60], Iter [6/633], LR: 0.005000, Loss: 3.6393, top1: 10.9375\n",
      "Epoch [2/60], Iter [7/633], LR: 0.005000, Loss: 3.7127, top1: 4.6875\n",
      "Epoch [2/60], Iter [8/633], LR: 0.005000, Loss: 3.6767, top1: 4.6875\n",
      "Epoch [2/60], Iter [9/633], LR: 0.005000, Loss: 3.6519, top1: 12.5000\n",
      "Epoch [2/60], Iter [10/633], LR: 0.005000, Loss: 3.6562, top1: 7.8125\n",
      "Epoch [2/60], Iter [11/633], LR: 0.005000, Loss: 3.6803, top1: 3.1250\n",
      "Epoch [2/60], Iter [12/633], LR: 0.005000, Loss: 3.6851, top1: 6.2500\n",
      "Epoch [2/60], Iter [13/633], LR: 0.005000, Loss: 3.6637, top1: 0.0000\n",
      "Epoch [2/60], Iter [14/633], LR: 0.005000, Loss: 3.6558, top1: 6.2500\n",
      "Epoch [2/60], Iter [15/633], LR: 0.005000, Loss: 3.6702, top1: 7.8125\n",
      "Epoch [2/60], Iter [16/633], LR: 0.005000, Loss: 3.6911, top1: 3.1250\n",
      "Epoch [2/60], Iter [17/633], LR: 0.005000, Loss: 3.6660, top1: 6.2500\n",
      "Epoch [2/60], Iter [18/633], LR: 0.005000, Loss: 3.6887, top1: 6.2500\n",
      "Epoch [2/60], Iter [19/633], LR: 0.005000, Loss: 3.6840, top1: 6.2500\n",
      "Epoch [2/60], Iter [20/633], LR: 0.005000, Loss: 3.6835, top1: 10.9375\n",
      "Epoch [2/60], Iter [21/633], LR: 0.005000, Loss: 3.6750, top1: 9.3750\n",
      "Epoch [2/60], Iter [22/633], LR: 0.005000, Loss: 3.6931, top1: 7.8125\n",
      "Epoch [2/60], Iter [23/633], LR: 0.005000, Loss: 3.6651, top1: 6.2500\n",
      "Epoch [2/60], Iter [24/633], LR: 0.005000, Loss: 3.6855, top1: 1.5625\n",
      "Epoch [2/60], Iter [25/633], LR: 0.005000, Loss: 3.6544, top1: 4.6875\n",
      "Epoch [2/60], Iter [26/633], LR: 0.005000, Loss: 3.6737, top1: 15.6250\n",
      "Epoch [2/60], Iter [27/633], LR: 0.005000, Loss: 3.6325, top1: 12.5000\n",
      "Epoch [2/60], Iter [28/633], LR: 0.005000, Loss: 3.6722, top1: 7.8125\n",
      "Epoch [2/60], Iter [29/633], LR: 0.005000, Loss: 3.6770, top1: 1.5625\n",
      "Epoch [2/60], Iter [30/633], LR: 0.005000, Loss: 3.6893, top1: 7.8125\n",
      "Epoch [2/60], Iter [31/633], LR: 0.005000, Loss: 3.6897, top1: 1.5625\n",
      "Epoch [2/60], Iter [32/633], LR: 0.005000, Loss: 3.6738, top1: 7.8125\n",
      "Epoch [2/60], Iter [33/633], LR: 0.005000, Loss: 3.6867, top1: 3.1250\n",
      "Epoch [2/60], Iter [34/633], LR: 0.005000, Loss: 3.6348, top1: 18.7500\n",
      "Epoch [2/60], Iter [35/633], LR: 0.005000, Loss: 3.6971, top1: 6.2500\n",
      "Epoch [2/60], Iter [36/633], LR: 0.005000, Loss: 3.7071, top1: 0.0000\n",
      "Epoch [2/60], Iter [37/633], LR: 0.005000, Loss: 3.6700, top1: 6.2500\n",
      "Epoch [2/60], Iter [38/633], LR: 0.005000, Loss: 3.6942, top1: 4.6875\n",
      "Epoch [2/60], Iter [39/633], LR: 0.005000, Loss: 3.6714, top1: 4.6875\n",
      "Epoch [2/60], Iter [40/633], LR: 0.005000, Loss: 3.7081, top1: 4.6875\n",
      "Epoch [2/60], Iter [41/633], LR: 0.005000, Loss: 3.6647, top1: 6.2500\n",
      "Epoch [2/60], Iter [42/633], LR: 0.005000, Loss: 3.6774, top1: 3.1250\n",
      "Epoch [2/60], Iter [43/633], LR: 0.005000, Loss: 3.6409, top1: 10.9375\n",
      "Epoch [2/60], Iter [44/633], LR: 0.005000, Loss: 3.6592, top1: 4.6875\n",
      "Epoch [2/60], Iter [45/633], LR: 0.005000, Loss: 3.7072, top1: 1.5625\n",
      "Epoch [2/60], Iter [46/633], LR: 0.005000, Loss: 3.6430, top1: 10.9375\n",
      "Epoch [2/60], Iter [47/633], LR: 0.005000, Loss: 3.6624, top1: 7.8125\n",
      "Epoch [2/60], Iter [48/633], LR: 0.005000, Loss: 3.6826, top1: 3.1250\n",
      "Epoch [2/60], Iter [49/633], LR: 0.005000, Loss: 3.6859, top1: 6.2500\n",
      "Epoch [2/60], Iter [50/633], LR: 0.005000, Loss: 3.6897, top1: 0.0000\n",
      "Epoch [2/60], Iter [51/633], LR: 0.005000, Loss: 3.6734, top1: 1.5625\n",
      "Epoch [2/60], Iter [52/633], LR: 0.005000, Loss: 3.6684, top1: 6.2500\n",
      "Epoch [2/60], Iter [53/633], LR: 0.005000, Loss: 3.6718, top1: 9.3750\n",
      "Epoch [2/60], Iter [54/633], LR: 0.005000, Loss: 3.6866, top1: 3.1250\n",
      "Epoch [2/60], Iter [55/633], LR: 0.005000, Loss: 3.6774, top1: 3.1250\n",
      "Epoch [2/60], Iter [56/633], LR: 0.005000, Loss: 3.6803, top1: 4.6875\n",
      "Epoch [2/60], Iter [57/633], LR: 0.005000, Loss: 3.6883, top1: 1.5625\n",
      "Epoch [2/60], Iter [58/633], LR: 0.005000, Loss: 3.6973, top1: 3.1250\n",
      "Epoch [2/60], Iter [59/633], LR: 0.005000, Loss: 3.6881, top1: 0.0000\n",
      "Epoch [2/60], Iter [60/633], LR: 0.005000, Loss: 3.6935, top1: 4.6875\n",
      "Epoch [2/60], Iter [61/633], LR: 0.005000, Loss: 3.6731, top1: 4.6875\n",
      "Epoch [2/60], Iter [62/633], LR: 0.005000, Loss: 3.7085, top1: 7.8125\n",
      "Epoch [2/60], Iter [63/633], LR: 0.005000, Loss: 3.6948, top1: 4.6875\n",
      "Epoch [2/60], Iter [64/633], LR: 0.005000, Loss: 3.6688, top1: 7.8125\n",
      "Epoch [2/60], Iter [65/633], LR: 0.005000, Loss: 3.6884, top1: 3.1250\n",
      "Epoch [2/60], Iter [66/633], LR: 0.005000, Loss: 3.6251, top1: 7.8125\n",
      "Epoch [2/60], Iter [67/633], LR: 0.005000, Loss: 3.6669, top1: 7.8125\n",
      "Epoch [2/60], Iter [68/633], LR: 0.005000, Loss: 3.6391, top1: 4.6875\n",
      "Epoch [2/60], Iter [69/633], LR: 0.005000, Loss: 3.6933, top1: 3.1250\n",
      "Epoch [2/60], Iter [70/633], LR: 0.005000, Loss: 3.6882, top1: 7.8125\n",
      "Epoch [2/60], Iter [71/633], LR: 0.005000, Loss: 3.6598, top1: 1.5625\n",
      "Epoch [2/60], Iter [72/633], LR: 0.005000, Loss: 3.6901, top1: 7.8125\n",
      "Epoch [2/60], Iter [73/633], LR: 0.005000, Loss: 3.6710, top1: 6.2500\n",
      "Epoch [2/60], Iter [74/633], LR: 0.005000, Loss: 3.6912, top1: 4.6875\n",
      "Epoch [2/60], Iter [75/633], LR: 0.005000, Loss: 3.6798, top1: 3.1250\n",
      "Epoch [2/60], Iter [76/633], LR: 0.005000, Loss: 3.7029, top1: 0.0000\n",
      "Epoch [2/60], Iter [77/633], LR: 0.005000, Loss: 3.6776, top1: 4.6875\n",
      "Epoch [2/60], Iter [78/633], LR: 0.005000, Loss: 3.6798, top1: 4.6875\n",
      "Epoch [2/60], Iter [79/633], LR: 0.005000, Loss: 3.6467, top1: 7.8125\n",
      "Epoch [2/60], Iter [80/633], LR: 0.005000, Loss: 3.6475, top1: 12.5000\n",
      "Epoch [2/60], Iter [81/633], LR: 0.005000, Loss: 3.6856, top1: 3.1250\n",
      "Epoch [2/60], Iter [82/633], LR: 0.005000, Loss: 3.6661, top1: 4.6875\n",
      "Epoch [2/60], Iter [83/633], LR: 0.005000, Loss: 3.6720, top1: 6.2500\n",
      "Epoch [2/60], Iter [84/633], LR: 0.005000, Loss: 3.6520, top1: 1.5625\n",
      "Epoch [2/60], Iter [85/633], LR: 0.005000, Loss: 3.6475, top1: 3.1250\n",
      "Epoch [2/60], Iter [86/633], LR: 0.005000, Loss: 3.6675, top1: 3.1250\n",
      "Epoch [2/60], Iter [87/633], LR: 0.005000, Loss: 3.7273, top1: 1.5625\n",
      "Epoch [2/60], Iter [88/633], LR: 0.005000, Loss: 3.6811, top1: 4.6875\n",
      "Epoch [2/60], Iter [89/633], LR: 0.005000, Loss: 3.6735, top1: 9.3750\n",
      "Epoch [2/60], Iter [90/633], LR: 0.005000, Loss: 3.6788, top1: 4.6875\n",
      "Epoch [2/60], Iter [91/633], LR: 0.005000, Loss: 3.6653, top1: 7.8125\n",
      "Epoch [2/60], Iter [92/633], LR: 0.005000, Loss: 3.6770, top1: 6.2500\n",
      "Epoch [2/60], Iter [93/633], LR: 0.005000, Loss: 3.6652, top1: 3.1250\n",
      "Epoch [2/60], Iter [94/633], LR: 0.005000, Loss: 3.6547, top1: 9.3750\n",
      "Epoch [2/60], Iter [95/633], LR: 0.005000, Loss: 3.6460, top1: 7.8125\n",
      "Epoch [2/60], Iter [96/633], LR: 0.005000, Loss: 3.6797, top1: 6.2500\n",
      "Epoch [2/60], Iter [97/633], LR: 0.005000, Loss: 3.6829, top1: 3.1250\n",
      "Epoch [2/60], Iter [98/633], LR: 0.005000, Loss: 3.6839, top1: 3.1250\n",
      "Epoch [2/60], Iter [99/633], LR: 0.005000, Loss: 3.6765, top1: 3.1250\n",
      "Epoch [2/60], Iter [100/633], LR: 0.005000, Loss: 3.6802, top1: 7.8125\n",
      "Epoch [2/60], Iter [101/633], LR: 0.005000, Loss: 3.6581, top1: 6.2500\n",
      "Epoch [2/60], Iter [102/633], LR: 0.005000, Loss: 3.6675, top1: 7.8125\n",
      "Epoch [2/60], Iter [103/633], LR: 0.005000, Loss: 3.6356, top1: 7.8125\n",
      "Epoch [2/60], Iter [104/633], LR: 0.005000, Loss: 3.6809, top1: 3.1250\n",
      "Epoch [2/60], Iter [105/633], LR: 0.005000, Loss: 3.6957, top1: 4.6875\n",
      "Epoch [2/60], Iter [106/633], LR: 0.005000, Loss: 3.6583, top1: 9.3750\n",
      "Epoch [2/60], Iter [107/633], LR: 0.005000, Loss: 3.6504, top1: 6.2500\n",
      "Epoch [2/60], Iter [108/633], LR: 0.005000, Loss: 3.6767, top1: 6.2500\n",
      "Epoch [2/60], Iter [109/633], LR: 0.005000, Loss: 3.6919, top1: 6.2500\n",
      "Epoch [2/60], Iter [110/633], LR: 0.005000, Loss: 3.6784, top1: 7.8125\n",
      "Epoch [2/60], Iter [111/633], LR: 0.005000, Loss: 3.6675, top1: 7.8125\n",
      "Epoch [2/60], Iter [112/633], LR: 0.005000, Loss: 3.6317, top1: 14.0625\n",
      "Epoch [2/60], Iter [113/633], LR: 0.005000, Loss: 3.6246, top1: 7.8125\n",
      "Epoch [2/60], Iter [114/633], LR: 0.005000, Loss: 3.6623, top1: 6.2500\n",
      "Epoch [2/60], Iter [115/633], LR: 0.005000, Loss: 3.6835, top1: 6.2500\n",
      "Epoch [2/60], Iter [116/633], LR: 0.005000, Loss: 3.6594, top1: 7.8125\n",
      "Epoch [2/60], Iter [117/633], LR: 0.005000, Loss: 3.6732, top1: 4.6875\n",
      "Epoch [2/60], Iter [118/633], LR: 0.005000, Loss: 3.6534, top1: 9.3750\n",
      "Epoch [2/60], Iter [119/633], LR: 0.005000, Loss: 3.6514, top1: 6.2500\n",
      "Epoch [2/60], Iter [120/633], LR: 0.005000, Loss: 3.6809, top1: 6.2500\n",
      "Epoch [2/60], Iter [121/633], LR: 0.005000, Loss: 3.6696, top1: 9.3750\n",
      "Epoch [2/60], Iter [122/633], LR: 0.005000, Loss: 3.7252, top1: 4.6875\n",
      "Epoch [2/60], Iter [123/633], LR: 0.005000, Loss: 3.7053, top1: 4.6875\n",
      "Epoch [2/60], Iter [124/633], LR: 0.005000, Loss: 3.6836, top1: 1.5625\n",
      "Epoch [2/60], Iter [125/633], LR: 0.005000, Loss: 3.6866, top1: 6.2500\n",
      "Epoch [2/60], Iter [126/633], LR: 0.005000, Loss: 3.6681, top1: 4.6875\n",
      "Epoch [2/60], Iter [127/633], LR: 0.005000, Loss: 3.6893, top1: 3.1250\n",
      "Epoch [2/60], Iter [128/633], LR: 0.005000, Loss: 3.6619, top1: 10.9375\n",
      "Epoch [2/60], Iter [129/633], LR: 0.005000, Loss: 3.6725, top1: 4.6875\n",
      "Epoch [2/60], Iter [130/633], LR: 0.005000, Loss: 3.6705, top1: 3.1250\n",
      "Epoch [2/60], Iter [131/633], LR: 0.005000, Loss: 3.7062, top1: 4.6875\n",
      "Epoch [2/60], Iter [132/633], LR: 0.005000, Loss: 3.6592, top1: 6.2500\n",
      "Epoch [2/60], Iter [133/633], LR: 0.005000, Loss: 3.6656, top1: 7.8125\n",
      "Epoch [2/60], Iter [134/633], LR: 0.005000, Loss: 3.6610, top1: 9.3750\n",
      "Epoch [2/60], Iter [135/633], LR: 0.005000, Loss: 3.6781, top1: 6.2500\n",
      "Epoch [2/60], Iter [136/633], LR: 0.005000, Loss: 3.6872, top1: 7.8125\n",
      "Epoch [2/60], Iter [137/633], LR: 0.005000, Loss: 3.6806, top1: 6.2500\n",
      "Epoch [2/60], Iter [138/633], LR: 0.005000, Loss: 3.6568, top1: 6.2500\n",
      "Epoch [2/60], Iter [139/633], LR: 0.005000, Loss: 3.6751, top1: 12.5000\n",
      "Epoch [2/60], Iter [140/633], LR: 0.005000, Loss: 3.6382, top1: 15.6250\n",
      "Epoch [2/60], Iter [141/633], LR: 0.005000, Loss: 3.6696, top1: 6.2500\n",
      "Epoch [2/60], Iter [142/633], LR: 0.005000, Loss: 3.6760, top1: 6.2500\n",
      "Epoch [2/60], Iter [143/633], LR: 0.005000, Loss: 3.6629, top1: 6.2500\n",
      "Epoch [2/60], Iter [144/633], LR: 0.005000, Loss: 3.6923, top1: 6.2500\n",
      "Epoch [2/60], Iter [145/633], LR: 0.005000, Loss: 3.6520, top1: 4.6875\n",
      "Epoch [2/60], Iter [146/633], LR: 0.005000, Loss: 3.6585, top1: 4.6875\n",
      "Epoch [2/60], Iter [147/633], LR: 0.005000, Loss: 3.6827, top1: 6.2500\n",
      "Epoch [2/60], Iter [148/633], LR: 0.005000, Loss: 3.6900, top1: 9.3750\n",
      "Epoch [2/60], Iter [149/633], LR: 0.005000, Loss: 3.6490, top1: 9.3750\n",
      "Epoch [2/60], Iter [150/633], LR: 0.005000, Loss: 3.6519, top1: 6.2500\n",
      "Epoch [2/60], Iter [151/633], LR: 0.005000, Loss: 3.6940, top1: 3.1250\n",
      "Epoch [2/60], Iter [152/633], LR: 0.005000, Loss: 3.6283, top1: 10.9375\n",
      "Epoch [2/60], Iter [153/633], LR: 0.005000, Loss: 3.6571, top1: 6.2500\n",
      "Epoch [2/60], Iter [154/633], LR: 0.005000, Loss: 3.6740, top1: 9.3750\n",
      "Epoch [2/60], Iter [155/633], LR: 0.005000, Loss: 3.6758, top1: 4.6875\n",
      "Epoch [2/60], Iter [156/633], LR: 0.005000, Loss: 3.6547, top1: 4.6875\n",
      "Epoch [2/60], Iter [157/633], LR: 0.005000, Loss: 3.6889, top1: 4.6875\n",
      "Epoch [2/60], Iter [158/633], LR: 0.005000, Loss: 3.6860, top1: 6.2500\n",
      "Epoch [2/60], Iter [159/633], LR: 0.005000, Loss: 3.6463, top1: 6.2500\n",
      "Epoch [2/60], Iter [160/633], LR: 0.005000, Loss: 3.7053, top1: 3.1250\n",
      "Epoch [2/60], Iter [161/633], LR: 0.005000, Loss: 3.6631, top1: 3.1250\n",
      "Epoch [2/60], Iter [162/633], LR: 0.005000, Loss: 3.6778, top1: 4.6875\n",
      "Epoch [2/60], Iter [163/633], LR: 0.005000, Loss: 3.6784, top1: 0.0000\n",
      "Epoch [2/60], Iter [164/633], LR: 0.005000, Loss: 3.6697, top1: 3.1250\n",
      "Epoch [2/60], Iter [165/633], LR: 0.005000, Loss: 3.7012, top1: 3.1250\n",
      "Epoch [2/60], Iter [166/633], LR: 0.005000, Loss: 3.6887, top1: 7.8125\n",
      "Epoch [2/60], Iter [167/633], LR: 0.005000, Loss: 3.6702, top1: 1.5625\n",
      "Epoch [2/60], Iter [168/633], LR: 0.005000, Loss: 3.7006, top1: 4.6875\n",
      "Epoch [2/60], Iter [169/633], LR: 0.005000, Loss: 3.7020, top1: 1.5625\n",
      "Epoch [2/60], Iter [170/633], LR: 0.005000, Loss: 3.6643, top1: 9.3750\n",
      "Epoch [2/60], Iter [171/633], LR: 0.005000, Loss: 3.6324, top1: 10.9375\n",
      "Epoch [2/60], Iter [172/633], LR: 0.005000, Loss: 3.6795, top1: 3.1250\n",
      "Epoch [2/60], Iter [173/633], LR: 0.005000, Loss: 3.6453, top1: 9.3750\n",
      "Epoch [2/60], Iter [174/633], LR: 0.005000, Loss: 3.6444, top1: 4.6875\n",
      "Epoch [2/60], Iter [175/633], LR: 0.005000, Loss: 3.6450, top1: 7.8125\n",
      "Epoch [2/60], Iter [176/633], LR: 0.005000, Loss: 3.6922, top1: 6.2500\n",
      "Epoch [2/60], Iter [177/633], LR: 0.005000, Loss: 3.6886, top1: 0.0000\n",
      "Epoch [2/60], Iter [178/633], LR: 0.005000, Loss: 3.6591, top1: 7.8125\n",
      "Epoch [2/60], Iter [179/633], LR: 0.005000, Loss: 3.6648, top1: 6.2500\n",
      "Epoch [2/60], Iter [180/633], LR: 0.005000, Loss: 3.6270, top1: 9.3750\n",
      "Epoch [2/60], Iter [181/633], LR: 0.005000, Loss: 3.6562, top1: 4.6875\n",
      "Epoch [2/60], Iter [182/633], LR: 0.005000, Loss: 3.6950, top1: 4.6875\n",
      "Epoch [2/60], Iter [183/633], LR: 0.005000, Loss: 3.6448, top1: 7.8125\n",
      "Epoch [2/60], Iter [184/633], LR: 0.005000, Loss: 3.6995, top1: 6.2500\n",
      "Epoch [2/60], Iter [185/633], LR: 0.005000, Loss: 3.6860, top1: 9.3750\n",
      "Epoch [2/60], Iter [186/633], LR: 0.005000, Loss: 3.6608, top1: 10.9375\n",
      "Epoch [2/60], Iter [187/633], LR: 0.005000, Loss: 3.6789, top1: 6.2500\n",
      "Epoch [2/60], Iter [188/633], LR: 0.005000, Loss: 3.6925, top1: 6.2500\n",
      "Epoch [2/60], Iter [189/633], LR: 0.005000, Loss: 3.6641, top1: 6.2500\n",
      "Epoch [2/60], Iter [190/633], LR: 0.005000, Loss: 3.6063, top1: 15.6250\n",
      "Epoch [2/60], Iter [191/633], LR: 0.005000, Loss: 3.6826, top1: 6.2500\n",
      "Epoch [2/60], Iter [192/633], LR: 0.005000, Loss: 3.6519, top1: 6.2500\n",
      "Epoch [2/60], Iter [193/633], LR: 0.005000, Loss: 3.6391, top1: 7.8125\n",
      "Epoch [2/60], Iter [194/633], LR: 0.005000, Loss: 3.6710, top1: 7.8125\n",
      "Epoch [2/60], Iter [195/633], LR: 0.005000, Loss: 3.6713, top1: 6.2500\n",
      "Epoch [2/60], Iter [196/633], LR: 0.005000, Loss: 3.6570, top1: 4.6875\n",
      "Epoch [2/60], Iter [197/633], LR: 0.005000, Loss: 3.7143, top1: 4.6875\n",
      "Epoch [2/60], Iter [198/633], LR: 0.005000, Loss: 3.6458, top1: 4.6875\n",
      "Epoch [2/60], Iter [199/633], LR: 0.005000, Loss: 3.6499, top1: 6.2500\n",
      "Epoch [2/60], Iter [200/633], LR: 0.005000, Loss: 3.6383, top1: 4.6875\n",
      "Epoch [2/60], Iter [201/633], LR: 0.005000, Loss: 3.6826, top1: 6.2500\n",
      "Epoch [2/60], Iter [202/633], LR: 0.005000, Loss: 3.6729, top1: 7.8125\n",
      "Epoch [2/60], Iter [203/633], LR: 0.005000, Loss: 3.6752, top1: 4.6875\n",
      "Epoch [2/60], Iter [204/633], LR: 0.005000, Loss: 3.6704, top1: 3.1250\n",
      "Epoch [2/60], Iter [205/633], LR: 0.005000, Loss: 3.6800, top1: 3.1250\n",
      "Epoch [2/60], Iter [206/633], LR: 0.005000, Loss: 3.6653, top1: 7.8125\n",
      "Epoch [2/60], Iter [207/633], LR: 0.005000, Loss: 3.6543, top1: 7.8125\n",
      "Epoch [2/60], Iter [208/633], LR: 0.005000, Loss: 3.6414, top1: 4.6875\n",
      "Epoch [2/60], Iter [209/633], LR: 0.005000, Loss: 3.6792, top1: 9.3750\n",
      "Epoch [2/60], Iter [210/633], LR: 0.005000, Loss: 3.6849, top1: 3.1250\n",
      "Epoch [2/60], Iter [211/633], LR: 0.005000, Loss: 3.6634, top1: 4.6875\n",
      "Epoch [2/60], Iter [212/633], LR: 0.005000, Loss: 3.6660, top1: 10.9375\n",
      "Epoch [2/60], Iter [213/633], LR: 0.005000, Loss: 3.6542, top1: 6.2500\n",
      "Epoch [2/60], Iter [214/633], LR: 0.005000, Loss: 3.6737, top1: 9.3750\n",
      "Epoch [2/60], Iter [215/633], LR: 0.005000, Loss: 3.7122, top1: 1.5625\n",
      "Epoch [2/60], Iter [216/633], LR: 0.005000, Loss: 3.6629, top1: 7.8125\n",
      "Epoch [2/60], Iter [217/633], LR: 0.005000, Loss: 3.6630, top1: 4.6875\n",
      "Epoch [2/60], Iter [218/633], LR: 0.005000, Loss: 3.6913, top1: 4.6875\n",
      "Epoch [2/60], Iter [219/633], LR: 0.005000, Loss: 3.6365, top1: 3.1250\n",
      "Epoch [2/60], Iter [220/633], LR: 0.005000, Loss: 3.6657, top1: 7.8125\n",
      "Epoch [2/60], Iter [221/633], LR: 0.005000, Loss: 3.6865, top1: 1.5625\n",
      "Epoch [2/60], Iter [222/633], LR: 0.005000, Loss: 3.6656, top1: 6.2500\n",
      "Epoch [2/60], Iter [223/633], LR: 0.005000, Loss: 3.6946, top1: 3.1250\n",
      "Epoch [2/60], Iter [224/633], LR: 0.005000, Loss: 3.6269, top1: 12.5000\n",
      "Epoch [2/60], Iter [225/633], LR: 0.005000, Loss: 3.6575, top1: 7.8125\n",
      "Epoch [2/60], Iter [226/633], LR: 0.005000, Loss: 3.6445, top1: 7.8125\n",
      "Epoch [2/60], Iter [227/633], LR: 0.005000, Loss: 3.6451, top1: 12.5000\n",
      "Epoch [2/60], Iter [228/633], LR: 0.005000, Loss: 3.6803, top1: 3.1250\n",
      "Epoch [2/60], Iter [229/633], LR: 0.005000, Loss: 3.6406, top1: 9.3750\n",
      "Epoch [2/60], Iter [230/633], LR: 0.005000, Loss: 3.6332, top1: 10.9375\n",
      "Epoch [2/60], Iter [231/633], LR: 0.005000, Loss: 3.6470, top1: 1.5625\n",
      "Epoch [2/60], Iter [232/633], LR: 0.005000, Loss: 3.6813, top1: 7.8125\n",
      "Epoch [2/60], Iter [233/633], LR: 0.005000, Loss: 3.6927, top1: 3.1250\n",
      "Epoch [2/60], Iter [234/633], LR: 0.005000, Loss: 3.6936, top1: 3.1250\n",
      "Epoch [2/60], Iter [235/633], LR: 0.005000, Loss: 3.6689, top1: 7.8125\n",
      "Epoch [2/60], Iter [236/633], LR: 0.005000, Loss: 3.6786, top1: 7.8125\n",
      "Epoch [2/60], Iter [237/633], LR: 0.005000, Loss: 3.6725, top1: 6.2500\n",
      "Epoch [2/60], Iter [238/633], LR: 0.005000, Loss: 3.6419, top1: 7.8125\n",
      "Epoch [2/60], Iter [239/633], LR: 0.005000, Loss: 3.6673, top1: 4.6875\n",
      "Epoch [2/60], Iter [240/633], LR: 0.005000, Loss: 3.6474, top1: 7.8125\n",
      "Epoch [2/60], Iter [241/633], LR: 0.005000, Loss: 3.6608, top1: 6.2500\n",
      "Epoch [2/60], Iter [242/633], LR: 0.005000, Loss: 3.6542, top1: 12.5000\n",
      "Epoch [2/60], Iter [243/633], LR: 0.005000, Loss: 3.6827, top1: 4.6875\n",
      "Epoch [2/60], Iter [244/633], LR: 0.005000, Loss: 3.6617, top1: 7.8125\n",
      "Epoch [2/60], Iter [245/633], LR: 0.005000, Loss: 3.6820, top1: 7.8125\n",
      "Epoch [2/60], Iter [246/633], LR: 0.005000, Loss: 3.6352, top1: 1.5625\n",
      "Epoch [2/60], Iter [247/633], LR: 0.005000, Loss: 3.6992, top1: 9.3750\n",
      "Epoch [2/60], Iter [248/633], LR: 0.005000, Loss: 3.6758, top1: 6.2500\n",
      "Epoch [2/60], Iter [249/633], LR: 0.005000, Loss: 3.6558, top1: 9.3750\n",
      "Epoch [2/60], Iter [250/633], LR: 0.005000, Loss: 3.6724, top1: 4.6875\n",
      "Epoch [2/60], Iter [251/633], LR: 0.005000, Loss: 3.6659, top1: 4.6875\n",
      "Epoch [2/60], Iter [252/633], LR: 0.005000, Loss: 3.6615, top1: 10.9375\n",
      "Epoch [2/60], Iter [253/633], LR: 0.005000, Loss: 3.6609, top1: 4.6875\n",
      "Epoch [2/60], Iter [254/633], LR: 0.005000, Loss: 3.6451, top1: 7.8125\n",
      "Epoch [2/60], Iter [255/633], LR: 0.005000, Loss: 3.6642, top1: 6.2500\n",
      "Epoch [2/60], Iter [256/633], LR: 0.005000, Loss: 3.6423, top1: 7.8125\n",
      "Epoch [2/60], Iter [257/633], LR: 0.005000, Loss: 3.6744, top1: 9.3750\n",
      "Epoch [2/60], Iter [258/633], LR: 0.005000, Loss: 3.6428, top1: 6.2500\n",
      "Epoch [2/60], Iter [259/633], LR: 0.005000, Loss: 3.6449, top1: 9.3750\n",
      "Epoch [2/60], Iter [260/633], LR: 0.005000, Loss: 3.6648, top1: 3.1250\n",
      "Epoch [2/60], Iter [261/633], LR: 0.005000, Loss: 3.6412, top1: 7.8125\n",
      "Epoch [2/60], Iter [262/633], LR: 0.005000, Loss: 3.6446, top1: 10.9375\n",
      "Epoch [2/60], Iter [263/633], LR: 0.005000, Loss: 3.6374, top1: 14.0625\n",
      "Epoch [2/60], Iter [264/633], LR: 0.005000, Loss: 3.6612, top1: 1.5625\n",
      "Epoch [2/60], Iter [265/633], LR: 0.005000, Loss: 3.6541, top1: 4.6875\n",
      "Epoch [2/60], Iter [266/633], LR: 0.005000, Loss: 3.6528, top1: 7.8125\n",
      "Epoch [2/60], Iter [267/633], LR: 0.005000, Loss: 3.6490, top1: 10.9375\n",
      "Epoch [2/60], Iter [268/633], LR: 0.005000, Loss: 3.6462, top1: 4.6875\n",
      "Epoch [2/60], Iter [269/633], LR: 0.005000, Loss: 3.6311, top1: 14.0625\n",
      "Epoch [2/60], Iter [270/633], LR: 0.005000, Loss: 3.6633, top1: 7.8125\n",
      "Epoch [2/60], Iter [271/633], LR: 0.005000, Loss: 3.6405, top1: 7.8125\n",
      "Epoch [2/60], Iter [272/633], LR: 0.005000, Loss: 3.6331, top1: 6.2500\n",
      "Epoch [2/60], Iter [273/633], LR: 0.005000, Loss: 3.6476, top1: 6.2500\n",
      "Epoch [2/60], Iter [274/633], LR: 0.005000, Loss: 3.6657, top1: 3.1250\n",
      "Epoch [2/60], Iter [275/633], LR: 0.005000, Loss: 3.6357, top1: 14.0625\n",
      "Epoch [2/60], Iter [276/633], LR: 0.005000, Loss: 3.6435, top1: 10.9375\n",
      "Epoch [2/60], Iter [277/633], LR: 0.005000, Loss: 3.7018, top1: 3.1250\n",
      "Epoch [2/60], Iter [278/633], LR: 0.005000, Loss: 3.6715, top1: 1.5625\n",
      "Epoch [2/60], Iter [279/633], LR: 0.005000, Loss: 3.6571, top1: 1.5625\n",
      "Epoch [2/60], Iter [280/633], LR: 0.005000, Loss: 3.6330, top1: 6.2500\n",
      "Epoch [2/60], Iter [281/633], LR: 0.005000, Loss: 3.6489, top1: 10.9375\n",
      "Epoch [2/60], Iter [282/633], LR: 0.005000, Loss: 3.6686, top1: 6.2500\n",
      "Epoch [2/60], Iter [283/633], LR: 0.005000, Loss: 3.6692, top1: 6.2500\n",
      "Epoch [2/60], Iter [284/633], LR: 0.005000, Loss: 3.6632, top1: 3.1250\n",
      "Epoch [2/60], Iter [285/633], LR: 0.005000, Loss: 3.6407, top1: 6.2500\n",
      "Epoch [2/60], Iter [286/633], LR: 0.005000, Loss: 3.6610, top1: 1.5625\n",
      "Epoch [2/60], Iter [287/633], LR: 0.005000, Loss: 3.6656, top1: 4.6875\n",
      "Epoch [2/60], Iter [288/633], LR: 0.005000, Loss: 3.6621, top1: 3.1250\n",
      "Epoch [2/60], Iter [289/633], LR: 0.005000, Loss: 3.6464, top1: 1.5625\n",
      "Epoch [2/60], Iter [290/633], LR: 0.005000, Loss: 3.6842, top1: 10.9375\n",
      "Epoch [2/60], Iter [291/633], LR: 0.005000, Loss: 3.6264, top1: 7.8125\n",
      "Epoch [2/60], Iter [292/633], LR: 0.005000, Loss: 3.6735, top1: 6.2500\n",
      "Epoch [2/60], Iter [293/633], LR: 0.005000, Loss: 3.6618, top1: 7.8125\n",
      "Epoch [2/60], Iter [294/633], LR: 0.005000, Loss: 3.6747, top1: 9.3750\n",
      "Epoch [2/60], Iter [295/633], LR: 0.005000, Loss: 3.6270, top1: 15.6250\n",
      "Epoch [2/60], Iter [296/633], LR: 0.005000, Loss: 3.6534, top1: 7.8125\n",
      "Epoch [2/60], Iter [297/633], LR: 0.005000, Loss: 3.6147, top1: 12.5000\n",
      "Epoch [2/60], Iter [298/633], LR: 0.005000, Loss: 3.6699, top1: 10.9375\n",
      "Epoch [2/60], Iter [299/633], LR: 0.005000, Loss: 3.6569, top1: 7.8125\n",
      "Epoch [2/60], Iter [300/633], LR: 0.005000, Loss: 3.6476, top1: 12.5000\n",
      "Epoch [2/60], Iter [301/633], LR: 0.005000, Loss: 3.6595, top1: 4.6875\n",
      "Epoch [2/60], Iter [302/633], LR: 0.005000, Loss: 3.6419, top1: 6.2500\n",
      "Epoch [2/60], Iter [303/633], LR: 0.005000, Loss: 3.6276, top1: 9.3750\n",
      "Epoch [2/60], Iter [304/633], LR: 0.005000, Loss: 3.6776, top1: 3.1250\n",
      "Epoch [2/60], Iter [305/633], LR: 0.005000, Loss: 3.6207, top1: 9.3750\n",
      "Epoch [2/60], Iter [306/633], LR: 0.005000, Loss: 3.6360, top1: 9.3750\n",
      "Epoch [2/60], Iter [307/633], LR: 0.005000, Loss: 3.6578, top1: 6.2500\n",
      "Epoch [2/60], Iter [308/633], LR: 0.005000, Loss: 3.6539, top1: 6.2500\n",
      "Epoch [2/60], Iter [309/633], LR: 0.005000, Loss: 3.6361, top1: 14.0625\n",
      "Epoch [2/60], Iter [310/633], LR: 0.005000, Loss: 3.6695, top1: 7.8125\n",
      "Epoch [2/60], Iter [311/633], LR: 0.005000, Loss: 3.6377, top1: 7.8125\n",
      "Epoch [2/60], Iter [312/633], LR: 0.005000, Loss: 3.6405, top1: 6.2500\n",
      "Epoch [2/60], Iter [313/633], LR: 0.005000, Loss: 3.6608, top1: 12.5000\n",
      "Epoch [2/60], Iter [314/633], LR: 0.005000, Loss: 3.6521, top1: 10.9375\n",
      "Epoch [2/60], Iter [315/633], LR: 0.005000, Loss: 3.6513, top1: 7.8125\n",
      "Epoch [2/60], Iter [316/633], LR: 0.005000, Loss: 3.6416, top1: 12.5000\n",
      "Epoch [2/60], Iter [317/633], LR: 0.005000, Loss: 3.6598, top1: 7.8125\n",
      "Epoch [2/60], Iter [318/633], LR: 0.005000, Loss: 3.6347, top1: 20.3125\n",
      "Epoch [2/60], Iter [319/633], LR: 0.005000, Loss: 3.6692, top1: 6.2500\n",
      "Epoch [2/60], Iter [320/633], LR: 0.005000, Loss: 3.6824, top1: 6.2500\n",
      "Epoch [2/60], Iter [321/633], LR: 0.005000, Loss: 3.6684, top1: 6.2500\n",
      "Epoch [2/60], Iter [322/633], LR: 0.005000, Loss: 3.6424, top1: 9.3750\n",
      "Epoch [2/60], Iter [323/633], LR: 0.005000, Loss: 3.6626, top1: 9.3750\n",
      "Epoch [2/60], Iter [324/633], LR: 0.005000, Loss: 3.6832, top1: 6.2500\n",
      "Epoch [2/60], Iter [325/633], LR: 0.005000, Loss: 3.6618, top1: 7.8125\n",
      "Epoch [2/60], Iter [326/633], LR: 0.005000, Loss: 3.6642, top1: 6.2500\n",
      "Epoch [2/60], Iter [327/633], LR: 0.005000, Loss: 3.6512, top1: 10.9375\n",
      "Epoch [2/60], Iter [328/633], LR: 0.005000, Loss: 3.6255, top1: 10.9375\n",
      "Epoch [2/60], Iter [329/633], LR: 0.005000, Loss: 3.6209, top1: 10.9375\n",
      "Epoch [2/60], Iter [330/633], LR: 0.005000, Loss: 3.6678, top1: 4.6875\n",
      "Epoch [2/60], Iter [331/633], LR: 0.005000, Loss: 3.6508, top1: 7.8125\n",
      "Epoch [2/60], Iter [332/633], LR: 0.005000, Loss: 3.6672, top1: 6.2500\n",
      "Epoch [2/60], Iter [333/633], LR: 0.005000, Loss: 3.6356, top1: 7.8125\n",
      "Epoch [2/60], Iter [334/633], LR: 0.005000, Loss: 3.6825, top1: 10.9375\n",
      "Epoch [2/60], Iter [335/633], LR: 0.005000, Loss: 3.6381, top1: 10.9375\n",
      "Epoch [2/60], Iter [336/633], LR: 0.005000, Loss: 3.6763, top1: 4.6875\n",
      "Epoch [2/60], Iter [337/633], LR: 0.005000, Loss: 3.6487, top1: 7.8125\n",
      "Epoch [2/60], Iter [338/633], LR: 0.005000, Loss: 3.6434, top1: 4.6875\n",
      "Epoch [2/60], Iter [339/633], LR: 0.005000, Loss: 3.6540, top1: 3.1250\n",
      "Epoch [2/60], Iter [340/633], LR: 0.005000, Loss: 3.6667, top1: 1.5625\n",
      "Epoch [2/60], Iter [341/633], LR: 0.005000, Loss: 3.6853, top1: 4.6875\n",
      "Epoch [2/60], Iter [342/633], LR: 0.005000, Loss: 3.6798, top1: 9.3750\n",
      "Epoch [2/60], Iter [343/633], LR: 0.005000, Loss: 3.6665, top1: 14.0625\n",
      "Epoch [2/60], Iter [344/633], LR: 0.005000, Loss: 3.6589, top1: 9.3750\n",
      "Epoch [2/60], Iter [345/633], LR: 0.005000, Loss: 3.6546, top1: 7.8125\n",
      "Epoch [2/60], Iter [346/633], LR: 0.005000, Loss: 3.6801, top1: 6.2500\n",
      "Epoch [2/60], Iter [347/633], LR: 0.005000, Loss: 3.6428, top1: 10.9375\n",
      "Epoch [2/60], Iter [348/633], LR: 0.005000, Loss: 3.6797, top1: 6.2500\n",
      "Epoch [2/60], Iter [349/633], LR: 0.005000, Loss: 3.6184, top1: 10.9375\n",
      "Epoch [2/60], Iter [350/633], LR: 0.005000, Loss: 3.6523, top1: 4.6875\n",
      "Epoch [2/60], Iter [351/633], LR: 0.005000, Loss: 3.6771, top1: 6.2500\n",
      "Epoch [2/60], Iter [352/633], LR: 0.005000, Loss: 3.6666, top1: 4.6875\n",
      "Epoch [2/60], Iter [353/633], LR: 0.005000, Loss: 3.6784, top1: 6.2500\n",
      "Epoch [2/60], Iter [354/633], LR: 0.005000, Loss: 3.6689, top1: 3.1250\n",
      "Epoch [2/60], Iter [355/633], LR: 0.005000, Loss: 3.6476, top1: 7.8125\n",
      "Epoch [2/60], Iter [356/633], LR: 0.005000, Loss: 3.6790, top1: 10.9375\n",
      "Epoch [2/60], Iter [357/633], LR: 0.005000, Loss: 3.6581, top1: 10.9375\n",
      "Epoch [2/60], Iter [358/633], LR: 0.005000, Loss: 3.6349, top1: 7.8125\n",
      "Epoch [2/60], Iter [359/633], LR: 0.005000, Loss: 3.6416, top1: 7.8125\n",
      "Epoch [2/60], Iter [360/633], LR: 0.005000, Loss: 3.6699, top1: 1.5625\n",
      "Epoch [2/60], Iter [361/633], LR: 0.005000, Loss: 3.6632, top1: 6.2500\n",
      "Epoch [2/60], Iter [362/633], LR: 0.005000, Loss: 3.6542, top1: 6.2500\n",
      "Epoch [2/60], Iter [363/633], LR: 0.005000, Loss: 3.6356, top1: 9.3750\n",
      "Epoch [2/60], Iter [364/633], LR: 0.005000, Loss: 3.6821, top1: 1.5625\n",
      "Epoch [2/60], Iter [365/633], LR: 0.005000, Loss: 3.6349, top1: 10.9375\n",
      "Epoch [2/60], Iter [366/633], LR: 0.005000, Loss: 3.6798, top1: 1.5625\n",
      "Epoch [2/60], Iter [367/633], LR: 0.005000, Loss: 3.6687, top1: 7.8125\n",
      "Epoch [2/60], Iter [368/633], LR: 0.005000, Loss: 3.6666, top1: 9.3750\n",
      "Epoch [2/60], Iter [369/633], LR: 0.005000, Loss: 3.6693, top1: 3.1250\n",
      "Epoch [2/60], Iter [370/633], LR: 0.005000, Loss: 3.6439, top1: 7.8125\n",
      "Epoch [2/60], Iter [371/633], LR: 0.005000, Loss: 3.6754, top1: 3.1250\n",
      "Epoch [2/60], Iter [372/633], LR: 0.005000, Loss: 3.6363, top1: 9.3750\n",
      "Epoch [2/60], Iter [373/633], LR: 0.005000, Loss: 3.6214, top1: 10.9375\n",
      "Epoch [2/60], Iter [374/633], LR: 0.005000, Loss: 3.6460, top1: 9.3750\n",
      "Epoch [2/60], Iter [375/633], LR: 0.005000, Loss: 3.6489, top1: 4.6875\n",
      "Epoch [2/60], Iter [376/633], LR: 0.005000, Loss: 3.6942, top1: 3.1250\n",
      "Epoch [2/60], Iter [377/633], LR: 0.005000, Loss: 3.6852, top1: 4.6875\n",
      "Epoch [2/60], Iter [378/633], LR: 0.005000, Loss: 3.6472, top1: 14.0625\n",
      "Epoch [2/60], Iter [379/633], LR: 0.005000, Loss: 3.6563, top1: 3.1250\n",
      "Epoch [2/60], Iter [380/633], LR: 0.005000, Loss: 3.6490, top1: 4.6875\n",
      "Epoch [2/60], Iter [381/633], LR: 0.005000, Loss: 3.6418, top1: 6.2500\n",
      "Epoch [2/60], Iter [382/633], LR: 0.005000, Loss: 3.6512, top1: 6.2500\n",
      "Epoch [2/60], Iter [383/633], LR: 0.005000, Loss: 3.6585, top1: 10.9375\n",
      "Epoch [2/60], Iter [384/633], LR: 0.005000, Loss: 3.6626, top1: 6.2500\n",
      "Epoch [2/60], Iter [385/633], LR: 0.005000, Loss: 3.6435, top1: 7.8125\n",
      "Epoch [2/60], Iter [386/633], LR: 0.005000, Loss: 3.6734, top1: 6.2500\n",
      "Epoch [2/60], Iter [387/633], LR: 0.005000, Loss: 3.6582, top1: 7.8125\n",
      "Epoch [2/60], Iter [388/633], LR: 0.005000, Loss: 3.6547, top1: 1.5625\n",
      "Epoch [2/60], Iter [389/633], LR: 0.005000, Loss: 3.6853, top1: 3.1250\n",
      "Epoch [2/60], Iter [390/633], LR: 0.005000, Loss: 3.6612, top1: 10.9375\n",
      "Epoch [2/60], Iter [391/633], LR: 0.005000, Loss: 3.6655, top1: 3.1250\n",
      "Epoch [2/60], Iter [392/633], LR: 0.005000, Loss: 3.6935, top1: 4.6875\n",
      "Epoch [2/60], Iter [393/633], LR: 0.005000, Loss: 3.6185, top1: 9.3750\n",
      "Epoch [2/60], Iter [394/633], LR: 0.005000, Loss: 3.6309, top1: 9.3750\n",
      "Epoch [2/60], Iter [395/633], LR: 0.005000, Loss: 3.6276, top1: 17.1875\n",
      "Epoch [2/60], Iter [396/633], LR: 0.005000, Loss: 3.6291, top1: 9.3750\n",
      "Epoch [2/60], Iter [397/633], LR: 0.005000, Loss: 3.6481, top1: 9.3750\n",
      "Epoch [2/60], Iter [398/633], LR: 0.005000, Loss: 3.6416, top1: 10.9375\n",
      "Epoch [2/60], Iter [399/633], LR: 0.005000, Loss: 3.6611, top1: 6.2500\n",
      "Epoch [2/60], Iter [400/633], LR: 0.005000, Loss: 3.6616, top1: 9.3750\n",
      "Epoch [2/60], Iter [401/633], LR: 0.005000, Loss: 3.6528, top1: 4.6875\n",
      "Epoch [2/60], Iter [402/633], LR: 0.005000, Loss: 3.6497, top1: 4.6875\n",
      "Epoch [2/60], Iter [403/633], LR: 0.005000, Loss: 3.6539, top1: 9.3750\n",
      "Epoch [2/60], Iter [404/633], LR: 0.005000, Loss: 3.6229, top1: 10.9375\n",
      "Epoch [2/60], Iter [405/633], LR: 0.005000, Loss: 3.6807, top1: 4.6875\n",
      "Epoch [2/60], Iter [406/633], LR: 0.005000, Loss: 3.6624, top1: 10.9375\n",
      "Epoch [2/60], Iter [407/633], LR: 0.005000, Loss: 3.7125, top1: 6.2500\n",
      "Epoch [2/60], Iter [408/633], LR: 0.005000, Loss: 3.6494, top1: 7.8125\n",
      "Epoch [2/60], Iter [409/633], LR: 0.005000, Loss: 3.6706, top1: 6.2500\n",
      "Epoch [2/60], Iter [410/633], LR: 0.005000, Loss: 3.6508, top1: 9.3750\n",
      "Epoch [2/60], Iter [411/633], LR: 0.005000, Loss: 3.6273, top1: 7.8125\n",
      "Epoch [2/60], Iter [412/633], LR: 0.005000, Loss: 3.6529, top1: 9.3750\n",
      "Epoch [2/60], Iter [413/633], LR: 0.005000, Loss: 3.6477, top1: 3.1250\n",
      "Epoch [2/60], Iter [414/633], LR: 0.005000, Loss: 3.6519, top1: 10.9375\n",
      "Epoch [2/60], Iter [415/633], LR: 0.005000, Loss: 3.6454, top1: 7.8125\n",
      "Epoch [2/60], Iter [416/633], LR: 0.005000, Loss: 3.6505, top1: 7.8125\n",
      "Epoch [2/60], Iter [417/633], LR: 0.005000, Loss: 3.6773, top1: 10.9375\n",
      "Epoch [2/60], Iter [418/633], LR: 0.005000, Loss: 3.6551, top1: 9.3750\n",
      "Epoch [2/60], Iter [419/633], LR: 0.005000, Loss: 3.6218, top1: 14.0625\n",
      "Epoch [2/60], Iter [420/633], LR: 0.005000, Loss: 3.6568, top1: 3.1250\n",
      "Epoch [2/60], Iter [421/633], LR: 0.005000, Loss: 3.6581, top1: 7.8125\n",
      "Epoch [2/60], Iter [422/633], LR: 0.005000, Loss: 3.6557, top1: 6.2500\n",
      "Epoch [2/60], Iter [423/633], LR: 0.005000, Loss: 3.6610, top1: 12.5000\n",
      "Epoch [2/60], Iter [424/633], LR: 0.005000, Loss: 3.6235, top1: 9.3750\n",
      "Epoch [2/60], Iter [425/633], LR: 0.005000, Loss: 3.6664, top1: 3.1250\n",
      "Epoch [2/60], Iter [426/633], LR: 0.005000, Loss: 3.6748, top1: 6.2500\n",
      "Epoch [2/60], Iter [427/633], LR: 0.005000, Loss: 3.6509, top1: 7.8125\n",
      "Epoch [2/60], Iter [428/633], LR: 0.005000, Loss: 3.6831, top1: 3.1250\n",
      "Epoch [2/60], Iter [429/633], LR: 0.005000, Loss: 3.6640, top1: 9.3750\n",
      "Epoch [2/60], Iter [430/633], LR: 0.005000, Loss: 3.6538, top1: 7.8125\n",
      "Epoch [2/60], Iter [431/633], LR: 0.005000, Loss: 3.6996, top1: 1.5625\n",
      "Epoch [2/60], Iter [432/633], LR: 0.005000, Loss: 3.6697, top1: 3.1250\n",
      "Epoch [2/60], Iter [433/633], LR: 0.005000, Loss: 3.6806, top1: 6.2500\n",
      "Epoch [2/60], Iter [434/633], LR: 0.005000, Loss: 3.6444, top1: 4.6875\n",
      "Epoch [2/60], Iter [435/633], LR: 0.005000, Loss: 3.6560, top1: 9.3750\n",
      "Epoch [2/60], Iter [436/633], LR: 0.005000, Loss: 3.6213, top1: 7.8125\n",
      "Epoch [2/60], Iter [437/633], LR: 0.005000, Loss: 3.6578, top1: 4.6875\n",
      "Epoch [2/60], Iter [438/633], LR: 0.005000, Loss: 3.6869, top1: 6.2500\n",
      "Epoch [2/60], Iter [439/633], LR: 0.005000, Loss: 3.5824, top1: 15.6250\n",
      "Epoch [2/60], Iter [440/633], LR: 0.005000, Loss: 3.6447, top1: 4.6875\n",
      "Epoch [2/60], Iter [441/633], LR: 0.005000, Loss: 3.6299, top1: 9.3750\n",
      "Epoch [2/60], Iter [442/633], LR: 0.005000, Loss: 3.6238, top1: 10.9375\n",
      "Epoch [2/60], Iter [443/633], LR: 0.005000, Loss: 3.6345, top1: 6.2500\n",
      "Epoch [2/60], Iter [444/633], LR: 0.005000, Loss: 3.6756, top1: 4.6875\n",
      "Epoch [2/60], Iter [445/633], LR: 0.005000, Loss: 3.6230, top1: 9.3750\n",
      "Epoch [2/60], Iter [446/633], LR: 0.005000, Loss: 3.6330, top1: 10.9375\n",
      "Epoch [2/60], Iter [447/633], LR: 0.005000, Loss: 3.6510, top1: 12.5000\n",
      "Epoch [2/60], Iter [448/633], LR: 0.005000, Loss: 3.6534, top1: 4.6875\n",
      "Epoch [2/60], Iter [449/633], LR: 0.005000, Loss: 3.6430, top1: 10.9375\n",
      "Epoch [2/60], Iter [450/633], LR: 0.005000, Loss: 3.6605, top1: 6.2500\n",
      "Epoch [2/60], Iter [451/633], LR: 0.005000, Loss: 3.6593, top1: 7.8125\n",
      "Epoch [2/60], Iter [452/633], LR: 0.005000, Loss: 3.6544, top1: 9.3750\n",
      "Epoch [2/60], Iter [453/633], LR: 0.005000, Loss: 3.6740, top1: 3.1250\n",
      "Epoch [2/60], Iter [454/633], LR: 0.005000, Loss: 3.6312, top1: 9.3750\n",
      "Epoch [2/60], Iter [455/633], LR: 0.005000, Loss: 3.6504, top1: 3.1250\n",
      "Epoch [2/60], Iter [456/633], LR: 0.005000, Loss: 3.6497, top1: 3.1250\n",
      "Epoch [2/60], Iter [457/633], LR: 0.005000, Loss: 3.6626, top1: 7.8125\n",
      "Epoch [2/60], Iter [458/633], LR: 0.005000, Loss: 3.6372, top1: 6.2500\n",
      "Epoch [2/60], Iter [459/633], LR: 0.005000, Loss: 3.6721, top1: 4.6875\n",
      "Epoch [2/60], Iter [460/633], LR: 0.005000, Loss: 3.6431, top1: 6.2500\n",
      "Epoch [2/60], Iter [461/633], LR: 0.005000, Loss: 3.6617, top1: 6.2500\n",
      "Epoch [2/60], Iter [462/633], LR: 0.005000, Loss: 3.6098, top1: 6.2500\n",
      "Epoch [2/60], Iter [463/633], LR: 0.005000, Loss: 3.6469, top1: 7.8125\n",
      "Epoch [2/60], Iter [464/633], LR: 0.005000, Loss: 3.6481, top1: 6.2500\n",
      "Epoch [2/60], Iter [465/633], LR: 0.005000, Loss: 3.6354, top1: 7.8125\n",
      "Epoch [2/60], Iter [466/633], LR: 0.005000, Loss: 3.6674, top1: 4.6875\n",
      "Epoch [2/60], Iter [467/633], LR: 0.005000, Loss: 3.6447, top1: 4.6875\n",
      "Epoch [2/60], Iter [468/633], LR: 0.005000, Loss: 3.6423, top1: 10.9375\n",
      "Epoch [2/60], Iter [469/633], LR: 0.005000, Loss: 3.6605, top1: 4.6875\n",
      "Epoch [2/60], Iter [470/633], LR: 0.005000, Loss: 3.6594, top1: 7.8125\n",
      "Epoch [2/60], Iter [471/633], LR: 0.005000, Loss: 3.6807, top1: 6.2500\n",
      "Epoch [2/60], Iter [472/633], LR: 0.005000, Loss: 3.6408, top1: 7.8125\n",
      "Epoch [2/60], Iter [473/633], LR: 0.005000, Loss: 3.6321, top1: 10.9375\n",
      "Epoch [2/60], Iter [474/633], LR: 0.005000, Loss: 3.6540, top1: 6.2500\n",
      "Epoch [2/60], Iter [475/633], LR: 0.005000, Loss: 3.6487, top1: 7.8125\n",
      "Epoch [2/60], Iter [476/633], LR: 0.005000, Loss: 3.6497, top1: 7.8125\n",
      "Epoch [2/60], Iter [477/633], LR: 0.005000, Loss: 3.6539, top1: 6.2500\n",
      "Epoch [2/60], Iter [478/633], LR: 0.005000, Loss: 3.6816, top1: 3.1250\n",
      "Epoch [2/60], Iter [479/633], LR: 0.005000, Loss: 3.6554, top1: 6.2500\n",
      "Epoch [2/60], Iter [480/633], LR: 0.005000, Loss: 3.6852, top1: 3.1250\n",
      "Epoch [2/60], Iter [481/633], LR: 0.005000, Loss: 3.6643, top1: 1.5625\n",
      "Epoch [2/60], Iter [482/633], LR: 0.005000, Loss: 3.6156, top1: 10.9375\n",
      "Epoch [2/60], Iter [483/633], LR: 0.005000, Loss: 3.6229, top1: 10.9375\n",
      "Epoch [2/60], Iter [484/633], LR: 0.005000, Loss: 3.6304, top1: 15.6250\n",
      "Epoch [2/60], Iter [485/633], LR: 0.005000, Loss: 3.6599, top1: 4.6875\n",
      "Epoch [2/60], Iter [486/633], LR: 0.005000, Loss: 3.6584, top1: 3.1250\n",
      "Epoch [2/60], Iter [487/633], LR: 0.005000, Loss: 3.6564, top1: 9.3750\n",
      "Epoch [2/60], Iter [488/633], LR: 0.005000, Loss: 3.6584, top1: 4.6875\n",
      "Epoch [2/60], Iter [489/633], LR: 0.005000, Loss: 3.6415, top1: 4.6875\n",
      "Epoch [2/60], Iter [490/633], LR: 0.005000, Loss: 3.6661, top1: 7.8125\n",
      "Epoch [2/60], Iter [491/633], LR: 0.005000, Loss: 3.6414, top1: 9.3750\n",
      "Epoch [2/60], Iter [492/633], LR: 0.005000, Loss: 3.6552, top1: 4.6875\n",
      "Epoch [2/60], Iter [493/633], LR: 0.005000, Loss: 3.6720, top1: 6.2500\n",
      "Epoch [2/60], Iter [494/633], LR: 0.005000, Loss: 3.6437, top1: 4.6875\n",
      "Epoch [2/60], Iter [495/633], LR: 0.005000, Loss: 3.6493, top1: 3.1250\n",
      "Epoch [2/60], Iter [496/633], LR: 0.005000, Loss: 3.6763, top1: 4.6875\n",
      "Epoch [2/60], Iter [497/633], LR: 0.005000, Loss: 3.6481, top1: 7.8125\n",
      "Epoch [2/60], Iter [498/633], LR: 0.005000, Loss: 3.6522, top1: 9.3750\n",
      "Epoch [2/60], Iter [499/633], LR: 0.005000, Loss: 3.6345, top1: 9.3750\n",
      "Epoch [2/60], Iter [500/633], LR: 0.005000, Loss: 3.6136, top1: 7.8125\n",
      "Epoch [2/60], Iter [501/633], LR: 0.005000, Loss: 3.6233, top1: 10.9375\n",
      "Epoch [2/60], Iter [502/633], LR: 0.005000, Loss: 3.6324, top1: 9.3750\n",
      "Epoch [2/60], Iter [503/633], LR: 0.005000, Loss: 3.6238, top1: 3.1250\n",
      "Epoch [2/60], Iter [504/633], LR: 0.005000, Loss: 3.6493, top1: 6.2500\n",
      "Epoch [2/60], Iter [505/633], LR: 0.005000, Loss: 3.6509, top1: 3.1250\n",
      "Epoch [2/60], Iter [506/633], LR: 0.005000, Loss: 3.6454, top1: 12.5000\n",
      "Epoch [2/60], Iter [507/633], LR: 0.005000, Loss: 3.6735, top1: 6.2500\n",
      "Epoch [2/60], Iter [508/633], LR: 0.005000, Loss: 3.6729, top1: 4.6875\n",
      "Epoch [2/60], Iter [509/633], LR: 0.005000, Loss: 3.6630, top1: 10.9375\n",
      "Epoch [2/60], Iter [510/633], LR: 0.005000, Loss: 3.6150, top1: 10.9375\n",
      "Epoch [2/60], Iter [511/633], LR: 0.005000, Loss: 3.6488, top1: 7.8125\n",
      "Epoch [2/60], Iter [512/633], LR: 0.005000, Loss: 3.6397, top1: 4.6875\n",
      "Epoch [2/60], Iter [513/633], LR: 0.005000, Loss: 3.6319, top1: 4.6875\n",
      "Epoch [2/60], Iter [514/633], LR: 0.005000, Loss: 3.6277, top1: 10.9375\n",
      "Epoch [2/60], Iter [515/633], LR: 0.005000, Loss: 3.6499, top1: 4.6875\n",
      "Epoch [2/60], Iter [516/633], LR: 0.005000, Loss: 3.6434, top1: 9.3750\n",
      "Epoch [2/60], Iter [517/633], LR: 0.005000, Loss: 3.6582, top1: 6.2500\n",
      "Epoch [2/60], Iter [518/633], LR: 0.005000, Loss: 3.6646, top1: 4.6875\n",
      "Epoch [2/60], Iter [519/633], LR: 0.005000, Loss: 3.6723, top1: 9.3750\n",
      "Epoch [2/60], Iter [520/633], LR: 0.005000, Loss: 3.6392, top1: 7.8125\n",
      "Epoch [2/60], Iter [521/633], LR: 0.005000, Loss: 3.6560, top1: 4.6875\n",
      "Epoch [2/60], Iter [522/633], LR: 0.005000, Loss: 3.6284, top1: 9.3750\n",
      "Epoch [2/60], Iter [523/633], LR: 0.005000, Loss: 3.6772, top1: 6.2500\n",
      "Epoch [2/60], Iter [524/633], LR: 0.005000, Loss: 3.6540, top1: 9.3750\n",
      "Epoch [2/60], Iter [525/633], LR: 0.005000, Loss: 3.6090, top1: 12.5000\n",
      "Epoch [2/60], Iter [526/633], LR: 0.005000, Loss: 3.6368, top1: 10.9375\n",
      "Epoch [2/60], Iter [527/633], LR: 0.005000, Loss: 3.6637, top1: 4.6875\n",
      "Epoch [2/60], Iter [528/633], LR: 0.005000, Loss: 3.6562, top1: 4.6875\n",
      "Epoch [2/60], Iter [529/633], LR: 0.005000, Loss: 3.6399, top1: 7.8125\n",
      "Epoch [2/60], Iter [530/633], LR: 0.005000, Loss: 3.6550, top1: 3.1250\n",
      "Epoch [2/60], Iter [531/633], LR: 0.005000, Loss: 3.6552, top1: 3.1250\n",
      "Epoch [2/60], Iter [532/633], LR: 0.005000, Loss: 3.6485, top1: 10.9375\n",
      "Epoch [2/60], Iter [533/633], LR: 0.005000, Loss: 3.6053, top1: 10.9375\n",
      "Epoch [2/60], Iter [534/633], LR: 0.005000, Loss: 3.6246, top1: 7.8125\n",
      "Epoch [2/60], Iter [535/633], LR: 0.005000, Loss: 3.6359, top1: 10.9375\n",
      "Epoch [2/60], Iter [536/633], LR: 0.005000, Loss: 3.6855, top1: 1.5625\n",
      "Epoch [2/60], Iter [537/633], LR: 0.005000, Loss: 3.6377, top1: 9.3750\n",
      "Epoch [2/60], Iter [538/633], LR: 0.005000, Loss: 3.6401, top1: 3.1250\n",
      "Epoch [2/60], Iter [539/633], LR: 0.005000, Loss: 3.6687, top1: 1.5625\n",
      "Epoch [2/60], Iter [540/633], LR: 0.005000, Loss: 3.6526, top1: 7.8125\n",
      "Epoch [2/60], Iter [541/633], LR: 0.005000, Loss: 3.6150, top1: 7.8125\n",
      "Epoch [2/60], Iter [542/633], LR: 0.005000, Loss: 3.6683, top1: 3.1250\n",
      "Epoch [2/60], Iter [543/633], LR: 0.005000, Loss: 3.6439, top1: 12.5000\n",
      "Epoch [2/60], Iter [544/633], LR: 0.005000, Loss: 3.6677, top1: 3.1250\n",
      "Epoch [2/60], Iter [545/633], LR: 0.005000, Loss: 3.6292, top1: 6.2500\n",
      "Epoch [2/60], Iter [546/633], LR: 0.005000, Loss: 3.6560, top1: 6.2500\n",
      "Epoch [2/60], Iter [547/633], LR: 0.005000, Loss: 3.6324, top1: 6.2500\n",
      "Epoch [2/60], Iter [548/633], LR: 0.005000, Loss: 3.6363, top1: 9.3750\n",
      "Epoch [2/60], Iter [549/633], LR: 0.005000, Loss: 3.6358, top1: 7.8125\n",
      "Epoch [2/60], Iter [550/633], LR: 0.005000, Loss: 3.6376, top1: 7.8125\n",
      "Epoch [2/60], Iter [551/633], LR: 0.005000, Loss: 3.6651, top1: 7.8125\n",
      "Epoch [2/60], Iter [552/633], LR: 0.005000, Loss: 3.6972, top1: 3.1250\n",
      "Epoch [2/60], Iter [553/633], LR: 0.005000, Loss: 3.6579, top1: 6.2500\n",
      "Epoch [2/60], Iter [554/633], LR: 0.005000, Loss: 3.6489, top1: 6.2500\n",
      "Epoch [2/60], Iter [555/633], LR: 0.005000, Loss: 3.6649, top1: 4.6875\n",
      "Epoch [2/60], Iter [556/633], LR: 0.005000, Loss: 3.6424, top1: 9.3750\n",
      "Epoch [2/60], Iter [557/633], LR: 0.005000, Loss: 3.6575, top1: 7.8125\n",
      "Epoch [2/60], Iter [558/633], LR: 0.005000, Loss: 3.6180, top1: 12.5000\n",
      "Epoch [2/60], Iter [559/633], LR: 0.005000, Loss: 3.6473, top1: 6.2500\n",
      "Epoch [2/60], Iter [560/633], LR: 0.005000, Loss: 3.6797, top1: 4.6875\n",
      "Epoch [2/60], Iter [561/633], LR: 0.005000, Loss: 3.6642, top1: 4.6875\n",
      "Epoch [2/60], Iter [562/633], LR: 0.005000, Loss: 3.6375, top1: 6.2500\n",
      "Epoch [2/60], Iter [563/633], LR: 0.005000, Loss: 3.6671, top1: 10.9375\n",
      "Epoch [2/60], Iter [564/633], LR: 0.005000, Loss: 3.6448, top1: 10.9375\n",
      "Epoch [2/60], Iter [565/633], LR: 0.005000, Loss: 3.6590, top1: 9.3750\n",
      "Epoch [2/60], Iter [566/633], LR: 0.005000, Loss: 3.6127, top1: 10.9375\n",
      "Epoch [2/60], Iter [567/633], LR: 0.005000, Loss: 3.6257, top1: 10.9375\n",
      "Epoch [2/60], Iter [568/633], LR: 0.005000, Loss: 3.6620, top1: 10.9375\n",
      "Epoch [2/60], Iter [569/633], LR: 0.005000, Loss: 3.6027, top1: 12.5000\n",
      "Epoch [2/60], Iter [570/633], LR: 0.005000, Loss: 3.6358, top1: 7.8125\n",
      "Epoch [2/60], Iter [571/633], LR: 0.005000, Loss: 3.6758, top1: 4.6875\n",
      "Epoch [2/60], Iter [572/633], LR: 0.005000, Loss: 3.6385, top1: 14.0625\n",
      "Epoch [2/60], Iter [573/633], LR: 0.005000, Loss: 3.6558, top1: 7.8125\n",
      "Epoch [2/60], Iter [574/633], LR: 0.005000, Loss: 3.6354, top1: 7.8125\n",
      "Epoch [2/60], Iter [575/633], LR: 0.005000, Loss: 3.6172, top1: 14.0625\n",
      "Epoch [2/60], Iter [576/633], LR: 0.005000, Loss: 3.6212, top1: 9.3750\n",
      "Epoch [2/60], Iter [577/633], LR: 0.005000, Loss: 3.6370, top1: 7.8125\n",
      "Epoch [2/60], Iter [578/633], LR: 0.005000, Loss: 3.6210, top1: 14.0625\n",
      "Epoch [2/60], Iter [579/633], LR: 0.005000, Loss: 3.6942, top1: 1.5625\n",
      "Epoch [2/60], Iter [580/633], LR: 0.005000, Loss: 3.6631, top1: 4.6875\n",
      "Epoch [2/60], Iter [581/633], LR: 0.005000, Loss: 3.6384, top1: 14.0625\n",
      "Epoch [2/60], Iter [582/633], LR: 0.005000, Loss: 3.6268, top1: 7.8125\n",
      "Epoch [2/60], Iter [583/633], LR: 0.005000, Loss: 3.6664, top1: 6.2500\n",
      "Epoch [2/60], Iter [584/633], LR: 0.005000, Loss: 3.6594, top1: 4.6875\n",
      "Epoch [2/60], Iter [585/633], LR: 0.005000, Loss: 3.6307, top1: 10.9375\n",
      "Epoch [2/60], Iter [586/633], LR: 0.005000, Loss: 3.6485, top1: 3.1250\n",
      "Epoch [2/60], Iter [587/633], LR: 0.005000, Loss: 3.6605, top1: 9.3750\n",
      "Epoch [2/60], Iter [588/633], LR: 0.005000, Loss: 3.6495, top1: 6.2500\n",
      "Epoch [2/60], Iter [589/633], LR: 0.005000, Loss: 3.6918, top1: 4.6875\n",
      "Epoch [2/60], Iter [590/633], LR: 0.005000, Loss: 3.6556, top1: 6.2500\n",
      "Epoch [2/60], Iter [591/633], LR: 0.005000, Loss: 3.6682, top1: 6.2500\n",
      "Epoch [2/60], Iter [592/633], LR: 0.005000, Loss: 3.6399, top1: 10.9375\n",
      "Epoch [2/60], Iter [593/633], LR: 0.005000, Loss: 3.6771, top1: 7.8125\n",
      "Epoch [2/60], Iter [594/633], LR: 0.005000, Loss: 3.6666, top1: 6.2500\n",
      "Epoch [2/60], Iter [595/633], LR: 0.005000, Loss: 3.6481, top1: 4.6875\n",
      "Epoch [2/60], Iter [596/633], LR: 0.005000, Loss: 3.6339, top1: 10.9375\n",
      "Epoch [2/60], Iter [597/633], LR: 0.005000, Loss: 3.6321, top1: 14.0625\n",
      "Epoch [2/60], Iter [598/633], LR: 0.005000, Loss: 3.6444, top1: 6.2500\n",
      "Epoch [2/60], Iter [599/633], LR: 0.005000, Loss: 3.6490, top1: 7.8125\n",
      "Epoch [2/60], Iter [600/633], LR: 0.005000, Loss: 3.6440, top1: 7.8125\n",
      "Epoch [2/60], Iter [601/633], LR: 0.005000, Loss: 3.6492, top1: 4.6875\n",
      "Epoch [2/60], Iter [602/633], LR: 0.005000, Loss: 3.6355, top1: 12.5000\n",
      "Epoch [2/60], Iter [603/633], LR: 0.005000, Loss: 3.6225, top1: 9.3750\n",
      "Epoch [2/60], Iter [604/633], LR: 0.005000, Loss: 3.6592, top1: 7.8125\n",
      "Epoch [2/60], Iter [605/633], LR: 0.005000, Loss: 3.6342, top1: 9.3750\n",
      "Epoch [2/60], Iter [606/633], LR: 0.005000, Loss: 3.6478, top1: 3.1250\n",
      "Epoch [2/60], Iter [607/633], LR: 0.005000, Loss: 3.6506, top1: 9.3750\n",
      "Epoch [2/60], Iter [608/633], LR: 0.005000, Loss: 3.6227, top1: 4.6875\n",
      "Epoch [2/60], Iter [609/633], LR: 0.005000, Loss: 3.6502, top1: 4.6875\n",
      "Epoch [2/60], Iter [610/633], LR: 0.005000, Loss: 3.6764, top1: 7.8125\n",
      "Epoch [2/60], Iter [611/633], LR: 0.005000, Loss: 3.6085, top1: 18.7500\n",
      "Epoch [2/60], Iter [612/633], LR: 0.005000, Loss: 3.6383, top1: 9.3750\n",
      "Epoch [2/60], Iter [613/633], LR: 0.005000, Loss: 3.6269, top1: 12.5000\n",
      "Epoch [2/60], Iter [614/633], LR: 0.005000, Loss: 3.6062, top1: 14.0625\n",
      "Epoch [2/60], Iter [615/633], LR: 0.005000, Loss: 3.6316, top1: 6.2500\n",
      "Epoch [2/60], Iter [616/633], LR: 0.005000, Loss: 3.6271, top1: 15.6250\n",
      "Epoch [2/60], Iter [617/633], LR: 0.005000, Loss: 3.6367, top1: 9.3750\n",
      "Epoch [2/60], Iter [618/633], LR: 0.005000, Loss: 3.6300, top1: 3.1250\n",
      "Epoch [2/60], Iter [619/633], LR: 0.005000, Loss: 3.6205, top1: 10.9375\n",
      "Epoch [2/60], Iter [620/633], LR: 0.005000, Loss: 3.6445, top1: 7.8125\n",
      "Epoch [2/60], Iter [621/633], LR: 0.005000, Loss: 3.6138, top1: 10.9375\n",
      "Epoch [2/60], Iter [622/633], LR: 0.005000, Loss: 3.6581, top1: 6.2500\n",
      "Epoch [2/60], Iter [623/633], LR: 0.005000, Loss: 3.6714, top1: 3.1250\n",
      "Epoch [2/60], Iter [624/633], LR: 0.005000, Loss: 3.6572, top1: 10.9375\n",
      "Epoch [2/60], Iter [625/633], LR: 0.005000, Loss: 3.6561, top1: 9.3750\n",
      "Epoch [2/60], Iter [626/633], LR: 0.005000, Loss: 3.6182, top1: 10.9375\n",
      "Epoch [2/60], Iter [627/633], LR: 0.005000, Loss: 3.6529, top1: 4.6875\n",
      "Epoch [2/60], Iter [628/633], LR: 0.005000, Loss: 3.6617, top1: 4.6875\n",
      "Epoch [2/60], Iter [629/633], LR: 0.005000, Loss: 3.6057, top1: 9.3750\n",
      "Epoch [2/60], Iter [630/633], LR: 0.005000, Loss: 3.6575, top1: 4.6875\n",
      "Epoch [2/60], Iter [631/633], LR: 0.005000, Loss: 3.6322, top1: 9.3750\n",
      "Epoch [2/60], Iter [632/633], LR: 0.005000, Loss: 3.6505, top1: 6.2500\n",
      "Epoch [2/60], Iter [633/633], LR: 0.005000, Loss: 3.6458, top1: 6.2500\n",
      "Epoch [2/60], Iter [634/633], LR: 0.005000, Loss: 3.6557, top1: 6.4516\n",
      "Epoch [2/60], Val_Loss: 3.6512, Val_top1: 7.1523, best_top1: 6.3600\n",
      "epoch time: 4.478229025999705 min\n",
      "Taking top1 snapshot...\n",
      "Epoch [3/60], Iter [1/633], LR: 0.005000, Loss: 3.6384, top1: 10.9375\n",
      "Epoch [3/60], Iter [2/633], LR: 0.005000, Loss: 3.6578, top1: 12.5000\n",
      "Epoch [3/60], Iter [3/633], LR: 0.005000, Loss: 3.6494, top1: 4.6875\n",
      "Epoch [3/60], Iter [4/633], LR: 0.005000, Loss: 3.6410, top1: 14.0625\n",
      "Epoch [3/60], Iter [5/633], LR: 0.005000, Loss: 3.6795, top1: 6.2500\n",
      "Epoch [3/60], Iter [6/633], LR: 0.005000, Loss: 3.6757, top1: 7.8125\n",
      "Epoch [3/60], Iter [7/633], LR: 0.005000, Loss: 3.6535, top1: 10.9375\n",
      "Epoch [3/60], Iter [8/633], LR: 0.005000, Loss: 3.6233, top1: 14.0625\n",
      "Epoch [3/60], Iter [9/633], LR: 0.005000, Loss: 3.6447, top1: 7.8125\n",
      "Epoch [3/60], Iter [10/633], LR: 0.005000, Loss: 3.6669, top1: 6.2500\n",
      "Epoch [3/60], Iter [11/633], LR: 0.005000, Loss: 3.6429, top1: 7.8125\n",
      "Epoch [3/60], Iter [12/633], LR: 0.005000, Loss: 3.6551, top1: 4.6875\n",
      "Epoch [3/60], Iter [13/633], LR: 0.005000, Loss: 3.6386, top1: 4.6875\n",
      "Epoch [3/60], Iter [14/633], LR: 0.005000, Loss: 3.6078, top1: 17.1875\n",
      "Epoch [3/60], Iter [15/633], LR: 0.005000, Loss: 3.6783, top1: 4.6875\n",
      "Epoch [3/60], Iter [16/633], LR: 0.005000, Loss: 3.6240, top1: 7.8125\n",
      "Epoch [3/60], Iter [17/633], LR: 0.005000, Loss: 3.6466, top1: 12.5000\n",
      "Epoch [3/60], Iter [18/633], LR: 0.005000, Loss: 3.6809, top1: 4.6875\n",
      "Epoch [3/60], Iter [19/633], LR: 0.005000, Loss: 3.6464, top1: 7.8125\n",
      "Epoch [3/60], Iter [20/633], LR: 0.005000, Loss: 3.6676, top1: 1.5625\n",
      "Epoch [3/60], Iter [21/633], LR: 0.005000, Loss: 3.6441, top1: 7.8125\n",
      "Epoch [3/60], Iter [22/633], LR: 0.005000, Loss: 3.6400, top1: 7.8125\n",
      "Epoch [3/60], Iter [23/633], LR: 0.005000, Loss: 3.6239, top1: 9.3750\n",
      "Epoch [3/60], Iter [24/633], LR: 0.005000, Loss: 3.6648, top1: 4.6875\n",
      "Epoch [3/60], Iter [25/633], LR: 0.005000, Loss: 3.6525, top1: 9.3750\n",
      "Epoch [3/60], Iter [26/633], LR: 0.005000, Loss: 3.6136, top1: 15.6250\n",
      "Epoch [3/60], Iter [27/633], LR: 0.005000, Loss: 3.6410, top1: 3.1250\n",
      "Epoch [3/60], Iter [28/633], LR: 0.005000, Loss: 3.6424, top1: 9.3750\n",
      "Epoch [3/60], Iter [29/633], LR: 0.005000, Loss: 3.6165, top1: 7.8125\n",
      "Epoch [3/60], Iter [30/633], LR: 0.005000, Loss: 3.6201, top1: 12.5000\n",
      "Epoch [3/60], Iter [31/633], LR: 0.005000, Loss: 3.6456, top1: 9.3750\n",
      "Epoch [3/60], Iter [32/633], LR: 0.005000, Loss: 3.6269, top1: 12.5000\n",
      "Epoch [3/60], Iter [33/633], LR: 0.005000, Loss: 3.6554, top1: 9.3750\n",
      "Epoch [3/60], Iter [34/633], LR: 0.005000, Loss: 3.6045, top1: 9.3750\n",
      "Epoch [3/60], Iter [35/633], LR: 0.005000, Loss: 3.6638, top1: 3.1250\n",
      "Epoch [3/60], Iter [36/633], LR: 0.005000, Loss: 3.6443, top1: 3.1250\n",
      "Epoch [3/60], Iter [37/633], LR: 0.005000, Loss: 3.6536, top1: 9.3750\n",
      "Epoch [3/60], Iter [38/633], LR: 0.005000, Loss: 3.6083, top1: 7.8125\n",
      "Epoch [3/60], Iter [39/633], LR: 0.005000, Loss: 3.6480, top1: 10.9375\n",
      "Epoch [3/60], Iter [40/633], LR: 0.005000, Loss: 3.6512, top1: 7.8125\n",
      "Epoch [3/60], Iter [41/633], LR: 0.005000, Loss: 3.6727, top1: 9.3750\n",
      "Epoch [3/60], Iter [42/633], LR: 0.005000, Loss: 3.6593, top1: 7.8125\n",
      "Epoch [3/60], Iter [43/633], LR: 0.005000, Loss: 3.6771, top1: 4.6875\n",
      "Epoch [3/60], Iter [44/633], LR: 0.005000, Loss: 3.6227, top1: 12.5000\n",
      "Epoch [3/60], Iter [45/633], LR: 0.005000, Loss: 3.5843, top1: 15.6250\n",
      "Epoch [3/60], Iter [46/633], LR: 0.005000, Loss: 3.6548, top1: 9.3750\n",
      "Epoch [3/60], Iter [47/633], LR: 0.005000, Loss: 3.6539, top1: 3.1250\n",
      "Epoch [3/60], Iter [48/633], LR: 0.005000, Loss: 3.6586, top1: 10.9375\n",
      "Epoch [3/60], Iter [49/633], LR: 0.005000, Loss: 3.6583, top1: 6.2500\n",
      "Epoch [3/60], Iter [50/633], LR: 0.005000, Loss: 3.6561, top1: 4.6875\n",
      "Epoch [3/60], Iter [51/633], LR: 0.005000, Loss: 3.6375, top1: 6.2500\n",
      "Epoch [3/60], Iter [52/633], LR: 0.005000, Loss: 3.6287, top1: 9.3750\n",
      "Epoch [3/60], Iter [53/633], LR: 0.005000, Loss: 3.6557, top1: 6.2500\n",
      "Epoch [3/60], Iter [54/633], LR: 0.005000, Loss: 3.6138, top1: 14.0625\n",
      "Epoch [3/60], Iter [55/633], LR: 0.005000, Loss: 3.6378, top1: 6.2500\n",
      "Epoch [3/60], Iter [56/633], LR: 0.005000, Loss: 3.6393, top1: 7.8125\n",
      "Epoch [3/60], Iter [57/633], LR: 0.005000, Loss: 3.6408, top1: 6.2500\n",
      "Epoch [3/60], Iter [58/633], LR: 0.005000, Loss: 3.6516, top1: 6.2500\n",
      "Epoch [3/60], Iter [59/633], LR: 0.005000, Loss: 3.6608, top1: 6.2500\n",
      "Epoch [3/60], Iter [60/633], LR: 0.005000, Loss: 3.6433, top1: 7.8125\n",
      "Epoch [3/60], Iter [61/633], LR: 0.005000, Loss: 3.6416, top1: 10.9375\n",
      "Epoch [3/60], Iter [62/633], LR: 0.005000, Loss: 3.6759, top1: 7.8125\n",
      "Epoch [3/60], Iter [63/633], LR: 0.005000, Loss: 3.6769, top1: 10.9375\n",
      "Epoch [3/60], Iter [64/633], LR: 0.005000, Loss: 3.6668, top1: 10.9375\n",
      "Epoch [3/60], Iter [65/633], LR: 0.005000, Loss: 3.6356, top1: 4.6875\n",
      "Epoch [3/60], Iter [66/633], LR: 0.005000, Loss: 3.6424, top1: 9.3750\n",
      "Epoch [3/60], Iter [67/633], LR: 0.005000, Loss: 3.6554, top1: 7.8125\n",
      "Epoch [3/60], Iter [68/633], LR: 0.005000, Loss: 3.6258, top1: 9.3750\n",
      "Epoch [3/60], Iter [69/633], LR: 0.005000, Loss: 3.6487, top1: 6.2500\n",
      "Epoch [3/60], Iter [70/633], LR: 0.005000, Loss: 3.6539, top1: 6.2500\n",
      "Epoch [3/60], Iter [71/633], LR: 0.005000, Loss: 3.6426, top1: 6.2500\n",
      "Epoch [3/60], Iter [72/633], LR: 0.005000, Loss: 3.6720, top1: 1.5625\n",
      "Epoch [3/60], Iter [73/633], LR: 0.005000, Loss: 3.6562, top1: 9.3750\n",
      "Epoch [3/60], Iter [74/633], LR: 0.005000, Loss: 3.6212, top1: 18.7500\n",
      "Epoch [3/60], Iter [75/633], LR: 0.005000, Loss: 3.6616, top1: 4.6875\n",
      "Epoch [3/60], Iter [76/633], LR: 0.005000, Loss: 3.5990, top1: 10.9375\n",
      "Epoch [3/60], Iter [77/633], LR: 0.005000, Loss: 3.6281, top1: 10.9375\n",
      "Epoch [3/60], Iter [78/633], LR: 0.005000, Loss: 3.6753, top1: 6.2500\n",
      "Epoch [3/60], Iter [79/633], LR: 0.005000, Loss: 3.6533, top1: 10.9375\n",
      "Epoch [3/60], Iter [80/633], LR: 0.005000, Loss: 3.6806, top1: 3.1250\n",
      "Epoch [3/60], Iter [81/633], LR: 0.005000, Loss: 3.6821, top1: 3.1250\n",
      "Epoch [3/60], Iter [82/633], LR: 0.005000, Loss: 3.6352, top1: 7.8125\n",
      "Epoch [3/60], Iter [83/633], LR: 0.005000, Loss: 3.6146, top1: 14.0625\n",
      "Epoch [3/60], Iter [84/633], LR: 0.005000, Loss: 3.6473, top1: 7.8125\n",
      "Epoch [3/60], Iter [85/633], LR: 0.005000, Loss: 3.5942, top1: 14.0625\n",
      "Epoch [3/60], Iter [86/633], LR: 0.005000, Loss: 3.6388, top1: 7.8125\n",
      "Epoch [3/60], Iter [87/633], LR: 0.005000, Loss: 3.6076, top1: 14.0625\n",
      "Epoch [3/60], Iter [88/633], LR: 0.005000, Loss: 3.6538, top1: 4.6875\n",
      "Epoch [3/60], Iter [89/633], LR: 0.005000, Loss: 3.6252, top1: 4.6875\n",
      "Epoch [3/60], Iter [90/633], LR: 0.005000, Loss: 3.6753, top1: 4.6875\n",
      "Epoch [3/60], Iter [91/633], LR: 0.005000, Loss: 3.6511, top1: 6.2500\n",
      "Epoch [3/60], Iter [92/633], LR: 0.005000, Loss: 3.6044, top1: 14.0625\n",
      "Epoch [3/60], Iter [93/633], LR: 0.005000, Loss: 3.6882, top1: 3.1250\n",
      "Epoch [3/60], Iter [94/633], LR: 0.005000, Loss: 3.6529, top1: 4.6875\n",
      "Epoch [3/60], Iter [95/633], LR: 0.005000, Loss: 3.6349, top1: 9.3750\n",
      "Epoch [3/60], Iter [96/633], LR: 0.005000, Loss: 3.6517, top1: 6.2500\n",
      "Epoch [3/60], Iter [97/633], LR: 0.005000, Loss: 3.6421, top1: 10.9375\n",
      "Epoch [3/60], Iter [98/633], LR: 0.005000, Loss: 3.6283, top1: 9.3750\n",
      "Epoch [3/60], Iter [99/633], LR: 0.005000, Loss: 3.6266, top1: 10.9375\n",
      "Epoch [3/60], Iter [100/633], LR: 0.005000, Loss: 3.6259, top1: 12.5000\n",
      "Epoch [3/60], Iter [101/633], LR: 0.005000, Loss: 3.6446, top1: 10.9375\n",
      "Epoch [3/60], Iter [102/633], LR: 0.005000, Loss: 3.6343, top1: 12.5000\n",
      "Epoch [3/60], Iter [103/633], LR: 0.005000, Loss: 3.6932, top1: 3.1250\n",
      "Epoch [3/60], Iter [104/633], LR: 0.005000, Loss: 3.6588, top1: 7.8125\n",
      "Epoch [3/60], Iter [105/633], LR: 0.005000, Loss: 3.6459, top1: 4.6875\n",
      "Epoch [3/60], Iter [106/633], LR: 0.005000, Loss: 3.6495, top1: 6.2500\n",
      "Epoch [3/60], Iter [107/633], LR: 0.005000, Loss: 3.6206, top1: 12.5000\n",
      "Epoch [3/60], Iter [108/633], LR: 0.005000, Loss: 3.6522, top1: 7.8125\n",
      "Epoch [3/60], Iter [109/633], LR: 0.005000, Loss: 3.6706, top1: 9.3750\n",
      "Epoch [3/60], Iter [110/633], LR: 0.005000, Loss: 3.6278, top1: 17.1875\n",
      "Epoch [3/60], Iter [111/633], LR: 0.005000, Loss: 3.6629, top1: 3.1250\n",
      "Epoch [3/60], Iter [112/633], LR: 0.005000, Loss: 3.6502, top1: 9.3750\n",
      "Epoch [3/60], Iter [113/633], LR: 0.005000, Loss: 3.6339, top1: 12.5000\n",
      "Epoch [3/60], Iter [114/633], LR: 0.005000, Loss: 3.6259, top1: 7.8125\n",
      "Epoch [3/60], Iter [115/633], LR: 0.005000, Loss: 3.6583, top1: 9.3750\n",
      "Epoch [3/60], Iter [116/633], LR: 0.005000, Loss: 3.6402, top1: 10.9375\n",
      "Epoch [3/60], Iter [117/633], LR: 0.005000, Loss: 3.6582, top1: 9.3750\n",
      "Epoch [3/60], Iter [118/633], LR: 0.005000, Loss: 3.6252, top1: 12.5000\n",
      "Epoch [3/60], Iter [119/633], LR: 0.005000, Loss: 3.6188, top1: 7.8125\n",
      "Epoch [3/60], Iter [120/633], LR: 0.005000, Loss: 3.6387, top1: 9.3750\n",
      "Epoch [3/60], Iter [121/633], LR: 0.005000, Loss: 3.6390, top1: 9.3750\n",
      "Epoch [3/60], Iter [122/633], LR: 0.005000, Loss: 3.6448, top1: 6.2500\n",
      "Epoch [3/60], Iter [123/633], LR: 0.005000, Loss: 3.6382, top1: 10.9375\n",
      "Epoch [3/60], Iter [124/633], LR: 0.005000, Loss: 3.6456, top1: 6.2500\n",
      "Epoch [3/60], Iter [125/633], LR: 0.005000, Loss: 3.6895, top1: 3.1250\n",
      "Epoch [3/60], Iter [126/633], LR: 0.005000, Loss: 3.6427, top1: 12.5000\n",
      "Epoch [3/60], Iter [127/633], LR: 0.005000, Loss: 3.6296, top1: 10.9375\n",
      "Epoch [3/60], Iter [128/633], LR: 0.005000, Loss: 3.6370, top1: 10.9375\n",
      "Epoch [3/60], Iter [129/633], LR: 0.005000, Loss: 3.6504, top1: 7.8125\n",
      "Epoch [3/60], Iter [130/633], LR: 0.005000, Loss: 3.6682, top1: 7.8125\n",
      "Epoch [3/60], Iter [131/633], LR: 0.005000, Loss: 3.6211, top1: 6.2500\n",
      "Epoch [3/60], Iter [132/633], LR: 0.005000, Loss: 3.6209, top1: 10.9375\n",
      "Epoch [3/60], Iter [133/633], LR: 0.005000, Loss: 3.6469, top1: 7.8125\n",
      "Epoch [3/60], Iter [134/633], LR: 0.005000, Loss: 3.5935, top1: 9.3750\n",
      "Epoch [3/60], Iter [135/633], LR: 0.005000, Loss: 3.6102, top1: 9.3750\n",
      "Epoch [3/60], Iter [136/633], LR: 0.005000, Loss: 3.6280, top1: 4.6875\n",
      "Epoch [3/60], Iter [137/633], LR: 0.005000, Loss: 3.6392, top1: 7.8125\n",
      "Epoch [3/60], Iter [138/633], LR: 0.005000, Loss: 3.6178, top1: 12.5000\n",
      "Epoch [3/60], Iter [139/633], LR: 0.005000, Loss: 3.6334, top1: 10.9375\n",
      "Epoch [3/60], Iter [140/633], LR: 0.005000, Loss: 3.6180, top1: 7.8125\n",
      "Epoch [3/60], Iter [141/633], LR: 0.005000, Loss: 3.5926, top1: 14.0625\n",
      "Epoch [3/60], Iter [142/633], LR: 0.005000, Loss: 3.6361, top1: 12.5000\n",
      "Epoch [3/60], Iter [143/633], LR: 0.005000, Loss: 3.6366, top1: 10.9375\n",
      "Epoch [3/60], Iter [144/633], LR: 0.005000, Loss: 3.6102, top1: 10.9375\n",
      "Epoch [3/60], Iter [145/633], LR: 0.005000, Loss: 3.6276, top1: 15.6250\n",
      "Epoch [3/60], Iter [146/633], LR: 0.005000, Loss: 3.6566, top1: 6.2500\n",
      "Epoch [3/60], Iter [147/633], LR: 0.005000, Loss: 3.6357, top1: 12.5000\n",
      "Epoch [3/60], Iter [148/633], LR: 0.005000, Loss: 3.6210, top1: 9.3750\n",
      "Epoch [3/60], Iter [149/633], LR: 0.005000, Loss: 3.6166, top1: 12.5000\n",
      "Epoch [3/60], Iter [150/633], LR: 0.005000, Loss: 3.6621, top1: 4.6875\n",
      "Epoch [3/60], Iter [151/633], LR: 0.005000, Loss: 3.6290, top1: 10.9375\n",
      "Epoch [3/60], Iter [152/633], LR: 0.005000, Loss: 3.6424, top1: 10.9375\n",
      "Epoch [3/60], Iter [153/633], LR: 0.005000, Loss: 3.6654, top1: 6.2500\n",
      "Epoch [3/60], Iter [154/633], LR: 0.005000, Loss: 3.6499, top1: 9.3750\n",
      "Epoch [3/60], Iter [155/633], LR: 0.005000, Loss: 3.6244, top1: 6.2500\n",
      "Epoch [3/60], Iter [156/633], LR: 0.005000, Loss: 3.6096, top1: 7.8125\n",
      "Epoch [3/60], Iter [157/633], LR: 0.005000, Loss: 3.6540, top1: 3.1250\n",
      "Epoch [3/60], Iter [158/633], LR: 0.005000, Loss: 3.6283, top1: 12.5000\n",
      "Epoch [3/60], Iter [159/633], LR: 0.005000, Loss: 3.6169, top1: 10.9375\n",
      "Epoch [3/60], Iter [160/633], LR: 0.005000, Loss: 3.6284, top1: 12.5000\n",
      "Epoch [3/60], Iter [161/633], LR: 0.005000, Loss: 3.6349, top1: 9.3750\n",
      "Epoch [3/60], Iter [162/633], LR: 0.005000, Loss: 3.6678, top1: 6.2500\n",
      "Epoch [3/60], Iter [163/633], LR: 0.005000, Loss: 3.6027, top1: 9.3750\n",
      "Epoch [3/60], Iter [164/633], LR: 0.005000, Loss: 3.6715, top1: 10.9375\n",
      "Epoch [3/60], Iter [165/633], LR: 0.005000, Loss: 3.6745, top1: 7.8125\n",
      "Epoch [3/60], Iter [166/633], LR: 0.005000, Loss: 3.6421, top1: 10.9375\n",
      "Epoch [3/60], Iter [167/633], LR: 0.005000, Loss: 3.6663, top1: 1.5625\n",
      "Epoch [3/60], Iter [168/633], LR: 0.005000, Loss: 3.6350, top1: 4.6875\n",
      "Epoch [3/60], Iter [169/633], LR: 0.005000, Loss: 3.6388, top1: 7.8125\n",
      "Epoch [3/60], Iter [170/633], LR: 0.005000, Loss: 3.6668, top1: 9.3750\n",
      "Epoch [3/60], Iter [171/633], LR: 0.005000, Loss: 3.6623, top1: 6.2500\n",
      "Epoch [3/60], Iter [172/633], LR: 0.005000, Loss: 3.6432, top1: 9.3750\n",
      "Epoch [3/60], Iter [173/633], LR: 0.005000, Loss: 3.6400, top1: 10.9375\n",
      "Epoch [3/60], Iter [174/633], LR: 0.005000, Loss: 3.6740, top1: 4.6875\n",
      "Epoch [3/60], Iter [175/633], LR: 0.005000, Loss: 3.6704, top1: 6.2500\n",
      "Epoch [3/60], Iter [176/633], LR: 0.005000, Loss: 3.6422, top1: 6.2500\n",
      "Epoch [3/60], Iter [177/633], LR: 0.005000, Loss: 3.6280, top1: 10.9375\n",
      "Epoch [3/60], Iter [178/633], LR: 0.005000, Loss: 3.6653, top1: 4.6875\n",
      "Epoch [3/60], Iter [179/633], LR: 0.005000, Loss: 3.6743, top1: 6.2500\n",
      "Epoch [3/60], Iter [180/633], LR: 0.005000, Loss: 3.6358, top1: 9.3750\n",
      "Epoch [3/60], Iter [181/633], LR: 0.005000, Loss: 3.6254, top1: 6.2500\n",
      "Epoch [3/60], Iter [182/633], LR: 0.005000, Loss: 3.6617, top1: 7.8125\n",
      "Epoch [3/60], Iter [183/633], LR: 0.005000, Loss: 3.6212, top1: 9.3750\n",
      "Epoch [3/60], Iter [184/633], LR: 0.005000, Loss: 3.6251, top1: 7.8125\n",
      "Epoch [3/60], Iter [185/633], LR: 0.005000, Loss: 3.6442, top1: 10.9375\n",
      "Epoch [3/60], Iter [186/633], LR: 0.005000, Loss: 3.6542, top1: 6.2500\n",
      "Epoch [3/60], Iter [187/633], LR: 0.005000, Loss: 3.6386, top1: 7.8125\n",
      "Epoch [3/60], Iter [188/633], LR: 0.005000, Loss: 3.6448, top1: 6.2500\n",
      "Epoch [3/60], Iter [189/633], LR: 0.005000, Loss: 3.6619, top1: 7.8125\n",
      "Epoch [3/60], Iter [190/633], LR: 0.005000, Loss: 3.6639, top1: 4.6875\n",
      "Epoch [3/60], Iter [191/633], LR: 0.005000, Loss: 3.6505, top1: 9.3750\n",
      "Epoch [3/60], Iter [192/633], LR: 0.005000, Loss: 3.6274, top1: 15.6250\n",
      "Epoch [3/60], Iter [193/633], LR: 0.005000, Loss: 3.6228, top1: 15.6250\n",
      "Epoch [3/60], Iter [194/633], LR: 0.005000, Loss: 3.5899, top1: 15.6250\n",
      "Epoch [3/60], Iter [195/633], LR: 0.005000, Loss: 3.6467, top1: 7.8125\n",
      "Epoch [3/60], Iter [196/633], LR: 0.005000, Loss: 3.6540, top1: 6.2500\n",
      "Epoch [3/60], Iter [197/633], LR: 0.005000, Loss: 3.6696, top1: 6.2500\n",
      "Epoch [3/60], Iter [198/633], LR: 0.005000, Loss: 3.6527, top1: 1.5625\n",
      "Epoch [3/60], Iter [199/633], LR: 0.005000, Loss: 3.5962, top1: 10.9375\n",
      "Epoch [3/60], Iter [200/633], LR: 0.005000, Loss: 3.6217, top1: 9.3750\n",
      "Epoch [3/60], Iter [201/633], LR: 0.005000, Loss: 3.6488, top1: 10.9375\n",
      "Epoch [3/60], Iter [202/633], LR: 0.005000, Loss: 3.6367, top1: 12.5000\n",
      "Epoch [3/60], Iter [203/633], LR: 0.005000, Loss: 3.6330, top1: 9.3750\n",
      "Epoch [3/60], Iter [204/633], LR: 0.005000, Loss: 3.6408, top1: 4.6875\n",
      "Epoch [3/60], Iter [205/633], LR: 0.005000, Loss: 3.6383, top1: 7.8125\n",
      "Epoch [3/60], Iter [206/633], LR: 0.005000, Loss: 3.6733, top1: 4.6875\n",
      "Epoch [3/60], Iter [207/633], LR: 0.005000, Loss: 3.6192, top1: 10.9375\n",
      "Epoch [3/60], Iter [208/633], LR: 0.005000, Loss: 3.6164, top1: 6.2500\n",
      "Epoch [3/60], Iter [209/633], LR: 0.005000, Loss: 3.6044, top1: 12.5000\n",
      "Epoch [3/60], Iter [210/633], LR: 0.005000, Loss: 3.6462, top1: 7.8125\n",
      "Epoch [3/60], Iter [211/633], LR: 0.005000, Loss: 3.6597, top1: 6.2500\n",
      "Epoch [3/60], Iter [212/633], LR: 0.005000, Loss: 3.6820, top1: 9.3750\n",
      "Epoch [3/60], Iter [213/633], LR: 0.005000, Loss: 3.6602, top1: 7.8125\n",
      "Epoch [3/60], Iter [214/633], LR: 0.005000, Loss: 3.6286, top1: 10.9375\n",
      "Epoch [3/60], Iter [215/633], LR: 0.005000, Loss: 3.6366, top1: 6.2500\n",
      "Epoch [3/60], Iter [216/633], LR: 0.005000, Loss: 3.6243, top1: 7.8125\n",
      "Epoch [3/60], Iter [217/633], LR: 0.005000, Loss: 3.6706, top1: 4.6875\n",
      "Epoch [3/60], Iter [218/633], LR: 0.005000, Loss: 3.6035, top1: 17.1875\n",
      "Epoch [3/60], Iter [219/633], LR: 0.005000, Loss: 3.6201, top1: 12.5000\n",
      "Epoch [3/60], Iter [220/633], LR: 0.005000, Loss: 3.6475, top1: 7.8125\n",
      "Epoch [3/60], Iter [221/633], LR: 0.005000, Loss: 3.6382, top1: 6.2500\n",
      "Epoch [3/60], Iter [222/633], LR: 0.005000, Loss: 3.6214, top1: 10.9375\n",
      "Epoch [3/60], Iter [223/633], LR: 0.005000, Loss: 3.6325, top1: 7.8125\n",
      "Epoch [3/60], Iter [224/633], LR: 0.005000, Loss: 3.6042, top1: 6.2500\n",
      "Epoch [3/60], Iter [225/633], LR: 0.005000, Loss: 3.6963, top1: 4.6875\n",
      "Epoch [3/60], Iter [226/633], LR: 0.005000, Loss: 3.6330, top1: 12.5000\n",
      "Epoch [3/60], Iter [227/633], LR: 0.005000, Loss: 3.6076, top1: 15.6250\n",
      "Epoch [3/60], Iter [228/633], LR: 0.005000, Loss: 3.6564, top1: 7.8125\n",
      "Epoch [3/60], Iter [229/633], LR: 0.005000, Loss: 3.6225, top1: 6.2500\n",
      "Epoch [3/60], Iter [230/633], LR: 0.005000, Loss: 3.6167, top1: 9.3750\n",
      "Epoch [3/60], Iter [231/633], LR: 0.005000, Loss: 3.6404, top1: 14.0625\n",
      "Epoch [3/60], Iter [232/633], LR: 0.005000, Loss: 3.6456, top1: 9.3750\n",
      "Epoch [3/60], Iter [233/633], LR: 0.005000, Loss: 3.6070, top1: 10.9375\n",
      "Epoch [3/60], Iter [234/633], LR: 0.005000, Loss: 3.6642, top1: 6.2500\n",
      "Epoch [3/60], Iter [235/633], LR: 0.005000, Loss: 3.6495, top1: 10.9375\n",
      "Epoch [3/60], Iter [236/633], LR: 0.005000, Loss: 3.6371, top1: 6.2500\n",
      "Epoch [3/60], Iter [237/633], LR: 0.005000, Loss: 3.6602, top1: 4.6875\n",
      "Epoch [3/60], Iter [238/633], LR: 0.005000, Loss: 3.6284, top1: 12.5000\n",
      "Epoch [3/60], Iter [239/633], LR: 0.005000, Loss: 3.6332, top1: 12.5000\n",
      "Epoch [3/60], Iter [240/633], LR: 0.005000, Loss: 3.6276, top1: 4.6875\n",
      "Epoch [3/60], Iter [241/633], LR: 0.005000, Loss: 3.6757, top1: 4.6875\n",
      "Epoch [3/60], Iter [242/633], LR: 0.005000, Loss: 3.6492, top1: 9.3750\n",
      "Epoch [3/60], Iter [243/633], LR: 0.005000, Loss: 3.6542, top1: 6.2500\n",
      "Epoch [3/60], Iter [244/633], LR: 0.005000, Loss: 3.6193, top1: 4.6875\n",
      "Epoch [3/60], Iter [245/633], LR: 0.005000, Loss: 3.6144, top1: 6.2500\n",
      "Epoch [3/60], Iter [246/633], LR: 0.005000, Loss: 3.6660, top1: 4.6875\n",
      "Epoch [3/60], Iter [247/633], LR: 0.005000, Loss: 3.6499, top1: 4.6875\n",
      "Epoch [3/60], Iter [248/633], LR: 0.005000, Loss: 3.6702, top1: 4.6875\n",
      "Epoch [3/60], Iter [249/633], LR: 0.005000, Loss: 3.6373, top1: 9.3750\n",
      "Epoch [3/60], Iter [250/633], LR: 0.005000, Loss: 3.6172, top1: 9.3750\n",
      "Epoch [3/60], Iter [251/633], LR: 0.005000, Loss: 3.6586, top1: 3.1250\n",
      "Epoch [3/60], Iter [252/633], LR: 0.005000, Loss: 3.6223, top1: 9.3750\n",
      "Epoch [3/60], Iter [253/633], LR: 0.005000, Loss: 3.6367, top1: 6.2500\n",
      "Epoch [3/60], Iter [254/633], LR: 0.005000, Loss: 3.6545, top1: 14.0625\n",
      "Epoch [3/60], Iter [255/633], LR: 0.005000, Loss: 3.6195, top1: 10.9375\n",
      "Epoch [3/60], Iter [256/633], LR: 0.005000, Loss: 3.6239, top1: 7.8125\n",
      "Epoch [3/60], Iter [257/633], LR: 0.005000, Loss: 3.6596, top1: 7.8125\n",
      "Epoch [3/60], Iter [258/633], LR: 0.005000, Loss: 3.6769, top1: 4.6875\n",
      "Epoch [3/60], Iter [259/633], LR: 0.005000, Loss: 3.6283, top1: 12.5000\n",
      "Epoch [3/60], Iter [260/633], LR: 0.005000, Loss: 3.6326, top1: 7.8125\n",
      "Epoch [3/60], Iter [261/633], LR: 0.005000, Loss: 3.6623, top1: 6.2500\n",
      "Epoch [3/60], Iter [262/633], LR: 0.005000, Loss: 3.6605, top1: 4.6875\n",
      "Epoch [3/60], Iter [263/633], LR: 0.005000, Loss: 3.6408, top1: 10.9375\n",
      "Epoch [3/60], Iter [264/633], LR: 0.005000, Loss: 3.6291, top1: 10.9375\n",
      "Epoch [3/60], Iter [265/633], LR: 0.005000, Loss: 3.6811, top1: 7.8125\n",
      "Epoch [3/60], Iter [266/633], LR: 0.005000, Loss: 3.6680, top1: 3.1250\n",
      "Epoch [3/60], Iter [267/633], LR: 0.005000, Loss: 3.6529, top1: 6.2500\n",
      "Epoch [3/60], Iter [268/633], LR: 0.005000, Loss: 3.6879, top1: 4.6875\n",
      "Epoch [3/60], Iter [269/633], LR: 0.005000, Loss: 3.6364, top1: 9.3750\n",
      "Epoch [3/60], Iter [270/633], LR: 0.005000, Loss: 3.6584, top1: 7.8125\n",
      "Epoch [3/60], Iter [271/633], LR: 0.005000, Loss: 3.6459, top1: 12.5000\n",
      "Epoch [3/60], Iter [272/633], LR: 0.005000, Loss: 3.6242, top1: 15.6250\n",
      "Epoch [3/60], Iter [273/633], LR: 0.005000, Loss: 3.6281, top1: 12.5000\n",
      "Epoch [3/60], Iter [274/633], LR: 0.005000, Loss: 3.6474, top1: 7.8125\n",
      "Epoch [3/60], Iter [275/633], LR: 0.005000, Loss: 3.5878, top1: 15.6250\n",
      "Epoch [3/60], Iter [276/633], LR: 0.005000, Loss: 3.6322, top1: 6.2500\n",
      "Epoch [3/60], Iter [277/633], LR: 0.005000, Loss: 3.6740, top1: 4.6875\n",
      "Epoch [3/60], Iter [278/633], LR: 0.005000, Loss: 3.6661, top1: 7.8125\n",
      "Epoch [3/60], Iter [279/633], LR: 0.005000, Loss: 3.6535, top1: 7.8125\n",
      "Epoch [3/60], Iter [280/633], LR: 0.005000, Loss: 3.5974, top1: 18.7500\n",
      "Epoch [3/60], Iter [281/633], LR: 0.005000, Loss: 3.6427, top1: 4.6875\n",
      "Epoch [3/60], Iter [282/633], LR: 0.005000, Loss: 3.6418, top1: 7.8125\n",
      "Epoch [3/60], Iter [283/633], LR: 0.005000, Loss: 3.6037, top1: 9.3750\n",
      "Epoch [3/60], Iter [284/633], LR: 0.005000, Loss: 3.5678, top1: 17.1875\n",
      "Epoch [3/60], Iter [285/633], LR: 0.005000, Loss: 3.6424, top1: 3.1250\n",
      "Epoch [3/60], Iter [286/633], LR: 0.005000, Loss: 3.6892, top1: 1.5625\n",
      "Epoch [3/60], Iter [287/633], LR: 0.005000, Loss: 3.6479, top1: 10.9375\n",
      "Epoch [3/60], Iter [288/633], LR: 0.005000, Loss: 3.6471, top1: 10.9375\n",
      "Epoch [3/60], Iter [289/633], LR: 0.005000, Loss: 3.6318, top1: 6.2500\n",
      "Epoch [3/60], Iter [290/633], LR: 0.005000, Loss: 3.6276, top1: 12.5000\n",
      "Epoch [3/60], Iter [291/633], LR: 0.005000, Loss: 3.6703, top1: 4.6875\n",
      "Epoch [3/60], Iter [292/633], LR: 0.005000, Loss: 3.6552, top1: 10.9375\n",
      "Epoch [3/60], Iter [293/633], LR: 0.005000, Loss: 3.6052, top1: 12.5000\n",
      "Epoch [3/60], Iter [294/633], LR: 0.005000, Loss: 3.6492, top1: 7.8125\n",
      "Epoch [3/60], Iter [295/633], LR: 0.005000, Loss: 3.6491, top1: 9.3750\n",
      "Epoch [3/60], Iter [296/633], LR: 0.005000, Loss: 3.6247, top1: 9.3750\n",
      "Epoch [3/60], Iter [297/633], LR: 0.005000, Loss: 3.6298, top1: 9.3750\n",
      "Epoch [3/60], Iter [298/633], LR: 0.005000, Loss: 3.6579, top1: 6.2500\n",
      "Epoch [3/60], Iter [299/633], LR: 0.005000, Loss: 3.6513, top1: 7.8125\n",
      "Epoch [3/60], Iter [300/633], LR: 0.005000, Loss: 3.6521, top1: 9.3750\n",
      "Epoch [3/60], Iter [301/633], LR: 0.005000, Loss: 3.6486, top1: 3.1250\n",
      "Epoch [3/60], Iter [302/633], LR: 0.005000, Loss: 3.6488, top1: 4.6875\n",
      "Epoch [3/60], Iter [303/633], LR: 0.005000, Loss: 3.6236, top1: 10.9375\n",
      "Epoch [3/60], Iter [304/633], LR: 0.005000, Loss: 3.6089, top1: 10.9375\n",
      "Epoch [3/60], Iter [305/633], LR: 0.005000, Loss: 3.6191, top1: 7.8125\n",
      "Epoch [3/60], Iter [306/633], LR: 0.005000, Loss: 3.6594, top1: 6.2500\n",
      "Epoch [3/60], Iter [307/633], LR: 0.005000, Loss: 3.6230, top1: 10.9375\n",
      "Epoch [3/60], Iter [308/633], LR: 0.005000, Loss: 3.6788, top1: 6.2500\n",
      "Epoch [3/60], Iter [309/633], LR: 0.005000, Loss: 3.6434, top1: 7.8125\n",
      "Epoch [3/60], Iter [310/633], LR: 0.005000, Loss: 3.6085, top1: 14.0625\n",
      "Epoch [3/60], Iter [311/633], LR: 0.005000, Loss: 3.6480, top1: 7.8125\n",
      "Epoch [3/60], Iter [312/633], LR: 0.005000, Loss: 3.6419, top1: 7.8125\n",
      "Epoch [3/60], Iter [313/633], LR: 0.005000, Loss: 3.6149, top1: 15.6250\n",
      "Epoch [3/60], Iter [314/633], LR: 0.005000, Loss: 3.6848, top1: 3.1250\n",
      "Epoch [3/60], Iter [315/633], LR: 0.005000, Loss: 3.6279, top1: 10.9375\n",
      "Epoch [3/60], Iter [316/633], LR: 0.005000, Loss: 3.6745, top1: 4.6875\n",
      "Epoch [3/60], Iter [317/633], LR: 0.005000, Loss: 3.6646, top1: 4.6875\n",
      "Epoch [3/60], Iter [318/633], LR: 0.005000, Loss: 3.6656, top1: 4.6875\n",
      "Epoch [3/60], Iter [319/633], LR: 0.005000, Loss: 3.6607, top1: 4.6875\n",
      "Epoch [3/60], Iter [320/633], LR: 0.005000, Loss: 3.6245, top1: 9.3750\n",
      "Epoch [3/60], Iter [321/633], LR: 0.005000, Loss: 3.6227, top1: 12.5000\n",
      "Epoch [3/60], Iter [322/633], LR: 0.005000, Loss: 3.6745, top1: 4.6875\n",
      "Epoch [3/60], Iter [323/633], LR: 0.005000, Loss: 3.6911, top1: 1.5625\n",
      "Epoch [3/60], Iter [324/633], LR: 0.005000, Loss: 3.6144, top1: 10.9375\n",
      "Epoch [3/60], Iter [325/633], LR: 0.005000, Loss: 3.6321, top1: 9.3750\n",
      "Epoch [3/60], Iter [326/633], LR: 0.005000, Loss: 3.6354, top1: 9.3750\n",
      "Epoch [3/60], Iter [327/633], LR: 0.005000, Loss: 3.6655, top1: 3.1250\n",
      "Epoch [3/60], Iter [328/633], LR: 0.005000, Loss: 3.6119, top1: 9.3750\n",
      "Epoch [3/60], Iter [329/633], LR: 0.005000, Loss: 3.6526, top1: 3.1250\n",
      "Epoch [3/60], Iter [330/633], LR: 0.005000, Loss: 3.6220, top1: 10.9375\n",
      "Epoch [3/60], Iter [331/633], LR: 0.005000, Loss: 3.6759, top1: 10.9375\n",
      "Epoch [3/60], Iter [332/633], LR: 0.005000, Loss: 3.6536, top1: 7.8125\n",
      "Epoch [3/60], Iter [333/633], LR: 0.005000, Loss: 3.6192, top1: 10.9375\n",
      "Epoch [3/60], Iter [334/633], LR: 0.005000, Loss: 3.6521, top1: 6.2500\n",
      "Epoch [3/60], Iter [335/633], LR: 0.005000, Loss: 3.6121, top1: 7.8125\n",
      "Epoch [3/60], Iter [336/633], LR: 0.005000, Loss: 3.6061, top1: 17.1875\n",
      "Epoch [3/60], Iter [337/633], LR: 0.005000, Loss: 3.6074, top1: 18.7500\n",
      "Epoch [3/60], Iter [338/633], LR: 0.005000, Loss: 3.6318, top1: 7.8125\n",
      "Epoch [3/60], Iter [339/633], LR: 0.005000, Loss: 3.6064, top1: 12.5000\n",
      "Epoch [3/60], Iter [340/633], LR: 0.005000, Loss: 3.6663, top1: 6.2500\n",
      "Epoch [3/60], Iter [341/633], LR: 0.005000, Loss: 3.6462, top1: 7.8125\n",
      "Epoch [3/60], Iter [342/633], LR: 0.005000, Loss: 3.6148, top1: 18.7500\n",
      "Epoch [3/60], Iter [343/633], LR: 0.005000, Loss: 3.6588, top1: 9.3750\n",
      "Epoch [3/60], Iter [344/633], LR: 0.005000, Loss: 3.6181, top1: 9.3750\n",
      "Epoch [3/60], Iter [345/633], LR: 0.005000, Loss: 3.6363, top1: 6.2500\n",
      "Epoch [3/60], Iter [346/633], LR: 0.005000, Loss: 3.6412, top1: 7.8125\n",
      "Epoch [3/60], Iter [347/633], LR: 0.005000, Loss: 3.6191, top1: 17.1875\n",
      "Epoch [3/60], Iter [348/633], LR: 0.005000, Loss: 3.6082, top1: 6.2500\n",
      "Epoch [3/60], Iter [349/633], LR: 0.005000, Loss: 3.6543, top1: 3.1250\n",
      "Epoch [3/60], Iter [350/633], LR: 0.005000, Loss: 3.6064, top1: 10.9375\n",
      "Epoch [3/60], Iter [351/633], LR: 0.005000, Loss: 3.6302, top1: 17.1875\n",
      "Epoch [3/60], Iter [352/633], LR: 0.005000, Loss: 3.6262, top1: 9.3750\n",
      "Epoch [3/60], Iter [353/633], LR: 0.005000, Loss: 3.6156, top1: 14.0625\n",
      "Epoch [3/60], Iter [354/633], LR: 0.005000, Loss: 3.6570, top1: 7.8125\n",
      "Epoch [3/60], Iter [355/633], LR: 0.005000, Loss: 3.6439, top1: 3.1250\n",
      "Epoch [3/60], Iter [356/633], LR: 0.005000, Loss: 3.6488, top1: 10.9375\n",
      "Epoch [3/60], Iter [357/633], LR: 0.005000, Loss: 3.6615, top1: 9.3750\n",
      "Epoch [3/60], Iter [358/633], LR: 0.005000, Loss: 3.6271, top1: 9.3750\n",
      "Epoch [3/60], Iter [359/633], LR: 0.005000, Loss: 3.6339, top1: 7.8125\n",
      "Epoch [3/60], Iter [360/633], LR: 0.005000, Loss: 3.6407, top1: 4.6875\n",
      "Epoch [3/60], Iter [361/633], LR: 0.005000, Loss: 3.6507, top1: 3.1250\n",
      "Epoch [3/60], Iter [362/633], LR: 0.005000, Loss: 3.6048, top1: 15.6250\n",
      "Epoch [3/60], Iter [363/633], LR: 0.005000, Loss: 3.6131, top1: 12.5000\n",
      "Epoch [3/60], Iter [364/633], LR: 0.005000, Loss: 3.6464, top1: 10.9375\n",
      "Epoch [3/60], Iter [365/633], LR: 0.005000, Loss: 3.6463, top1: 9.3750\n",
      "Epoch [3/60], Iter [366/633], LR: 0.005000, Loss: 3.6262, top1: 4.6875\n",
      "Epoch [3/60], Iter [367/633], LR: 0.005000, Loss: 3.6925, top1: 3.1250\n",
      "Epoch [3/60], Iter [368/633], LR: 0.005000, Loss: 3.6388, top1: 6.2500\n",
      "Epoch [3/60], Iter [369/633], LR: 0.005000, Loss: 3.6348, top1: 7.8125\n",
      "Epoch [3/60], Iter [370/633], LR: 0.005000, Loss: 3.6288, top1: 10.9375\n",
      "Epoch [3/60], Iter [371/633], LR: 0.005000, Loss: 3.6474, top1: 10.9375\n",
      "Epoch [3/60], Iter [372/633], LR: 0.005000, Loss: 3.6333, top1: 9.3750\n",
      "Epoch [3/60], Iter [373/633], LR: 0.005000, Loss: 3.5937, top1: 14.0625\n",
      "Epoch [3/60], Iter [374/633], LR: 0.005000, Loss: 3.5881, top1: 14.0625\n",
      "Epoch [3/60], Iter [375/633], LR: 0.005000, Loss: 3.6299, top1: 9.3750\n",
      "Epoch [3/60], Iter [376/633], LR: 0.005000, Loss: 3.6754, top1: 3.1250\n",
      "Epoch [3/60], Iter [377/633], LR: 0.005000, Loss: 3.6391, top1: 10.9375\n",
      "Epoch [3/60], Iter [378/633], LR: 0.005000, Loss: 3.6049, top1: 14.0625\n",
      "Epoch [3/60], Iter [379/633], LR: 0.005000, Loss: 3.6445, top1: 9.3750\n",
      "Epoch [3/60], Iter [380/633], LR: 0.005000, Loss: 3.6074, top1: 7.8125\n",
      "Epoch [3/60], Iter [381/633], LR: 0.005000, Loss: 3.6482, top1: 12.5000\n",
      "Epoch [3/60], Iter [382/633], LR: 0.005000, Loss: 3.6244, top1: 14.0625\n",
      "Epoch [3/60], Iter [383/633], LR: 0.005000, Loss: 3.6273, top1: 4.6875\n",
      "Epoch [3/60], Iter [384/633], LR: 0.005000, Loss: 3.5804, top1: 14.0625\n",
      "Epoch [3/60], Iter [385/633], LR: 0.005000, Loss: 3.5982, top1: 15.6250\n",
      "Epoch [3/60], Iter [386/633], LR: 0.005000, Loss: 3.6601, top1: 7.8125\n",
      "Epoch [3/60], Iter [387/633], LR: 0.005000, Loss: 3.6357, top1: 14.0625\n",
      "Epoch [3/60], Iter [388/633], LR: 0.005000, Loss: 3.6387, top1: 9.3750\n",
      "Epoch [3/60], Iter [389/633], LR: 0.005000, Loss: 3.6385, top1: 7.8125\n",
      "Epoch [3/60], Iter [390/633], LR: 0.005000, Loss: 3.6350, top1: 10.9375\n",
      "Epoch [3/60], Iter [391/633], LR: 0.005000, Loss: 3.6550, top1: 4.6875\n",
      "Epoch [3/60], Iter [392/633], LR: 0.005000, Loss: 3.6572, top1: 10.9375\n",
      "Epoch [3/60], Iter [393/633], LR: 0.005000, Loss: 3.6821, top1: 3.1250\n",
      "Epoch [3/60], Iter [394/633], LR: 0.005000, Loss: 3.5912, top1: 17.1875\n",
      "Epoch [3/60], Iter [395/633], LR: 0.005000, Loss: 3.6848, top1: 3.1250\n",
      "Epoch [3/60], Iter [396/633], LR: 0.005000, Loss: 3.6468, top1: 7.8125\n",
      "Epoch [3/60], Iter [397/633], LR: 0.005000, Loss: 3.6173, top1: 10.9375\n",
      "Epoch [3/60], Iter [398/633], LR: 0.005000, Loss: 3.6271, top1: 9.3750\n",
      "Epoch [3/60], Iter [399/633], LR: 0.005000, Loss: 3.6779, top1: 3.1250\n",
      "Epoch [3/60], Iter [400/633], LR: 0.005000, Loss: 3.6548, top1: 7.8125\n",
      "Epoch [3/60], Iter [401/633], LR: 0.005000, Loss: 3.6491, top1: 9.3750\n",
      "Epoch [3/60], Iter [402/633], LR: 0.005000, Loss: 3.6477, top1: 1.5625\n",
      "Epoch [3/60], Iter [403/633], LR: 0.005000, Loss: 3.6622, top1: 9.3750\n",
      "Epoch [3/60], Iter [404/633], LR: 0.005000, Loss: 3.6272, top1: 14.0625\n",
      "Epoch [3/60], Iter [405/633], LR: 0.005000, Loss: 3.6500, top1: 9.3750\n",
      "Epoch [3/60], Iter [406/633], LR: 0.005000, Loss: 3.6453, top1: 6.2500\n",
      "Epoch [3/60], Iter [407/633], LR: 0.005000, Loss: 3.6765, top1: 7.8125\n",
      "Epoch [3/60], Iter [408/633], LR: 0.005000, Loss: 3.6410, top1: 7.8125\n",
      "Epoch [3/60], Iter [409/633], LR: 0.005000, Loss: 3.6670, top1: 3.1250\n",
      "Epoch [3/60], Iter [410/633], LR: 0.005000, Loss: 3.6384, top1: 6.2500\n",
      "Epoch [3/60], Iter [411/633], LR: 0.005000, Loss: 3.6537, top1: 1.5625\n",
      "Epoch [3/60], Iter [412/633], LR: 0.005000, Loss: 3.6511, top1: 4.6875\n",
      "Epoch [3/60], Iter [413/633], LR: 0.005000, Loss: 3.6276, top1: 6.2500\n",
      "Epoch [3/60], Iter [414/633], LR: 0.005000, Loss: 3.6649, top1: 7.8125\n",
      "Epoch [3/60], Iter [415/633], LR: 0.005000, Loss: 3.6369, top1: 7.8125\n",
      "Epoch [3/60], Iter [416/633], LR: 0.005000, Loss: 3.6561, top1: 6.2500\n",
      "Epoch [3/60], Iter [417/633], LR: 0.005000, Loss: 3.6294, top1: 12.5000\n",
      "Epoch [3/60], Iter [418/633], LR: 0.005000, Loss: 3.6737, top1: 4.6875\n",
      "Epoch [3/60], Iter [419/633], LR: 0.005000, Loss: 3.6561, top1: 9.3750\n",
      "Epoch [3/60], Iter [420/633], LR: 0.005000, Loss: 3.6367, top1: 9.3750\n",
      "Epoch [3/60], Iter [421/633], LR: 0.005000, Loss: 3.6061, top1: 9.3750\n",
      "Epoch [3/60], Iter [422/633], LR: 0.005000, Loss: 3.6184, top1: 10.9375\n",
      "Epoch [3/60], Iter [423/633], LR: 0.005000, Loss: 3.6425, top1: 9.3750\n",
      "Epoch [3/60], Iter [424/633], LR: 0.005000, Loss: 3.5903, top1: 17.1875\n",
      "Epoch [3/60], Iter [425/633], LR: 0.005000, Loss: 3.6531, top1: 6.2500\n",
      "Epoch [3/60], Iter [426/633], LR: 0.005000, Loss: 3.6079, top1: 10.9375\n",
      "Epoch [3/60], Iter [427/633], LR: 0.005000, Loss: 3.6260, top1: 12.5000\n",
      "Epoch [3/60], Iter [428/633], LR: 0.005000, Loss: 3.6631, top1: 9.3750\n",
      "Epoch [3/60], Iter [429/633], LR: 0.005000, Loss: 3.6300, top1: 9.3750\n",
      "Epoch [3/60], Iter [430/633], LR: 0.005000, Loss: 3.6365, top1: 9.3750\n",
      "Epoch [3/60], Iter [431/633], LR: 0.005000, Loss: 3.6354, top1: 7.8125\n",
      "Epoch [3/60], Iter [432/633], LR: 0.005000, Loss: 3.6162, top1: 10.9375\n",
      "Epoch [3/60], Iter [433/633], LR: 0.005000, Loss: 3.6161, top1: 10.9375\n",
      "Epoch [3/60], Iter [434/633], LR: 0.005000, Loss: 3.6095, top1: 12.5000\n",
      "Epoch [3/60], Iter [435/633], LR: 0.005000, Loss: 3.6568, top1: 7.8125\n",
      "Epoch [3/60], Iter [436/633], LR: 0.005000, Loss: 3.6145, top1: 9.3750\n",
      "Epoch [3/60], Iter [437/633], LR: 0.005000, Loss: 3.6199, top1: 9.3750\n",
      "Epoch [3/60], Iter [438/633], LR: 0.005000, Loss: 3.6579, top1: 7.8125\n",
      "Epoch [3/60], Iter [439/633], LR: 0.005000, Loss: 3.6484, top1: 7.8125\n",
      "Epoch [3/60], Iter [440/633], LR: 0.005000, Loss: 3.6600, top1: 6.2500\n",
      "Epoch [3/60], Iter [441/633], LR: 0.005000, Loss: 3.6223, top1: 10.9375\n",
      "Epoch [3/60], Iter [442/633], LR: 0.005000, Loss: 3.6159, top1: 9.3750\n",
      "Epoch [3/60], Iter [443/633], LR: 0.005000, Loss: 3.6629, top1: 6.2500\n",
      "Epoch [3/60], Iter [444/633], LR: 0.005000, Loss: 3.6286, top1: 6.2500\n",
      "Epoch [3/60], Iter [445/633], LR: 0.005000, Loss: 3.6286, top1: 12.5000\n",
      "Epoch [3/60], Iter [446/633], LR: 0.005000, Loss: 3.5916, top1: 15.6250\n",
      "Epoch [3/60], Iter [447/633], LR: 0.005000, Loss: 3.6084, top1: 12.5000\n",
      "Epoch [3/60], Iter [448/633], LR: 0.005000, Loss: 3.6741, top1: 0.0000\n",
      "Epoch [3/60], Iter [449/633], LR: 0.005000, Loss: 3.6361, top1: 6.2500\n",
      "Epoch [3/60], Iter [450/633], LR: 0.005000, Loss: 3.6581, top1: 7.8125\n",
      "Epoch [3/60], Iter [451/633], LR: 0.005000, Loss: 3.6491, top1: 14.0625\n",
      "Epoch [3/60], Iter [452/633], LR: 0.005000, Loss: 3.5587, top1: 15.6250\n",
      "Epoch [3/60], Iter [453/633], LR: 0.005000, Loss: 3.6597, top1: 4.6875\n",
      "Epoch [3/60], Iter [454/633], LR: 0.005000, Loss: 3.6362, top1: 14.0625\n",
      "Epoch [3/60], Iter [455/633], LR: 0.005000, Loss: 3.6489, top1: 9.3750\n",
      "Epoch [3/60], Iter [456/633], LR: 0.005000, Loss: 3.6731, top1: 4.6875\n",
      "Epoch [3/60], Iter [457/633], LR: 0.005000, Loss: 3.5950, top1: 20.3125\n",
      "Epoch [3/60], Iter [458/633], LR: 0.005000, Loss: 3.6329, top1: 7.8125\n",
      "Epoch [3/60], Iter [459/633], LR: 0.005000, Loss: 3.6172, top1: 12.5000\n",
      "Epoch [3/60], Iter [460/633], LR: 0.005000, Loss: 3.6572, top1: 7.8125\n",
      "Epoch [3/60], Iter [461/633], LR: 0.005000, Loss: 3.6491, top1: 9.3750\n",
      "Epoch [3/60], Iter [462/633], LR: 0.005000, Loss: 3.6198, top1: 14.0625\n",
      "Epoch [3/60], Iter [463/633], LR: 0.005000, Loss: 3.6074, top1: 14.0625\n",
      "Epoch [3/60], Iter [464/633], LR: 0.005000, Loss: 3.6405, top1: 4.6875\n",
      "Epoch [3/60], Iter [465/633], LR: 0.005000, Loss: 3.6072, top1: 9.3750\n",
      "Epoch [3/60], Iter [466/633], LR: 0.005000, Loss: 3.6523, top1: 9.3750\n",
      "Epoch [3/60], Iter [467/633], LR: 0.005000, Loss: 3.6359, top1: 12.5000\n",
      "Epoch [3/60], Iter [468/633], LR: 0.005000, Loss: 3.6753, top1: 1.5625\n",
      "Epoch [3/60], Iter [469/633], LR: 0.005000, Loss: 3.6293, top1: 10.9375\n",
      "Epoch [3/60], Iter [470/633], LR: 0.005000, Loss: 3.6303, top1: 10.9375\n",
      "Epoch [3/60], Iter [471/633], LR: 0.005000, Loss: 3.6259, top1: 9.3750\n",
      "Epoch [3/60], Iter [472/633], LR: 0.005000, Loss: 3.6146, top1: 12.5000\n",
      "Epoch [3/60], Iter [473/633], LR: 0.005000, Loss: 3.6471, top1: 3.1250\n",
      "Epoch [3/60], Iter [474/633], LR: 0.005000, Loss: 3.6337, top1: 7.8125\n",
      "Epoch [3/60], Iter [475/633], LR: 0.005000, Loss: 3.6333, top1: 7.8125\n",
      "Epoch [3/60], Iter [476/633], LR: 0.005000, Loss: 3.6403, top1: 6.2500\n",
      "Epoch [3/60], Iter [477/633], LR: 0.005000, Loss: 3.6483, top1: 9.3750\n",
      "Epoch [3/60], Iter [478/633], LR: 0.005000, Loss: 3.6154, top1: 14.0625\n",
      "Epoch [3/60], Iter [479/633], LR: 0.005000, Loss: 3.6442, top1: 12.5000\n",
      "Epoch [3/60], Iter [480/633], LR: 0.005000, Loss: 3.6529, top1: 9.3750\n",
      "Epoch [3/60], Iter [481/633], LR: 0.005000, Loss: 3.6073, top1: 14.0625\n",
      "Epoch [3/60], Iter [482/633], LR: 0.005000, Loss: 3.6347, top1: 7.8125\n",
      "Epoch [3/60], Iter [483/633], LR: 0.005000, Loss: 3.6548, top1: 10.9375\n",
      "Epoch [3/60], Iter [484/633], LR: 0.005000, Loss: 3.6566, top1: 7.8125\n",
      "Epoch [3/60], Iter [485/633], LR: 0.005000, Loss: 3.6589, top1: 3.1250\n",
      "Epoch [3/60], Iter [486/633], LR: 0.005000, Loss: 3.6528, top1: 9.3750\n",
      "Epoch [3/60], Iter [487/633], LR: 0.005000, Loss: 3.6166, top1: 3.1250\n",
      "Epoch [3/60], Iter [488/633], LR: 0.005000, Loss: 3.6725, top1: 3.1250\n",
      "Epoch [3/60], Iter [489/633], LR: 0.005000, Loss: 3.6913, top1: 4.6875\n",
      "Epoch [3/60], Iter [490/633], LR: 0.005000, Loss: 3.6331, top1: 4.6875\n",
      "Epoch [3/60], Iter [491/633], LR: 0.005000, Loss: 3.6651, top1: 3.1250\n",
      "Epoch [3/60], Iter [492/633], LR: 0.005000, Loss: 3.6148, top1: 9.3750\n",
      "Epoch [3/60], Iter [493/633], LR: 0.005000, Loss: 3.6588, top1: 7.8125\n",
      "Epoch [3/60], Iter [494/633], LR: 0.005000, Loss: 3.6114, top1: 12.5000\n",
      "Epoch [3/60], Iter [495/633], LR: 0.005000, Loss: 3.6518, top1: 4.6875\n",
      "Epoch [3/60], Iter [496/633], LR: 0.005000, Loss: 3.5967, top1: 12.5000\n",
      "Epoch [3/60], Iter [497/633], LR: 0.005000, Loss: 3.6283, top1: 14.0625\n",
      "Epoch [3/60], Iter [498/633], LR: 0.005000, Loss: 3.6554, top1: 4.6875\n",
      "Epoch [3/60], Iter [499/633], LR: 0.005000, Loss: 3.5942, top1: 10.9375\n",
      "Epoch [3/60], Iter [500/633], LR: 0.005000, Loss: 3.6544, top1: 7.8125\n",
      "Epoch [3/60], Iter [501/633], LR: 0.005000, Loss: 3.6375, top1: 6.2500\n",
      "Epoch [3/60], Iter [502/633], LR: 0.005000, Loss: 3.6416, top1: 7.8125\n",
      "Epoch [3/60], Iter [503/633], LR: 0.005000, Loss: 3.6277, top1: 12.5000\n",
      "Epoch [3/60], Iter [504/633], LR: 0.005000, Loss: 3.6357, top1: 12.5000\n",
      "Epoch [3/60], Iter [505/633], LR: 0.005000, Loss: 3.6242, top1: 9.3750\n",
      "Epoch [3/60], Iter [506/633], LR: 0.005000, Loss: 3.6361, top1: 6.2500\n",
      "Epoch [3/60], Iter [507/633], LR: 0.005000, Loss: 3.6027, top1: 6.2500\n",
      "Epoch [3/60], Iter [508/633], LR: 0.005000, Loss: 3.6635, top1: 9.3750\n",
      "Epoch [3/60], Iter [509/633], LR: 0.005000, Loss: 3.6451, top1: 7.8125\n",
      "Epoch [3/60], Iter [510/633], LR: 0.005000, Loss: 3.6767, top1: 4.6875\n",
      "Epoch [3/60], Iter [511/633], LR: 0.005000, Loss: 3.5952, top1: 10.9375\n",
      "Epoch [3/60], Iter [512/633], LR: 0.005000, Loss: 3.6458, top1: 4.6875\n",
      "Epoch [3/60], Iter [513/633], LR: 0.005000, Loss: 3.6399, top1: 6.2500\n",
      "Epoch [3/60], Iter [514/633], LR: 0.005000, Loss: 3.6442, top1: 7.8125\n",
      "Epoch [3/60], Iter [515/633], LR: 0.005000, Loss: 3.6197, top1: 17.1875\n",
      "Epoch [3/60], Iter [516/633], LR: 0.005000, Loss: 3.6307, top1: 14.0625\n",
      "Epoch [3/60], Iter [517/633], LR: 0.005000, Loss: 3.6640, top1: 6.2500\n",
      "Epoch [3/60], Iter [518/633], LR: 0.005000, Loss: 3.6442, top1: 7.8125\n",
      "Epoch [3/60], Iter [519/633], LR: 0.005000, Loss: 3.6748, top1: 4.6875\n",
      "Epoch [3/60], Iter [520/633], LR: 0.005000, Loss: 3.6271, top1: 6.2500\n",
      "Epoch [3/60], Iter [521/633], LR: 0.005000, Loss: 3.6270, top1: 7.8125\n",
      "Epoch [3/60], Iter [522/633], LR: 0.005000, Loss: 3.6604, top1: 6.2500\n",
      "Epoch [3/60], Iter [523/633], LR: 0.005000, Loss: 3.6414, top1: 9.3750\n",
      "Epoch [3/60], Iter [524/633], LR: 0.005000, Loss: 3.6780, top1: 10.9375\n",
      "Epoch [3/60], Iter [525/633], LR: 0.005000, Loss: 3.6515, top1: 12.5000\n",
      "Epoch [3/60], Iter [526/633], LR: 0.005000, Loss: 3.6277, top1: 14.0625\n",
      "Epoch [3/60], Iter [527/633], LR: 0.005000, Loss: 3.6706, top1: 6.2500\n",
      "Epoch [3/60], Iter [528/633], LR: 0.005000, Loss: 3.6470, top1: 9.3750\n",
      "Epoch [3/60], Iter [529/633], LR: 0.005000, Loss: 3.6050, top1: 17.1875\n",
      "Epoch [3/60], Iter [530/633], LR: 0.005000, Loss: 3.5991, top1: 14.0625\n",
      "Epoch [3/60], Iter [531/633], LR: 0.005000, Loss: 3.6427, top1: 12.5000\n",
      "Epoch [3/60], Iter [532/633], LR: 0.005000, Loss: 3.6260, top1: 7.8125\n",
      "Epoch [3/60], Iter [533/633], LR: 0.005000, Loss: 3.6660, top1: 4.6875\n",
      "Epoch [3/60], Iter [534/633], LR: 0.005000, Loss: 3.6147, top1: 10.9375\n",
      "Epoch [3/60], Iter [535/633], LR: 0.005000, Loss: 3.6610, top1: 9.3750\n",
      "Epoch [3/60], Iter [536/633], LR: 0.005000, Loss: 3.6384, top1: 3.1250\n",
      "Epoch [3/60], Iter [537/633], LR: 0.005000, Loss: 3.6452, top1: 7.8125\n",
      "Epoch [3/60], Iter [538/633], LR: 0.005000, Loss: 3.6449, top1: 7.8125\n",
      "Epoch [3/60], Iter [539/633], LR: 0.005000, Loss: 3.6402, top1: 18.7500\n",
      "Epoch [3/60], Iter [540/633], LR: 0.005000, Loss: 3.6364, top1: 7.8125\n",
      "Epoch [3/60], Iter [541/633], LR: 0.005000, Loss: 3.6145, top1: 10.9375\n",
      "Epoch [3/60], Iter [542/633], LR: 0.005000, Loss: 3.6376, top1: 6.2500\n",
      "Epoch [3/60], Iter [543/633], LR: 0.005000, Loss: 3.6506, top1: 4.6875\n",
      "Epoch [3/60], Iter [544/633], LR: 0.005000, Loss: 3.6032, top1: 7.8125\n",
      "Epoch [3/60], Iter [545/633], LR: 0.005000, Loss: 3.6886, top1: 3.1250\n",
      "Epoch [3/60], Iter [546/633], LR: 0.005000, Loss: 3.6332, top1: 14.0625\n",
      "Epoch [3/60], Iter [547/633], LR: 0.005000, Loss: 3.6616, top1: 7.8125\n",
      "Epoch [3/60], Iter [548/633], LR: 0.005000, Loss: 3.6457, top1: 7.8125\n",
      "Epoch [3/60], Iter [549/633], LR: 0.005000, Loss: 3.6416, top1: 4.6875\n",
      "Epoch [3/60], Iter [550/633], LR: 0.005000, Loss: 3.6520, top1: 6.2500\n",
      "Epoch [3/60], Iter [551/633], LR: 0.005000, Loss: 3.6034, top1: 10.9375\n",
      "Epoch [3/60], Iter [552/633], LR: 0.005000, Loss: 3.6622, top1: 6.2500\n",
      "Epoch [3/60], Iter [553/633], LR: 0.005000, Loss: 3.6300, top1: 9.3750\n",
      "Epoch [3/60], Iter [554/633], LR: 0.005000, Loss: 3.6497, top1: 7.8125\n",
      "Epoch [3/60], Iter [555/633], LR: 0.005000, Loss: 3.6493, top1: 4.6875\n",
      "Epoch [3/60], Iter [556/633], LR: 0.005000, Loss: 3.6892, top1: 6.2500\n",
      "Epoch [3/60], Iter [557/633], LR: 0.005000, Loss: 3.6914, top1: 1.5625\n",
      "Epoch [3/60], Iter [558/633], LR: 0.005000, Loss: 3.6403, top1: 12.5000\n",
      "Epoch [3/60], Iter [559/633], LR: 0.005000, Loss: 3.6406, top1: 10.9375\n",
      "Epoch [3/60], Iter [560/633], LR: 0.005000, Loss: 3.6294, top1: 6.2500\n",
      "Epoch [3/60], Iter [561/633], LR: 0.005000, Loss: 3.6565, top1: 9.3750\n",
      "Epoch [3/60], Iter [562/633], LR: 0.005000, Loss: 3.6185, top1: 15.6250\n",
      "Epoch [3/60], Iter [563/633], LR: 0.005000, Loss: 3.6171, top1: 9.3750\n",
      "Epoch [3/60], Iter [564/633], LR: 0.005000, Loss: 3.6005, top1: 17.1875\n",
      "Epoch [3/60], Iter [565/633], LR: 0.005000, Loss: 3.6392, top1: 9.3750\n",
      "Epoch [3/60], Iter [566/633], LR: 0.005000, Loss: 3.6396, top1: 9.3750\n",
      "Epoch [3/60], Iter [567/633], LR: 0.005000, Loss: 3.6032, top1: 10.9375\n",
      "Epoch [3/60], Iter [568/633], LR: 0.005000, Loss: 3.6480, top1: 9.3750\n",
      "Epoch [3/60], Iter [569/633], LR: 0.005000, Loss: 3.6445, top1: 9.3750\n",
      "Epoch [3/60], Iter [570/633], LR: 0.005000, Loss: 3.6210, top1: 9.3750\n",
      "Epoch [3/60], Iter [571/633], LR: 0.005000, Loss: 3.6266, top1: 10.9375\n",
      "Epoch [3/60], Iter [572/633], LR: 0.005000, Loss: 3.6313, top1: 9.3750\n",
      "Epoch [3/60], Iter [573/633], LR: 0.005000, Loss: 3.6304, top1: 9.3750\n",
      "Epoch [3/60], Iter [574/633], LR: 0.005000, Loss: 3.6285, top1: 6.2500\n",
      "Epoch [3/60], Iter [575/633], LR: 0.005000, Loss: 3.6337, top1: 12.5000\n",
      "Epoch [3/60], Iter [576/633], LR: 0.005000, Loss: 3.6012, top1: 10.9375\n",
      "Epoch [3/60], Iter [577/633], LR: 0.005000, Loss: 3.6370, top1: 10.9375\n",
      "Epoch [3/60], Iter [578/633], LR: 0.005000, Loss: 3.6710, top1: 7.8125\n",
      "Epoch [3/60], Iter [579/633], LR: 0.005000, Loss: 3.6341, top1: 9.3750\n",
      "Epoch [3/60], Iter [580/633], LR: 0.005000, Loss: 3.6323, top1: 15.6250\n",
      "Epoch [3/60], Iter [581/633], LR: 0.005000, Loss: 3.6597, top1: 9.3750\n",
      "Epoch [3/60], Iter [582/633], LR: 0.005000, Loss: 3.6745, top1: 6.2500\n",
      "Epoch [3/60], Iter [583/633], LR: 0.005000, Loss: 3.6389, top1: 6.2500\n",
      "Epoch [3/60], Iter [584/633], LR: 0.005000, Loss: 3.6309, top1: 7.8125\n",
      "Epoch [3/60], Iter [585/633], LR: 0.005000, Loss: 3.6662, top1: 3.1250\n",
      "Epoch [3/60], Iter [586/633], LR: 0.005000, Loss: 3.6281, top1: 7.8125\n",
      "Epoch [3/60], Iter [587/633], LR: 0.005000, Loss: 3.6066, top1: 12.5000\n",
      "Epoch [3/60], Iter [588/633], LR: 0.005000, Loss: 3.6627, top1: 3.1250\n",
      "Epoch [3/60], Iter [589/633], LR: 0.005000, Loss: 3.6532, top1: 7.8125\n",
      "Epoch [3/60], Iter [590/633], LR: 0.005000, Loss: 3.6608, top1: 10.9375\n",
      "Epoch [3/60], Iter [591/633], LR: 0.005000, Loss: 3.6235, top1: 7.8125\n",
      "Epoch [3/60], Iter [592/633], LR: 0.005000, Loss: 3.6270, top1: 10.9375\n",
      "Epoch [3/60], Iter [593/633], LR: 0.005000, Loss: 3.6538, top1: 7.8125\n",
      "Epoch [3/60], Iter [594/633], LR: 0.005000, Loss: 3.6493, top1: 10.9375\n",
      "Epoch [3/60], Iter [595/633], LR: 0.005000, Loss: 3.6492, top1: 6.2500\n",
      "Epoch [3/60], Iter [596/633], LR: 0.005000, Loss: 3.6540, top1: 7.8125\n",
      "Epoch [3/60], Iter [597/633], LR: 0.005000, Loss: 3.6709, top1: 6.2500\n",
      "Epoch [3/60], Iter [598/633], LR: 0.005000, Loss: 3.6023, top1: 15.6250\n",
      "Epoch [3/60], Iter [599/633], LR: 0.005000, Loss: 3.6265, top1: 10.9375\n",
      "Epoch [3/60], Iter [600/633], LR: 0.005000, Loss: 3.6359, top1: 14.0625\n",
      "Epoch [3/60], Iter [601/633], LR: 0.005000, Loss: 3.5926, top1: 10.9375\n",
      "Epoch [3/60], Iter [602/633], LR: 0.005000, Loss: 3.6578, top1: 6.2500\n",
      "Epoch [3/60], Iter [603/633], LR: 0.005000, Loss: 3.6330, top1: 9.3750\n",
      "Epoch [3/60], Iter [604/633], LR: 0.005000, Loss: 3.6139, top1: 10.9375\n",
      "Epoch [3/60], Iter [605/633], LR: 0.005000, Loss: 3.6649, top1: 4.6875\n",
      "Epoch [3/60], Iter [606/633], LR: 0.005000, Loss: 3.6336, top1: 9.3750\n",
      "Epoch [3/60], Iter [607/633], LR: 0.005000, Loss: 3.6465, top1: 9.3750\n",
      "Epoch [3/60], Iter [608/633], LR: 0.005000, Loss: 3.6097, top1: 9.3750\n",
      "Epoch [3/60], Iter [609/633], LR: 0.005000, Loss: 3.6137, top1: 10.9375\n",
      "Epoch [3/60], Iter [610/633], LR: 0.005000, Loss: 3.6145, top1: 10.9375\n",
      "Epoch [3/60], Iter [611/633], LR: 0.005000, Loss: 3.6228, top1: 9.3750\n",
      "Epoch [3/60], Iter [612/633], LR: 0.005000, Loss: 3.6388, top1: 7.8125\n",
      "Epoch [3/60], Iter [613/633], LR: 0.005000, Loss: 3.6181, top1: 7.8125\n",
      "Epoch [3/60], Iter [614/633], LR: 0.005000, Loss: 3.6423, top1: 7.8125\n",
      "Epoch [3/60], Iter [615/633], LR: 0.005000, Loss: 3.6423, top1: 3.1250\n",
      "Epoch [3/60], Iter [616/633], LR: 0.005000, Loss: 3.6518, top1: 3.1250\n",
      "Epoch [3/60], Iter [617/633], LR: 0.005000, Loss: 3.6190, top1: 10.9375\n",
      "Epoch [3/60], Iter [618/633], LR: 0.005000, Loss: 3.6393, top1: 10.9375\n",
      "Epoch [3/60], Iter [619/633], LR: 0.005000, Loss: 3.6091, top1: 12.5000\n",
      "Epoch [3/60], Iter [620/633], LR: 0.005000, Loss: 3.6758, top1: 7.8125\n",
      "Epoch [3/60], Iter [621/633], LR: 0.005000, Loss: 3.6083, top1: 12.5000\n",
      "Epoch [3/60], Iter [622/633], LR: 0.005000, Loss: 3.6280, top1: 12.5000\n",
      "Epoch [3/60], Iter [623/633], LR: 0.005000, Loss: 3.6361, top1: 9.3750\n",
      "Epoch [3/60], Iter [624/633], LR: 0.005000, Loss: 3.6279, top1: 10.9375\n",
      "Epoch [3/60], Iter [625/633], LR: 0.005000, Loss: 3.6366, top1: 10.9375\n",
      "Epoch [3/60], Iter [626/633], LR: 0.005000, Loss: 3.6463, top1: 14.0625\n",
      "Epoch [3/60], Iter [627/633], LR: 0.005000, Loss: 3.6623, top1: 3.1250\n",
      "Epoch [3/60], Iter [628/633], LR: 0.005000, Loss: 3.6389, top1: 9.3750\n",
      "Epoch [3/60], Iter [629/633], LR: 0.005000, Loss: 3.6065, top1: 15.6250\n",
      "Epoch [3/60], Iter [630/633], LR: 0.005000, Loss: 3.6035, top1: 15.6250\n",
      "Epoch [3/60], Iter [631/633], LR: 0.005000, Loss: 3.6068, top1: 9.3750\n",
      "Epoch [3/60], Iter [632/633], LR: 0.005000, Loss: 3.6664, top1: 4.6875\n",
      "Epoch [3/60], Iter [633/633], LR: 0.005000, Loss: 3.6258, top1: 6.2500\n",
      "Epoch [3/60], Iter [634/633], LR: 0.005000, Loss: 3.6477, top1: 6.4516\n",
      "Epoch [3/60], Val_Loss: 3.6254, Val_top1: 10.2773, best_top1: 7.1523\n",
      "epoch time: 4.459580870469411 min\n",
      "Taking top1 snapshot...\n",
      "Epoch [4/60], Iter [1/633], LR: 0.005000, Loss: 3.6251, top1: 7.8125\n",
      "Epoch [4/60], Iter [2/633], LR: 0.005000, Loss: 3.6521, top1: 10.9375\n",
      "Epoch [4/60], Iter [3/633], LR: 0.005000, Loss: 3.6615, top1: 7.8125\n",
      "Epoch [4/60], Iter [4/633], LR: 0.005000, Loss: 3.6654, top1: 4.6875\n",
      "Epoch [4/60], Iter [5/633], LR: 0.005000, Loss: 3.6173, top1: 14.0625\n",
      "Epoch [4/60], Iter [6/633], LR: 0.005000, Loss: 3.6305, top1: 10.9375\n",
      "Epoch [4/60], Iter [7/633], LR: 0.005000, Loss: 3.6399, top1: 6.2500\n",
      "Epoch [4/60], Iter [8/633], LR: 0.005000, Loss: 3.6601, top1: 12.5000\n",
      "Epoch [4/60], Iter [9/633], LR: 0.005000, Loss: 3.6416, top1: 10.9375\n",
      "Epoch [4/60], Iter [10/633], LR: 0.005000, Loss: 3.6248, top1: 6.2500\n",
      "Epoch [4/60], Iter [11/633], LR: 0.005000, Loss: 3.6278, top1: 14.0625\n",
      "Epoch [4/60], Iter [12/633], LR: 0.005000, Loss: 3.6394, top1: 7.8125\n",
      "Epoch [4/60], Iter [13/633], LR: 0.005000, Loss: 3.6202, top1: 15.6250\n",
      "Epoch [4/60], Iter [14/633], LR: 0.005000, Loss: 3.6407, top1: 10.9375\n",
      "Epoch [4/60], Iter [15/633], LR: 0.005000, Loss: 3.6498, top1: 7.8125\n",
      "Epoch [4/60], Iter [16/633], LR: 0.005000, Loss: 3.6344, top1: 7.8125\n",
      "Epoch [4/60], Iter [17/633], LR: 0.005000, Loss: 3.6260, top1: 12.5000\n",
      "Epoch [4/60], Iter [18/633], LR: 0.005000, Loss: 3.6576, top1: 6.2500\n",
      "Epoch [4/60], Iter [19/633], LR: 0.005000, Loss: 3.6548, top1: 7.8125\n",
      "Epoch [4/60], Iter [20/633], LR: 0.005000, Loss: 3.6492, top1: 4.6875\n",
      "Epoch [4/60], Iter [21/633], LR: 0.005000, Loss: 3.6609, top1: 6.2500\n",
      "Epoch [4/60], Iter [22/633], LR: 0.005000, Loss: 3.6540, top1: 9.3750\n",
      "Epoch [4/60], Iter [23/633], LR: 0.005000, Loss: 3.5922, top1: 17.1875\n",
      "Epoch [4/60], Iter [24/633], LR: 0.005000, Loss: 3.6689, top1: 4.6875\n",
      "Epoch [4/60], Iter [25/633], LR: 0.005000, Loss: 3.6773, top1: 4.6875\n",
      "Epoch [4/60], Iter [26/633], LR: 0.005000, Loss: 3.6592, top1: 9.3750\n",
      "Epoch [4/60], Iter [27/633], LR: 0.005000, Loss: 3.6435, top1: 10.9375\n",
      "Epoch [4/60], Iter [28/633], LR: 0.005000, Loss: 3.6553, top1: 6.2500\n",
      "Epoch [4/60], Iter [29/633], LR: 0.005000, Loss: 3.6634, top1: 3.1250\n",
      "Epoch [4/60], Iter [30/633], LR: 0.005000, Loss: 3.6498, top1: 6.2500\n",
      "Epoch [4/60], Iter [31/633], LR: 0.005000, Loss: 3.6627, top1: 4.6875\n",
      "Epoch [4/60], Iter [32/633], LR: 0.005000, Loss: 3.6322, top1: 12.5000\n",
      "Epoch [4/60], Iter [33/633], LR: 0.005000, Loss: 3.6576, top1: 6.2500\n",
      "Epoch [4/60], Iter [34/633], LR: 0.005000, Loss: 3.6697, top1: 6.2500\n",
      "Epoch [4/60], Iter [35/633], LR: 0.005000, Loss: 3.6519, top1: 7.8125\n",
      "Epoch [4/60], Iter [36/633], LR: 0.005000, Loss: 3.6081, top1: 14.0625\n",
      "Epoch [4/60], Iter [37/633], LR: 0.005000, Loss: 3.6144, top1: 10.9375\n",
      "Epoch [4/60], Iter [38/633], LR: 0.005000, Loss: 3.6383, top1: 9.3750\n",
      "Epoch [4/60], Iter [39/633], LR: 0.005000, Loss: 3.6495, top1: 7.8125\n",
      "Epoch [4/60], Iter [40/633], LR: 0.005000, Loss: 3.6354, top1: 6.2500\n",
      "Epoch [4/60], Iter [41/633], LR: 0.005000, Loss: 3.6361, top1: 9.3750\n",
      "Epoch [4/60], Iter [42/633], LR: 0.005000, Loss: 3.6331, top1: 6.2500\n",
      "Epoch [4/60], Iter [43/633], LR: 0.005000, Loss: 3.6440, top1: 9.3750\n",
      "Epoch [4/60], Iter [44/633], LR: 0.005000, Loss: 3.5938, top1: 9.3750\n",
      "Epoch [4/60], Iter [45/633], LR: 0.005000, Loss: 3.6462, top1: 4.6875\n",
      "Epoch [4/60], Iter [46/633], LR: 0.005000, Loss: 3.6463, top1: 4.6875\n",
      "Epoch [4/60], Iter [47/633], LR: 0.005000, Loss: 3.6324, top1: 17.1875\n",
      "Epoch [4/60], Iter [48/633], LR: 0.005000, Loss: 3.6190, top1: 14.0625\n",
      "Epoch [4/60], Iter [49/633], LR: 0.005000, Loss: 3.6442, top1: 9.3750\n",
      "Epoch [4/60], Iter [50/633], LR: 0.005000, Loss: 3.6330, top1: 7.8125\n",
      "Epoch [4/60], Iter [51/633], LR: 0.005000, Loss: 3.6202, top1: 4.6875\n",
      "Epoch [4/60], Iter [52/633], LR: 0.005000, Loss: 3.6287, top1: 7.8125\n",
      "Epoch [4/60], Iter [53/633], LR: 0.005000, Loss: 3.6348, top1: 7.8125\n",
      "Epoch [4/60], Iter [54/633], LR: 0.005000, Loss: 3.6302, top1: 6.2500\n",
      "Epoch [4/60], Iter [55/633], LR: 0.005000, Loss: 3.6272, top1: 9.3750\n",
      "Epoch [4/60], Iter [56/633], LR: 0.005000, Loss: 3.6421, top1: 9.3750\n",
      "Epoch [4/60], Iter [57/633], LR: 0.005000, Loss: 3.6166, top1: 17.1875\n",
      "Epoch [4/60], Iter [58/633], LR: 0.005000, Loss: 3.6417, top1: 9.3750\n",
      "Epoch [4/60], Iter [59/633], LR: 0.005000, Loss: 3.6199, top1: 6.2500\n",
      "Epoch [4/60], Iter [60/633], LR: 0.005000, Loss: 3.5984, top1: 10.9375\n",
      "Epoch [4/60], Iter [61/633], LR: 0.005000, Loss: 3.5983, top1: 15.6250\n",
      "Epoch [4/60], Iter [62/633], LR: 0.005000, Loss: 3.6326, top1: 9.3750\n",
      "Epoch [4/60], Iter [63/633], LR: 0.005000, Loss: 3.6491, top1: 6.2500\n",
      "Epoch [4/60], Iter [64/633], LR: 0.005000, Loss: 3.6106, top1: 10.9375\n",
      "Epoch [4/60], Iter [65/633], LR: 0.005000, Loss: 3.5647, top1: 15.6250\n",
      "Epoch [4/60], Iter [66/633], LR: 0.005000, Loss: 3.6613, top1: 6.2500\n",
      "Epoch [4/60], Iter [67/633], LR: 0.005000, Loss: 3.6206, top1: 9.3750\n",
      "Epoch [4/60], Iter [68/633], LR: 0.005000, Loss: 3.6239, top1: 10.9375\n",
      "Epoch [4/60], Iter [69/633], LR: 0.005000, Loss: 3.6716, top1: 6.2500\n",
      "Epoch [4/60], Iter [70/633], LR: 0.005000, Loss: 3.6428, top1: 9.3750\n",
      "Epoch [4/60], Iter [71/633], LR: 0.005000, Loss: 3.6435, top1: 3.1250\n",
      "Epoch [4/60], Iter [72/633], LR: 0.005000, Loss: 3.6385, top1: 10.9375\n",
      "Epoch [4/60], Iter [73/633], LR: 0.005000, Loss: 3.6384, top1: 10.9375\n",
      "Epoch [4/60], Iter [74/633], LR: 0.005000, Loss: 3.6140, top1: 9.3750\n",
      "Epoch [4/60], Iter [75/633], LR: 0.005000, Loss: 3.6139, top1: 15.6250\n",
      "Epoch [4/60], Iter [76/633], LR: 0.005000, Loss: 3.6523, top1: 7.8125\n",
      "Epoch [4/60], Iter [77/633], LR: 0.005000, Loss: 3.6655, top1: 7.8125\n",
      "Epoch [4/60], Iter [78/633], LR: 0.005000, Loss: 3.6369, top1: 9.3750\n",
      "Epoch [4/60], Iter [79/633], LR: 0.005000, Loss: 3.6503, top1: 7.8125\n",
      "Epoch [4/60], Iter [80/633], LR: 0.005000, Loss: 3.6483, top1: 12.5000\n",
      "Epoch [4/60], Iter [81/633], LR: 0.005000, Loss: 3.6654, top1: 6.2500\n",
      "Epoch [4/60], Iter [82/633], LR: 0.005000, Loss: 3.6515, top1: 7.8125\n",
      "Epoch [4/60], Iter [83/633], LR: 0.005000, Loss: 3.6447, top1: 10.9375\n",
      "Epoch [4/60], Iter [84/633], LR: 0.005000, Loss: 3.5930, top1: 12.5000\n",
      "Epoch [4/60], Iter [85/633], LR: 0.005000, Loss: 3.6343, top1: 10.9375\n",
      "Epoch [4/60], Iter [86/633], LR: 0.005000, Loss: 3.6105, top1: 7.8125\n",
      "Epoch [4/60], Iter [87/633], LR: 0.005000, Loss: 3.6256, top1: 10.9375\n",
      "Epoch [4/60], Iter [88/633], LR: 0.005000, Loss: 3.6373, top1: 9.3750\n",
      "Epoch [4/60], Iter [89/633], LR: 0.005000, Loss: 3.6623, top1: 3.1250\n",
      "Epoch [4/60], Iter [90/633], LR: 0.005000, Loss: 3.6380, top1: 6.2500\n",
      "Epoch [4/60], Iter [91/633], LR: 0.005000, Loss: 3.6247, top1: 14.0625\n",
      "Epoch [4/60], Iter [92/633], LR: 0.005000, Loss: 3.6377, top1: 12.5000\n",
      "Epoch [4/60], Iter [93/633], LR: 0.005000, Loss: 3.6544, top1: 6.2500\n",
      "Epoch [4/60], Iter [94/633], LR: 0.005000, Loss: 3.6463, top1: 4.6875\n",
      "Epoch [4/60], Iter [95/633], LR: 0.005000, Loss: 3.6422, top1: 12.5000\n",
      "Epoch [4/60], Iter [96/633], LR: 0.005000, Loss: 3.6404, top1: 12.5000\n",
      "Epoch [4/60], Iter [97/633], LR: 0.005000, Loss: 3.6427, top1: 6.2500\n",
      "Epoch [4/60], Iter [98/633], LR: 0.005000, Loss: 3.6115, top1: 10.9375\n",
      "Epoch [4/60], Iter [99/633], LR: 0.005000, Loss: 3.6524, top1: 4.6875\n",
      "Epoch [4/60], Iter [100/633], LR: 0.005000, Loss: 3.6328, top1: 6.2500\n",
      "Epoch [4/60], Iter [101/633], LR: 0.005000, Loss: 3.6362, top1: 9.3750\n",
      "Epoch [4/60], Iter [102/633], LR: 0.005000, Loss: 3.6288, top1: 6.2500\n",
      "Epoch [4/60], Iter [103/633], LR: 0.005000, Loss: 3.6068, top1: 14.0625\n",
      "Epoch [4/60], Iter [104/633], LR: 0.005000, Loss: 3.6286, top1: 7.8125\n",
      "Epoch [4/60], Iter [105/633], LR: 0.005000, Loss: 3.6323, top1: 7.8125\n",
      "Epoch [4/60], Iter [106/633], LR: 0.005000, Loss: 3.6639, top1: 3.1250\n",
      "Epoch [4/60], Iter [107/633], LR: 0.005000, Loss: 3.6203, top1: 9.3750\n",
      "Epoch [4/60], Iter [108/633], LR: 0.005000, Loss: 3.6396, top1: 9.3750\n",
      "Epoch [4/60], Iter [109/633], LR: 0.005000, Loss: 3.6070, top1: 15.6250\n",
      "Epoch [4/60], Iter [110/633], LR: 0.005000, Loss: 3.5870, top1: 9.3750\n",
      "Epoch [4/60], Iter [111/633], LR: 0.005000, Loss: 3.6573, top1: 6.2500\n",
      "Epoch [4/60], Iter [112/633], LR: 0.005000, Loss: 3.6184, top1: 10.9375\n",
      "Epoch [4/60], Iter [113/633], LR: 0.005000, Loss: 3.6461, top1: 9.3750\n",
      "Epoch [4/60], Iter [114/633], LR: 0.005000, Loss: 3.6053, top1: 10.9375\n",
      "Epoch [4/60], Iter [115/633], LR: 0.005000, Loss: 3.6146, top1: 12.5000\n",
      "Epoch [4/60], Iter [116/633], LR: 0.005000, Loss: 3.6087, top1: 7.8125\n",
      "Epoch [4/60], Iter [117/633], LR: 0.005000, Loss: 3.6194, top1: 9.3750\n",
      "Epoch [4/60], Iter [118/633], LR: 0.005000, Loss: 3.6106, top1: 9.3750\n",
      "Epoch [4/60], Iter [119/633], LR: 0.005000, Loss: 3.6269, top1: 12.5000\n",
      "Epoch [4/60], Iter [120/633], LR: 0.005000, Loss: 3.6336, top1: 10.9375\n",
      "Epoch [4/60], Iter [121/633], LR: 0.005000, Loss: 3.6491, top1: 7.8125\n",
      "Epoch [4/60], Iter [122/633], LR: 0.005000, Loss: 3.6504, top1: 4.6875\n",
      "Epoch [4/60], Iter [123/633], LR: 0.005000, Loss: 3.6424, top1: 9.3750\n",
      "Epoch [4/60], Iter [124/633], LR: 0.005000, Loss: 3.6428, top1: 1.5625\n",
      "Epoch [4/60], Iter [125/633], LR: 0.005000, Loss: 3.6009, top1: 17.1875\n",
      "Epoch [4/60], Iter [126/633], LR: 0.005000, Loss: 3.6473, top1: 6.2500\n",
      "Epoch [4/60], Iter [127/633], LR: 0.005000, Loss: 3.6490, top1: 12.5000\n",
      "Epoch [4/60], Iter [128/633], LR: 0.005000, Loss: 3.5729, top1: 15.6250\n",
      "Epoch [4/60], Iter [129/633], LR: 0.005000, Loss: 3.6164, top1: 10.9375\n",
      "Epoch [4/60], Iter [130/633], LR: 0.005000, Loss: 3.6301, top1: 7.8125\n",
      "Epoch [4/60], Iter [131/633], LR: 0.005000, Loss: 3.6067, top1: 12.5000\n",
      "Epoch [4/60], Iter [132/633], LR: 0.005000, Loss: 3.6184, top1: 14.0625\n",
      "Epoch [4/60], Iter [133/633], LR: 0.005000, Loss: 3.6553, top1: 4.6875\n",
      "Epoch [4/60], Iter [134/633], LR: 0.005000, Loss: 3.6480, top1: 7.8125\n",
      "Epoch [4/60], Iter [135/633], LR: 0.005000, Loss: 3.6522, top1: 9.3750\n",
      "Epoch [4/60], Iter [136/633], LR: 0.005000, Loss: 3.6503, top1: 6.2500\n",
      "Epoch [4/60], Iter [137/633], LR: 0.005000, Loss: 3.6699, top1: 9.3750\n",
      "Epoch [4/60], Iter [138/633], LR: 0.005000, Loss: 3.6336, top1: 10.9375\n",
      "Epoch [4/60], Iter [139/633], LR: 0.005000, Loss: 3.6325, top1: 4.6875\n",
      "Epoch [4/60], Iter [140/633], LR: 0.005000, Loss: 3.6465, top1: 9.3750\n",
      "Epoch [4/60], Iter [141/633], LR: 0.005000, Loss: 3.6284, top1: 7.8125\n",
      "Epoch [4/60], Iter [142/633], LR: 0.005000, Loss: 3.5900, top1: 10.9375\n",
      "Epoch [4/60], Iter [143/633], LR: 0.005000, Loss: 3.6401, top1: 10.9375\n",
      "Epoch [4/60], Iter [144/633], LR: 0.005000, Loss: 3.6546, top1: 6.2500\n",
      "Epoch [4/60], Iter [145/633], LR: 0.005000, Loss: 3.6001, top1: 14.0625\n",
      "Epoch [4/60], Iter [146/633], LR: 0.005000, Loss: 3.6314, top1: 6.2500\n",
      "Epoch [4/60], Iter [147/633], LR: 0.005000, Loss: 3.5879, top1: 21.8750\n",
      "Epoch [4/60], Iter [148/633], LR: 0.005000, Loss: 3.6250, top1: 12.5000\n",
      "Epoch [4/60], Iter [149/633], LR: 0.005000, Loss: 3.6401, top1: 4.6875\n",
      "Epoch [4/60], Iter [150/633], LR: 0.005000, Loss: 3.6144, top1: 12.5000\n",
      "Epoch [4/60], Iter [151/633], LR: 0.005000, Loss: 3.6482, top1: 10.9375\n",
      "Epoch [4/60], Iter [152/633], LR: 0.005000, Loss: 3.6077, top1: 15.6250\n",
      "Epoch [4/60], Iter [153/633], LR: 0.005000, Loss: 3.6125, top1: 10.9375\n",
      "Epoch [4/60], Iter [154/633], LR: 0.005000, Loss: 3.6300, top1: 3.1250\n",
      "Epoch [4/60], Iter [155/633], LR: 0.005000, Loss: 3.6591, top1: 4.6875\n",
      "Epoch [4/60], Iter [156/633], LR: 0.005000, Loss: 3.6104, top1: 7.8125\n",
      "Epoch [4/60], Iter [157/633], LR: 0.005000, Loss: 3.5984, top1: 12.5000\n",
      "Epoch [4/60], Iter [158/633], LR: 0.005000, Loss: 3.6109, top1: 14.0625\n",
      "Epoch [4/60], Iter [159/633], LR: 0.005000, Loss: 3.6143, top1: 9.3750\n",
      "Epoch [4/60], Iter [160/633], LR: 0.005000, Loss: 3.6122, top1: 12.5000\n",
      "Epoch [4/60], Iter [161/633], LR: 0.005000, Loss: 3.5936, top1: 17.1875\n",
      "Epoch [4/60], Iter [162/633], LR: 0.005000, Loss: 3.6057, top1: 10.9375\n",
      "Epoch [4/60], Iter [163/633], LR: 0.005000, Loss: 3.6074, top1: 4.6875\n",
      "Epoch [4/60], Iter [164/633], LR: 0.005000, Loss: 3.6306, top1: 6.2500\n",
      "Epoch [4/60], Iter [165/633], LR: 0.005000, Loss: 3.6528, top1: 6.2500\n",
      "Epoch [4/60], Iter [166/633], LR: 0.005000, Loss: 3.6273, top1: 9.3750\n",
      "Epoch [4/60], Iter [167/633], LR: 0.005000, Loss: 3.6326, top1: 7.8125\n",
      "Epoch [4/60], Iter [168/633], LR: 0.005000, Loss: 3.6269, top1: 10.9375\n",
      "Epoch [4/60], Iter [169/633], LR: 0.005000, Loss: 3.6232, top1: 4.6875\n",
      "Epoch [4/60], Iter [170/633], LR: 0.005000, Loss: 3.6375, top1: 14.0625\n",
      "Epoch [4/60], Iter [171/633], LR: 0.005000, Loss: 3.6219, top1: 12.5000\n",
      "Epoch [4/60], Iter [172/633], LR: 0.005000, Loss: 3.6571, top1: 6.2500\n",
      "Epoch [4/60], Iter [173/633], LR: 0.005000, Loss: 3.6086, top1: 14.0625\n",
      "Epoch [4/60], Iter [174/633], LR: 0.005000, Loss: 3.6269, top1: 7.8125\n",
      "Epoch [4/60], Iter [175/633], LR: 0.005000, Loss: 3.6360, top1: 9.3750\n",
      "Epoch [4/60], Iter [176/633], LR: 0.005000, Loss: 3.6152, top1: 12.5000\n",
      "Epoch [4/60], Iter [177/633], LR: 0.005000, Loss: 3.5850, top1: 17.1875\n",
      "Epoch [4/60], Iter [178/633], LR: 0.005000, Loss: 3.6254, top1: 6.2500\n",
      "Epoch [4/60], Iter [179/633], LR: 0.005000, Loss: 3.6403, top1: 4.6875\n",
      "Epoch [4/60], Iter [180/633], LR: 0.005000, Loss: 3.6451, top1: 6.2500\n",
      "Epoch [4/60], Iter [181/633], LR: 0.005000, Loss: 3.6865, top1: 3.1250\n",
      "Epoch [4/60], Iter [182/633], LR: 0.005000, Loss: 3.6398, top1: 14.0625\n",
      "Epoch [4/60], Iter [183/633], LR: 0.005000, Loss: 3.5790, top1: 17.1875\n",
      "Epoch [4/60], Iter [184/633], LR: 0.005000, Loss: 3.6366, top1: 7.8125\n",
      "Epoch [4/60], Iter [185/633], LR: 0.005000, Loss: 3.6509, top1: 9.3750\n",
      "Epoch [4/60], Iter [186/633], LR: 0.005000, Loss: 3.6555, top1: 6.2500\n",
      "Epoch [4/60], Iter [187/633], LR: 0.005000, Loss: 3.6422, top1: 9.3750\n",
      "Epoch [4/60], Iter [188/633], LR: 0.005000, Loss: 3.6475, top1: 6.2500\n",
      "Epoch [4/60], Iter [189/633], LR: 0.005000, Loss: 3.6634, top1: 7.8125\n",
      "Epoch [4/60], Iter [190/633], LR: 0.005000, Loss: 3.6625, top1: 6.2500\n",
      "Epoch [4/60], Iter [191/633], LR: 0.005000, Loss: 3.6397, top1: 9.3750\n",
      "Epoch [4/60], Iter [192/633], LR: 0.005000, Loss: 3.6460, top1: 7.8125\n",
      "Epoch [4/60], Iter [193/633], LR: 0.005000, Loss: 3.6278, top1: 9.3750\n",
      "Epoch [4/60], Iter [194/633], LR: 0.005000, Loss: 3.6416, top1: 7.8125\n",
      "Epoch [4/60], Iter [195/633], LR: 0.005000, Loss: 3.6029, top1: 12.5000\n",
      "Epoch [4/60], Iter [196/633], LR: 0.005000, Loss: 3.6188, top1: 12.5000\n",
      "Epoch [4/60], Iter [197/633], LR: 0.005000, Loss: 3.6349, top1: 6.2500\n",
      "Epoch [4/60], Iter [198/633], LR: 0.005000, Loss: 3.6326, top1: 9.3750\n",
      "Epoch [4/60], Iter [199/633], LR: 0.005000, Loss: 3.6202, top1: 7.8125\n",
      "Epoch [4/60], Iter [200/633], LR: 0.005000, Loss: 3.6297, top1: 12.5000\n",
      "Epoch [4/60], Iter [201/633], LR: 0.005000, Loss: 3.6572, top1: 6.2500\n",
      "Epoch [4/60], Iter [202/633], LR: 0.005000, Loss: 3.6095, top1: 14.0625\n",
      "Epoch [4/60], Iter [203/633], LR: 0.005000, Loss: 3.5799, top1: 14.0625\n",
      "Epoch [4/60], Iter [204/633], LR: 0.005000, Loss: 3.6458, top1: 10.9375\n",
      "Epoch [4/60], Iter [205/633], LR: 0.005000, Loss: 3.6003, top1: 14.0625\n",
      "Epoch [4/60], Iter [206/633], LR: 0.005000, Loss: 3.6229, top1: 12.5000\n",
      "Epoch [4/60], Iter [207/633], LR: 0.005000, Loss: 3.6441, top1: 4.6875\n",
      "Epoch [4/60], Iter [208/633], LR: 0.005000, Loss: 3.6150, top1: 10.9375\n",
      "Epoch [4/60], Iter [209/633], LR: 0.005000, Loss: 3.6594, top1: 9.3750\n",
      "Epoch [4/60], Iter [210/633], LR: 0.005000, Loss: 3.6879, top1: 6.2500\n",
      "Epoch [4/60], Iter [211/633], LR: 0.005000, Loss: 3.6276, top1: 14.0625\n",
      "Epoch [4/60], Iter [212/633], LR: 0.005000, Loss: 3.6159, top1: 10.9375\n",
      "Epoch [4/60], Iter [213/633], LR: 0.005000, Loss: 3.6227, top1: 12.5000\n",
      "Epoch [4/60], Iter [214/633], LR: 0.005000, Loss: 3.6270, top1: 6.2500\n",
      "Epoch [4/60], Iter [215/633], LR: 0.005000, Loss: 3.6202, top1: 12.5000\n",
      "Epoch [4/60], Iter [216/633], LR: 0.005000, Loss: 3.6439, top1: 4.6875\n",
      "Epoch [4/60], Iter [217/633], LR: 0.005000, Loss: 3.6275, top1: 7.8125\n",
      "Epoch [4/60], Iter [218/633], LR: 0.005000, Loss: 3.6211, top1: 12.5000\n",
      "Epoch [4/60], Iter [219/633], LR: 0.005000, Loss: 3.6248, top1: 12.5000\n",
      "Epoch [4/60], Iter [220/633], LR: 0.005000, Loss: 3.6309, top1: 7.8125\n",
      "Epoch [4/60], Iter [221/633], LR: 0.005000, Loss: 3.6613, top1: 1.5625\n",
      "Epoch [4/60], Iter [222/633], LR: 0.005000, Loss: 3.6319, top1: 6.2500\n",
      "Epoch [4/60], Iter [223/633], LR: 0.005000, Loss: 3.6194, top1: 9.3750\n",
      "Epoch [4/60], Iter [224/633], LR: 0.005000, Loss: 3.6744, top1: 4.6875\n",
      "Epoch [4/60], Iter [225/633], LR: 0.005000, Loss: 3.6091, top1: 12.5000\n",
      "Epoch [4/60], Iter [226/633], LR: 0.005000, Loss: 3.6135, top1: 4.6875\n",
      "Epoch [4/60], Iter [227/633], LR: 0.005000, Loss: 3.6116, top1: 14.0625\n",
      "Epoch [4/60], Iter [228/633], LR: 0.005000, Loss: 3.6118, top1: 18.7500\n",
      "Epoch [4/60], Iter [229/633], LR: 0.005000, Loss: 3.6418, top1: 14.0625\n",
      "Epoch [4/60], Iter [230/633], LR: 0.005000, Loss: 3.5914, top1: 17.1875\n",
      "Epoch [4/60], Iter [231/633], LR: 0.005000, Loss: 3.6241, top1: 4.6875\n",
      "Epoch [4/60], Iter [232/633], LR: 0.005000, Loss: 3.6263, top1: 7.8125\n",
      "Epoch [4/60], Iter [233/633], LR: 0.005000, Loss: 3.6290, top1: 10.9375\n",
      "Epoch [4/60], Iter [234/633], LR: 0.005000, Loss: 3.6688, top1: 4.6875\n",
      "Epoch [4/60], Iter [235/633], LR: 0.005000, Loss: 3.6538, top1: 4.6875\n",
      "Epoch [4/60], Iter [236/633], LR: 0.005000, Loss: 3.6008, top1: 9.3750\n",
      "Epoch [4/60], Iter [237/633], LR: 0.005000, Loss: 3.6197, top1: 7.8125\n",
      "Epoch [4/60], Iter [238/633], LR: 0.005000, Loss: 3.6565, top1: 6.2500\n",
      "Epoch [4/60], Iter [239/633], LR: 0.005000, Loss: 3.6296, top1: 6.2500\n",
      "Epoch [4/60], Iter [240/633], LR: 0.005000, Loss: 3.6318, top1: 9.3750\n",
      "Epoch [4/60], Iter [241/633], LR: 0.005000, Loss: 3.6500, top1: 9.3750\n",
      "Epoch [4/60], Iter [242/633], LR: 0.005000, Loss: 3.5950, top1: 14.0625\n",
      "Epoch [4/60], Iter [243/633], LR: 0.005000, Loss: 3.6165, top1: 6.2500\n",
      "Epoch [4/60], Iter [244/633], LR: 0.005000, Loss: 3.6484, top1: 7.8125\n",
      "Epoch [4/60], Iter [245/633], LR: 0.005000, Loss: 3.6089, top1: 10.9375\n",
      "Epoch [4/60], Iter [246/633], LR: 0.005000, Loss: 3.6237, top1: 12.5000\n",
      "Epoch [4/60], Iter [247/633], LR: 0.005000, Loss: 3.6144, top1: 4.6875\n",
      "Epoch [4/60], Iter [248/633], LR: 0.005000, Loss: 3.6402, top1: 7.8125\n",
      "Epoch [4/60], Iter [249/633], LR: 0.005000, Loss: 3.6380, top1: 7.8125\n",
      "Epoch [4/60], Iter [250/633], LR: 0.005000, Loss: 3.6524, top1: 7.8125\n",
      "Epoch [4/60], Iter [251/633], LR: 0.005000, Loss: 3.6167, top1: 9.3750\n",
      "Epoch [4/60], Iter [252/633], LR: 0.005000, Loss: 3.5978, top1: 12.5000\n",
      "Epoch [4/60], Iter [253/633], LR: 0.005000, Loss: 3.6479, top1: 10.9375\n",
      "Epoch [4/60], Iter [254/633], LR: 0.005000, Loss: 3.6466, top1: 7.8125\n",
      "Epoch [4/60], Iter [255/633], LR: 0.005000, Loss: 3.6120, top1: 14.0625\n",
      "Epoch [4/60], Iter [256/633], LR: 0.005000, Loss: 3.5998, top1: 10.9375\n",
      "Epoch [4/60], Iter [257/633], LR: 0.005000, Loss: 3.5992, top1: 17.1875\n",
      "Epoch [4/60], Iter [258/633], LR: 0.005000, Loss: 3.6345, top1: 14.0625\n",
      "Epoch [4/60], Iter [259/633], LR: 0.005000, Loss: 3.5913, top1: 14.0625\n",
      "Epoch [4/60], Iter [260/633], LR: 0.005000, Loss: 3.6416, top1: 7.8125\n",
      "Epoch [4/60], Iter [261/633], LR: 0.005000, Loss: 3.6245, top1: 9.3750\n",
      "Epoch [4/60], Iter [262/633], LR: 0.005000, Loss: 3.6402, top1: 10.9375\n",
      "Epoch [4/60], Iter [263/633], LR: 0.005000, Loss: 3.6311, top1: 10.9375\n",
      "Epoch [4/60], Iter [264/633], LR: 0.005000, Loss: 3.6288, top1: 9.3750\n",
      "Epoch [4/60], Iter [265/633], LR: 0.005000, Loss: 3.6507, top1: 7.8125\n",
      "Epoch [4/60], Iter [266/633], LR: 0.005000, Loss: 3.6694, top1: 3.1250\n",
      "Epoch [4/60], Iter [267/633], LR: 0.005000, Loss: 3.6277, top1: 7.8125\n",
      "Epoch [4/60], Iter [268/633], LR: 0.005000, Loss: 3.6197, top1: 7.8125\n",
      "Epoch [4/60], Iter [269/633], LR: 0.005000, Loss: 3.6395, top1: 9.3750\n",
      "Epoch [4/60], Iter [270/633], LR: 0.005000, Loss: 3.6251, top1: 4.6875\n",
      "Epoch [4/60], Iter [271/633], LR: 0.005000, Loss: 3.6061, top1: 12.5000\n",
      "Epoch [4/60], Iter [272/633], LR: 0.005000, Loss: 3.6290, top1: 6.2500\n",
      "Epoch [4/60], Iter [273/633], LR: 0.005000, Loss: 3.6397, top1: 6.2500\n",
      "Epoch [4/60], Iter [274/633], LR: 0.005000, Loss: 3.6439, top1: 3.1250\n",
      "Epoch [4/60], Iter [275/633], LR: 0.005000, Loss: 3.6545, top1: 4.6875\n",
      "Epoch [4/60], Iter [276/633], LR: 0.005000, Loss: 3.6277, top1: 12.5000\n",
      "Epoch [4/60], Iter [277/633], LR: 0.005000, Loss: 3.6124, top1: 21.8750\n",
      "Epoch [4/60], Iter [278/633], LR: 0.005000, Loss: 3.6021, top1: 12.5000\n",
      "Epoch [4/60], Iter [279/633], LR: 0.005000, Loss: 3.5844, top1: 15.6250\n",
      "Epoch [4/60], Iter [280/633], LR: 0.005000, Loss: 3.6179, top1: 10.9375\n",
      "Epoch [4/60], Iter [281/633], LR: 0.005000, Loss: 3.5963, top1: 18.7500\n",
      "Epoch [4/60], Iter [282/633], LR: 0.005000, Loss: 3.6049, top1: 10.9375\n",
      "Epoch [4/60], Iter [283/633], LR: 0.005000, Loss: 3.6590, top1: 9.3750\n",
      "Epoch [4/60], Iter [284/633], LR: 0.005000, Loss: 3.6078, top1: 12.5000\n",
      "Epoch [4/60], Iter [285/633], LR: 0.005000, Loss: 3.6518, top1: 4.6875\n",
      "Epoch [4/60], Iter [286/633], LR: 0.005000, Loss: 3.6166, top1: 10.9375\n",
      "Epoch [4/60], Iter [287/633], LR: 0.005000, Loss: 3.6190, top1: 10.9375\n",
      "Epoch [4/60], Iter [288/633], LR: 0.005000, Loss: 3.5825, top1: 17.1875\n",
      "Epoch [4/60], Iter [289/633], LR: 0.005000, Loss: 3.6197, top1: 10.9375\n",
      "Epoch [4/60], Iter [290/633], LR: 0.005000, Loss: 3.6262, top1: 12.5000\n",
      "Epoch [4/60], Iter [291/633], LR: 0.005000, Loss: 3.6001, top1: 10.9375\n",
      "Epoch [4/60], Iter [292/633], LR: 0.005000, Loss: 3.6420, top1: 3.1250\n",
      "Epoch [4/60], Iter [293/633], LR: 0.005000, Loss: 3.6226, top1: 12.5000\n",
      "Epoch [4/60], Iter [294/633], LR: 0.005000, Loss: 3.6267, top1: 6.2500\n",
      "Epoch [4/60], Iter [295/633], LR: 0.005000, Loss: 3.6112, top1: 7.8125\n",
      "Epoch [4/60], Iter [296/633], LR: 0.005000, Loss: 3.6320, top1: 10.9375\n",
      "Epoch [4/60], Iter [297/633], LR: 0.005000, Loss: 3.6456, top1: 9.3750\n",
      "Epoch [4/60], Iter [298/633], LR: 0.005000, Loss: 3.6421, top1: 4.6875\n",
      "Epoch [4/60], Iter [299/633], LR: 0.005000, Loss: 3.6050, top1: 14.0625\n",
      "Epoch [4/60], Iter [300/633], LR: 0.005000, Loss: 3.6457, top1: 4.6875\n",
      "Epoch [4/60], Iter [301/633], LR: 0.005000, Loss: 3.6285, top1: 9.3750\n",
      "Epoch [4/60], Iter [302/633], LR: 0.005000, Loss: 3.6686, top1: 1.5625\n",
      "Epoch [4/60], Iter [303/633], LR: 0.005000, Loss: 3.6735, top1: 10.9375\n",
      "Epoch [4/60], Iter [304/633], LR: 0.005000, Loss: 3.6014, top1: 14.0625\n",
      "Epoch [4/60], Iter [305/633], LR: 0.005000, Loss: 3.6030, top1: 12.5000\n",
      "Epoch [4/60], Iter [306/633], LR: 0.005000, Loss: 3.6106, top1: 9.3750\n",
      "Epoch [4/60], Iter [307/633], LR: 0.005000, Loss: 3.5823, top1: 10.9375\n",
      "Epoch [4/60], Iter [308/633], LR: 0.005000, Loss: 3.6011, top1: 12.5000\n",
      "Epoch [4/60], Iter [309/633], LR: 0.005000, Loss: 3.6136, top1: 10.9375\n",
      "Epoch [4/60], Iter [310/633], LR: 0.005000, Loss: 3.5921, top1: 15.6250\n",
      "Epoch [4/60], Iter [311/633], LR: 0.005000, Loss: 3.6318, top1: 10.9375\n",
      "Epoch [4/60], Iter [312/633], LR: 0.005000, Loss: 3.6396, top1: 9.3750\n",
      "Epoch [4/60], Iter [313/633], LR: 0.005000, Loss: 3.5977, top1: 12.5000\n",
      "Epoch [4/60], Iter [314/633], LR: 0.005000, Loss: 3.5905, top1: 14.0625\n",
      "Epoch [4/60], Iter [315/633], LR: 0.005000, Loss: 3.5920, top1: 9.3750\n",
      "Epoch [4/60], Iter [316/633], LR: 0.005000, Loss: 3.6261, top1: 12.5000\n",
      "Epoch [4/60], Iter [317/633], LR: 0.005000, Loss: 3.6603, top1: 4.6875\n",
      "Epoch [4/60], Iter [318/633], LR: 0.005000, Loss: 3.6204, top1: 7.8125\n",
      "Epoch [4/60], Iter [319/633], LR: 0.005000, Loss: 3.6614, top1: 9.3750\n",
      "Epoch [4/60], Iter [320/633], LR: 0.005000, Loss: 3.6009, top1: 9.3750\n",
      "Epoch [4/60], Iter [321/633], LR: 0.005000, Loss: 3.6619, top1: 6.2500\n",
      "Epoch [4/60], Iter [322/633], LR: 0.005000, Loss: 3.6346, top1: 3.1250\n",
      "Epoch [4/60], Iter [323/633], LR: 0.005000, Loss: 3.5944, top1: 15.6250\n",
      "Epoch [4/60], Iter [324/633], LR: 0.005000, Loss: 3.6486, top1: 7.8125\n",
      "Epoch [4/60], Iter [325/633], LR: 0.005000, Loss: 3.6055, top1: 9.3750\n",
      "Epoch [4/60], Iter [326/633], LR: 0.005000, Loss: 3.6368, top1: 14.0625\n",
      "Epoch [4/60], Iter [327/633], LR: 0.005000, Loss: 3.6489, top1: 4.6875\n",
      "Epoch [4/60], Iter [328/633], LR: 0.005000, Loss: 3.5924, top1: 14.0625\n",
      "Epoch [4/60], Iter [329/633], LR: 0.005000, Loss: 3.6279, top1: 6.2500\n",
      "Epoch [4/60], Iter [330/633], LR: 0.005000, Loss: 3.6431, top1: 4.6875\n",
      "Epoch [4/60], Iter [331/633], LR: 0.005000, Loss: 3.6520, top1: 12.5000\n",
      "Epoch [4/60], Iter [332/633], LR: 0.005000, Loss: 3.6585, top1: 4.6875\n",
      "Epoch [4/60], Iter [333/633], LR: 0.005000, Loss: 3.6010, top1: 10.9375\n",
      "Epoch [4/60], Iter [334/633], LR: 0.005000, Loss: 3.6089, top1: 12.5000\n",
      "Epoch [4/60], Iter [335/633], LR: 0.005000, Loss: 3.6093, top1: 9.3750\n",
      "Epoch [4/60], Iter [336/633], LR: 0.005000, Loss: 3.6345, top1: 6.2500\n",
      "Epoch [4/60], Iter [337/633], LR: 0.005000, Loss: 3.6291, top1: 10.9375\n",
      "Epoch [4/60], Iter [338/633], LR: 0.005000, Loss: 3.6224, top1: 4.6875\n",
      "Epoch [4/60], Iter [339/633], LR: 0.005000, Loss: 3.6459, top1: 9.3750\n",
      "Epoch [4/60], Iter [340/633], LR: 0.005000, Loss: 3.6332, top1: 10.9375\n",
      "Epoch [4/60], Iter [341/633], LR: 0.005000, Loss: 3.5996, top1: 15.6250\n",
      "Epoch [4/60], Iter [342/633], LR: 0.005000, Loss: 3.5937, top1: 20.3125\n",
      "Epoch [4/60], Iter [343/633], LR: 0.005000, Loss: 3.6348, top1: 7.8125\n",
      "Epoch [4/60], Iter [344/633], LR: 0.005000, Loss: 3.6319, top1: 3.1250\n",
      "Epoch [4/60], Iter [345/633], LR: 0.005000, Loss: 3.6373, top1: 7.8125\n",
      "Epoch [4/60], Iter [346/633], LR: 0.005000, Loss: 3.6546, top1: 6.2500\n",
      "Epoch [4/60], Iter [347/633], LR: 0.005000, Loss: 3.6255, top1: 9.3750\n",
      "Epoch [4/60], Iter [348/633], LR: 0.005000, Loss: 3.5879, top1: 12.5000\n",
      "Epoch [4/60], Iter [349/633], LR: 0.005000, Loss: 3.6773, top1: 4.6875\n",
      "Epoch [4/60], Iter [350/633], LR: 0.005000, Loss: 3.6166, top1: 7.8125\n",
      "Epoch [4/60], Iter [351/633], LR: 0.005000, Loss: 3.6307, top1: 9.3750\n",
      "Epoch [4/60], Iter [352/633], LR: 0.005000, Loss: 3.6424, top1: 6.2500\n",
      "Epoch [4/60], Iter [353/633], LR: 0.005000, Loss: 3.5959, top1: 12.5000\n",
      "Epoch [4/60], Iter [354/633], LR: 0.005000, Loss: 3.6271, top1: 7.8125\n",
      "Epoch [4/60], Iter [355/633], LR: 0.005000, Loss: 3.5859, top1: 12.5000\n",
      "Epoch [4/60], Iter [356/633], LR: 0.005000, Loss: 3.6359, top1: 12.5000\n",
      "Epoch [4/60], Iter [357/633], LR: 0.005000, Loss: 3.6128, top1: 9.3750\n",
      "Epoch [4/60], Iter [358/633], LR: 0.005000, Loss: 3.6354, top1: 7.8125\n",
      "Epoch [4/60], Iter [359/633], LR: 0.005000, Loss: 3.6267, top1: 7.8125\n",
      "Epoch [4/60], Iter [360/633], LR: 0.005000, Loss: 3.6526, top1: 14.0625\n",
      "Epoch [4/60], Iter [361/633], LR: 0.005000, Loss: 3.6478, top1: 6.2500\n",
      "Epoch [4/60], Iter [362/633], LR: 0.005000, Loss: 3.6305, top1: 9.3750\n",
      "Epoch [4/60], Iter [363/633], LR: 0.005000, Loss: 3.5837, top1: 9.3750\n",
      "Epoch [4/60], Iter [364/633], LR: 0.005000, Loss: 3.6038, top1: 12.5000\n",
      "Epoch [4/60], Iter [365/633], LR: 0.005000, Loss: 3.6120, top1: 10.9375\n",
      "Epoch [4/60], Iter [366/633], LR: 0.005000, Loss: 3.6132, top1: 14.0625\n",
      "Epoch [4/60], Iter [367/633], LR: 0.005000, Loss: 3.6685, top1: 7.8125\n",
      "Epoch [4/60], Iter [368/633], LR: 0.005000, Loss: 3.6370, top1: 6.2500\n",
      "Epoch [4/60], Iter [369/633], LR: 0.005000, Loss: 3.5846, top1: 14.0625\n",
      "Epoch [4/60], Iter [370/633], LR: 0.005000, Loss: 3.6271, top1: 9.3750\n",
      "Epoch [4/60], Iter [371/633], LR: 0.005000, Loss: 3.6306, top1: 10.9375\n",
      "Epoch [4/60], Iter [372/633], LR: 0.005000, Loss: 3.6365, top1: 15.6250\n",
      "Epoch [4/60], Iter [373/633], LR: 0.005000, Loss: 3.6044, top1: 12.5000\n",
      "Epoch [4/60], Iter [374/633], LR: 0.005000, Loss: 3.6083, top1: 12.5000\n",
      "Epoch [4/60], Iter [375/633], LR: 0.005000, Loss: 3.6242, top1: 9.3750\n",
      "Epoch [4/60], Iter [376/633], LR: 0.005000, Loss: 3.6003, top1: 15.6250\n",
      "Epoch [4/60], Iter [377/633], LR: 0.005000, Loss: 3.6432, top1: 7.8125\n",
      "Epoch [4/60], Iter [378/633], LR: 0.005000, Loss: 3.6517, top1: 7.8125\n",
      "Epoch [4/60], Iter [379/633], LR: 0.005000, Loss: 3.6310, top1: 10.9375\n",
      "Epoch [4/60], Iter [380/633], LR: 0.005000, Loss: 3.6502, top1: 6.2500\n",
      "Epoch [4/60], Iter [381/633], LR: 0.005000, Loss: 3.6558, top1: 12.5000\n",
      "Epoch [4/60], Iter [382/633], LR: 0.005000, Loss: 3.6587, top1: 3.1250\n",
      "Epoch [4/60], Iter [383/633], LR: 0.005000, Loss: 3.6223, top1: 14.0625\n",
      "Epoch [4/60], Iter [384/633], LR: 0.005000, Loss: 3.6302, top1: 7.8125\n",
      "Epoch [4/60], Iter [385/633], LR: 0.005000, Loss: 3.6544, top1: 10.9375\n",
      "Epoch [4/60], Iter [386/633], LR: 0.005000, Loss: 3.6015, top1: 7.8125\n",
      "Epoch [4/60], Iter [387/633], LR: 0.005000, Loss: 3.6123, top1: 7.8125\n",
      "Epoch [4/60], Iter [388/633], LR: 0.005000, Loss: 3.5971, top1: 10.9375\n",
      "Epoch [4/60], Iter [389/633], LR: 0.005000, Loss: 3.6397, top1: 6.2500\n",
      "Epoch [4/60], Iter [390/633], LR: 0.005000, Loss: 3.6274, top1: 7.8125\n",
      "Epoch [4/60], Iter [391/633], LR: 0.005000, Loss: 3.6310, top1: 10.9375\n",
      "Epoch [4/60], Iter [392/633], LR: 0.005000, Loss: 3.6267, top1: 9.3750\n",
      "Epoch [4/60], Iter [393/633], LR: 0.005000, Loss: 3.6244, top1: 6.2500\n",
      "Epoch [4/60], Iter [394/633], LR: 0.005000, Loss: 3.6224, top1: 9.3750\n",
      "Epoch [4/60], Iter [395/633], LR: 0.005000, Loss: 3.6548, top1: 4.6875\n",
      "Epoch [4/60], Iter [396/633], LR: 0.005000, Loss: 3.6119, top1: 12.5000\n",
      "Epoch [4/60], Iter [397/633], LR: 0.005000, Loss: 3.6261, top1: 14.0625\n",
      "Epoch [4/60], Iter [398/633], LR: 0.005000, Loss: 3.6552, top1: 7.8125\n",
      "Epoch [4/60], Iter [399/633], LR: 0.005000, Loss: 3.6487, top1: 9.3750\n",
      "Epoch [4/60], Iter [400/633], LR: 0.005000, Loss: 3.6033, top1: 12.5000\n",
      "Epoch [4/60], Iter [401/633], LR: 0.005000, Loss: 3.5882, top1: 10.9375\n",
      "Epoch [4/60], Iter [402/633], LR: 0.005000, Loss: 3.6314, top1: 7.8125\n",
      "Epoch [4/60], Iter [403/633], LR: 0.005000, Loss: 3.6208, top1: 9.3750\n",
      "Epoch [4/60], Iter [404/633], LR: 0.005000, Loss: 3.6391, top1: 14.0625\n",
      "Epoch [4/60], Iter [405/633], LR: 0.005000, Loss: 3.6554, top1: 9.3750\n",
      "Epoch [4/60], Iter [406/633], LR: 0.005000, Loss: 3.6483, top1: 9.3750\n",
      "Epoch [4/60], Iter [407/633], LR: 0.005000, Loss: 3.6294, top1: 4.6875\n",
      "Epoch [4/60], Iter [408/633], LR: 0.005000, Loss: 3.6456, top1: 10.9375\n",
      "Epoch [4/60], Iter [409/633], LR: 0.005000, Loss: 3.6222, top1: 7.8125\n",
      "Epoch [4/60], Iter [410/633], LR: 0.005000, Loss: 3.6138, top1: 9.3750\n",
      "Epoch [4/60], Iter [411/633], LR: 0.005000, Loss: 3.6410, top1: 6.2500\n",
      "Epoch [4/60], Iter [412/633], LR: 0.005000, Loss: 3.6066, top1: 10.9375\n",
      "Epoch [4/60], Iter [413/633], LR: 0.005000, Loss: 3.6585, top1: 4.6875\n",
      "Epoch [4/60], Iter [414/633], LR: 0.005000, Loss: 3.6696, top1: 9.3750\n",
      "Epoch [4/60], Iter [415/633], LR: 0.005000, Loss: 3.6166, top1: 12.5000\n",
      "Epoch [4/60], Iter [416/633], LR: 0.005000, Loss: 3.6652, top1: 0.0000\n",
      "Epoch [4/60], Iter [417/633], LR: 0.005000, Loss: 3.6063, top1: 10.9375\n",
      "Epoch [4/60], Iter [418/633], LR: 0.005000, Loss: 3.5961, top1: 12.5000\n",
      "Epoch [4/60], Iter [419/633], LR: 0.005000, Loss: 3.6367, top1: 6.2500\n",
      "Epoch [4/60], Iter [420/633], LR: 0.005000, Loss: 3.6288, top1: 12.5000\n",
      "Epoch [4/60], Iter [421/633], LR: 0.005000, Loss: 3.6626, top1: 6.2500\n",
      "Epoch [4/60], Iter [422/633], LR: 0.005000, Loss: 3.6610, top1: 9.3750\n",
      "Epoch [4/60], Iter [423/633], LR: 0.005000, Loss: 3.6114, top1: 14.0625\n",
      "Epoch [4/60], Iter [424/633], LR: 0.005000, Loss: 3.6663, top1: 6.2500\n",
      "Epoch [4/60], Iter [425/633], LR: 0.005000, Loss: 3.6309, top1: 9.3750\n",
      "Epoch [4/60], Iter [426/633], LR: 0.005000, Loss: 3.6137, top1: 15.6250\n",
      "Epoch [4/60], Iter [427/633], LR: 0.005000, Loss: 3.6078, top1: 10.9375\n",
      "Epoch [4/60], Iter [428/633], LR: 0.005000, Loss: 3.6736, top1: 4.6875\n",
      "Epoch [4/60], Iter [429/633], LR: 0.005000, Loss: 3.6646, top1: 3.1250\n",
      "Epoch [4/60], Iter [430/633], LR: 0.005000, Loss: 3.6305, top1: 10.9375\n",
      "Epoch [4/60], Iter [431/633], LR: 0.005000, Loss: 3.6009, top1: 23.4375\n",
      "Epoch [4/60], Iter [432/633], LR: 0.005000, Loss: 3.6320, top1: 7.8125\n",
      "Epoch [4/60], Iter [433/633], LR: 0.005000, Loss: 3.6479, top1: 14.0625\n",
      "Epoch [4/60], Iter [434/633], LR: 0.005000, Loss: 3.6423, top1: 6.2500\n",
      "Epoch [4/60], Iter [435/633], LR: 0.005000, Loss: 3.6218, top1: 12.5000\n",
      "Epoch [4/60], Iter [436/633], LR: 0.005000, Loss: 3.6474, top1: 4.6875\n",
      "Epoch [4/60], Iter [437/633], LR: 0.005000, Loss: 3.6095, top1: 15.6250\n",
      "Epoch [4/60], Iter [438/633], LR: 0.005000, Loss: 3.6463, top1: 9.3750\n",
      "Epoch [4/60], Iter [439/633], LR: 0.005000, Loss: 3.6208, top1: 6.2500\n",
      "Epoch [4/60], Iter [440/633], LR: 0.005000, Loss: 3.6282, top1: 12.5000\n",
      "Epoch [4/60], Iter [441/633], LR: 0.005000, Loss: 3.6300, top1: 7.8125\n",
      "Epoch [4/60], Iter [442/633], LR: 0.005000, Loss: 3.6256, top1: 10.9375\n",
      "Epoch [4/60], Iter [443/633], LR: 0.005000, Loss: 3.6305, top1: 10.9375\n",
      "Epoch [4/60], Iter [444/633], LR: 0.005000, Loss: 3.6215, top1: 12.5000\n",
      "Epoch [4/60], Iter [445/633], LR: 0.005000, Loss: 3.6327, top1: 7.8125\n",
      "Epoch [4/60], Iter [446/633], LR: 0.005000, Loss: 3.6474, top1: 7.8125\n",
      "Epoch [4/60], Iter [447/633], LR: 0.005000, Loss: 3.5997, top1: 7.8125\n",
      "Epoch [4/60], Iter [448/633], LR: 0.005000, Loss: 3.6115, top1: 15.6250\n",
      "Epoch [4/60], Iter [449/633], LR: 0.005000, Loss: 3.5722, top1: 17.1875\n",
      "Epoch [4/60], Iter [450/633], LR: 0.005000, Loss: 3.5788, top1: 12.5000\n",
      "Epoch [4/60], Iter [451/633], LR: 0.005000, Loss: 3.6160, top1: 9.3750\n",
      "Epoch [4/60], Iter [452/633], LR: 0.005000, Loss: 3.6444, top1: 4.6875\n",
      "Epoch [4/60], Iter [453/633], LR: 0.005000, Loss: 3.5962, top1: 9.3750\n",
      "Epoch [4/60], Iter [454/633], LR: 0.005000, Loss: 3.6357, top1: 1.5625\n",
      "Epoch [4/60], Iter [455/633], LR: 0.005000, Loss: 3.6043, top1: 10.9375\n",
      "Epoch [4/60], Iter [456/633], LR: 0.005000, Loss: 3.6316, top1: 12.5000\n",
      "Epoch [4/60], Iter [457/633], LR: 0.005000, Loss: 3.6582, top1: 6.2500\n",
      "Epoch [4/60], Iter [458/633], LR: 0.005000, Loss: 3.5967, top1: 10.9375\n",
      "Epoch [4/60], Iter [459/633], LR: 0.005000, Loss: 3.6447, top1: 1.5625\n",
      "Epoch [4/60], Iter [460/633], LR: 0.005000, Loss: 3.6312, top1: 6.2500\n",
      "Epoch [4/60], Iter [461/633], LR: 0.005000, Loss: 3.6176, top1: 10.9375\n",
      "Epoch [4/60], Iter [462/633], LR: 0.005000, Loss: 3.6399, top1: 7.8125\n",
      "Epoch [4/60], Iter [463/633], LR: 0.005000, Loss: 3.6010, top1: 14.0625\n",
      "Epoch [4/60], Iter [464/633], LR: 0.005000, Loss: 3.5967, top1: 17.1875\n",
      "Epoch [4/60], Iter [465/633], LR: 0.005000, Loss: 3.6638, top1: 7.8125\n",
      "Epoch [4/60], Iter [466/633], LR: 0.005000, Loss: 3.6078, top1: 10.9375\n",
      "Epoch [4/60], Iter [467/633], LR: 0.005000, Loss: 3.6236, top1: 9.3750\n",
      "Epoch [4/60], Iter [468/633], LR: 0.005000, Loss: 3.6096, top1: 9.3750\n",
      "Epoch [4/60], Iter [469/633], LR: 0.005000, Loss: 3.6386, top1: 10.9375\n",
      "Epoch [4/60], Iter [470/633], LR: 0.005000, Loss: 3.6163, top1: 10.9375\n",
      "Epoch [4/60], Iter [471/633], LR: 0.005000, Loss: 3.5907, top1: 15.6250\n",
      "Epoch [4/60], Iter [472/633], LR: 0.005000, Loss: 3.6052, top1: 12.5000\n",
      "Epoch [4/60], Iter [473/633], LR: 0.005000, Loss: 3.6182, top1: 9.3750\n",
      "Epoch [4/60], Iter [474/633], LR: 0.005000, Loss: 3.6364, top1: 4.6875\n",
      "Epoch [4/60], Iter [475/633], LR: 0.005000, Loss: 3.5847, top1: 18.7500\n",
      "Epoch [4/60], Iter [476/633], LR: 0.005000, Loss: 3.6231, top1: 12.5000\n",
      "Epoch [4/60], Iter [477/633], LR: 0.005000, Loss: 3.6423, top1: 9.3750\n",
      "Epoch [4/60], Iter [478/633], LR: 0.005000, Loss: 3.5916, top1: 14.0625\n",
      "Epoch [4/60], Iter [479/633], LR: 0.005000, Loss: 3.6087, top1: 18.7500\n",
      "Epoch [4/60], Iter [480/633], LR: 0.005000, Loss: 3.6005, top1: 15.6250\n",
      "Epoch [4/60], Iter [481/633], LR: 0.005000, Loss: 3.6303, top1: 10.9375\n",
      "Epoch [4/60], Iter [482/633], LR: 0.005000, Loss: 3.6451, top1: 9.3750\n",
      "Epoch [4/60], Iter [483/633], LR: 0.005000, Loss: 3.6137, top1: 14.0625\n",
      "Epoch [4/60], Iter [484/633], LR: 0.005000, Loss: 3.6014, top1: 9.3750\n",
      "Epoch [4/60], Iter [485/633], LR: 0.005000, Loss: 3.6583, top1: 12.5000\n",
      "Epoch [4/60], Iter [486/633], LR: 0.005000, Loss: 3.6322, top1: 7.8125\n",
      "Epoch [4/60], Iter [487/633], LR: 0.005000, Loss: 3.6367, top1: 7.8125\n",
      "Epoch [4/60], Iter [488/633], LR: 0.005000, Loss: 3.5846, top1: 18.7500\n",
      "Epoch [4/60], Iter [489/633], LR: 0.005000, Loss: 3.6256, top1: 15.6250\n",
      "Epoch [4/60], Iter [490/633], LR: 0.005000, Loss: 3.6394, top1: 7.8125\n",
      "Epoch [4/60], Iter [491/633], LR: 0.005000, Loss: 3.6343, top1: 7.8125\n",
      "Epoch [4/60], Iter [492/633], LR: 0.005000, Loss: 3.6758, top1: 4.6875\n",
      "Epoch [4/60], Iter [493/633], LR: 0.005000, Loss: 3.6260, top1: 9.3750\n",
      "Epoch [4/60], Iter [494/633], LR: 0.005000, Loss: 3.5970, top1: 14.0625\n",
      "Epoch [4/60], Iter [495/633], LR: 0.005000, Loss: 3.6624, top1: 7.8125\n",
      "Epoch [4/60], Iter [496/633], LR: 0.005000, Loss: 3.6386, top1: 6.2500\n",
      "Epoch [4/60], Iter [497/633], LR: 0.005000, Loss: 3.6675, top1: 3.1250\n",
      "Epoch [4/60], Iter [498/633], LR: 0.005000, Loss: 3.6251, top1: 14.0625\n",
      "Epoch [4/60], Iter [499/633], LR: 0.005000, Loss: 3.6141, top1: 10.9375\n",
      "Epoch [4/60], Iter [500/633], LR: 0.005000, Loss: 3.6060, top1: 14.0625\n",
      "Epoch [4/60], Iter [501/633], LR: 0.005000, Loss: 3.6436, top1: 7.8125\n",
      "Epoch [4/60], Iter [502/633], LR: 0.005000, Loss: 3.6420, top1: 6.2500\n",
      "Epoch [4/60], Iter [503/633], LR: 0.005000, Loss: 3.6132, top1: 9.3750\n",
      "Epoch [4/60], Iter [504/633], LR: 0.005000, Loss: 3.5841, top1: 12.5000\n",
      "Epoch [4/60], Iter [505/633], LR: 0.005000, Loss: 3.6366, top1: 9.3750\n",
      "Epoch [4/60], Iter [506/633], LR: 0.005000, Loss: 3.6490, top1: 6.2500\n",
      "Epoch [4/60], Iter [507/633], LR: 0.005000, Loss: 3.6201, top1: 7.8125\n",
      "Epoch [4/60], Iter [508/633], LR: 0.005000, Loss: 3.6486, top1: 7.8125\n",
      "Epoch [4/60], Iter [509/633], LR: 0.005000, Loss: 3.6296, top1: 9.3750\n",
      "Epoch [4/60], Iter [510/633], LR: 0.005000, Loss: 3.5925, top1: 17.1875\n",
      "Epoch [4/60], Iter [511/633], LR: 0.005000, Loss: 3.6441, top1: 10.9375\n",
      "Epoch [4/60], Iter [512/633], LR: 0.005000, Loss: 3.6292, top1: 4.6875\n",
      "Epoch [4/60], Iter [513/633], LR: 0.005000, Loss: 3.6362, top1: 10.9375\n",
      "Epoch [4/60], Iter [514/633], LR: 0.005000, Loss: 3.6527, top1: 7.8125\n",
      "Epoch [4/60], Iter [515/633], LR: 0.005000, Loss: 3.6133, top1: 7.8125\n",
      "Epoch [4/60], Iter [516/633], LR: 0.005000, Loss: 3.6490, top1: 9.3750\n",
      "Epoch [4/60], Iter [517/633], LR: 0.005000, Loss: 3.6107, top1: 7.8125\n",
      "Epoch [4/60], Iter [518/633], LR: 0.005000, Loss: 3.6330, top1: 6.2500\n",
      "Epoch [4/60], Iter [519/633], LR: 0.005000, Loss: 3.6237, top1: 10.9375\n",
      "Epoch [4/60], Iter [520/633], LR: 0.005000, Loss: 3.6178, top1: 10.9375\n",
      "Epoch [4/60], Iter [521/633], LR: 0.005000, Loss: 3.6167, top1: 9.3750\n",
      "Epoch [4/60], Iter [522/633], LR: 0.005000, Loss: 3.6040, top1: 10.9375\n",
      "Epoch [4/60], Iter [523/633], LR: 0.005000, Loss: 3.5716, top1: 15.6250\n",
      "Epoch [4/60], Iter [524/633], LR: 0.005000, Loss: 3.6430, top1: 3.1250\n",
      "Epoch [4/60], Iter [525/633], LR: 0.005000, Loss: 3.5423, top1: 20.3125\n",
      "Epoch [4/60], Iter [526/633], LR: 0.005000, Loss: 3.6549, top1: 1.5625\n",
      "Epoch [4/60], Iter [527/633], LR: 0.005000, Loss: 3.6304, top1: 14.0625\n",
      "Epoch [4/60], Iter [528/633], LR: 0.005000, Loss: 3.6185, top1: 10.9375\n",
      "Epoch [4/60], Iter [529/633], LR: 0.005000, Loss: 3.6299, top1: 7.8125\n",
      "Epoch [4/60], Iter [530/633], LR: 0.005000, Loss: 3.6135, top1: 9.3750\n",
      "Epoch [4/60], Iter [531/633], LR: 0.005000, Loss: 3.6210, top1: 10.9375\n",
      "Epoch [4/60], Iter [532/633], LR: 0.005000, Loss: 3.5903, top1: 14.0625\n",
      "Epoch [4/60], Iter [533/633], LR: 0.005000, Loss: 3.6459, top1: 6.2500\n",
      "Epoch [4/60], Iter [534/633], LR: 0.005000, Loss: 3.6393, top1: 9.3750\n",
      "Epoch [4/60], Iter [535/633], LR: 0.005000, Loss: 3.6107, top1: 12.5000\n",
      "Epoch [4/60], Iter [536/633], LR: 0.005000, Loss: 3.6012, top1: 14.0625\n",
      "Epoch [4/60], Iter [537/633], LR: 0.005000, Loss: 3.6221, top1: 9.3750\n",
      "Epoch [4/60], Iter [538/633], LR: 0.005000, Loss: 3.6195, top1: 17.1875\n",
      "Epoch [4/60], Iter [539/633], LR: 0.005000, Loss: 3.6052, top1: 10.9375\n",
      "Epoch [4/60], Iter [540/633], LR: 0.005000, Loss: 3.6370, top1: 7.8125\n",
      "Epoch [4/60], Iter [541/633], LR: 0.005000, Loss: 3.6120, top1: 9.3750\n",
      "Epoch [4/60], Iter [542/633], LR: 0.005000, Loss: 3.5988, top1: 7.8125\n",
      "Epoch [4/60], Iter [543/633], LR: 0.005000, Loss: 3.5899, top1: 7.8125\n",
      "Epoch [4/60], Iter [544/633], LR: 0.005000, Loss: 3.6283, top1: 10.9375\n",
      "Epoch [4/60], Iter [545/633], LR: 0.005000, Loss: 3.6342, top1: 6.2500\n",
      "Epoch [4/60], Iter [546/633], LR: 0.005000, Loss: 3.6239, top1: 10.9375\n",
      "Epoch [4/60], Iter [547/633], LR: 0.005000, Loss: 3.6016, top1: 14.0625\n",
      "Epoch [4/60], Iter [548/633], LR: 0.005000, Loss: 3.5975, top1: 10.9375\n",
      "Epoch [4/60], Iter [549/633], LR: 0.005000, Loss: 3.5934, top1: 15.6250\n",
      "Epoch [4/60], Iter [550/633], LR: 0.005000, Loss: 3.6208, top1: 9.3750\n",
      "Epoch [4/60], Iter [551/633], LR: 0.005000, Loss: 3.5967, top1: 15.6250\n",
      "Epoch [4/60], Iter [552/633], LR: 0.005000, Loss: 3.6809, top1: 3.1250\n",
      "Epoch [4/60], Iter [553/633], LR: 0.005000, Loss: 3.6072, top1: 17.1875\n",
      "Epoch [4/60], Iter [554/633], LR: 0.005000, Loss: 3.6577, top1: 3.1250\n",
      "Epoch [4/60], Iter [555/633], LR: 0.005000, Loss: 3.6109, top1: 12.5000\n",
      "Epoch [4/60], Iter [556/633], LR: 0.005000, Loss: 3.6295, top1: 10.9375\n",
      "Epoch [4/60], Iter [557/633], LR: 0.005000, Loss: 3.6258, top1: 12.5000\n",
      "Epoch [4/60], Iter [558/633], LR: 0.005000, Loss: 3.6033, top1: 14.0625\n",
      "Epoch [4/60], Iter [559/633], LR: 0.005000, Loss: 3.6098, top1: 9.3750\n",
      "Epoch [4/60], Iter [560/633], LR: 0.005000, Loss: 3.6020, top1: 10.9375\n",
      "Epoch [4/60], Iter [561/633], LR: 0.005000, Loss: 3.6108, top1: 18.7500\n",
      "Epoch [4/60], Iter [562/633], LR: 0.005000, Loss: 3.6461, top1: 12.5000\n",
      "Epoch [4/60], Iter [563/633], LR: 0.005000, Loss: 3.6165, top1: 12.5000\n",
      "Epoch [4/60], Iter [564/633], LR: 0.005000, Loss: 3.6230, top1: 7.8125\n",
      "Epoch [4/60], Iter [565/633], LR: 0.005000, Loss: 3.5811, top1: 17.1875\n",
      "Epoch [4/60], Iter [566/633], LR: 0.005000, Loss: 3.6319, top1: 7.8125\n",
      "Epoch [4/60], Iter [567/633], LR: 0.005000, Loss: 3.6305, top1: 9.3750\n",
      "Epoch [4/60], Iter [568/633], LR: 0.005000, Loss: 3.5792, top1: 17.1875\n",
      "Epoch [4/60], Iter [569/633], LR: 0.005000, Loss: 3.5821, top1: 10.9375\n",
      "Epoch [4/60], Iter [570/633], LR: 0.005000, Loss: 3.6088, top1: 9.3750\n",
      "Epoch [4/60], Iter [571/633], LR: 0.005000, Loss: 3.6101, top1: 6.2500\n",
      "Epoch [4/60], Iter [572/633], LR: 0.005000, Loss: 3.6376, top1: 12.5000\n",
      "Epoch [4/60], Iter [573/633], LR: 0.005000, Loss: 3.6738, top1: 3.1250\n",
      "Epoch [4/60], Iter [574/633], LR: 0.005000, Loss: 3.6173, top1: 10.9375\n",
      "Epoch [4/60], Iter [575/633], LR: 0.005000, Loss: 3.6326, top1: 7.8125\n",
      "Epoch [4/60], Iter [576/633], LR: 0.005000, Loss: 3.6243, top1: 4.6875\n",
      "Epoch [4/60], Iter [577/633], LR: 0.005000, Loss: 3.5827, top1: 15.6250\n",
      "Epoch [4/60], Iter [578/633], LR: 0.005000, Loss: 3.6207, top1: 10.9375\n",
      "Epoch [4/60], Iter [579/633], LR: 0.005000, Loss: 3.6369, top1: 6.2500\n",
      "Epoch [4/60], Iter [580/633], LR: 0.005000, Loss: 3.6646, top1: 6.2500\n",
      "Epoch [4/60], Iter [581/633], LR: 0.005000, Loss: 3.6087, top1: 10.9375\n",
      "Epoch [4/60], Iter [582/633], LR: 0.005000, Loss: 3.6582, top1: 10.9375\n",
      "Epoch [4/60], Iter [583/633], LR: 0.005000, Loss: 3.6003, top1: 14.0625\n",
      "Epoch [4/60], Iter [584/633], LR: 0.005000, Loss: 3.6399, top1: 12.5000\n",
      "Epoch [4/60], Iter [585/633], LR: 0.005000, Loss: 3.6582, top1: 7.8125\n",
      "Epoch [4/60], Iter [586/633], LR: 0.005000, Loss: 3.6254, top1: 12.5000\n",
      "Epoch [4/60], Iter [587/633], LR: 0.005000, Loss: 3.6446, top1: 9.3750\n",
      "Epoch [4/60], Iter [588/633], LR: 0.005000, Loss: 3.6215, top1: 9.3750\n",
      "Epoch [4/60], Iter [589/633], LR: 0.005000, Loss: 3.6008, top1: 10.9375\n",
      "Epoch [4/60], Iter [590/633], LR: 0.005000, Loss: 3.6151, top1: 10.9375\n",
      "Epoch [4/60], Iter [591/633], LR: 0.005000, Loss: 3.5835, top1: 17.1875\n",
      "Epoch [4/60], Iter [592/633], LR: 0.005000, Loss: 3.6009, top1: 9.3750\n",
      "Epoch [4/60], Iter [593/633], LR: 0.005000, Loss: 3.6145, top1: 9.3750\n",
      "Epoch [4/60], Iter [594/633], LR: 0.005000, Loss: 3.6167, top1: 6.2500\n",
      "Epoch [4/60], Iter [595/633], LR: 0.005000, Loss: 3.6281, top1: 7.8125\n",
      "Epoch [4/60], Iter [596/633], LR: 0.005000, Loss: 3.6281, top1: 6.2500\n",
      "Epoch [4/60], Iter [597/633], LR: 0.005000, Loss: 3.5919, top1: 17.1875\n",
      "Epoch [4/60], Iter [598/633], LR: 0.005000, Loss: 3.6383, top1: 6.2500\n",
      "Epoch [4/60], Iter [599/633], LR: 0.005000, Loss: 3.6207, top1: 9.3750\n",
      "Epoch [4/60], Iter [600/633], LR: 0.005000, Loss: 3.6198, top1: 14.0625\n",
      "Epoch [4/60], Iter [601/633], LR: 0.005000, Loss: 3.6281, top1: 4.6875\n",
      "Epoch [4/60], Iter [602/633], LR: 0.005000, Loss: 3.5797, top1: 10.9375\n",
      "Epoch [4/60], Iter [603/633], LR: 0.005000, Loss: 3.6510, top1: 6.2500\n",
      "Epoch [4/60], Iter [604/633], LR: 0.005000, Loss: 3.6163, top1: 10.9375\n",
      "Epoch [4/60], Iter [605/633], LR: 0.005000, Loss: 3.6501, top1: 0.0000\n",
      "Epoch [4/60], Iter [606/633], LR: 0.005000, Loss: 3.6438, top1: 9.3750\n",
      "Epoch [4/60], Iter [607/633], LR: 0.005000, Loss: 3.6197, top1: 14.0625\n",
      "Epoch [4/60], Iter [608/633], LR: 0.005000, Loss: 3.5747, top1: 14.0625\n",
      "Epoch [4/60], Iter [609/633], LR: 0.005000, Loss: 3.6102, top1: 10.9375\n",
      "Epoch [4/60], Iter [610/633], LR: 0.005000, Loss: 3.6531, top1: 4.6875\n",
      "Epoch [4/60], Iter [611/633], LR: 0.005000, Loss: 3.6096, top1: 15.6250\n",
      "Epoch [4/60], Iter [612/633], LR: 0.005000, Loss: 3.6269, top1: 6.2500\n",
      "Epoch [4/60], Iter [613/633], LR: 0.005000, Loss: 3.6476, top1: 9.3750\n",
      "Epoch [4/60], Iter [614/633], LR: 0.005000, Loss: 3.6125, top1: 12.5000\n",
      "Epoch [4/60], Iter [615/633], LR: 0.005000, Loss: 3.6178, top1: 7.8125\n",
      "Epoch [4/60], Iter [616/633], LR: 0.005000, Loss: 3.5818, top1: 14.0625\n",
      "Epoch [4/60], Iter [617/633], LR: 0.005000, Loss: 3.6597, top1: 10.9375\n",
      "Epoch [4/60], Iter [618/633], LR: 0.005000, Loss: 3.5811, top1: 10.9375\n",
      "Epoch [4/60], Iter [619/633], LR: 0.005000, Loss: 3.6210, top1: 12.5000\n",
      "Epoch [4/60], Iter [620/633], LR: 0.005000, Loss: 3.6032, top1: 9.3750\n",
      "Epoch [4/60], Iter [621/633], LR: 0.005000, Loss: 3.6524, top1: 7.8125\n",
      "Epoch [4/60], Iter [622/633], LR: 0.005000, Loss: 3.6030, top1: 15.6250\n",
      "Epoch [4/60], Iter [623/633], LR: 0.005000, Loss: 3.6388, top1: 7.8125\n",
      "Epoch [4/60], Iter [624/633], LR: 0.005000, Loss: 3.5952, top1: 15.6250\n",
      "Epoch [4/60], Iter [625/633], LR: 0.005000, Loss: 3.6115, top1: 14.0625\n",
      "Epoch [4/60], Iter [626/633], LR: 0.005000, Loss: 3.6426, top1: 12.5000\n",
      "Epoch [4/60], Iter [627/633], LR: 0.005000, Loss: 3.6185, top1: 14.0625\n",
      "Epoch [4/60], Iter [628/633], LR: 0.005000, Loss: 3.6416, top1: 10.9375\n",
      "Epoch [4/60], Iter [629/633], LR: 0.005000, Loss: 3.6012, top1: 12.5000\n",
      "Epoch [4/60], Iter [630/633], LR: 0.005000, Loss: 3.6126, top1: 10.9375\n",
      "Epoch [4/60], Iter [631/633], LR: 0.005000, Loss: 3.6051, top1: 21.8750\n",
      "Epoch [4/60], Iter [632/633], LR: 0.005000, Loss: 3.6386, top1: 4.6875\n",
      "Epoch [4/60], Iter [633/633], LR: 0.005000, Loss: 3.6499, top1: 12.5000\n",
      "Epoch [4/60], Iter [634/633], LR: 0.005000, Loss: 3.6342, top1: 6.4516\n",
      "Epoch [4/60], Val_Loss: 3.6286, Val_top1: 9.2210, best_top1: 10.2773\n",
      "epoch time: 4.5220749537150065 min\n",
      "Epoch [5/60], Iter [1/633], LR: 0.005000, Loss: 3.6306, top1: 6.2500\n",
      "Epoch [5/60], Iter [2/633], LR: 0.005000, Loss: 3.6254, top1: 10.9375\n",
      "Epoch [5/60], Iter [3/633], LR: 0.005000, Loss: 3.6059, top1: 4.6875\n",
      "Epoch [5/60], Iter [4/633], LR: 0.005000, Loss: 3.5850, top1: 17.1875\n",
      "Epoch [5/60], Iter [5/633], LR: 0.005000, Loss: 3.6482, top1: 4.6875\n",
      "Epoch [5/60], Iter [6/633], LR: 0.005000, Loss: 3.6724, top1: 7.8125\n",
      "Epoch [5/60], Iter [7/633], LR: 0.005000, Loss: 3.6269, top1: 9.3750\n",
      "Epoch [5/60], Iter [8/633], LR: 0.005000, Loss: 3.6785, top1: 6.2500\n",
      "Epoch [5/60], Iter [9/633], LR: 0.005000, Loss: 3.6023, top1: 17.1875\n",
      "Epoch [5/60], Iter [10/633], LR: 0.005000, Loss: 3.6390, top1: 9.3750\n",
      "Epoch [5/60], Iter [11/633], LR: 0.005000, Loss: 3.6276, top1: 9.3750\n",
      "Epoch [5/60], Iter [12/633], LR: 0.005000, Loss: 3.6257, top1: 15.6250\n",
      "Epoch [5/60], Iter [13/633], LR: 0.005000, Loss: 3.6446, top1: 9.3750\n",
      "Epoch [5/60], Iter [14/633], LR: 0.005000, Loss: 3.6432, top1: 12.5000\n",
      "Epoch [5/60], Iter [15/633], LR: 0.005000, Loss: 3.6167, top1: 6.2500\n",
      "Epoch [5/60], Iter [16/633], LR: 0.005000, Loss: 3.6103, top1: 12.5000\n",
      "Epoch [5/60], Iter [17/633], LR: 0.005000, Loss: 3.6270, top1: 6.2500\n",
      "Epoch [5/60], Iter [18/633], LR: 0.005000, Loss: 3.6058, top1: 12.5000\n",
      "Epoch [5/60], Iter [19/633], LR: 0.005000, Loss: 3.6180, top1: 9.3750\n",
      "Epoch [5/60], Iter [20/633], LR: 0.005000, Loss: 3.5925, top1: 17.1875\n",
      "Epoch [5/60], Iter [21/633], LR: 0.005000, Loss: 3.6210, top1: 6.2500\n",
      "Epoch [5/60], Iter [22/633], LR: 0.005000, Loss: 3.6466, top1: 6.2500\n",
      "Epoch [5/60], Iter [23/633], LR: 0.005000, Loss: 3.6282, top1: 9.3750\n",
      "Epoch [5/60], Iter [24/633], LR: 0.005000, Loss: 3.6206, top1: 12.5000\n",
      "Epoch [5/60], Iter [25/633], LR: 0.005000, Loss: 3.6345, top1: 6.2500\n",
      "Epoch [5/60], Iter [26/633], LR: 0.005000, Loss: 3.6389, top1: 9.3750\n",
      "Epoch [5/60], Iter [27/633], LR: 0.005000, Loss: 3.5968, top1: 15.6250\n",
      "Epoch [5/60], Iter [28/633], LR: 0.005000, Loss: 3.6216, top1: 12.5000\n",
      "Epoch [5/60], Iter [29/633], LR: 0.005000, Loss: 3.6487, top1: 6.2500\n",
      "Epoch [5/60], Iter [30/633], LR: 0.005000, Loss: 3.6353, top1: 9.3750\n",
      "Epoch [5/60], Iter [31/633], LR: 0.005000, Loss: 3.5652, top1: 15.6250\n",
      "Epoch [5/60], Iter [32/633], LR: 0.005000, Loss: 3.5874, top1: 17.1875\n",
      "Epoch [5/60], Iter [33/633], LR: 0.005000, Loss: 3.6030, top1: 15.6250\n",
      "Epoch [5/60], Iter [34/633], LR: 0.005000, Loss: 3.6666, top1: 4.6875\n",
      "Epoch [5/60], Iter [35/633], LR: 0.005000, Loss: 3.6306, top1: 9.3750\n",
      "Epoch [5/60], Iter [36/633], LR: 0.005000, Loss: 3.6402, top1: 7.8125\n",
      "Epoch [5/60], Iter [37/633], LR: 0.005000, Loss: 3.6542, top1: 12.5000\n",
      "Epoch [5/60], Iter [38/633], LR: 0.005000, Loss: 3.5625, top1: 14.0625\n",
      "Epoch [5/60], Iter [39/633], LR: 0.005000, Loss: 3.6267, top1: 9.3750\n",
      "Epoch [5/60], Iter [40/633], LR: 0.005000, Loss: 3.6077, top1: 14.0625\n",
      "Epoch [5/60], Iter [41/633], LR: 0.005000, Loss: 3.6230, top1: 15.6250\n",
      "Epoch [5/60], Iter [42/633], LR: 0.005000, Loss: 3.6020, top1: 12.5000\n",
      "Epoch [5/60], Iter [43/633], LR: 0.005000, Loss: 3.6116, top1: 14.0625\n",
      "Epoch [5/60], Iter [44/633], LR: 0.005000, Loss: 3.6130, top1: 7.8125\n",
      "Epoch [5/60], Iter [45/633], LR: 0.005000, Loss: 3.6488, top1: 10.9375\n",
      "Epoch [5/60], Iter [46/633], LR: 0.005000, Loss: 3.6461, top1: 9.3750\n",
      "Epoch [5/60], Iter [47/633], LR: 0.005000, Loss: 3.6186, top1: 4.6875\n",
      "Epoch [5/60], Iter [48/633], LR: 0.005000, Loss: 3.5845, top1: 17.1875\n",
      "Epoch [5/60], Iter [49/633], LR: 0.005000, Loss: 3.6230, top1: 9.3750\n",
      "Epoch [5/60], Iter [50/633], LR: 0.005000, Loss: 3.6332, top1: 10.9375\n",
      "Epoch [5/60], Iter [51/633], LR: 0.005000, Loss: 3.6407, top1: 12.5000\n",
      "Epoch [5/60], Iter [52/633], LR: 0.005000, Loss: 3.6535, top1: 7.8125\n",
      "Epoch [5/60], Iter [53/633], LR: 0.005000, Loss: 3.6295, top1: 9.3750\n",
      "Epoch [5/60], Iter [54/633], LR: 0.005000, Loss: 3.6320, top1: 3.1250\n",
      "Epoch [5/60], Iter [55/633], LR: 0.005000, Loss: 3.6445, top1: 9.3750\n",
      "Epoch [5/60], Iter [56/633], LR: 0.005000, Loss: 3.6356, top1: 12.5000\n",
      "Epoch [5/60], Iter [57/633], LR: 0.005000, Loss: 3.6207, top1: 9.3750\n",
      "Epoch [5/60], Iter [58/633], LR: 0.005000, Loss: 3.5955, top1: 17.1875\n",
      "Epoch [5/60], Iter [59/633], LR: 0.005000, Loss: 3.6377, top1: 10.9375\n",
      "Epoch [5/60], Iter [60/633], LR: 0.005000, Loss: 3.5971, top1: 14.0625\n",
      "Epoch [5/60], Iter [61/633], LR: 0.005000, Loss: 3.5750, top1: 15.6250\n",
      "Epoch [5/60], Iter [62/633], LR: 0.005000, Loss: 3.6199, top1: 9.3750\n",
      "Epoch [5/60], Iter [63/633], LR: 0.005000, Loss: 3.6062, top1: 12.5000\n",
      "Epoch [5/60], Iter [64/633], LR: 0.005000, Loss: 3.5921, top1: 18.7500\n",
      "Epoch [5/60], Iter [65/633], LR: 0.005000, Loss: 3.6237, top1: 10.9375\n",
      "Epoch [5/60], Iter [66/633], LR: 0.005000, Loss: 3.6292, top1: 6.2500\n",
      "Epoch [5/60], Iter [67/633], LR: 0.005000, Loss: 3.6381, top1: 7.8125\n",
      "Epoch [5/60], Iter [68/633], LR: 0.005000, Loss: 3.6142, top1: 9.3750\n",
      "Epoch [5/60], Iter [69/633], LR: 0.005000, Loss: 3.5496, top1: 20.3125\n",
      "Epoch [5/60], Iter [70/633], LR: 0.005000, Loss: 3.6467, top1: 6.2500\n",
      "Epoch [5/60], Iter [71/633], LR: 0.005000, Loss: 3.6509, top1: 9.3750\n",
      "Epoch [5/60], Iter [72/633], LR: 0.005000, Loss: 3.6317, top1: 6.2500\n",
      "Epoch [5/60], Iter [73/633], LR: 0.005000, Loss: 3.6170, top1: 12.5000\n",
      "Epoch [5/60], Iter [74/633], LR: 0.005000, Loss: 3.5620, top1: 14.0625\n",
      "Epoch [5/60], Iter [75/633], LR: 0.005000, Loss: 3.6499, top1: 7.8125\n",
      "Epoch [5/60], Iter [76/633], LR: 0.005000, Loss: 3.5729, top1: 12.5000\n",
      "Epoch [5/60], Iter [77/633], LR: 0.005000, Loss: 3.5902, top1: 10.9375\n",
      "Epoch [5/60], Iter [78/633], LR: 0.005000, Loss: 3.6473, top1: 9.3750\n",
      "Epoch [5/60], Iter [79/633], LR: 0.005000, Loss: 3.6462, top1: 7.8125\n",
      "Epoch [5/60], Iter [80/633], LR: 0.005000, Loss: 3.6224, top1: 9.3750\n",
      "Epoch [5/60], Iter [81/633], LR: 0.005000, Loss: 3.6056, top1: 9.3750\n",
      "Epoch [5/60], Iter [82/633], LR: 0.005000, Loss: 3.6242, top1: 9.3750\n",
      "Epoch [5/60], Iter [83/633], LR: 0.005000, Loss: 3.6373, top1: 9.3750\n",
      "Epoch [5/60], Iter [84/633], LR: 0.005000, Loss: 3.6259, top1: 10.9375\n",
      "Epoch [5/60], Iter [85/633], LR: 0.005000, Loss: 3.5961, top1: 9.3750\n",
      "Epoch [5/60], Iter [86/633], LR: 0.005000, Loss: 3.6416, top1: 14.0625\n",
      "Epoch [5/60], Iter [87/633], LR: 0.005000, Loss: 3.5993, top1: 14.0625\n",
      "Epoch [5/60], Iter [88/633], LR: 0.005000, Loss: 3.5743, top1: 17.1875\n",
      "Epoch [5/60], Iter [89/633], LR: 0.005000, Loss: 3.6542, top1: 6.2500\n",
      "Epoch [5/60], Iter [90/633], LR: 0.005000, Loss: 3.5722, top1: 15.6250\n",
      "Epoch [5/60], Iter [91/633], LR: 0.005000, Loss: 3.6264, top1: 9.3750\n",
      "Epoch [5/60], Iter [92/633], LR: 0.005000, Loss: 3.6067, top1: 15.6250\n",
      "Epoch [5/60], Iter [93/633], LR: 0.005000, Loss: 3.5856, top1: 9.3750\n",
      "Epoch [5/60], Iter [94/633], LR: 0.005000, Loss: 3.6098, top1: 12.5000\n",
      "Epoch [5/60], Iter [95/633], LR: 0.005000, Loss: 3.6350, top1: 6.2500\n",
      "Epoch [5/60], Iter [96/633], LR: 0.005000, Loss: 3.6099, top1: 14.0625\n",
      "Epoch [5/60], Iter [97/633], LR: 0.005000, Loss: 3.5801, top1: 12.5000\n",
      "Epoch [5/60], Iter [98/633], LR: 0.005000, Loss: 3.6134, top1: 10.9375\n",
      "Epoch [5/60], Iter [99/633], LR: 0.005000, Loss: 3.6355, top1: 7.8125\n",
      "Epoch [5/60], Iter [100/633], LR: 0.005000, Loss: 3.6011, top1: 17.1875\n",
      "Epoch [5/60], Iter [101/633], LR: 0.005000, Loss: 3.6236, top1: 10.9375\n",
      "Epoch [5/60], Iter [102/633], LR: 0.005000, Loss: 3.6249, top1: 7.8125\n",
      "Epoch [5/60], Iter [103/633], LR: 0.005000, Loss: 3.6015, top1: 18.7500\n",
      "Epoch [5/60], Iter [104/633], LR: 0.005000, Loss: 3.6456, top1: 9.3750\n",
      "Epoch [5/60], Iter [105/633], LR: 0.005000, Loss: 3.6205, top1: 6.2500\n",
      "Epoch [5/60], Iter [106/633], LR: 0.005000, Loss: 3.6343, top1: 4.6875\n",
      "Epoch [5/60], Iter [107/633], LR: 0.005000, Loss: 3.6170, top1: 7.8125\n",
      "Epoch [5/60], Iter [108/633], LR: 0.005000, Loss: 3.6131, top1: 17.1875\n",
      "Epoch [5/60], Iter [109/633], LR: 0.005000, Loss: 3.5963, top1: 9.3750\n",
      "Epoch [5/60], Iter [110/633], LR: 0.005000, Loss: 3.5873, top1: 12.5000\n",
      "Epoch [5/60], Iter [111/633], LR: 0.005000, Loss: 3.6143, top1: 12.5000\n",
      "Epoch [5/60], Iter [112/633], LR: 0.005000, Loss: 3.6550, top1: 9.3750\n",
      "Epoch [5/60], Iter [113/633], LR: 0.005000, Loss: 3.6034, top1: 14.0625\n",
      "Epoch [5/60], Iter [114/633], LR: 0.005000, Loss: 3.5807, top1: 12.5000\n",
      "Epoch [5/60], Iter [115/633], LR: 0.005000, Loss: 3.6224, top1: 10.9375\n",
      "Epoch [5/60], Iter [116/633], LR: 0.005000, Loss: 3.6148, top1: 10.9375\n",
      "Epoch [5/60], Iter [117/633], LR: 0.005000, Loss: 3.6145, top1: 7.8125\n",
      "Epoch [5/60], Iter [118/633], LR: 0.005000, Loss: 3.6496, top1: 7.8125\n",
      "Epoch [5/60], Iter [119/633], LR: 0.005000, Loss: 3.6098, top1: 15.6250\n",
      "Epoch [5/60], Iter [120/633], LR: 0.005000, Loss: 3.6070, top1: 12.5000\n",
      "Epoch [5/60], Iter [121/633], LR: 0.005000, Loss: 3.6070, top1: 9.3750\n",
      "Epoch [5/60], Iter [122/633], LR: 0.005000, Loss: 3.5823, top1: 18.7500\n",
      "Epoch [5/60], Iter [123/633], LR: 0.005000, Loss: 3.6133, top1: 7.8125\n",
      "Epoch [5/60], Iter [124/633], LR: 0.005000, Loss: 3.6088, top1: 12.5000\n",
      "Epoch [5/60], Iter [125/633], LR: 0.005000, Loss: 3.6550, top1: 6.2500\n",
      "Epoch [5/60], Iter [126/633], LR: 0.005000, Loss: 3.5972, top1: 14.0625\n",
      "Epoch [5/60], Iter [127/633], LR: 0.005000, Loss: 3.6345, top1: 7.8125\n",
      "Epoch [5/60], Iter [128/633], LR: 0.005000, Loss: 3.6336, top1: 10.9375\n",
      "Epoch [5/60], Iter [129/633], LR: 0.005000, Loss: 3.6167, top1: 10.9375\n",
      "Epoch [5/60], Iter [130/633], LR: 0.005000, Loss: 3.6154, top1: 15.6250\n",
      "Epoch [5/60], Iter [131/633], LR: 0.005000, Loss: 3.6199, top1: 7.8125\n",
      "Epoch [5/60], Iter [132/633], LR: 0.005000, Loss: 3.6277, top1: 9.3750\n",
      "Epoch [5/60], Iter [133/633], LR: 0.005000, Loss: 3.6206, top1: 12.5000\n",
      "Epoch [5/60], Iter [134/633], LR: 0.005000, Loss: 3.5674, top1: 21.8750\n",
      "Epoch [5/60], Iter [135/633], LR: 0.005000, Loss: 3.6552, top1: 7.8125\n",
      "Epoch [5/60], Iter [136/633], LR: 0.005000, Loss: 3.5654, top1: 18.7500\n",
      "Epoch [5/60], Iter [137/633], LR: 0.005000, Loss: 3.6323, top1: 9.3750\n",
      "Epoch [5/60], Iter [138/633], LR: 0.005000, Loss: 3.5716, top1: 17.1875\n",
      "Epoch [5/60], Iter [139/633], LR: 0.005000, Loss: 3.6446, top1: 4.6875\n",
      "Epoch [5/60], Iter [140/633], LR: 0.005000, Loss: 3.6086, top1: 15.6250\n",
      "Epoch [5/60], Iter [141/633], LR: 0.005000, Loss: 3.5843, top1: 14.0625\n",
      "Epoch [5/60], Iter [142/633], LR: 0.005000, Loss: 3.6365, top1: 7.8125\n",
      "Epoch [5/60], Iter [143/633], LR: 0.005000, Loss: 3.6354, top1: 7.8125\n",
      "Epoch [5/60], Iter [144/633], LR: 0.005000, Loss: 3.6262, top1: 7.8125\n",
      "Epoch [5/60], Iter [145/633], LR: 0.005000, Loss: 3.6256, top1: 10.9375\n",
      "Epoch [5/60], Iter [146/633], LR: 0.005000, Loss: 3.5960, top1: 17.1875\n",
      "Epoch [5/60], Iter [147/633], LR: 0.005000, Loss: 3.5805, top1: 15.6250\n",
      "Epoch [5/60], Iter [148/633], LR: 0.005000, Loss: 3.6222, top1: 12.5000\n",
      "Epoch [5/60], Iter [149/633], LR: 0.005000, Loss: 3.5907, top1: 15.6250\n",
      "Epoch [5/60], Iter [150/633], LR: 0.005000, Loss: 3.5769, top1: 15.6250\n",
      "Epoch [5/60], Iter [151/633], LR: 0.005000, Loss: 3.6239, top1: 10.9375\n",
      "Epoch [5/60], Iter [152/633], LR: 0.005000, Loss: 3.6078, top1: 7.8125\n",
      "Epoch [5/60], Iter [153/633], LR: 0.005000, Loss: 3.6426, top1: 12.5000\n",
      "Epoch [5/60], Iter [154/633], LR: 0.005000, Loss: 3.6119, top1: 14.0625\n",
      "Epoch [5/60], Iter [155/633], LR: 0.005000, Loss: 3.6275, top1: 12.5000\n",
      "Epoch [5/60], Iter [156/633], LR: 0.005000, Loss: 3.6447, top1: 12.5000\n",
      "Epoch [5/60], Iter [157/633], LR: 0.005000, Loss: 3.6047, top1: 9.3750\n",
      "Epoch [5/60], Iter [158/633], LR: 0.005000, Loss: 3.6123, top1: 9.3750\n",
      "Epoch [5/60], Iter [159/633], LR: 0.005000, Loss: 3.6149, top1: 15.6250\n",
      "Epoch [5/60], Iter [160/633], LR: 0.005000, Loss: 3.5924, top1: 18.7500\n",
      "Epoch [5/60], Iter [161/633], LR: 0.005000, Loss: 3.6446, top1: 10.9375\n",
      "Epoch [5/60], Iter [162/633], LR: 0.005000, Loss: 3.6167, top1: 14.0625\n",
      "Epoch [5/60], Iter [163/633], LR: 0.005000, Loss: 3.6036, top1: 15.6250\n",
      "Epoch [5/60], Iter [164/633], LR: 0.005000, Loss: 3.6008, top1: 17.1875\n",
      "Epoch [5/60], Iter [165/633], LR: 0.005000, Loss: 3.5678, top1: 17.1875\n",
      "Epoch [5/60], Iter [166/633], LR: 0.005000, Loss: 3.5892, top1: 9.3750\n",
      "Epoch [5/60], Iter [167/633], LR: 0.005000, Loss: 3.6121, top1: 6.2500\n",
      "Epoch [5/60], Iter [168/633], LR: 0.005000, Loss: 3.6514, top1: 9.3750\n",
      "Epoch [5/60], Iter [169/633], LR: 0.005000, Loss: 3.5811, top1: 15.6250\n",
      "Epoch [5/60], Iter [170/633], LR: 0.005000, Loss: 3.6446, top1: 9.3750\n",
      "Epoch [5/60], Iter [171/633], LR: 0.005000, Loss: 3.5981, top1: 14.0625\n",
      "Epoch [5/60], Iter [172/633], LR: 0.005000, Loss: 3.6069, top1: 12.5000\n",
      "Epoch [5/60], Iter [173/633], LR: 0.005000, Loss: 3.6067, top1: 10.9375\n",
      "Epoch [5/60], Iter [174/633], LR: 0.005000, Loss: 3.5727, top1: 15.6250\n",
      "Epoch [5/60], Iter [175/633], LR: 0.005000, Loss: 3.6266, top1: 15.6250\n",
      "Epoch [5/60], Iter [176/633], LR: 0.005000, Loss: 3.6299, top1: 12.5000\n",
      "Epoch [5/60], Iter [177/633], LR: 0.005000, Loss: 3.5942, top1: 14.0625\n",
      "Epoch [5/60], Iter [178/633], LR: 0.005000, Loss: 3.6170, top1: 10.9375\n",
      "Epoch [5/60], Iter [179/633], LR: 0.005000, Loss: 3.6591, top1: 7.8125\n",
      "Epoch [5/60], Iter [180/633], LR: 0.005000, Loss: 3.6705, top1: 4.6875\n",
      "Epoch [5/60], Iter [181/633], LR: 0.005000, Loss: 3.6501, top1: 7.8125\n",
      "Epoch [5/60], Iter [182/633], LR: 0.005000, Loss: 3.6256, top1: 7.8125\n",
      "Epoch [5/60], Iter [183/633], LR: 0.005000, Loss: 3.6747, top1: 6.2500\n",
      "Epoch [5/60], Iter [184/633], LR: 0.005000, Loss: 3.5731, top1: 17.1875\n",
      "Epoch [5/60], Iter [185/633], LR: 0.005000, Loss: 3.6115, top1: 12.5000\n",
      "Epoch [5/60], Iter [186/633], LR: 0.005000, Loss: 3.6453, top1: 4.6875\n",
      "Epoch [5/60], Iter [187/633], LR: 0.005000, Loss: 3.6522, top1: 9.3750\n",
      "Epoch [5/60], Iter [188/633], LR: 0.005000, Loss: 3.6158, top1: 12.5000\n",
      "Epoch [5/60], Iter [189/633], LR: 0.005000, Loss: 3.5861, top1: 7.8125\n",
      "Epoch [5/60], Iter [190/633], LR: 0.005000, Loss: 3.6174, top1: 7.8125\n",
      "Epoch [5/60], Iter [191/633], LR: 0.005000, Loss: 3.5843, top1: 15.6250\n",
      "Epoch [5/60], Iter [192/633], LR: 0.005000, Loss: 3.6215, top1: 10.9375\n",
      "Epoch [5/60], Iter [193/633], LR: 0.005000, Loss: 3.6599, top1: 3.1250\n",
      "Epoch [5/60], Iter [194/633], LR: 0.005000, Loss: 3.5835, top1: 17.1875\n",
      "Epoch [5/60], Iter [195/633], LR: 0.005000, Loss: 3.6254, top1: 12.5000\n",
      "Epoch [5/60], Iter [196/633], LR: 0.005000, Loss: 3.6093, top1: 7.8125\n",
      "Epoch [5/60], Iter [197/633], LR: 0.005000, Loss: 3.6257, top1: 12.5000\n",
      "Epoch [5/60], Iter [198/633], LR: 0.005000, Loss: 3.5772, top1: 15.6250\n",
      "Epoch [5/60], Iter [199/633], LR: 0.005000, Loss: 3.6518, top1: 6.2500\n",
      "Epoch [5/60], Iter [200/633], LR: 0.005000, Loss: 3.5977, top1: 14.0625\n",
      "Epoch [5/60], Iter [201/633], LR: 0.005000, Loss: 3.6225, top1: 10.9375\n",
      "Epoch [5/60], Iter [202/633], LR: 0.005000, Loss: 3.6019, top1: 10.9375\n",
      "Epoch [5/60], Iter [203/633], LR: 0.005000, Loss: 3.6240, top1: 14.0625\n",
      "Epoch [5/60], Iter [204/633], LR: 0.005000, Loss: 3.5993, top1: 9.3750\n",
      "Epoch [5/60], Iter [205/633], LR: 0.005000, Loss: 3.6240, top1: 7.8125\n",
      "Epoch [5/60], Iter [206/633], LR: 0.005000, Loss: 3.6041, top1: 15.6250\n",
      "Epoch [5/60], Iter [207/633], LR: 0.005000, Loss: 3.6135, top1: 6.2500\n",
      "Epoch [5/60], Iter [208/633], LR: 0.005000, Loss: 3.5844, top1: 18.7500\n",
      "Epoch [5/60], Iter [209/633], LR: 0.005000, Loss: 3.6329, top1: 10.9375\n",
      "Epoch [5/60], Iter [210/633], LR: 0.005000, Loss: 3.6044, top1: 10.9375\n",
      "Epoch [5/60], Iter [211/633], LR: 0.005000, Loss: 3.6249, top1: 4.6875\n",
      "Epoch [5/60], Iter [212/633], LR: 0.005000, Loss: 3.6390, top1: 6.2500\n",
      "Epoch [5/60], Iter [213/633], LR: 0.005000, Loss: 3.6367, top1: 9.3750\n",
      "Epoch [5/60], Iter [214/633], LR: 0.005000, Loss: 3.5817, top1: 17.1875\n",
      "Epoch [5/60], Iter [215/633], LR: 0.005000, Loss: 3.6295, top1: 6.2500\n",
      "Epoch [5/60], Iter [216/633], LR: 0.005000, Loss: 3.6242, top1: 10.9375\n",
      "Epoch [5/60], Iter [217/633], LR: 0.005000, Loss: 3.5696, top1: 17.1875\n",
      "Epoch [5/60], Iter [218/633], LR: 0.005000, Loss: 3.6222, top1: 10.9375\n",
      "Epoch [5/60], Iter [219/633], LR: 0.005000, Loss: 3.6165, top1: 12.5000\n",
      "Epoch [5/60], Iter [220/633], LR: 0.005000, Loss: 3.6239, top1: 7.8125\n",
      "Epoch [5/60], Iter [221/633], LR: 0.005000, Loss: 3.6271, top1: 7.8125\n",
      "Epoch [5/60], Iter [222/633], LR: 0.005000, Loss: 3.5917, top1: 15.6250\n",
      "Epoch [5/60], Iter [223/633], LR: 0.005000, Loss: 3.5983, top1: 10.9375\n",
      "Epoch [5/60], Iter [224/633], LR: 0.005000, Loss: 3.6471, top1: 6.2500\n",
      "Epoch [5/60], Iter [225/633], LR: 0.005000, Loss: 3.5812, top1: 15.6250\n",
      "Epoch [5/60], Iter [226/633], LR: 0.005000, Loss: 3.6027, top1: 9.3750\n",
      "Epoch [5/60], Iter [227/633], LR: 0.005000, Loss: 3.6181, top1: 10.9375\n",
      "Epoch [5/60], Iter [228/633], LR: 0.005000, Loss: 3.6231, top1: 9.3750\n",
      "Epoch [5/60], Iter [229/633], LR: 0.005000, Loss: 3.6082, top1: 17.1875\n",
      "Epoch [5/60], Iter [230/633], LR: 0.005000, Loss: 3.6138, top1: 6.2500\n",
      "Epoch [5/60], Iter [231/633], LR: 0.005000, Loss: 3.6042, top1: 18.7500\n",
      "Epoch [5/60], Iter [232/633], LR: 0.005000, Loss: 3.6395, top1: 7.8125\n",
      "Epoch [5/60], Iter [233/633], LR: 0.005000, Loss: 3.6161, top1: 6.2500\n",
      "Epoch [5/60], Iter [234/633], LR: 0.005000, Loss: 3.6162, top1: 7.8125\n",
      "Epoch [5/60], Iter [235/633], LR: 0.005000, Loss: 3.6662, top1: 7.8125\n",
      "Epoch [5/60], Iter [236/633], LR: 0.005000, Loss: 3.6162, top1: 14.0625\n",
      "Epoch [5/60], Iter [237/633], LR: 0.005000, Loss: 3.6135, top1: 7.8125\n",
      "Epoch [5/60], Iter [238/633], LR: 0.005000, Loss: 3.6373, top1: 7.8125\n",
      "Epoch [5/60], Iter [239/633], LR: 0.005000, Loss: 3.6267, top1: 4.6875\n",
      "Epoch [5/60], Iter [240/633], LR: 0.005000, Loss: 3.5855, top1: 18.7500\n",
      "Epoch [5/60], Iter [241/633], LR: 0.005000, Loss: 3.5897, top1: 14.0625\n",
      "Epoch [5/60], Iter [242/633], LR: 0.005000, Loss: 3.6471, top1: 7.8125\n",
      "Epoch [5/60], Iter [243/633], LR: 0.005000, Loss: 3.6041, top1: 15.6250\n",
      "Epoch [5/60], Iter [244/633], LR: 0.005000, Loss: 3.5884, top1: 12.5000\n",
      "Epoch [5/60], Iter [245/633], LR: 0.005000, Loss: 3.6176, top1: 6.2500\n",
      "Epoch [5/60], Iter [246/633], LR: 0.005000, Loss: 3.6025, top1: 15.6250\n",
      "Epoch [5/60], Iter [247/633], LR: 0.005000, Loss: 3.6239, top1: 12.5000\n",
      "Epoch [5/60], Iter [248/633], LR: 0.005000, Loss: 3.6081, top1: 9.3750\n",
      "Epoch [5/60], Iter [249/633], LR: 0.005000, Loss: 3.6096, top1: 12.5000\n",
      "Epoch [5/60], Iter [250/633], LR: 0.005000, Loss: 3.6565, top1: 9.3750\n",
      "Epoch [5/60], Iter [251/633], LR: 0.005000, Loss: 3.5602, top1: 17.1875\n",
      "Epoch [5/60], Iter [252/633], LR: 0.005000, Loss: 3.6451, top1: 7.8125\n",
      "Epoch [5/60], Iter [253/633], LR: 0.005000, Loss: 3.5644, top1: 20.3125\n",
      "Epoch [5/60], Iter [254/633], LR: 0.005000, Loss: 3.6235, top1: 7.8125\n",
      "Epoch [5/60], Iter [255/633], LR: 0.005000, Loss: 3.5846, top1: 15.6250\n",
      "Epoch [5/60], Iter [256/633], LR: 0.005000, Loss: 3.5901, top1: 12.5000\n",
      "Epoch [5/60], Iter [257/633], LR: 0.005000, Loss: 3.5980, top1: 10.9375\n",
      "Epoch [5/60], Iter [258/633], LR: 0.005000, Loss: 3.5675, top1: 17.1875\n",
      "Epoch [5/60], Iter [259/633], LR: 0.005000, Loss: 3.6417, top1: 9.3750\n",
      "Epoch [5/60], Iter [260/633], LR: 0.005000, Loss: 3.5964, top1: 20.3125\n",
      "Epoch [5/60], Iter [261/633], LR: 0.005000, Loss: 3.6240, top1: 9.3750\n",
      "Epoch [5/60], Iter [262/633], LR: 0.005000, Loss: 3.6332, top1: 4.6875\n",
      "Epoch [5/60], Iter [263/633], LR: 0.005000, Loss: 3.5965, top1: 12.5000\n",
      "Epoch [5/60], Iter [264/633], LR: 0.005000, Loss: 3.6126, top1: 10.9375\n",
      "Epoch [5/60], Iter [265/633], LR: 0.005000, Loss: 3.6122, top1: 7.8125\n",
      "Epoch [5/60], Iter [266/633], LR: 0.005000, Loss: 3.5820, top1: 14.0625\n",
      "Epoch [5/60], Iter [267/633], LR: 0.005000, Loss: 3.6176, top1: 7.8125\n",
      "Epoch [5/60], Iter [268/633], LR: 0.005000, Loss: 3.5881, top1: 18.7500\n",
      "Epoch [5/60], Iter [269/633], LR: 0.005000, Loss: 3.6415, top1: 7.8125\n",
      "Epoch [5/60], Iter [270/633], LR: 0.005000, Loss: 3.5744, top1: 14.0625\n",
      "Epoch [5/60], Iter [271/633], LR: 0.005000, Loss: 3.6182, top1: 12.5000\n",
      "Epoch [5/60], Iter [272/633], LR: 0.005000, Loss: 3.5916, top1: 14.0625\n",
      "Epoch [5/60], Iter [273/633], LR: 0.005000, Loss: 3.6199, top1: 12.5000\n",
      "Epoch [5/60], Iter [274/633], LR: 0.005000, Loss: 3.6075, top1: 10.9375\n",
      "Epoch [5/60], Iter [275/633], LR: 0.005000, Loss: 3.6109, top1: 6.2500\n",
      "Epoch [5/60], Iter [276/633], LR: 0.005000, Loss: 3.6112, top1: 10.9375\n",
      "Epoch [5/60], Iter [277/633], LR: 0.005000, Loss: 3.6429, top1: 7.8125\n",
      "Epoch [5/60], Iter [278/633], LR: 0.005000, Loss: 3.6412, top1: 7.8125\n",
      "Epoch [5/60], Iter [279/633], LR: 0.005000, Loss: 3.6029, top1: 10.9375\n",
      "Epoch [5/60], Iter [280/633], LR: 0.005000, Loss: 3.6045, top1: 9.3750\n",
      "Epoch [5/60], Iter [281/633], LR: 0.005000, Loss: 3.6408, top1: 10.9375\n",
      "Epoch [5/60], Iter [282/633], LR: 0.005000, Loss: 3.6047, top1: 9.3750\n",
      "Epoch [5/60], Iter [283/633], LR: 0.005000, Loss: 3.6274, top1: 7.8125\n",
      "Epoch [5/60], Iter [284/633], LR: 0.005000, Loss: 3.6423, top1: 1.5625\n",
      "Epoch [5/60], Iter [285/633], LR: 0.005000, Loss: 3.6101, top1: 12.5000\n",
      "Epoch [5/60], Iter [286/633], LR: 0.005000, Loss: 3.5946, top1: 17.1875\n",
      "Epoch [5/60], Iter [287/633], LR: 0.005000, Loss: 3.6063, top1: 7.8125\n",
      "Epoch [5/60], Iter [288/633], LR: 0.005000, Loss: 3.6020, top1: 12.5000\n",
      "Epoch [5/60], Iter [289/633], LR: 0.005000, Loss: 3.6326, top1: 10.9375\n",
      "Epoch [5/60], Iter [290/633], LR: 0.005000, Loss: 3.5895, top1: 12.5000\n",
      "Epoch [5/60], Iter [291/633], LR: 0.005000, Loss: 3.6145, top1: 15.6250\n",
      "Epoch [5/60], Iter [292/633], LR: 0.005000, Loss: 3.6337, top1: 10.9375\n",
      "Epoch [5/60], Iter [293/633], LR: 0.005000, Loss: 3.6068, top1: 7.8125\n",
      "Epoch [5/60], Iter [294/633], LR: 0.005000, Loss: 3.6461, top1: 14.0625\n",
      "Epoch [5/60], Iter [295/633], LR: 0.005000, Loss: 3.6173, top1: 14.0625\n",
      "Epoch [5/60], Iter [296/633], LR: 0.005000, Loss: 3.6090, top1: 10.9375\n",
      "Epoch [5/60], Iter [297/633], LR: 0.005000, Loss: 3.5855, top1: 14.0625\n",
      "Epoch [5/60], Iter [298/633], LR: 0.005000, Loss: 3.6204, top1: 7.8125\n",
      "Epoch [5/60], Iter [299/633], LR: 0.005000, Loss: 3.6696, top1: 6.2500\n",
      "Epoch [5/60], Iter [300/633], LR: 0.005000, Loss: 3.6417, top1: 6.2500\n",
      "Epoch [5/60], Iter [301/633], LR: 0.005000, Loss: 3.6129, top1: 9.3750\n",
      "Epoch [5/60], Iter [302/633], LR: 0.005000, Loss: 3.5845, top1: 14.0625\n",
      "Epoch [5/60], Iter [303/633], LR: 0.005000, Loss: 3.6321, top1: 10.9375\n",
      "Epoch [5/60], Iter [304/633], LR: 0.005000, Loss: 3.6393, top1: 12.5000\n",
      "Epoch [5/60], Iter [305/633], LR: 0.005000, Loss: 3.5867, top1: 12.5000\n",
      "Epoch [5/60], Iter [306/633], LR: 0.005000, Loss: 3.6337, top1: 7.8125\n",
      "Epoch [5/60], Iter [307/633], LR: 0.005000, Loss: 3.6335, top1: 10.9375\n",
      "Epoch [5/60], Iter [308/633], LR: 0.005000, Loss: 3.5781, top1: 14.0625\n",
      "Epoch [5/60], Iter [309/633], LR: 0.005000, Loss: 3.6536, top1: 7.8125\n",
      "Epoch [5/60], Iter [310/633], LR: 0.005000, Loss: 3.5980, top1: 12.5000\n",
      "Epoch [5/60], Iter [311/633], LR: 0.005000, Loss: 3.5945, top1: 12.5000\n",
      "Epoch [5/60], Iter [312/633], LR: 0.005000, Loss: 3.6148, top1: 1.5625\n",
      "Epoch [5/60], Iter [313/633], LR: 0.005000, Loss: 3.5860, top1: 17.1875\n",
      "Epoch [5/60], Iter [314/633], LR: 0.005000, Loss: 3.5843, top1: 9.3750\n",
      "Epoch [5/60], Iter [315/633], LR: 0.005000, Loss: 3.6304, top1: 9.3750\n",
      "Epoch [5/60], Iter [316/633], LR: 0.005000, Loss: 3.6339, top1: 12.5000\n",
      "Epoch [5/60], Iter [317/633], LR: 0.005000, Loss: 3.6627, top1: 12.5000\n",
      "Epoch [5/60], Iter [318/633], LR: 0.005000, Loss: 3.6127, top1: 12.5000\n",
      "Epoch [5/60], Iter [319/633], LR: 0.005000, Loss: 3.6007, top1: 15.6250\n",
      "Epoch [5/60], Iter [320/633], LR: 0.005000, Loss: 3.6179, top1: 7.8125\n",
      "Epoch [5/60], Iter [321/633], LR: 0.005000, Loss: 3.6167, top1: 9.3750\n",
      "Epoch [5/60], Iter [322/633], LR: 0.005000, Loss: 3.6187, top1: 6.2500\n",
      "Epoch [5/60], Iter [323/633], LR: 0.005000, Loss: 3.5996, top1: 12.5000\n",
      "Epoch [5/60], Iter [324/633], LR: 0.005000, Loss: 3.6211, top1: 9.3750\n",
      "Epoch [5/60], Iter [325/633], LR: 0.005000, Loss: 3.6026, top1: 12.5000\n",
      "Epoch [5/60], Iter [326/633], LR: 0.005000, Loss: 3.5896, top1: 17.1875\n",
      "Epoch [5/60], Iter [327/633], LR: 0.005000, Loss: 3.6186, top1: 9.3750\n",
      "Epoch [5/60], Iter [328/633], LR: 0.005000, Loss: 3.6186, top1: 9.3750\n",
      "Epoch [5/60], Iter [329/633], LR: 0.005000, Loss: 3.6293, top1: 9.3750\n",
      "Epoch [5/60], Iter [330/633], LR: 0.005000, Loss: 3.5870, top1: 15.6250\n",
      "Epoch [5/60], Iter [331/633], LR: 0.005000, Loss: 3.6325, top1: 10.9375\n",
      "Epoch [5/60], Iter [332/633], LR: 0.005000, Loss: 3.6505, top1: 6.2500\n",
      "Epoch [5/60], Iter [333/633], LR: 0.005000, Loss: 3.6358, top1: 4.6875\n",
      "Epoch [5/60], Iter [334/633], LR: 0.005000, Loss: 3.6374, top1: 4.6875\n",
      "Epoch [5/60], Iter [335/633], LR: 0.005000, Loss: 3.5595, top1: 21.8750\n",
      "Epoch [5/60], Iter [336/633], LR: 0.005000, Loss: 3.6121, top1: 6.2500\n",
      "Epoch [5/60], Iter [337/633], LR: 0.005000, Loss: 3.6520, top1: 3.1250\n",
      "Epoch [5/60], Iter [338/633], LR: 0.005000, Loss: 3.5836, top1: 12.5000\n",
      "Epoch [5/60], Iter [339/633], LR: 0.005000, Loss: 3.6175, top1: 6.2500\n",
      "Epoch [5/60], Iter [340/633], LR: 0.005000, Loss: 3.5716, top1: 18.7500\n",
      "Epoch [5/60], Iter [341/633], LR: 0.005000, Loss: 3.5586, top1: 14.0625\n",
      "Epoch [5/60], Iter [342/633], LR: 0.005000, Loss: 3.6551, top1: 6.2500\n",
      "Epoch [5/60], Iter [343/633], LR: 0.005000, Loss: 3.6162, top1: 9.3750\n",
      "Epoch [5/60], Iter [344/633], LR: 0.005000, Loss: 3.6802, top1: 7.8125\n",
      "Epoch [5/60], Iter [345/633], LR: 0.005000, Loss: 3.6257, top1: 15.6250\n",
      "Epoch [5/60], Iter [346/633], LR: 0.005000, Loss: 3.6011, top1: 15.6250\n",
      "Epoch [5/60], Iter [347/633], LR: 0.005000, Loss: 3.6357, top1: 7.8125\n",
      "Epoch [5/60], Iter [348/633], LR: 0.005000, Loss: 3.6066, top1: 12.5000\n",
      "Epoch [5/60], Iter [349/633], LR: 0.005000, Loss: 3.5948, top1: 9.3750\n",
      "Epoch [5/60], Iter [350/633], LR: 0.005000, Loss: 3.6457, top1: 4.6875\n",
      "Epoch [5/60], Iter [351/633], LR: 0.005000, Loss: 3.5992, top1: 15.6250\n",
      "Epoch [5/60], Iter [352/633], LR: 0.005000, Loss: 3.5943, top1: 10.9375\n",
      "Epoch [5/60], Iter [353/633], LR: 0.005000, Loss: 3.5811, top1: 17.1875\n",
      "Epoch [5/60], Iter [354/633], LR: 0.005000, Loss: 3.6316, top1: 10.9375\n",
      "Epoch [5/60], Iter [355/633], LR: 0.005000, Loss: 3.6308, top1: 12.5000\n",
      "Epoch [5/60], Iter [356/633], LR: 0.005000, Loss: 3.6365, top1: 9.3750\n",
      "Epoch [5/60], Iter [357/633], LR: 0.005000, Loss: 3.6073, top1: 7.8125\n",
      "Epoch [5/60], Iter [358/633], LR: 0.005000, Loss: 3.6379, top1: 9.3750\n",
      "Epoch [5/60], Iter [359/633], LR: 0.005000, Loss: 3.6066, top1: 7.8125\n",
      "Epoch [5/60], Iter [360/633], LR: 0.005000, Loss: 3.6614, top1: 7.8125\n",
      "Epoch [5/60], Iter [361/633], LR: 0.005000, Loss: 3.6187, top1: 4.6875\n",
      "Epoch [5/60], Iter [362/633], LR: 0.005000, Loss: 3.5984, top1: 14.0625\n",
      "Epoch [5/60], Iter [363/633], LR: 0.005000, Loss: 3.6264, top1: 10.9375\n",
      "Epoch [5/60], Iter [364/633], LR: 0.005000, Loss: 3.6458, top1: 9.3750\n",
      "Epoch [5/60], Iter [365/633], LR: 0.005000, Loss: 3.6086, top1: 10.9375\n",
      "Epoch [5/60], Iter [366/633], LR: 0.005000, Loss: 3.5768, top1: 14.0625\n",
      "Epoch [5/60], Iter [367/633], LR: 0.005000, Loss: 3.6125, top1: 10.9375\n",
      "Epoch [5/60], Iter [368/633], LR: 0.005000, Loss: 3.5931, top1: 14.0625\n",
      "Epoch [5/60], Iter [369/633], LR: 0.005000, Loss: 3.6131, top1: 9.3750\n",
      "Epoch [5/60], Iter [370/633], LR: 0.005000, Loss: 3.6306, top1: 10.9375\n",
      "Epoch [5/60], Iter [371/633], LR: 0.005000, Loss: 3.5790, top1: 14.0625\n",
      "Epoch [5/60], Iter [372/633], LR: 0.005000, Loss: 3.6421, top1: 7.8125\n",
      "Epoch [5/60], Iter [373/633], LR: 0.005000, Loss: 3.6376, top1: 12.5000\n",
      "Epoch [5/60], Iter [374/633], LR: 0.005000, Loss: 3.6427, top1: 10.9375\n",
      "Epoch [5/60], Iter [375/633], LR: 0.005000, Loss: 3.6129, top1: 10.9375\n",
      "Epoch [5/60], Iter [376/633], LR: 0.005000, Loss: 3.6134, top1: 14.0625\n",
      "Epoch [5/60], Iter [377/633], LR: 0.005000, Loss: 3.6085, top1: 12.5000\n",
      "Epoch [5/60], Iter [378/633], LR: 0.005000, Loss: 3.5847, top1: 12.5000\n",
      "Epoch [5/60], Iter [379/633], LR: 0.005000, Loss: 3.6259, top1: 10.9375\n",
      "Epoch [5/60], Iter [380/633], LR: 0.005000, Loss: 3.6275, top1: 12.5000\n",
      "Epoch [5/60], Iter [381/633], LR: 0.005000, Loss: 3.6492, top1: 3.1250\n",
      "Epoch [5/60], Iter [382/633], LR: 0.005000, Loss: 3.5826, top1: 15.6250\n",
      "Epoch [5/60], Iter [383/633], LR: 0.005000, Loss: 3.5660, top1: 14.0625\n",
      "Epoch [5/60], Iter [384/633], LR: 0.005000, Loss: 3.5903, top1: 17.1875\n",
      "Epoch [5/60], Iter [385/633], LR: 0.005000, Loss: 3.6217, top1: 12.5000\n",
      "Epoch [5/60], Iter [386/633], LR: 0.005000, Loss: 3.6402, top1: 12.5000\n",
      "Epoch [5/60], Iter [387/633], LR: 0.005000, Loss: 3.5610, top1: 20.3125\n",
      "Epoch [5/60], Iter [388/633], LR: 0.005000, Loss: 3.6010, top1: 14.0625\n",
      "Epoch [5/60], Iter [389/633], LR: 0.005000, Loss: 3.6171, top1: 10.9375\n",
      "Epoch [5/60], Iter [390/633], LR: 0.005000, Loss: 3.6041, top1: 10.9375\n",
      "Epoch [5/60], Iter [391/633], LR: 0.005000, Loss: 3.6118, top1: 7.8125\n",
      "Epoch [5/60], Iter [392/633], LR: 0.005000, Loss: 3.6631, top1: 4.6875\n",
      "Epoch [5/60], Iter [393/633], LR: 0.005000, Loss: 3.6032, top1: 14.0625\n",
      "Epoch [5/60], Iter [394/633], LR: 0.005000, Loss: 3.5666, top1: 14.0625\n",
      "Epoch [5/60], Iter [395/633], LR: 0.005000, Loss: 3.6297, top1: 6.2500\n",
      "Epoch [5/60], Iter [396/633], LR: 0.005000, Loss: 3.6316, top1: 12.5000\n",
      "Epoch [5/60], Iter [397/633], LR: 0.005000, Loss: 3.6320, top1: 9.3750\n",
      "Epoch [5/60], Iter [398/633], LR: 0.005000, Loss: 3.6264, top1: 6.2500\n",
      "Epoch [5/60], Iter [399/633], LR: 0.005000, Loss: 3.6492, top1: 3.1250\n",
      "Epoch [5/60], Iter [400/633], LR: 0.005000, Loss: 3.6165, top1: 6.2500\n",
      "Epoch [5/60], Iter [401/633], LR: 0.005000, Loss: 3.6261, top1: 10.9375\n",
      "Epoch [5/60], Iter [402/633], LR: 0.005000, Loss: 3.5885, top1: 9.3750\n",
      "Epoch [5/60], Iter [403/633], LR: 0.005000, Loss: 3.6091, top1: 7.8125\n",
      "Epoch [5/60], Iter [404/633], LR: 0.005000, Loss: 3.6051, top1: 10.9375\n",
      "Epoch [5/60], Iter [405/633], LR: 0.005000, Loss: 3.6585, top1: 6.2500\n",
      "Epoch [5/60], Iter [406/633], LR: 0.005000, Loss: 3.6341, top1: 7.8125\n",
      "Epoch [5/60], Iter [407/633], LR: 0.005000, Loss: 3.6155, top1: 9.3750\n",
      "Epoch [5/60], Iter [408/633], LR: 0.005000, Loss: 3.6252, top1: 10.9375\n",
      "Epoch [5/60], Iter [409/633], LR: 0.005000, Loss: 3.6267, top1: 4.6875\n",
      "Epoch [5/60], Iter [410/633], LR: 0.005000, Loss: 3.6481, top1: 4.6875\n",
      "Epoch [5/60], Iter [411/633], LR: 0.005000, Loss: 3.6252, top1: 9.3750\n",
      "Epoch [5/60], Iter [412/633], LR: 0.005000, Loss: 3.5721, top1: 12.5000\n",
      "Epoch [5/60], Iter [413/633], LR: 0.005000, Loss: 3.6114, top1: 9.3750\n",
      "Epoch [5/60], Iter [414/633], LR: 0.005000, Loss: 3.5948, top1: 10.9375\n",
      "Epoch [5/60], Iter [415/633], LR: 0.005000, Loss: 3.5990, top1: 6.2500\n",
      "Epoch [5/60], Iter [416/633], LR: 0.005000, Loss: 3.6085, top1: 9.3750\n",
      "Epoch [5/60], Iter [417/633], LR: 0.005000, Loss: 3.6250, top1: 7.8125\n",
      "Epoch [5/60], Iter [418/633], LR: 0.005000, Loss: 3.6260, top1: 12.5000\n",
      "Epoch [5/60], Iter [419/633], LR: 0.005000, Loss: 3.6018, top1: 10.9375\n",
      "Epoch [5/60], Iter [420/633], LR: 0.005000, Loss: 3.6367, top1: 9.3750\n",
      "Epoch [5/60], Iter [421/633], LR: 0.005000, Loss: 3.6178, top1: 9.3750\n",
      "Epoch [5/60], Iter [422/633], LR: 0.005000, Loss: 3.6549, top1: 7.8125\n",
      "Epoch [5/60], Iter [423/633], LR: 0.005000, Loss: 3.6446, top1: 7.8125\n",
      "Epoch [5/60], Iter [424/633], LR: 0.005000, Loss: 3.5819, top1: 12.5000\n",
      "Epoch [5/60], Iter [425/633], LR: 0.005000, Loss: 3.6180, top1: 10.9375\n",
      "Epoch [5/60], Iter [426/633], LR: 0.005000, Loss: 3.6346, top1: 6.2500\n",
      "Epoch [5/60], Iter [427/633], LR: 0.005000, Loss: 3.6333, top1: 9.3750\n",
      "Epoch [5/60], Iter [428/633], LR: 0.005000, Loss: 3.5969, top1: 14.0625\n",
      "Epoch [5/60], Iter [429/633], LR: 0.005000, Loss: 3.5913, top1: 12.5000\n",
      "Epoch [5/60], Iter [430/633], LR: 0.005000, Loss: 3.6491, top1: 6.2500\n",
      "Epoch [5/60], Iter [431/633], LR: 0.005000, Loss: 3.6064, top1: 12.5000\n",
      "Epoch [5/60], Iter [432/633], LR: 0.005000, Loss: 3.6074, top1: 14.0625\n",
      "Epoch [5/60], Iter [433/633], LR: 0.005000, Loss: 3.6173, top1: 14.0625\n",
      "Epoch [5/60], Iter [434/633], LR: 0.005000, Loss: 3.6007, top1: 7.8125\n",
      "Epoch [5/60], Iter [435/633], LR: 0.005000, Loss: 3.6656, top1: 7.8125\n",
      "Epoch [5/60], Iter [436/633], LR: 0.005000, Loss: 3.6447, top1: 14.0625\n",
      "Epoch [5/60], Iter [437/633], LR: 0.005000, Loss: 3.6265, top1: 12.5000\n",
      "Epoch [5/60], Iter [438/633], LR: 0.005000, Loss: 3.5941, top1: 10.9375\n",
      "Epoch [5/60], Iter [439/633], LR: 0.005000, Loss: 3.5825, top1: 10.9375\n",
      "Epoch [5/60], Iter [440/633], LR: 0.005000, Loss: 3.5857, top1: 15.6250\n",
      "Epoch [5/60], Iter [441/633], LR: 0.005000, Loss: 3.5980, top1: 12.5000\n",
      "Epoch [5/60], Iter [442/633], LR: 0.005000, Loss: 3.5665, top1: 17.1875\n",
      "Epoch [5/60], Iter [443/633], LR: 0.005000, Loss: 3.5894, top1: 14.0625\n",
      "Epoch [5/60], Iter [444/633], LR: 0.005000, Loss: 3.5910, top1: 14.0625\n",
      "Epoch [5/60], Iter [445/633], LR: 0.005000, Loss: 3.5912, top1: 14.0625\n",
      "Epoch [5/60], Iter [446/633], LR: 0.005000, Loss: 3.6178, top1: 9.3750\n",
      "Epoch [5/60], Iter [447/633], LR: 0.005000, Loss: 3.6006, top1: 15.6250\n",
      "Epoch [5/60], Iter [448/633], LR: 0.005000, Loss: 3.6325, top1: 6.2500\n",
      "Epoch [5/60], Iter [449/633], LR: 0.005000, Loss: 3.6619, top1: 0.0000\n",
      "Epoch [5/60], Iter [450/633], LR: 0.005000, Loss: 3.6448, top1: 4.6875\n",
      "Epoch [5/60], Iter [451/633], LR: 0.005000, Loss: 3.5890, top1: 14.0625\n",
      "Epoch [5/60], Iter [452/633], LR: 0.005000, Loss: 3.6074, top1: 12.5000\n",
      "Epoch [5/60], Iter [453/633], LR: 0.005000, Loss: 3.6379, top1: 9.3750\n",
      "Epoch [5/60], Iter [454/633], LR: 0.005000, Loss: 3.6136, top1: 9.3750\n",
      "Epoch [5/60], Iter [455/633], LR: 0.005000, Loss: 3.5916, top1: 15.6250\n",
      "Epoch [5/60], Iter [456/633], LR: 0.005000, Loss: 3.6524, top1: 7.8125\n",
      "Epoch [5/60], Iter [457/633], LR: 0.005000, Loss: 3.6024, top1: 10.9375\n",
      "Epoch [5/60], Iter [458/633], LR: 0.005000, Loss: 3.6174, top1: 9.3750\n",
      "Epoch [5/60], Iter [459/633], LR: 0.005000, Loss: 3.5789, top1: 12.5000\n",
      "Epoch [5/60], Iter [460/633], LR: 0.005000, Loss: 3.6110, top1: 14.0625\n",
      "Epoch [5/60], Iter [461/633], LR: 0.005000, Loss: 3.6568, top1: 14.0625\n",
      "Epoch [5/60], Iter [462/633], LR: 0.005000, Loss: 3.6237, top1: 9.3750\n",
      "Epoch [5/60], Iter [463/633], LR: 0.005000, Loss: 3.6302, top1: 9.3750\n",
      "Epoch [5/60], Iter [464/633], LR: 0.005000, Loss: 3.6143, top1: 14.0625\n",
      "Epoch [5/60], Iter [465/633], LR: 0.005000, Loss: 3.6251, top1: 12.5000\n",
      "Epoch [5/60], Iter [466/633], LR: 0.005000, Loss: 3.6328, top1: 7.8125\n",
      "Epoch [5/60], Iter [467/633], LR: 0.005000, Loss: 3.6076, top1: 9.3750\n",
      "Epoch [5/60], Iter [468/633], LR: 0.005000, Loss: 3.6230, top1: 10.9375\n",
      "Epoch [5/60], Iter [469/633], LR: 0.005000, Loss: 3.5978, top1: 12.5000\n",
      "Epoch [5/60], Iter [470/633], LR: 0.005000, Loss: 3.5913, top1: 10.9375\n",
      "Epoch [5/60], Iter [471/633], LR: 0.005000, Loss: 3.5822, top1: 15.6250\n",
      "Epoch [5/60], Iter [472/633], LR: 0.005000, Loss: 3.6358, top1: 12.5000\n",
      "Epoch [5/60], Iter [473/633], LR: 0.005000, Loss: 3.6025, top1: 7.8125\n",
      "Epoch [5/60], Iter [474/633], LR: 0.005000, Loss: 3.6402, top1: 4.6875\n",
      "Epoch [5/60], Iter [475/633], LR: 0.005000, Loss: 3.6479, top1: 4.6875\n",
      "Epoch [5/60], Iter [476/633], LR: 0.005000, Loss: 3.6259, top1: 9.3750\n",
      "Epoch [5/60], Iter [477/633], LR: 0.005000, Loss: 3.6408, top1: 6.2500\n",
      "Epoch [5/60], Iter [478/633], LR: 0.005000, Loss: 3.6170, top1: 14.0625\n",
      "Epoch [5/60], Iter [479/633], LR: 0.005000, Loss: 3.6085, top1: 7.8125\n",
      "Epoch [5/60], Iter [480/633], LR: 0.005000, Loss: 3.5756, top1: 12.5000\n",
      "Epoch [5/60], Iter [481/633], LR: 0.005000, Loss: 3.6469, top1: 6.2500\n",
      "Epoch [5/60], Iter [482/633], LR: 0.005000, Loss: 3.6316, top1: 6.2500\n",
      "Epoch [5/60], Iter [483/633], LR: 0.005000, Loss: 3.6490, top1: 9.3750\n",
      "Epoch [5/60], Iter [484/633], LR: 0.005000, Loss: 3.6030, top1: 7.8125\n",
      "Epoch [5/60], Iter [485/633], LR: 0.005000, Loss: 3.6212, top1: 7.8125\n",
      "Epoch [5/60], Iter [486/633], LR: 0.005000, Loss: 3.5923, top1: 15.6250\n",
      "Epoch [5/60], Iter [487/633], LR: 0.005000, Loss: 3.6693, top1: 6.2500\n",
      "Epoch [5/60], Iter [488/633], LR: 0.005000, Loss: 3.5981, top1: 14.0625\n",
      "Epoch [5/60], Iter [489/633], LR: 0.005000, Loss: 3.6046, top1: 9.3750\n",
      "Epoch [5/60], Iter [490/633], LR: 0.005000, Loss: 3.6404, top1: 9.3750\n",
      "Epoch [5/60], Iter [491/633], LR: 0.005000, Loss: 3.6553, top1: 6.2500\n",
      "Epoch [5/60], Iter [492/633], LR: 0.005000, Loss: 3.6264, top1: 12.5000\n",
      "Epoch [5/60], Iter [493/633], LR: 0.005000, Loss: 3.5938, top1: 9.3750\n",
      "Epoch [5/60], Iter [494/633], LR: 0.005000, Loss: 3.6281, top1: 6.2500\n",
      "Epoch [5/60], Iter [495/633], LR: 0.005000, Loss: 3.6010, top1: 10.9375\n",
      "Epoch [5/60], Iter [496/633], LR: 0.005000, Loss: 3.5805, top1: 17.1875\n",
      "Epoch [5/60], Iter [497/633], LR: 0.005000, Loss: 3.6509, top1: 6.2500\n",
      "Epoch [5/60], Iter [498/633], LR: 0.005000, Loss: 3.6001, top1: 7.8125\n",
      "Epoch [5/60], Iter [499/633], LR: 0.005000, Loss: 3.6017, top1: 7.8125\n",
      "Epoch [5/60], Iter [500/633], LR: 0.005000, Loss: 3.5709, top1: 17.1875\n",
      "Epoch [5/60], Iter [501/633], LR: 0.005000, Loss: 3.6089, top1: 12.5000\n",
      "Epoch [5/60], Iter [502/633], LR: 0.005000, Loss: 3.6681, top1: 4.6875\n",
      "Epoch [5/60], Iter [503/633], LR: 0.005000, Loss: 3.6289, top1: 12.5000\n",
      "Epoch [5/60], Iter [504/633], LR: 0.005000, Loss: 3.5926, top1: 14.0625\n",
      "Epoch [5/60], Iter [505/633], LR: 0.005000, Loss: 3.6602, top1: 4.6875\n",
      "Epoch [5/60], Iter [506/633], LR: 0.005000, Loss: 3.6228, top1: 15.6250\n",
      "Epoch [5/60], Iter [507/633], LR: 0.005000, Loss: 3.6220, top1: 9.3750\n",
      "Epoch [5/60], Iter [508/633], LR: 0.005000, Loss: 3.6277, top1: 7.8125\n",
      "Epoch [5/60], Iter [509/633], LR: 0.005000, Loss: 3.6267, top1: 10.9375\n",
      "Epoch [5/60], Iter [510/633], LR: 0.005000, Loss: 3.6410, top1: 9.3750\n",
      "Epoch [5/60], Iter [511/633], LR: 0.005000, Loss: 3.6307, top1: 10.9375\n",
      "Epoch [5/60], Iter [512/633], LR: 0.005000, Loss: 3.5859, top1: 15.6250\n",
      "Epoch [5/60], Iter [513/633], LR: 0.005000, Loss: 3.6313, top1: 6.2500\n",
      "Epoch [5/60], Iter [514/633], LR: 0.005000, Loss: 3.6021, top1: 17.1875\n",
      "Epoch [5/60], Iter [515/633], LR: 0.005000, Loss: 3.6083, top1: 15.6250\n",
      "Epoch [5/60], Iter [516/633], LR: 0.005000, Loss: 3.6280, top1: 10.9375\n",
      "Epoch [5/60], Iter [517/633], LR: 0.005000, Loss: 3.6163, top1: 9.3750\n",
      "Epoch [5/60], Iter [518/633], LR: 0.005000, Loss: 3.5854, top1: 9.3750\n",
      "Epoch [5/60], Iter [519/633], LR: 0.005000, Loss: 3.6049, top1: 10.9375\n",
      "Epoch [5/60], Iter [520/633], LR: 0.005000, Loss: 3.6468, top1: 6.2500\n",
      "Epoch [5/60], Iter [521/633], LR: 0.005000, Loss: 3.5906, top1: 9.3750\n",
      "Epoch [5/60], Iter [522/633], LR: 0.005000, Loss: 3.6345, top1: 6.2500\n",
      "Epoch [5/60], Iter [523/633], LR: 0.005000, Loss: 3.6127, top1: 10.9375\n",
      "Epoch [5/60], Iter [524/633], LR: 0.005000, Loss: 3.6301, top1: 10.9375\n",
      "Epoch [5/60], Iter [525/633], LR: 0.005000, Loss: 3.5636, top1: 12.5000\n",
      "Epoch [5/60], Iter [526/633], LR: 0.005000, Loss: 3.5791, top1: 14.0625\n",
      "Epoch [5/60], Iter [527/633], LR: 0.005000, Loss: 3.6432, top1: 7.8125\n",
      "Epoch [5/60], Iter [528/633], LR: 0.005000, Loss: 3.6406, top1: 7.8125\n",
      "Epoch [5/60], Iter [529/633], LR: 0.005000, Loss: 3.6305, top1: 14.0625\n",
      "Epoch [5/60], Iter [530/633], LR: 0.005000, Loss: 3.6676, top1: 3.1250\n",
      "Epoch [5/60], Iter [531/633], LR: 0.005000, Loss: 3.5839, top1: 12.5000\n",
      "Epoch [5/60], Iter [532/633], LR: 0.005000, Loss: 3.6467, top1: 10.9375\n",
      "Epoch [5/60], Iter [533/633], LR: 0.005000, Loss: 3.6020, top1: 10.9375\n",
      "Epoch [5/60], Iter [534/633], LR: 0.005000, Loss: 3.6200, top1: 15.6250\n",
      "Epoch [5/60], Iter [535/633], LR: 0.005000, Loss: 3.6320, top1: 9.3750\n",
      "Epoch [5/60], Iter [536/633], LR: 0.005000, Loss: 3.6380, top1: 15.6250\n",
      "Epoch [5/60], Iter [537/633], LR: 0.005000, Loss: 3.6243, top1: 9.3750\n",
      "Epoch [5/60], Iter [538/633], LR: 0.005000, Loss: 3.5871, top1: 14.0625\n",
      "Epoch [5/60], Iter [539/633], LR: 0.005000, Loss: 3.6012, top1: 9.3750\n",
      "Epoch [5/60], Iter [540/633], LR: 0.005000, Loss: 3.6396, top1: 7.8125\n",
      "Epoch [5/60], Iter [541/633], LR: 0.005000, Loss: 3.5992, top1: 9.3750\n",
      "Epoch [5/60], Iter [542/633], LR: 0.005000, Loss: 3.5947, top1: 9.3750\n",
      "Epoch [5/60], Iter [543/633], LR: 0.005000, Loss: 3.6272, top1: 4.6875\n",
      "Epoch [5/60], Iter [544/633], LR: 0.005000, Loss: 3.6470, top1: 7.8125\n",
      "Epoch [5/60], Iter [545/633], LR: 0.005000, Loss: 3.6182, top1: 7.8125\n",
      "Epoch [5/60], Iter [546/633], LR: 0.005000, Loss: 3.6370, top1: 7.8125\n",
      "Epoch [5/60], Iter [547/633], LR: 0.005000, Loss: 3.6657, top1: 9.3750\n",
      "Epoch [5/60], Iter [548/633], LR: 0.005000, Loss: 3.5784, top1: 10.9375\n",
      "Epoch [5/60], Iter [549/633], LR: 0.005000, Loss: 3.5852, top1: 10.9375\n",
      "Epoch [5/60], Iter [550/633], LR: 0.005000, Loss: 3.5998, top1: 7.8125\n",
      "Epoch [5/60], Iter [551/633], LR: 0.005000, Loss: 3.6638, top1: 6.2500\n",
      "Epoch [5/60], Iter [552/633], LR: 0.005000, Loss: 3.6314, top1: 6.2500\n",
      "Epoch [5/60], Iter [553/633], LR: 0.005000, Loss: 3.6424, top1: 12.5000\n",
      "Epoch [5/60], Iter [554/633], LR: 0.005000, Loss: 3.6493, top1: 3.1250\n",
      "Epoch [5/60], Iter [555/633], LR: 0.005000, Loss: 3.5845, top1: 15.6250\n",
      "Epoch [5/60], Iter [556/633], LR: 0.005000, Loss: 3.6029, top1: 12.5000\n",
      "Epoch [5/60], Iter [557/633], LR: 0.005000, Loss: 3.6296, top1: 14.0625\n",
      "Epoch [5/60], Iter [558/633], LR: 0.005000, Loss: 3.5992, top1: 7.8125\n",
      "Epoch [5/60], Iter [559/633], LR: 0.005000, Loss: 3.5656, top1: 12.5000\n",
      "Epoch [5/60], Iter [560/633], LR: 0.005000, Loss: 3.6009, top1: 10.9375\n",
      "Epoch [5/60], Iter [561/633], LR: 0.005000, Loss: 3.5680, top1: 20.3125\n",
      "Epoch [5/60], Iter [562/633], LR: 0.005000, Loss: 3.6251, top1: 12.5000\n",
      "Epoch [5/60], Iter [563/633], LR: 0.005000, Loss: 3.6247, top1: 9.3750\n",
      "Epoch [5/60], Iter [564/633], LR: 0.005000, Loss: 3.6333, top1: 10.9375\n",
      "Epoch [5/60], Iter [565/633], LR: 0.005000, Loss: 3.6266, top1: 9.3750\n",
      "Epoch [5/60], Iter [566/633], LR: 0.005000, Loss: 3.6601, top1: 6.2500\n",
      "Epoch [5/60], Iter [567/633], LR: 0.005000, Loss: 3.5905, top1: 14.0625\n",
      "Epoch [5/60], Iter [568/633], LR: 0.005000, Loss: 3.5864, top1: 10.9375\n",
      "Epoch [5/60], Iter [569/633], LR: 0.005000, Loss: 3.6510, top1: 4.6875\n",
      "Epoch [5/60], Iter [570/633], LR: 0.005000, Loss: 3.5565, top1: 20.3125\n",
      "Epoch [5/60], Iter [571/633], LR: 0.005000, Loss: 3.6341, top1: 9.3750\n",
      "Epoch [5/60], Iter [572/633], LR: 0.005000, Loss: 3.6230, top1: 14.0625\n",
      "Epoch [5/60], Iter [573/633], LR: 0.005000, Loss: 3.6018, top1: 14.0625\n",
      "Epoch [5/60], Iter [574/633], LR: 0.005000, Loss: 3.6322, top1: 9.3750\n",
      "Epoch [5/60], Iter [575/633], LR: 0.005000, Loss: 3.6336, top1: 4.6875\n",
      "Epoch [5/60], Iter [576/633], LR: 0.005000, Loss: 3.6172, top1: 7.8125\n",
      "Epoch [5/60], Iter [577/633], LR: 0.005000, Loss: 3.6195, top1: 10.9375\n",
      "Epoch [5/60], Iter [578/633], LR: 0.005000, Loss: 3.5830, top1: 12.5000\n",
      "Epoch [5/60], Iter [579/633], LR: 0.005000, Loss: 3.5968, top1: 12.5000\n",
      "Epoch [5/60], Iter [580/633], LR: 0.005000, Loss: 3.6380, top1: 6.2500\n",
      "Epoch [5/60], Iter [581/633], LR: 0.005000, Loss: 3.6031, top1: 9.3750\n",
      "Epoch [5/60], Iter [582/633], LR: 0.005000, Loss: 3.6004, top1: 18.7500\n",
      "Epoch [5/60], Iter [583/633], LR: 0.005000, Loss: 3.5863, top1: 12.5000\n",
      "Epoch [5/60], Iter [584/633], LR: 0.005000, Loss: 3.6232, top1: 10.9375\n",
      "Epoch [5/60], Iter [585/633], LR: 0.005000, Loss: 3.6258, top1: 9.3750\n",
      "Epoch [5/60], Iter [586/633], LR: 0.005000, Loss: 3.6127, top1: 10.9375\n",
      "Epoch [5/60], Iter [587/633], LR: 0.005000, Loss: 3.6394, top1: 4.6875\n",
      "Epoch [5/60], Iter [588/633], LR: 0.005000, Loss: 3.5843, top1: 20.3125\n",
      "Epoch [5/60], Iter [589/633], LR: 0.005000, Loss: 3.6220, top1: 10.9375\n",
      "Epoch [5/60], Iter [590/633], LR: 0.005000, Loss: 3.6172, top1: 7.8125\n",
      "Epoch [5/60], Iter [591/633], LR: 0.005000, Loss: 3.6098, top1: 9.3750\n",
      "Epoch [5/60], Iter [592/633], LR: 0.005000, Loss: 3.6422, top1: 9.3750\n",
      "Epoch [5/60], Iter [593/633], LR: 0.005000, Loss: 3.6305, top1: 7.8125\n",
      "Epoch [5/60], Iter [594/633], LR: 0.005000, Loss: 3.6332, top1: 6.2500\n",
      "Epoch [5/60], Iter [595/633], LR: 0.005000, Loss: 3.5854, top1: 14.0625\n",
      "Epoch [5/60], Iter [596/633], LR: 0.005000, Loss: 3.5908, top1: 18.7500\n",
      "Epoch [5/60], Iter [597/633], LR: 0.005000, Loss: 3.5807, top1: 12.5000\n",
      "Epoch [5/60], Iter [598/633], LR: 0.005000, Loss: 3.6334, top1: 9.3750\n",
      "Epoch [5/60], Iter [599/633], LR: 0.005000, Loss: 3.6241, top1: 12.5000\n",
      "Epoch [5/60], Iter [600/633], LR: 0.005000, Loss: 3.6256, top1: 7.8125\n",
      "Epoch [5/60], Iter [601/633], LR: 0.005000, Loss: 3.5668, top1: 20.3125\n",
      "Epoch [5/60], Iter [602/633], LR: 0.005000, Loss: 3.5971, top1: 14.0625\n",
      "Epoch [5/60], Iter [603/633], LR: 0.005000, Loss: 3.6650, top1: 4.6875\n",
      "Epoch [5/60], Iter [604/633], LR: 0.005000, Loss: 3.6260, top1: 7.8125\n",
      "Epoch [5/60], Iter [605/633], LR: 0.005000, Loss: 3.6102, top1: 10.9375\n",
      "Epoch [5/60], Iter [606/633], LR: 0.005000, Loss: 3.5898, top1: 4.6875\n",
      "Epoch [5/60], Iter [607/633], LR: 0.005000, Loss: 3.6081, top1: 9.3750\n",
      "Epoch [5/60], Iter [608/633], LR: 0.005000, Loss: 3.6093, top1: 4.6875\n",
      "Epoch [5/60], Iter [609/633], LR: 0.005000, Loss: 3.6258, top1: 12.5000\n",
      "Epoch [5/60], Iter [610/633], LR: 0.005000, Loss: 3.6013, top1: 14.0625\n",
      "Epoch [5/60], Iter [611/633], LR: 0.005000, Loss: 3.6147, top1: 4.6875\n",
      "Epoch [5/60], Iter [612/633], LR: 0.005000, Loss: 3.6057, top1: 7.8125\n",
      "Epoch [5/60], Iter [613/633], LR: 0.005000, Loss: 3.6031, top1: 10.9375\n",
      "Epoch [5/60], Iter [614/633], LR: 0.005000, Loss: 3.5974, top1: 17.1875\n",
      "Epoch [5/60], Iter [615/633], LR: 0.005000, Loss: 3.5975, top1: 9.3750\n",
      "Epoch [5/60], Iter [616/633], LR: 0.005000, Loss: 3.6142, top1: 6.2500\n",
      "Epoch [5/60], Iter [617/633], LR: 0.005000, Loss: 3.6192, top1: 6.2500\n",
      "Epoch [5/60], Iter [618/633], LR: 0.005000, Loss: 3.6207, top1: 10.9375\n",
      "Epoch [5/60], Iter [619/633], LR: 0.005000, Loss: 3.5709, top1: 17.1875\n",
      "Epoch [5/60], Iter [620/633], LR: 0.005000, Loss: 3.6174, top1: 6.2500\n",
      "Epoch [5/60], Iter [621/633], LR: 0.005000, Loss: 3.6143, top1: 12.5000\n",
      "Epoch [5/60], Iter [622/633], LR: 0.005000, Loss: 3.5871, top1: 9.3750\n",
      "Epoch [5/60], Iter [623/633], LR: 0.005000, Loss: 3.6445, top1: 7.8125\n",
      "Epoch [5/60], Iter [624/633], LR: 0.005000, Loss: 3.6372, top1: 10.9375\n",
      "Epoch [5/60], Iter [625/633], LR: 0.005000, Loss: 3.5644, top1: 17.1875\n",
      "Epoch [5/60], Iter [626/633], LR: 0.005000, Loss: 3.5652, top1: 20.3125\n",
      "Epoch [5/60], Iter [627/633], LR: 0.005000, Loss: 3.6114, top1: 14.0625\n",
      "Epoch [5/60], Iter [628/633], LR: 0.005000, Loss: 3.6120, top1: 9.3750\n",
      "Epoch [5/60], Iter [629/633], LR: 0.005000, Loss: 3.5747, top1: 14.0625\n",
      "Epoch [5/60], Iter [630/633], LR: 0.005000, Loss: 3.6270, top1: 10.9375\n",
      "Epoch [5/60], Iter [631/633], LR: 0.005000, Loss: 3.6280, top1: 12.5000\n",
      "Epoch [5/60], Iter [632/633], LR: 0.005000, Loss: 3.5925, top1: 14.0625\n",
      "Epoch [5/60], Iter [633/633], LR: 0.005000, Loss: 3.5960, top1: 10.9375\n",
      "Epoch [5/60], Iter [634/633], LR: 0.005000, Loss: 3.6299, top1: 11.2903\n",
      "Epoch [5/60], Val_Loss: 3.6064, Val_top1: 10.9050, best_top1: 10.2773\n",
      "epoch time: 4.477642103036245 min\n",
      "Taking top1 snapshot...\n",
      "Epoch [6/60], Iter [1/633], LR: 0.005000, Loss: 3.5754, top1: 9.3750\n",
      "Epoch [6/60], Iter [2/633], LR: 0.005000, Loss: 3.6244, top1: 3.1250\n",
      "Epoch [6/60], Iter [3/633], LR: 0.005000, Loss: 3.6207, top1: 9.3750\n",
      "Epoch [6/60], Iter [4/633], LR: 0.005000, Loss: 3.6232, top1: 10.9375\n",
      "Epoch [6/60], Iter [5/633], LR: 0.005000, Loss: 3.6373, top1: 9.3750\n",
      "Epoch [6/60], Iter [6/633], LR: 0.005000, Loss: 3.6108, top1: 14.0625\n",
      "Epoch [6/60], Iter [7/633], LR: 0.005000, Loss: 3.6304, top1: 9.3750\n",
      "Epoch [6/60], Iter [8/633], LR: 0.005000, Loss: 3.5929, top1: 12.5000\n",
      "Epoch [6/60], Iter [9/633], LR: 0.005000, Loss: 3.6142, top1: 7.8125\n",
      "Epoch [6/60], Iter [10/633], LR: 0.005000, Loss: 3.5747, top1: 15.6250\n",
      "Epoch [6/60], Iter [11/633], LR: 0.005000, Loss: 3.6169, top1: 14.0625\n",
      "Epoch [6/60], Iter [12/633], LR: 0.005000, Loss: 3.6031, top1: 12.5000\n",
      "Epoch [6/60], Iter [13/633], LR: 0.005000, Loss: 3.5635, top1: 17.1875\n",
      "Epoch [6/60], Iter [14/633], LR: 0.005000, Loss: 3.5943, top1: 12.5000\n",
      "Epoch [6/60], Iter [15/633], LR: 0.005000, Loss: 3.6291, top1: 7.8125\n",
      "Epoch [6/60], Iter [16/633], LR: 0.005000, Loss: 3.6512, top1: 6.2500\n",
      "Epoch [6/60], Iter [17/633], LR: 0.005000, Loss: 3.6223, top1: 9.3750\n",
      "Epoch [6/60], Iter [18/633], LR: 0.005000, Loss: 3.5710, top1: 21.8750\n",
      "Epoch [6/60], Iter [19/633], LR: 0.005000, Loss: 3.6226, top1: 7.8125\n",
      "Epoch [6/60], Iter [20/633], LR: 0.005000, Loss: 3.6494, top1: 7.8125\n",
      "Epoch [6/60], Iter [21/633], LR: 0.005000, Loss: 3.6123, top1: 15.6250\n",
      "Epoch [6/60], Iter [22/633], LR: 0.005000, Loss: 3.6073, top1: 15.6250\n",
      "Epoch [6/60], Iter [23/633], LR: 0.005000, Loss: 3.6344, top1: 7.8125\n",
      "Epoch [6/60], Iter [24/633], LR: 0.005000, Loss: 3.5986, top1: 14.0625\n",
      "Epoch [6/60], Iter [25/633], LR: 0.005000, Loss: 3.5577, top1: 15.6250\n",
      "Epoch [6/60], Iter [26/633], LR: 0.005000, Loss: 3.5760, top1: 10.9375\n",
      "Epoch [6/60], Iter [27/633], LR: 0.005000, Loss: 3.5911, top1: 14.0625\n",
      "Epoch [6/60], Iter [28/633], LR: 0.005000, Loss: 3.6363, top1: 6.2500\n",
      "Epoch [6/60], Iter [29/633], LR: 0.005000, Loss: 3.6032, top1: 14.0625\n",
      "Epoch [6/60], Iter [30/633], LR: 0.005000, Loss: 3.6221, top1: 9.3750\n",
      "Epoch [6/60], Iter [31/633], LR: 0.005000, Loss: 3.6122, top1: 7.8125\n",
      "Epoch [6/60], Iter [32/633], LR: 0.005000, Loss: 3.6269, top1: 12.5000\n",
      "Epoch [6/60], Iter [33/633], LR: 0.005000, Loss: 3.5939, top1: 12.5000\n",
      "Epoch [6/60], Iter [34/633], LR: 0.005000, Loss: 3.5854, top1: 12.5000\n",
      "Epoch [6/60], Iter [35/633], LR: 0.005000, Loss: 3.5792, top1: 18.7500\n",
      "Epoch [6/60], Iter [36/633], LR: 0.005000, Loss: 3.6347, top1: 4.6875\n",
      "Epoch [6/60], Iter [37/633], LR: 0.005000, Loss: 3.6484, top1: 6.2500\n",
      "Epoch [6/60], Iter [38/633], LR: 0.005000, Loss: 3.5498, top1: 20.3125\n",
      "Epoch [6/60], Iter [39/633], LR: 0.005000, Loss: 3.5701, top1: 15.6250\n",
      "Epoch [6/60], Iter [40/633], LR: 0.005000, Loss: 3.6350, top1: 9.3750\n",
      "Epoch [6/60], Iter [41/633], LR: 0.005000, Loss: 3.6118, top1: 6.2500\n",
      "Epoch [6/60], Iter [42/633], LR: 0.005000, Loss: 3.6101, top1: 10.9375\n",
      "Epoch [6/60], Iter [43/633], LR: 0.005000, Loss: 3.6608, top1: 3.1250\n",
      "Epoch [6/60], Iter [44/633], LR: 0.005000, Loss: 3.5946, top1: 14.0625\n",
      "Epoch [6/60], Iter [45/633], LR: 0.005000, Loss: 3.6017, top1: 7.8125\n",
      "Epoch [6/60], Iter [46/633], LR: 0.005000, Loss: 3.5554, top1: 12.5000\n",
      "Epoch [6/60], Iter [47/633], LR: 0.005000, Loss: 3.6291, top1: 10.9375\n",
      "Epoch [6/60], Iter [48/633], LR: 0.005000, Loss: 3.6203, top1: 9.3750\n",
      "Epoch [6/60], Iter [49/633], LR: 0.005000, Loss: 3.6291, top1: 7.8125\n",
      "Epoch [6/60], Iter [50/633], LR: 0.005000, Loss: 3.6004, top1: 7.8125\n",
      "Epoch [6/60], Iter [51/633], LR: 0.005000, Loss: 3.5781, top1: 17.1875\n",
      "Epoch [6/60], Iter [52/633], LR: 0.005000, Loss: 3.6182, top1: 12.5000\n",
      "Epoch [6/60], Iter [53/633], LR: 0.005000, Loss: 3.6538, top1: 9.3750\n",
      "Epoch [6/60], Iter [54/633], LR: 0.005000, Loss: 3.6463, top1: 14.0625\n",
      "Epoch [6/60], Iter [55/633], LR: 0.005000, Loss: 3.5951, top1: 9.3750\n",
      "Epoch [6/60], Iter [56/633], LR: 0.005000, Loss: 3.6204, top1: 7.8125\n",
      "Epoch [6/60], Iter [57/633], LR: 0.005000, Loss: 3.5682, top1: 17.1875\n",
      "Epoch [6/60], Iter [58/633], LR: 0.005000, Loss: 3.5890, top1: 10.9375\n",
      "Epoch [6/60], Iter [59/633], LR: 0.005000, Loss: 3.6056, top1: 9.3750\n",
      "Epoch [6/60], Iter [60/633], LR: 0.005000, Loss: 3.6362, top1: 6.2500\n",
      "Epoch [6/60], Iter [61/633], LR: 0.005000, Loss: 3.6242, top1: 12.5000\n",
      "Epoch [6/60], Iter [62/633], LR: 0.005000, Loss: 3.6018, top1: 12.5000\n",
      "Epoch [6/60], Iter [63/633], LR: 0.005000, Loss: 3.5692, top1: 10.9375\n",
      "Epoch [6/60], Iter [64/633], LR: 0.005000, Loss: 3.6172, top1: 9.3750\n",
      "Epoch [6/60], Iter [65/633], LR: 0.005000, Loss: 3.6735, top1: 3.1250\n",
      "Epoch [6/60], Iter [66/633], LR: 0.005000, Loss: 3.6318, top1: 14.0625\n",
      "Epoch [6/60], Iter [67/633], LR: 0.005000, Loss: 3.5756, top1: 18.7500\n",
      "Epoch [6/60], Iter [68/633], LR: 0.005000, Loss: 3.6199, top1: 14.0625\n",
      "Epoch [6/60], Iter [69/633], LR: 0.005000, Loss: 3.6165, top1: 10.9375\n",
      "Epoch [6/60], Iter [70/633], LR: 0.005000, Loss: 3.6224, top1: 3.1250\n",
      "Epoch [6/60], Iter [71/633], LR: 0.005000, Loss: 3.5964, top1: 10.9375\n",
      "Epoch [6/60], Iter [72/633], LR: 0.005000, Loss: 3.6463, top1: 4.6875\n",
      "Epoch [6/60], Iter [73/633], LR: 0.005000, Loss: 3.6142, top1: 7.8125\n",
      "Epoch [6/60], Iter [74/633], LR: 0.005000, Loss: 3.6246, top1: 6.2500\n",
      "Epoch [6/60], Iter [75/633], LR: 0.005000, Loss: 3.5978, top1: 12.5000\n",
      "Epoch [6/60], Iter [76/633], LR: 0.005000, Loss: 3.6294, top1: 4.6875\n",
      "Epoch [6/60], Iter [77/633], LR: 0.005000, Loss: 3.6187, top1: 9.3750\n",
      "Epoch [6/60], Iter [78/633], LR: 0.005000, Loss: 3.6009, top1: 7.8125\n",
      "Epoch [6/60], Iter [79/633], LR: 0.005000, Loss: 3.6003, top1: 18.7500\n",
      "Epoch [6/60], Iter [80/633], LR: 0.005000, Loss: 3.6283, top1: 9.3750\n",
      "Epoch [6/60], Iter [81/633], LR: 0.005000, Loss: 3.6139, top1: 7.8125\n",
      "Epoch [6/60], Iter [82/633], LR: 0.005000, Loss: 3.6294, top1: 12.5000\n",
      "Epoch [6/60], Iter [83/633], LR: 0.005000, Loss: 3.6233, top1: 7.8125\n",
      "Epoch [6/60], Iter [84/633], LR: 0.005000, Loss: 3.6423, top1: 9.3750\n",
      "Epoch [6/60], Iter [85/633], LR: 0.005000, Loss: 3.6328, top1: 12.5000\n",
      "Epoch [6/60], Iter [86/633], LR: 0.005000, Loss: 3.6059, top1: 18.7500\n",
      "Epoch [6/60], Iter [87/633], LR: 0.005000, Loss: 3.6564, top1: 7.8125\n",
      "Epoch [6/60], Iter [88/633], LR: 0.005000, Loss: 3.6651, top1: 10.9375\n",
      "Epoch [6/60], Iter [89/633], LR: 0.005000, Loss: 3.6585, top1: 7.8125\n",
      "Epoch [6/60], Iter [90/633], LR: 0.005000, Loss: 3.6071, top1: 12.5000\n",
      "Epoch [6/60], Iter [91/633], LR: 0.005000, Loss: 3.5987, top1: 12.5000\n",
      "Epoch [6/60], Iter [92/633], LR: 0.005000, Loss: 3.5866, top1: 10.9375\n",
      "Epoch [6/60], Iter [93/633], LR: 0.005000, Loss: 3.6284, top1: 7.8125\n",
      "Epoch [6/60], Iter [94/633], LR: 0.005000, Loss: 3.5847, top1: 9.3750\n",
      "Epoch [6/60], Iter [95/633], LR: 0.005000, Loss: 3.6287, top1: 7.8125\n",
      "Epoch [6/60], Iter [96/633], LR: 0.005000, Loss: 3.6117, top1: 9.3750\n",
      "Epoch [6/60], Iter [97/633], LR: 0.005000, Loss: 3.5611, top1: 17.1875\n",
      "Epoch [6/60], Iter [98/633], LR: 0.005000, Loss: 3.6269, top1: 10.9375\n",
      "Epoch [6/60], Iter [99/633], LR: 0.005000, Loss: 3.5716, top1: 18.7500\n",
      "Epoch [6/60], Iter [100/633], LR: 0.005000, Loss: 3.6229, top1: 9.3750\n",
      "Epoch [6/60], Iter [101/633], LR: 0.005000, Loss: 3.5985, top1: 10.9375\n",
      "Epoch [6/60], Iter [102/633], LR: 0.005000, Loss: 3.6224, top1: 12.5000\n",
      "Epoch [6/60], Iter [103/633], LR: 0.005000, Loss: 3.6025, top1: 14.0625\n",
      "Epoch [6/60], Iter [104/633], LR: 0.005000, Loss: 3.6063, top1: 4.6875\n",
      "Epoch [6/60], Iter [105/633], LR: 0.005000, Loss: 3.6305, top1: 10.9375\n",
      "Epoch [6/60], Iter [106/633], LR: 0.005000, Loss: 3.6202, top1: 9.3750\n",
      "Epoch [6/60], Iter [107/633], LR: 0.005000, Loss: 3.5853, top1: 10.9375\n",
      "Epoch [6/60], Iter [108/633], LR: 0.005000, Loss: 3.6020, top1: 10.9375\n",
      "Epoch [6/60], Iter [109/633], LR: 0.005000, Loss: 3.5997, top1: 20.3125\n",
      "Epoch [6/60], Iter [110/633], LR: 0.005000, Loss: 3.5954, top1: 14.0625\n",
      "Epoch [6/60], Iter [111/633], LR: 0.005000, Loss: 3.6106, top1: 12.5000\n",
      "Epoch [6/60], Iter [112/633], LR: 0.005000, Loss: 3.5969, top1: 7.8125\n",
      "Epoch [6/60], Iter [113/633], LR: 0.005000, Loss: 3.6205, top1: 10.9375\n",
      "Epoch [6/60], Iter [114/633], LR: 0.005000, Loss: 3.5898, top1: 14.0625\n",
      "Epoch [6/60], Iter [115/633], LR: 0.005000, Loss: 3.6275, top1: 9.3750\n",
      "Epoch [6/60], Iter [116/633], LR: 0.005000, Loss: 3.6213, top1: 9.3750\n",
      "Epoch [6/60], Iter [117/633], LR: 0.005000, Loss: 3.6514, top1: 6.2500\n",
      "Epoch [6/60], Iter [118/633], LR: 0.005000, Loss: 3.6286, top1: 9.3750\n",
      "Epoch [6/60], Iter [119/633], LR: 0.005000, Loss: 3.5504, top1: 18.7500\n",
      "Epoch [6/60], Iter [120/633], LR: 0.005000, Loss: 3.6006, top1: 10.9375\n",
      "Epoch [6/60], Iter [121/633], LR: 0.005000, Loss: 3.6091, top1: 15.6250\n",
      "Epoch [6/60], Iter [122/633], LR: 0.005000, Loss: 3.6034, top1: 10.9375\n",
      "Epoch [6/60], Iter [123/633], LR: 0.005000, Loss: 3.5893, top1: 9.3750\n",
      "Epoch [6/60], Iter [124/633], LR: 0.005000, Loss: 3.6030, top1: 15.6250\n",
      "Epoch [6/60], Iter [125/633], LR: 0.005000, Loss: 3.5753, top1: 18.7500\n",
      "Epoch [6/60], Iter [126/633], LR: 0.005000, Loss: 3.6038, top1: 12.5000\n",
      "Epoch [6/60], Iter [127/633], LR: 0.005000, Loss: 3.6373, top1: 12.5000\n",
      "Epoch [6/60], Iter [128/633], LR: 0.005000, Loss: 3.5876, top1: 18.7500\n",
      "Epoch [6/60], Iter [129/633], LR: 0.005000, Loss: 3.5678, top1: 18.7500\n",
      "Epoch [6/60], Iter [130/633], LR: 0.005000, Loss: 3.6170, top1: 17.1875\n",
      "Epoch [6/60], Iter [131/633], LR: 0.005000, Loss: 3.5780, top1: 9.3750\n",
      "Epoch [6/60], Iter [132/633], LR: 0.005000, Loss: 3.6041, top1: 9.3750\n",
      "Epoch [6/60], Iter [133/633], LR: 0.005000, Loss: 3.6479, top1: 4.6875\n",
      "Epoch [6/60], Iter [134/633], LR: 0.005000, Loss: 3.6030, top1: 6.2500\n",
      "Epoch [6/60], Iter [135/633], LR: 0.005000, Loss: 3.6340, top1: 6.2500\n",
      "Epoch [6/60], Iter [136/633], LR: 0.005000, Loss: 3.5887, top1: 15.6250\n",
      "Epoch [6/60], Iter [137/633], LR: 0.005000, Loss: 3.5853, top1: 10.9375\n",
      "Epoch [6/60], Iter [138/633], LR: 0.005000, Loss: 3.6333, top1: 4.6875\n",
      "Epoch [6/60], Iter [139/633], LR: 0.005000, Loss: 3.6051, top1: 14.0625\n",
      "Epoch [6/60], Iter [140/633], LR: 0.005000, Loss: 3.6219, top1: 9.3750\n",
      "Epoch [6/60], Iter [141/633], LR: 0.005000, Loss: 3.5791, top1: 10.9375\n",
      "Epoch [6/60], Iter [142/633], LR: 0.005000, Loss: 3.5999, top1: 14.0625\n",
      "Epoch [6/60], Iter [143/633], LR: 0.005000, Loss: 3.6247, top1: 7.8125\n",
      "Epoch [6/60], Iter [144/633], LR: 0.005000, Loss: 3.6072, top1: 18.7500\n",
      "Epoch [6/60], Iter [145/633], LR: 0.005000, Loss: 3.5831, top1: 12.5000\n",
      "Epoch [6/60], Iter [146/633], LR: 0.005000, Loss: 3.5869, top1: 18.7500\n",
      "Epoch [6/60], Iter [147/633], LR: 0.005000, Loss: 3.5859, top1: 9.3750\n",
      "Epoch [6/60], Iter [148/633], LR: 0.005000, Loss: 3.6301, top1: 12.5000\n",
      "Epoch [6/60], Iter [149/633], LR: 0.005000, Loss: 3.6051, top1: 7.8125\n",
      "Epoch [6/60], Iter [150/633], LR: 0.005000, Loss: 3.6003, top1: 10.9375\n",
      "Epoch [6/60], Iter [151/633], LR: 0.005000, Loss: 3.6243, top1: 10.9375\n",
      "Epoch [6/60], Iter [152/633], LR: 0.005000, Loss: 3.5646, top1: 15.6250\n",
      "Epoch [6/60], Iter [153/633], LR: 0.005000, Loss: 3.6032, top1: 12.5000\n",
      "Epoch [6/60], Iter [154/633], LR: 0.005000, Loss: 3.5724, top1: 14.0625\n",
      "Epoch [6/60], Iter [155/633], LR: 0.005000, Loss: 3.6311, top1: 4.6875\n",
      "Epoch [6/60], Iter [156/633], LR: 0.005000, Loss: 3.5849, top1: 17.1875\n",
      "Epoch [6/60], Iter [157/633], LR: 0.005000, Loss: 3.5748, top1: 18.7500\n",
      "Epoch [6/60], Iter [158/633], LR: 0.005000, Loss: 3.5863, top1: 14.0625\n",
      "Epoch [6/60], Iter [159/633], LR: 0.005000, Loss: 3.6180, top1: 9.3750\n",
      "Epoch [6/60], Iter [160/633], LR: 0.005000, Loss: 3.5867, top1: 7.8125\n",
      "Epoch [6/60], Iter [161/633], LR: 0.005000, Loss: 3.5662, top1: 18.7500\n",
      "Epoch [6/60], Iter [162/633], LR: 0.005000, Loss: 3.6105, top1: 10.9375\n",
      "Epoch [6/60], Iter [163/633], LR: 0.005000, Loss: 3.5848, top1: 12.5000\n",
      "Epoch [6/60], Iter [164/633], LR: 0.005000, Loss: 3.6363, top1: 7.8125\n",
      "Epoch [6/60], Iter [165/633], LR: 0.005000, Loss: 3.5955, top1: 15.6250\n",
      "Epoch [6/60], Iter [166/633], LR: 0.005000, Loss: 3.6026, top1: 10.9375\n",
      "Epoch [6/60], Iter [167/633], LR: 0.005000, Loss: 3.6118, top1: 9.3750\n",
      "Epoch [6/60], Iter [168/633], LR: 0.005000, Loss: 3.6398, top1: 7.8125\n",
      "Epoch [6/60], Iter [169/633], LR: 0.005000, Loss: 3.6158, top1: 15.6250\n",
      "Epoch [6/60], Iter [170/633], LR: 0.005000, Loss: 3.5987, top1: 14.0625\n",
      "Epoch [6/60], Iter [171/633], LR: 0.005000, Loss: 3.6402, top1: 4.6875\n",
      "Epoch [6/60], Iter [172/633], LR: 0.005000, Loss: 3.5743, top1: 10.9375\n",
      "Epoch [6/60], Iter [173/633], LR: 0.005000, Loss: 3.6447, top1: 1.5625\n",
      "Epoch [6/60], Iter [174/633], LR: 0.005000, Loss: 3.6010, top1: 7.8125\n",
      "Epoch [6/60], Iter [175/633], LR: 0.005000, Loss: 3.6040, top1: 10.9375\n",
      "Epoch [6/60], Iter [176/633], LR: 0.005000, Loss: 3.6255, top1: 7.8125\n",
      "Epoch [6/60], Iter [177/633], LR: 0.005000, Loss: 3.5922, top1: 15.6250\n",
      "Epoch [6/60], Iter [178/633], LR: 0.005000, Loss: 3.6108, top1: 7.8125\n",
      "Epoch [6/60], Iter [179/633], LR: 0.005000, Loss: 3.5821, top1: 12.5000\n",
      "Epoch [6/60], Iter [180/633], LR: 0.005000, Loss: 3.6369, top1: 7.8125\n",
      "Epoch [6/60], Iter [181/633], LR: 0.005000, Loss: 3.6235, top1: 14.0625\n",
      "Epoch [6/60], Iter [182/633], LR: 0.005000, Loss: 3.5805, top1: 14.0625\n",
      "Epoch [6/60], Iter [183/633], LR: 0.005000, Loss: 3.5911, top1: 9.3750\n",
      "Epoch [6/60], Iter [184/633], LR: 0.005000, Loss: 3.6021, top1: 15.6250\n",
      "Epoch [6/60], Iter [185/633], LR: 0.005000, Loss: 3.5574, top1: 21.8750\n",
      "Epoch [6/60], Iter [186/633], LR: 0.005000, Loss: 3.6307, top1: 9.3750\n",
      "Epoch [6/60], Iter [187/633], LR: 0.005000, Loss: 3.6247, top1: 9.3750\n",
      "Epoch [6/60], Iter [188/633], LR: 0.005000, Loss: 3.6061, top1: 7.8125\n",
      "Epoch [6/60], Iter [189/633], LR: 0.005000, Loss: 3.5742, top1: 10.9375\n",
      "Epoch [6/60], Iter [190/633], LR: 0.005000, Loss: 3.5970, top1: 17.1875\n",
      "Epoch [6/60], Iter [191/633], LR: 0.005000, Loss: 3.6041, top1: 15.6250\n",
      "Epoch [6/60], Iter [192/633], LR: 0.005000, Loss: 3.6538, top1: 1.5625\n",
      "Epoch [6/60], Iter [193/633], LR: 0.005000, Loss: 3.6208, top1: 12.5000\n",
      "Epoch [6/60], Iter [194/633], LR: 0.005000, Loss: 3.6122, top1: 7.8125\n",
      "Epoch [6/60], Iter [195/633], LR: 0.005000, Loss: 3.6404, top1: 4.6875\n",
      "Epoch [6/60], Iter [196/633], LR: 0.005000, Loss: 3.6087, top1: 7.8125\n",
      "Epoch [6/60], Iter [197/633], LR: 0.005000, Loss: 3.6212, top1: 7.8125\n",
      "Epoch [6/60], Iter [198/633], LR: 0.005000, Loss: 3.6037, top1: 10.9375\n",
      "Epoch [6/60], Iter [199/633], LR: 0.005000, Loss: 3.6194, top1: 14.0625\n",
      "Epoch [6/60], Iter [200/633], LR: 0.005000, Loss: 3.6127, top1: 10.9375\n",
      "Epoch [6/60], Iter [201/633], LR: 0.005000, Loss: 3.6305, top1: 10.9375\n",
      "Epoch [6/60], Iter [202/633], LR: 0.005000, Loss: 3.6574, top1: 7.8125\n",
      "Epoch [6/60], Iter [203/633], LR: 0.005000, Loss: 3.6449, top1: 7.8125\n",
      "Epoch [6/60], Iter [204/633], LR: 0.005000, Loss: 3.6046, top1: 12.5000\n",
      "Epoch [6/60], Iter [205/633], LR: 0.005000, Loss: 3.5863, top1: 10.9375\n",
      "Epoch [6/60], Iter [206/633], LR: 0.005000, Loss: 3.6206, top1: 12.5000\n",
      "Epoch [6/60], Iter [207/633], LR: 0.005000, Loss: 3.5950, top1: 17.1875\n",
      "Epoch [6/60], Iter [208/633], LR: 0.005000, Loss: 3.6168, top1: 9.3750\n",
      "Epoch [6/60], Iter [209/633], LR: 0.005000, Loss: 3.6223, top1: 6.2500\n",
      "Epoch [6/60], Iter [210/633], LR: 0.005000, Loss: 3.5986, top1: 7.8125\n",
      "Epoch [6/60], Iter [211/633], LR: 0.005000, Loss: 3.6185, top1: 12.5000\n",
      "Epoch [6/60], Iter [212/633], LR: 0.005000, Loss: 3.5607, top1: 18.7500\n",
      "Epoch [6/60], Iter [213/633], LR: 0.005000, Loss: 3.5764, top1: 17.1875\n",
      "Epoch [6/60], Iter [214/633], LR: 0.005000, Loss: 3.5928, top1: 15.6250\n",
      "Epoch [6/60], Iter [215/633], LR: 0.005000, Loss: 3.5918, top1: 14.0625\n",
      "Epoch [6/60], Iter [216/633], LR: 0.005000, Loss: 3.6201, top1: 6.2500\n",
      "Epoch [6/60], Iter [217/633], LR: 0.005000, Loss: 3.6307, top1: 9.3750\n",
      "Epoch [6/60], Iter [218/633], LR: 0.005000, Loss: 3.6193, top1: 14.0625\n",
      "Epoch [6/60], Iter [219/633], LR: 0.005000, Loss: 3.6274, top1: 4.6875\n",
      "Epoch [6/60], Iter [220/633], LR: 0.005000, Loss: 3.6293, top1: 9.3750\n",
      "Epoch [6/60], Iter [221/633], LR: 0.005000, Loss: 3.6579, top1: 1.5625\n",
      "Epoch [6/60], Iter [222/633], LR: 0.005000, Loss: 3.5999, top1: 12.5000\n",
      "Epoch [6/60], Iter [223/633], LR: 0.005000, Loss: 3.6138, top1: 12.5000\n",
      "Epoch [6/60], Iter [224/633], LR: 0.005000, Loss: 3.6354, top1: 9.3750\n",
      "Epoch [6/60], Iter [225/633], LR: 0.005000, Loss: 3.6008, top1: 14.0625\n",
      "Epoch [6/60], Iter [226/633], LR: 0.005000, Loss: 3.6038, top1: 7.8125\n",
      "Epoch [6/60], Iter [227/633], LR: 0.005000, Loss: 3.5383, top1: 18.7500\n",
      "Epoch [6/60], Iter [228/633], LR: 0.005000, Loss: 3.5924, top1: 12.5000\n",
      "Epoch [6/60], Iter [229/633], LR: 0.005000, Loss: 3.5978, top1: 6.2500\n",
      "Epoch [6/60], Iter [230/633], LR: 0.005000, Loss: 3.5821, top1: 12.5000\n",
      "Epoch [6/60], Iter [231/633], LR: 0.005000, Loss: 3.6233, top1: 10.9375\n",
      "Epoch [6/60], Iter [232/633], LR: 0.005000, Loss: 3.5939, top1: 18.7500\n",
      "Epoch [6/60], Iter [233/633], LR: 0.005000, Loss: 3.6136, top1: 12.5000\n",
      "Epoch [6/60], Iter [234/633], LR: 0.005000, Loss: 3.6540, top1: 6.2500\n",
      "Epoch [6/60], Iter [235/633], LR: 0.005000, Loss: 3.6377, top1: 4.6875\n",
      "Epoch [6/60], Iter [236/633], LR: 0.005000, Loss: 3.6134, top1: 14.0625\n",
      "Epoch [6/60], Iter [237/633], LR: 0.005000, Loss: 3.5756, top1: 15.6250\n",
      "Epoch [6/60], Iter [238/633], LR: 0.005000, Loss: 3.6522, top1: 7.8125\n",
      "Epoch [6/60], Iter [239/633], LR: 0.005000, Loss: 3.5943, top1: 10.9375\n",
      "Epoch [6/60], Iter [240/633], LR: 0.005000, Loss: 3.6228, top1: 12.5000\n",
      "Epoch [6/60], Iter [241/633], LR: 0.005000, Loss: 3.5824, top1: 14.0625\n",
      "Epoch [6/60], Iter [242/633], LR: 0.005000, Loss: 3.6395, top1: 1.5625\n",
      "Epoch [6/60], Iter [243/633], LR: 0.005000, Loss: 3.6137, top1: 4.6875\n",
      "Epoch [6/60], Iter [244/633], LR: 0.005000, Loss: 3.6264, top1: 9.3750\n",
      "Epoch [6/60], Iter [245/633], LR: 0.005000, Loss: 3.6197, top1: 14.0625\n",
      "Epoch [6/60], Iter [246/633], LR: 0.005000, Loss: 3.6149, top1: 14.0625\n",
      "Epoch [6/60], Iter [247/633], LR: 0.005000, Loss: 3.5880, top1: 9.3750\n",
      "Epoch [6/60], Iter [248/633], LR: 0.005000, Loss: 3.6091, top1: 12.5000\n",
      "Epoch [6/60], Iter [249/633], LR: 0.005000, Loss: 3.5234, top1: 25.0000\n",
      "Epoch [6/60], Iter [250/633], LR: 0.005000, Loss: 3.6093, top1: 9.3750\n",
      "Epoch [6/60], Iter [251/633], LR: 0.005000, Loss: 3.6123, top1: 9.3750\n",
      "Epoch [6/60], Iter [252/633], LR: 0.005000, Loss: 3.5348, top1: 18.7500\n",
      "Epoch [6/60], Iter [253/633], LR: 0.005000, Loss: 3.6072, top1: 15.6250\n",
      "Epoch [6/60], Iter [254/633], LR: 0.005000, Loss: 3.6358, top1: 7.8125\n",
      "Epoch [6/60], Iter [255/633], LR: 0.005000, Loss: 3.5909, top1: 12.5000\n",
      "Epoch [6/60], Iter [256/633], LR: 0.005000, Loss: 3.6304, top1: 7.8125\n",
      "Epoch [6/60], Iter [257/633], LR: 0.005000, Loss: 3.5985, top1: 12.5000\n",
      "Epoch [6/60], Iter [258/633], LR: 0.005000, Loss: 3.6126, top1: 12.5000\n",
      "Epoch [6/60], Iter [259/633], LR: 0.005000, Loss: 3.5878, top1: 12.5000\n",
      "Epoch [6/60], Iter [260/633], LR: 0.005000, Loss: 3.6172, top1: 10.9375\n",
      "Epoch [6/60], Iter [261/633], LR: 0.005000, Loss: 3.6114, top1: 9.3750\n",
      "Epoch [6/60], Iter [262/633], LR: 0.005000, Loss: 3.6082, top1: 14.0625\n",
      "Epoch [6/60], Iter [263/633], LR: 0.005000, Loss: 3.6178, top1: 10.9375\n",
      "Epoch [6/60], Iter [264/633], LR: 0.005000, Loss: 3.6575, top1: 7.8125\n",
      "Epoch [6/60], Iter [265/633], LR: 0.005000, Loss: 3.5840, top1: 12.5000\n",
      "Epoch [6/60], Iter [266/633], LR: 0.005000, Loss: 3.6282, top1: 12.5000\n",
      "Epoch [6/60], Iter [267/633], LR: 0.005000, Loss: 3.6168, top1: 7.8125\n",
      "Epoch [6/60], Iter [268/633], LR: 0.005000, Loss: 3.5689, top1: 17.1875\n",
      "Epoch [6/60], Iter [269/633], LR: 0.005000, Loss: 3.5961, top1: 12.5000\n",
      "Epoch [6/60], Iter [270/633], LR: 0.005000, Loss: 3.6379, top1: 9.3750\n",
      "Epoch [6/60], Iter [271/633], LR: 0.005000, Loss: 3.6479, top1: 9.3750\n",
      "Epoch [6/60], Iter [272/633], LR: 0.005000, Loss: 3.6152, top1: 10.9375\n",
      "Epoch [6/60], Iter [273/633], LR: 0.005000, Loss: 3.6407, top1: 10.9375\n",
      "Epoch [6/60], Iter [274/633], LR: 0.005000, Loss: 3.6075, top1: 10.9375\n",
      "Epoch [6/60], Iter [275/633], LR: 0.005000, Loss: 3.6607, top1: 6.2500\n",
      "Epoch [6/60], Iter [276/633], LR: 0.005000, Loss: 3.5983, top1: 10.9375\n",
      "Epoch [6/60], Iter [277/633], LR: 0.005000, Loss: 3.6228, top1: 9.3750\n",
      "Epoch [6/60], Iter [278/633], LR: 0.005000, Loss: 3.6189, top1: 9.3750\n",
      "Epoch [6/60], Iter [279/633], LR: 0.005000, Loss: 3.6423, top1: 10.9375\n",
      "Epoch [6/60], Iter [280/633], LR: 0.005000, Loss: 3.6294, top1: 6.2500\n",
      "Epoch [6/60], Iter [281/633], LR: 0.005000, Loss: 3.6661, top1: 6.2500\n",
      "Epoch [6/60], Iter [282/633], LR: 0.005000, Loss: 3.6333, top1: 10.9375\n",
      "Epoch [6/60], Iter [283/633], LR: 0.005000, Loss: 3.6162, top1: 10.9375\n",
      "Epoch [6/60], Iter [284/633], LR: 0.005000, Loss: 3.6108, top1: 15.6250\n",
      "Epoch [6/60], Iter [285/633], LR: 0.005000, Loss: 3.5810, top1: 17.1875\n",
      "Epoch [6/60], Iter [286/633], LR: 0.005000, Loss: 3.5749, top1: 15.6250\n",
      "Epoch [6/60], Iter [287/633], LR: 0.005000, Loss: 3.6178, top1: 10.9375\n",
      "Epoch [6/60], Iter [288/633], LR: 0.005000, Loss: 3.6196, top1: 15.6250\n",
      "Epoch [6/60], Iter [289/633], LR: 0.005000, Loss: 3.6176, top1: 9.3750\n",
      "Epoch [6/60], Iter [290/633], LR: 0.005000, Loss: 3.5664, top1: 17.1875\n",
      "Epoch [6/60], Iter [291/633], LR: 0.005000, Loss: 3.6082, top1: 10.9375\n",
      "Epoch [6/60], Iter [292/633], LR: 0.005000, Loss: 3.5634, top1: 17.1875\n",
      "Epoch [6/60], Iter [293/633], LR: 0.005000, Loss: 3.6199, top1: 7.8125\n",
      "Epoch [6/60], Iter [294/633], LR: 0.005000, Loss: 3.5993, top1: 6.2500\n",
      "Epoch [6/60], Iter [295/633], LR: 0.005000, Loss: 3.6139, top1: 15.6250\n",
      "Epoch [6/60], Iter [296/633], LR: 0.005000, Loss: 3.5862, top1: 15.6250\n",
      "Epoch [6/60], Iter [297/633], LR: 0.005000, Loss: 3.6164, top1: 9.3750\n",
      "Epoch [6/60], Iter [298/633], LR: 0.005000, Loss: 3.5890, top1: 9.3750\n",
      "Epoch [6/60], Iter [299/633], LR: 0.005000, Loss: 3.6424, top1: 7.8125\n",
      "Epoch [6/60], Iter [300/633], LR: 0.005000, Loss: 3.6107, top1: 10.9375\n",
      "Epoch [6/60], Iter [301/633], LR: 0.005000, Loss: 3.6130, top1: 12.5000\n",
      "Epoch [6/60], Iter [302/633], LR: 0.005000, Loss: 3.5920, top1: 7.8125\n",
      "Epoch [6/60], Iter [303/633], LR: 0.005000, Loss: 3.6285, top1: 9.3750\n",
      "Epoch [6/60], Iter [304/633], LR: 0.005000, Loss: 3.5972, top1: 10.9375\n",
      "Epoch [6/60], Iter [305/633], LR: 0.005000, Loss: 3.5961, top1: 14.0625\n",
      "Epoch [6/60], Iter [306/633], LR: 0.005000, Loss: 3.6331, top1: 10.9375\n",
      "Epoch [6/60], Iter [307/633], LR: 0.005000, Loss: 3.5979, top1: 12.5000\n",
      "Epoch [6/60], Iter [308/633], LR: 0.005000, Loss: 3.6330, top1: 4.6875\n",
      "Epoch [6/60], Iter [309/633], LR: 0.005000, Loss: 3.5921, top1: 12.5000\n",
      "Epoch [6/60], Iter [310/633], LR: 0.005000, Loss: 3.5988, top1: 15.6250\n",
      "Epoch [6/60], Iter [311/633], LR: 0.005000, Loss: 3.6250, top1: 10.9375\n",
      "Epoch [6/60], Iter [312/633], LR: 0.005000, Loss: 3.6031, top1: 7.8125\n",
      "Epoch [6/60], Iter [313/633], LR: 0.005000, Loss: 3.5880, top1: 14.0625\n",
      "Epoch [6/60], Iter [314/633], LR: 0.005000, Loss: 3.6180, top1: 6.2500\n",
      "Epoch [6/60], Iter [315/633], LR: 0.005000, Loss: 3.6024, top1: 14.0625\n",
      "Epoch [6/60], Iter [316/633], LR: 0.005000, Loss: 3.6399, top1: 10.9375\n",
      "Epoch [6/60], Iter [317/633], LR: 0.005000, Loss: 3.6253, top1: 10.9375\n",
      "Epoch [6/60], Iter [318/633], LR: 0.005000, Loss: 3.6057, top1: 9.3750\n",
      "Epoch [6/60], Iter [319/633], LR: 0.005000, Loss: 3.6105, top1: 9.3750\n",
      "Epoch [6/60], Iter [320/633], LR: 0.005000, Loss: 3.5622, top1: 14.0625\n",
      "Epoch [6/60], Iter [321/633], LR: 0.005000, Loss: 3.5545, top1: 21.8750\n",
      "Epoch [6/60], Iter [322/633], LR: 0.005000, Loss: 3.6713, top1: 9.3750\n",
      "Epoch [6/60], Iter [323/633], LR: 0.005000, Loss: 3.5932, top1: 12.5000\n",
      "Epoch [6/60], Iter [324/633], LR: 0.005000, Loss: 3.5889, top1: 12.5000\n",
      "Epoch [6/60], Iter [325/633], LR: 0.005000, Loss: 3.5818, top1: 20.3125\n",
      "Epoch [6/60], Iter [326/633], LR: 0.005000, Loss: 3.6258, top1: 15.6250\n",
      "Epoch [6/60], Iter [327/633], LR: 0.005000, Loss: 3.5782, top1: 15.6250\n",
      "Epoch [6/60], Iter [328/633], LR: 0.005000, Loss: 3.6142, top1: 10.9375\n",
      "Epoch [6/60], Iter [329/633], LR: 0.005000, Loss: 3.6309, top1: 7.8125\n",
      "Epoch [6/60], Iter [330/633], LR: 0.005000, Loss: 3.6072, top1: 15.6250\n",
      "Epoch [6/60], Iter [331/633], LR: 0.005000, Loss: 3.5854, top1: 18.7500\n",
      "Epoch [6/60], Iter [332/633], LR: 0.005000, Loss: 3.5982, top1: 15.6250\n",
      "Epoch [6/60], Iter [333/633], LR: 0.005000, Loss: 3.6436, top1: 6.2500\n",
      "Epoch [6/60], Iter [334/633], LR: 0.005000, Loss: 3.5807, top1: 10.9375\n",
      "Epoch [6/60], Iter [335/633], LR: 0.005000, Loss: 3.6112, top1: 7.8125\n",
      "Epoch [6/60], Iter [336/633], LR: 0.005000, Loss: 3.6044, top1: 12.5000\n",
      "Epoch [6/60], Iter [337/633], LR: 0.005000, Loss: 3.6533, top1: 6.2500\n",
      "Epoch [6/60], Iter [338/633], LR: 0.005000, Loss: 3.5881, top1: 10.9375\n",
      "Epoch [6/60], Iter [339/633], LR: 0.005000, Loss: 3.5971, top1: 15.6250\n",
      "Epoch [6/60], Iter [340/633], LR: 0.005000, Loss: 3.6300, top1: 9.3750\n",
      "Epoch [6/60], Iter [341/633], LR: 0.005000, Loss: 3.6652, top1: 4.6875\n",
      "Epoch [6/60], Iter [342/633], LR: 0.005000, Loss: 3.6163, top1: 9.3750\n",
      "Epoch [6/60], Iter [343/633], LR: 0.005000, Loss: 3.6460, top1: 7.8125\n",
      "Epoch [6/60], Iter [344/633], LR: 0.005000, Loss: 3.5981, top1: 12.5000\n",
      "Epoch [6/60], Iter [345/633], LR: 0.005000, Loss: 3.5862, top1: 18.7500\n",
      "Epoch [6/60], Iter [346/633], LR: 0.005000, Loss: 3.6130, top1: 7.8125\n",
      "Epoch [6/60], Iter [347/633], LR: 0.005000, Loss: 3.6238, top1: 7.8125\n",
      "Epoch [6/60], Iter [348/633], LR: 0.005000, Loss: 3.5795, top1: 14.0625\n",
      "Epoch [6/60], Iter [349/633], LR: 0.005000, Loss: 3.6119, top1: 12.5000\n",
      "Epoch [6/60], Iter [350/633], LR: 0.005000, Loss: 3.6421, top1: 4.6875\n",
      "Epoch [6/60], Iter [351/633], LR: 0.005000, Loss: 3.6063, top1: 9.3750\n",
      "Epoch [6/60], Iter [352/633], LR: 0.005000, Loss: 3.6271, top1: 9.3750\n",
      "Epoch [6/60], Iter [353/633], LR: 0.005000, Loss: 3.6394, top1: 10.9375\n",
      "Epoch [6/60], Iter [354/633], LR: 0.005000, Loss: 3.6283, top1: 12.5000\n",
      "Epoch [6/60], Iter [355/633], LR: 0.005000, Loss: 3.6190, top1: 12.5000\n",
      "Epoch [6/60], Iter [356/633], LR: 0.005000, Loss: 3.6426, top1: 12.5000\n",
      "Epoch [6/60], Iter [357/633], LR: 0.005000, Loss: 3.6371, top1: 10.9375\n",
      "Epoch [6/60], Iter [358/633], LR: 0.005000, Loss: 3.5698, top1: 14.0625\n",
      "Epoch [6/60], Iter [359/633], LR: 0.005000, Loss: 3.6278, top1: 14.0625\n",
      "Epoch [6/60], Iter [360/633], LR: 0.005000, Loss: 3.6021, top1: 12.5000\n",
      "Epoch [6/60], Iter [361/633], LR: 0.005000, Loss: 3.5772, top1: 14.0625\n",
      "Epoch [6/60], Iter [362/633], LR: 0.005000, Loss: 3.5859, top1: 14.0625\n",
      "Epoch [6/60], Iter [363/633], LR: 0.005000, Loss: 3.6363, top1: 6.2500\n",
      "Epoch [6/60], Iter [364/633], LR: 0.005000, Loss: 3.6390, top1: 6.2500\n",
      "Epoch [6/60], Iter [365/633], LR: 0.005000, Loss: 3.6208, top1: 12.5000\n",
      "Epoch [6/60], Iter [366/633], LR: 0.005000, Loss: 3.5895, top1: 15.6250\n",
      "Epoch [6/60], Iter [367/633], LR: 0.005000, Loss: 3.6281, top1: 4.6875\n",
      "Epoch [6/60], Iter [368/633], LR: 0.005000, Loss: 3.6319, top1: 9.3750\n",
      "Epoch [6/60], Iter [369/633], LR: 0.005000, Loss: 3.6213, top1: 7.8125\n",
      "Epoch [6/60], Iter [370/633], LR: 0.005000, Loss: 3.6077, top1: 12.5000\n",
      "Epoch [6/60], Iter [371/633], LR: 0.005000, Loss: 3.6156, top1: 17.1875\n",
      "Epoch [6/60], Iter [372/633], LR: 0.005000, Loss: 3.6134, top1: 10.9375\n",
      "Epoch [6/60], Iter [373/633], LR: 0.005000, Loss: 3.5963, top1: 10.9375\n",
      "Epoch [6/60], Iter [374/633], LR: 0.005000, Loss: 3.6407, top1: 7.8125\n",
      "Epoch [6/60], Iter [375/633], LR: 0.005000, Loss: 3.5579, top1: 14.0625\n",
      "Epoch [6/60], Iter [376/633], LR: 0.005000, Loss: 3.5779, top1: 12.5000\n",
      "Epoch [6/60], Iter [377/633], LR: 0.005000, Loss: 3.6137, top1: 7.8125\n",
      "Epoch [6/60], Iter [378/633], LR: 0.005000, Loss: 3.6206, top1: 7.8125\n",
      "Epoch [6/60], Iter [379/633], LR: 0.005000, Loss: 3.6107, top1: 7.8125\n",
      "Epoch [6/60], Iter [380/633], LR: 0.005000, Loss: 3.6283, top1: 10.9375\n",
      "Epoch [6/60], Iter [381/633], LR: 0.005000, Loss: 3.5897, top1: 12.5000\n",
      "Epoch [6/60], Iter [382/633], LR: 0.005000, Loss: 3.5929, top1: 10.9375\n",
      "Epoch [6/60], Iter [383/633], LR: 0.005000, Loss: 3.6272, top1: 9.3750\n",
      "Epoch [6/60], Iter [384/633], LR: 0.005000, Loss: 3.5851, top1: 14.0625\n",
      "Epoch [6/60], Iter [385/633], LR: 0.005000, Loss: 3.5753, top1: 18.7500\n",
      "Epoch [6/60], Iter [386/633], LR: 0.005000, Loss: 3.5791, top1: 7.8125\n",
      "Epoch [6/60], Iter [387/633], LR: 0.005000, Loss: 3.6292, top1: 12.5000\n",
      "Epoch [6/60], Iter [388/633], LR: 0.005000, Loss: 3.5976, top1: 9.3750\n",
      "Epoch [6/60], Iter [389/633], LR: 0.005000, Loss: 3.5539, top1: 23.4375\n",
      "Epoch [6/60], Iter [390/633], LR: 0.005000, Loss: 3.6099, top1: 9.3750\n",
      "Epoch [6/60], Iter [391/633], LR: 0.005000, Loss: 3.6081, top1: 12.5000\n",
      "Epoch [6/60], Iter [392/633], LR: 0.005000, Loss: 3.5947, top1: 7.8125\n",
      "Epoch [6/60], Iter [393/633], LR: 0.005000, Loss: 3.6022, top1: 15.6250\n",
      "Epoch [6/60], Iter [394/633], LR: 0.005000, Loss: 3.6211, top1: 4.6875\n",
      "Epoch [6/60], Iter [395/633], LR: 0.005000, Loss: 3.6292, top1: 9.3750\n",
      "Epoch [6/60], Iter [396/633], LR: 0.005000, Loss: 3.6018, top1: 14.0625\n",
      "Epoch [6/60], Iter [397/633], LR: 0.005000, Loss: 3.6147, top1: 9.3750\n",
      "Epoch [6/60], Iter [398/633], LR: 0.005000, Loss: 3.6219, top1: 12.5000\n",
      "Epoch [6/60], Iter [399/633], LR: 0.005000, Loss: 3.6002, top1: 10.9375\n",
      "Epoch [6/60], Iter [400/633], LR: 0.005000, Loss: 3.5752, top1: 20.3125\n",
      "Epoch [6/60], Iter [401/633], LR: 0.005000, Loss: 3.5945, top1: 9.3750\n",
      "Epoch [6/60], Iter [402/633], LR: 0.005000, Loss: 3.6407, top1: 9.3750\n",
      "Epoch [6/60], Iter [403/633], LR: 0.005000, Loss: 3.6185, top1: 7.8125\n",
      "Epoch [6/60], Iter [404/633], LR: 0.005000, Loss: 3.6155, top1: 12.5000\n",
      "Epoch [6/60], Iter [405/633], LR: 0.005000, Loss: 3.6055, top1: 10.9375\n",
      "Epoch [6/60], Iter [406/633], LR: 0.005000, Loss: 3.6225, top1: 9.3750\n",
      "Epoch [6/60], Iter [407/633], LR: 0.005000, Loss: 3.6174, top1: 14.0625\n",
      "Epoch [6/60], Iter [408/633], LR: 0.005000, Loss: 3.5822, top1: 12.5000\n",
      "Epoch [6/60], Iter [409/633], LR: 0.005000, Loss: 3.6575, top1: 7.8125\n",
      "Epoch [6/60], Iter [410/633], LR: 0.005000, Loss: 3.6510, top1: 9.3750\n",
      "Epoch [6/60], Iter [411/633], LR: 0.005000, Loss: 3.5989, top1: 10.9375\n",
      "Epoch [6/60], Iter [412/633], LR: 0.005000, Loss: 3.6038, top1: 14.0625\n",
      "Epoch [6/60], Iter [413/633], LR: 0.005000, Loss: 3.6014, top1: 7.8125\n",
      "Epoch [6/60], Iter [414/633], LR: 0.005000, Loss: 3.5932, top1: 12.5000\n",
      "Epoch [6/60], Iter [415/633], LR: 0.005000, Loss: 3.6055, top1: 10.9375\n",
      "Epoch [6/60], Iter [416/633], LR: 0.005000, Loss: 3.6231, top1: 10.9375\n",
      "Epoch [6/60], Iter [417/633], LR: 0.005000, Loss: 3.6319, top1: 9.3750\n",
      "Epoch [6/60], Iter [418/633], LR: 0.005000, Loss: 3.5735, top1: 14.0625\n",
      "Epoch [6/60], Iter [419/633], LR: 0.005000, Loss: 3.5843, top1: 15.6250\n",
      "Epoch [6/60], Iter [420/633], LR: 0.005000, Loss: 3.6279, top1: 6.2500\n",
      "Epoch [6/60], Iter [421/633], LR: 0.005000, Loss: 3.5827, top1: 9.3750\n",
      "Epoch [6/60], Iter [422/633], LR: 0.005000, Loss: 3.6095, top1: 10.9375\n",
      "Epoch [6/60], Iter [423/633], LR: 0.005000, Loss: 3.5989, top1: 12.5000\n",
      "Epoch [6/60], Iter [424/633], LR: 0.005000, Loss: 3.6199, top1: 10.9375\n",
      "Epoch [6/60], Iter [425/633], LR: 0.005000, Loss: 3.6220, top1: 7.8125\n",
      "Epoch [6/60], Iter [426/633], LR: 0.005000, Loss: 3.6207, top1: 12.5000\n",
      "Epoch [6/60], Iter [427/633], LR: 0.005000, Loss: 3.5963, top1: 17.1875\n",
      "Epoch [6/60], Iter [428/633], LR: 0.005000, Loss: 3.6137, top1: 14.0625\n",
      "Epoch [6/60], Iter [429/633], LR: 0.005000, Loss: 3.6006, top1: 10.9375\n",
      "Epoch [6/60], Iter [430/633], LR: 0.005000, Loss: 3.6177, top1: 9.3750\n",
      "Epoch [6/60], Iter [431/633], LR: 0.005000, Loss: 3.5903, top1: 9.3750\n",
      "Epoch [6/60], Iter [432/633], LR: 0.005000, Loss: 3.6016, top1: 15.6250\n",
      "Epoch [6/60], Iter [433/633], LR: 0.005000, Loss: 3.5969, top1: 10.9375\n",
      "Epoch [6/60], Iter [434/633], LR: 0.005000, Loss: 3.5873, top1: 15.6250\n",
      "Epoch [6/60], Iter [435/633], LR: 0.005000, Loss: 3.5954, top1: 9.3750\n",
      "Epoch [6/60], Iter [436/633], LR: 0.005000, Loss: 3.5832, top1: 12.5000\n",
      "Epoch [6/60], Iter [437/633], LR: 0.005000, Loss: 3.5714, top1: 23.4375\n",
      "Epoch [6/60], Iter [438/633], LR: 0.005000, Loss: 3.5934, top1: 9.3750\n",
      "Epoch [6/60], Iter [439/633], LR: 0.005000, Loss: 3.5599, top1: 18.7500\n",
      "Epoch [6/60], Iter [440/633], LR: 0.005000, Loss: 3.5756, top1: 14.0625\n",
      "Epoch [6/60], Iter [441/633], LR: 0.005000, Loss: 3.5958, top1: 14.0625\n",
      "Epoch [6/60], Iter [442/633], LR: 0.005000, Loss: 3.5694, top1: 20.3125\n",
      "Epoch [6/60], Iter [443/633], LR: 0.005000, Loss: 3.6011, top1: 12.5000\n",
      "Epoch [6/60], Iter [444/633], LR: 0.005000, Loss: 3.6380, top1: 3.1250\n",
      "Epoch [6/60], Iter [445/633], LR: 0.005000, Loss: 3.6258, top1: 7.8125\n",
      "Epoch [6/60], Iter [446/633], LR: 0.005000, Loss: 3.6056, top1: 12.5000\n",
      "Epoch [6/60], Iter [447/633], LR: 0.005000, Loss: 3.6304, top1: 9.3750\n",
      "Epoch [6/60], Iter [448/633], LR: 0.005000, Loss: 3.6082, top1: 9.3750\n",
      "Epoch [6/60], Iter [449/633], LR: 0.005000, Loss: 3.6247, top1: 7.8125\n",
      "Epoch [6/60], Iter [450/633], LR: 0.005000, Loss: 3.5963, top1: 17.1875\n",
      "Epoch [6/60], Iter [451/633], LR: 0.005000, Loss: 3.6052, top1: 6.2500\n",
      "Epoch [6/60], Iter [452/633], LR: 0.005000, Loss: 3.6078, top1: 9.3750\n",
      "Epoch [6/60], Iter [453/633], LR: 0.005000, Loss: 3.5523, top1: 18.7500\n",
      "Epoch [6/60], Iter [454/633], LR: 0.005000, Loss: 3.6213, top1: 7.8125\n",
      "Epoch [6/60], Iter [455/633], LR: 0.005000, Loss: 3.6188, top1: 9.3750\n",
      "Epoch [6/60], Iter [456/633], LR: 0.005000, Loss: 3.5682, top1: 14.0625\n",
      "Epoch [6/60], Iter [457/633], LR: 0.005000, Loss: 3.5913, top1: 12.5000\n",
      "Epoch [6/60], Iter [458/633], LR: 0.005000, Loss: 3.6535, top1: 4.6875\n",
      "Epoch [6/60], Iter [459/633], LR: 0.005000, Loss: 3.6067, top1: 9.3750\n",
      "Epoch [6/60], Iter [460/633], LR: 0.005000, Loss: 3.6410, top1: 6.2500\n",
      "Epoch [6/60], Iter [461/633], LR: 0.005000, Loss: 3.6466, top1: 6.2500\n",
      "Epoch [6/60], Iter [462/633], LR: 0.005000, Loss: 3.6177, top1: 10.9375\n",
      "Epoch [6/60], Iter [463/633], LR: 0.005000, Loss: 3.6184, top1: 12.5000\n",
      "Epoch [6/60], Iter [464/633], LR: 0.005000, Loss: 3.6113, top1: 15.6250\n",
      "Epoch [6/60], Iter [465/633], LR: 0.005000, Loss: 3.5948, top1: 10.9375\n",
      "Epoch [6/60], Iter [466/633], LR: 0.005000, Loss: 3.5943, top1: 10.9375\n",
      "Epoch [6/60], Iter [467/633], LR: 0.005000, Loss: 3.6125, top1: 14.0625\n",
      "Epoch [6/60], Iter [468/633], LR: 0.005000, Loss: 3.5949, top1: 17.1875\n",
      "Epoch [6/60], Iter [469/633], LR: 0.005000, Loss: 3.6331, top1: 4.6875\n",
      "Epoch [6/60], Iter [470/633], LR: 0.005000, Loss: 3.5880, top1: 12.5000\n",
      "Epoch [6/60], Iter [471/633], LR: 0.005000, Loss: 3.6553, top1: 6.2500\n",
      "Epoch [6/60], Iter [472/633], LR: 0.005000, Loss: 3.6116, top1: 9.3750\n",
      "Epoch [6/60], Iter [473/633], LR: 0.005000, Loss: 3.6095, top1: 10.9375\n",
      "Epoch [6/60], Iter [474/633], LR: 0.005000, Loss: 3.5798, top1: 18.7500\n",
      "Epoch [6/60], Iter [475/633], LR: 0.005000, Loss: 3.6572, top1: 7.8125\n",
      "Epoch [6/60], Iter [476/633], LR: 0.005000, Loss: 3.5945, top1: 10.9375\n",
      "Epoch [6/60], Iter [477/633], LR: 0.005000, Loss: 3.5757, top1: 4.6875\n",
      "Epoch [6/60], Iter [478/633], LR: 0.005000, Loss: 3.6019, top1: 9.3750\n",
      "Epoch [6/60], Iter [479/633], LR: 0.005000, Loss: 3.5791, top1: 17.1875\n",
      "Epoch [6/60], Iter [480/633], LR: 0.005000, Loss: 3.5907, top1: 7.8125\n",
      "Epoch [6/60], Iter [481/633], LR: 0.005000, Loss: 3.6135, top1: 10.9375\n",
      "Epoch [6/60], Iter [482/633], LR: 0.005000, Loss: 3.5952, top1: 15.6250\n",
      "Epoch [6/60], Iter [483/633], LR: 0.005000, Loss: 3.5914, top1: 9.3750\n",
      "Epoch [6/60], Iter [484/633], LR: 0.005000, Loss: 3.5822, top1: 17.1875\n",
      "Epoch [6/60], Iter [485/633], LR: 0.005000, Loss: 3.5983, top1: 12.5000\n",
      "Epoch [6/60], Iter [486/633], LR: 0.005000, Loss: 3.6132, top1: 9.3750\n",
      "Epoch [6/60], Iter [487/633], LR: 0.005000, Loss: 3.5992, top1: 9.3750\n",
      "Epoch [6/60], Iter [488/633], LR: 0.005000, Loss: 3.5835, top1: 14.0625\n",
      "Epoch [6/60], Iter [489/633], LR: 0.005000, Loss: 3.6185, top1: 7.8125\n",
      "Epoch [6/60], Iter [490/633], LR: 0.005000, Loss: 3.5989, top1: 7.8125\n",
      "Epoch [6/60], Iter [491/633], LR: 0.005000, Loss: 3.6368, top1: 6.2500\n",
      "Epoch [6/60], Iter [492/633], LR: 0.005000, Loss: 3.5584, top1: 20.3125\n",
      "Epoch [6/60], Iter [493/633], LR: 0.005000, Loss: 3.5839, top1: 15.6250\n",
      "Epoch [6/60], Iter [494/633], LR: 0.005000, Loss: 3.6011, top1: 15.6250\n",
      "Epoch [6/60], Iter [495/633], LR: 0.005000, Loss: 3.5776, top1: 17.1875\n",
      "Epoch [6/60], Iter [496/633], LR: 0.005000, Loss: 3.5818, top1: 12.5000\n",
      "Epoch [6/60], Iter [497/633], LR: 0.005000, Loss: 3.6128, top1: 9.3750\n",
      "Epoch [6/60], Iter [498/633], LR: 0.005000, Loss: 3.5825, top1: 12.5000\n",
      "Epoch [6/60], Iter [499/633], LR: 0.005000, Loss: 3.5706, top1: 15.6250\n",
      "Epoch [6/60], Iter [500/633], LR: 0.005000, Loss: 3.6482, top1: 6.2500\n",
      "Epoch [6/60], Iter [501/633], LR: 0.005000, Loss: 3.6441, top1: 7.8125\n",
      "Epoch [6/60], Iter [502/633], LR: 0.005000, Loss: 3.6410, top1: 4.6875\n",
      "Epoch [6/60], Iter [503/633], LR: 0.005000, Loss: 3.6307, top1: 20.3125\n",
      "Epoch [6/60], Iter [504/633], LR: 0.005000, Loss: 3.6014, top1: 12.5000\n",
      "Epoch [6/60], Iter [505/633], LR: 0.005000, Loss: 3.6198, top1: 9.3750\n",
      "Epoch [6/60], Iter [506/633], LR: 0.005000, Loss: 3.6028, top1: 10.9375\n",
      "Epoch [6/60], Iter [507/633], LR: 0.005000, Loss: 3.6590, top1: 1.5625\n",
      "Epoch [6/60], Iter [508/633], LR: 0.005000, Loss: 3.6049, top1: 14.0625\n",
      "Epoch [6/60], Iter [509/633], LR: 0.005000, Loss: 3.6320, top1: 9.3750\n",
      "Epoch [6/60], Iter [510/633], LR: 0.005000, Loss: 3.5897, top1: 9.3750\n",
      "Epoch [6/60], Iter [511/633], LR: 0.005000, Loss: 3.6405, top1: 7.8125\n",
      "Epoch [6/60], Iter [512/633], LR: 0.005000, Loss: 3.6168, top1: 9.3750\n",
      "Epoch [6/60], Iter [513/633], LR: 0.005000, Loss: 3.5891, top1: 9.3750\n",
      "Epoch [6/60], Iter [514/633], LR: 0.005000, Loss: 3.6042, top1: 9.3750\n",
      "Epoch [6/60], Iter [515/633], LR: 0.005000, Loss: 3.5806, top1: 14.0625\n",
      "Epoch [6/60], Iter [516/633], LR: 0.005000, Loss: 3.6082, top1: 12.5000\n",
      "Epoch [6/60], Iter [517/633], LR: 0.005000, Loss: 3.6077, top1: 12.5000\n",
      "Epoch [6/60], Iter [518/633], LR: 0.005000, Loss: 3.6171, top1: 10.9375\n",
      "Epoch [6/60], Iter [519/633], LR: 0.005000, Loss: 3.6258, top1: 14.0625\n",
      "Epoch [6/60], Iter [520/633], LR: 0.005000, Loss: 3.6043, top1: 9.3750\n",
      "Epoch [6/60], Iter [521/633], LR: 0.005000, Loss: 3.6211, top1: 10.9375\n",
      "Epoch [6/60], Iter [522/633], LR: 0.005000, Loss: 3.6130, top1: 7.8125\n",
      "Epoch [6/60], Iter [523/633], LR: 0.005000, Loss: 3.6090, top1: 12.5000\n",
      "Epoch [6/60], Iter [524/633], LR: 0.005000, Loss: 3.6363, top1: 15.6250\n",
      "Epoch [6/60], Iter [525/633], LR: 0.005000, Loss: 3.6022, top1: 12.5000\n",
      "Epoch [6/60], Iter [526/633], LR: 0.005000, Loss: 3.6232, top1: 10.9375\n",
      "Epoch [6/60], Iter [527/633], LR: 0.005000, Loss: 3.5749, top1: 17.1875\n",
      "Epoch [6/60], Iter [528/633], LR: 0.005000, Loss: 3.5972, top1: 10.9375\n",
      "Epoch [6/60], Iter [529/633], LR: 0.005000, Loss: 3.6303, top1: 14.0625\n",
      "Epoch [6/60], Iter [530/633], LR: 0.005000, Loss: 3.5815, top1: 15.6250\n",
      "Epoch [6/60], Iter [531/633], LR: 0.005000, Loss: 3.5940, top1: 15.6250\n",
      "Epoch [6/60], Iter [532/633], LR: 0.005000, Loss: 3.5783, top1: 9.3750\n",
      "Epoch [6/60], Iter [533/633], LR: 0.005000, Loss: 3.5671, top1: 17.1875\n",
      "Epoch [6/60], Iter [534/633], LR: 0.005000, Loss: 3.5893, top1: 17.1875\n",
      "Epoch [6/60], Iter [535/633], LR: 0.005000, Loss: 3.5601, top1: 18.7500\n",
      "Epoch [6/60], Iter [536/633], LR: 0.005000, Loss: 3.6334, top1: 10.9375\n",
      "Epoch [6/60], Iter [537/633], LR: 0.005000, Loss: 3.6354, top1: 12.5000\n",
      "Epoch [6/60], Iter [538/633], LR: 0.005000, Loss: 3.6247, top1: 12.5000\n",
      "Epoch [6/60], Iter [539/633], LR: 0.005000, Loss: 3.6032, top1: 10.9375\n",
      "Epoch [6/60], Iter [540/633], LR: 0.005000, Loss: 3.6382, top1: 7.8125\n",
      "Epoch [6/60], Iter [541/633], LR: 0.005000, Loss: 3.6327, top1: 7.8125\n",
      "Epoch [6/60], Iter [542/633], LR: 0.005000, Loss: 3.6008, top1: 6.2500\n",
      "Epoch [6/60], Iter [543/633], LR: 0.005000, Loss: 3.6087, top1: 12.5000\n",
      "Epoch [6/60], Iter [544/633], LR: 0.005000, Loss: 3.6027, top1: 12.5000\n",
      "Epoch [6/60], Iter [545/633], LR: 0.005000, Loss: 3.6054, top1: 15.6250\n",
      "Epoch [6/60], Iter [546/633], LR: 0.005000, Loss: 3.6289, top1: 6.2500\n",
      "Epoch [6/60], Iter [547/633], LR: 0.005000, Loss: 3.5692, top1: 14.0625\n",
      "Epoch [6/60], Iter [548/633], LR: 0.005000, Loss: 3.5989, top1: 15.6250\n",
      "Epoch [6/60], Iter [549/633], LR: 0.005000, Loss: 3.6434, top1: 9.3750\n",
      "Epoch [6/60], Iter [550/633], LR: 0.005000, Loss: 3.5997, top1: 9.3750\n",
      "Epoch [6/60], Iter [551/633], LR: 0.005000, Loss: 3.6065, top1: 12.5000\n",
      "Epoch [6/60], Iter [552/633], LR: 0.005000, Loss: 3.6147, top1: 9.3750\n",
      "Epoch [6/60], Iter [553/633], LR: 0.005000, Loss: 3.5876, top1: 15.6250\n",
      "Epoch [6/60], Iter [554/633], LR: 0.005000, Loss: 3.5807, top1: 17.1875\n",
      "Epoch [6/60], Iter [555/633], LR: 0.005000, Loss: 3.5790, top1: 12.5000\n",
      "Epoch [6/60], Iter [556/633], LR: 0.005000, Loss: 3.5854, top1: 14.0625\n",
      "Epoch [6/60], Iter [557/633], LR: 0.005000, Loss: 3.6078, top1: 10.9375\n",
      "Epoch [6/60], Iter [558/633], LR: 0.005000, Loss: 3.6069, top1: 10.9375\n",
      "Epoch [6/60], Iter [559/633], LR: 0.005000, Loss: 3.6370, top1: 7.8125\n",
      "Epoch [6/60], Iter [560/633], LR: 0.005000, Loss: 3.6193, top1: 10.9375\n",
      "Epoch [6/60], Iter [561/633], LR: 0.005000, Loss: 3.6032, top1: 14.0625\n",
      "Epoch [6/60], Iter [562/633], LR: 0.005000, Loss: 3.6068, top1: 12.5000\n",
      "Epoch [6/60], Iter [563/633], LR: 0.005000, Loss: 3.5887, top1: 14.0625\n",
      "Epoch [6/60], Iter [564/633], LR: 0.005000, Loss: 3.6075, top1: 7.8125\n",
      "Epoch [6/60], Iter [565/633], LR: 0.005000, Loss: 3.6034, top1: 17.1875\n",
      "Epoch [6/60], Iter [566/633], LR: 0.005000, Loss: 3.5303, top1: 18.7500\n",
      "Epoch [6/60], Iter [567/633], LR: 0.005000, Loss: 3.5746, top1: 12.5000\n",
      "Epoch [6/60], Iter [568/633], LR: 0.005000, Loss: 3.6187, top1: 10.9375\n",
      "Epoch [6/60], Iter [569/633], LR: 0.005000, Loss: 3.5950, top1: 17.1875\n",
      "Epoch [6/60], Iter [570/633], LR: 0.005000, Loss: 3.6101, top1: 7.8125\n",
      "Epoch [6/60], Iter [571/633], LR: 0.005000, Loss: 3.6026, top1: 7.8125\n",
      "Epoch [6/60], Iter [572/633], LR: 0.005000, Loss: 3.5894, top1: 15.6250\n",
      "Epoch [6/60], Iter [573/633], LR: 0.005000, Loss: 3.5925, top1: 15.6250\n",
      "Epoch [6/60], Iter [574/633], LR: 0.005000, Loss: 3.5960, top1: 18.7500\n",
      "Epoch [6/60], Iter [575/633], LR: 0.005000, Loss: 3.6258, top1: 6.2500\n",
      "Epoch [6/60], Iter [576/633], LR: 0.005000, Loss: 3.5943, top1: 14.0625\n",
      "Epoch [6/60], Iter [577/633], LR: 0.005000, Loss: 3.6018, top1: 9.3750\n",
      "Epoch [6/60], Iter [578/633], LR: 0.005000, Loss: 3.5813, top1: 9.3750\n",
      "Epoch [6/60], Iter [579/633], LR: 0.005000, Loss: 3.5746, top1: 10.9375\n",
      "Epoch [6/60], Iter [580/633], LR: 0.005000, Loss: 3.6055, top1: 9.3750\n",
      "Epoch [6/60], Iter [581/633], LR: 0.005000, Loss: 3.5972, top1: 7.8125\n",
      "Epoch [6/60], Iter [582/633], LR: 0.005000, Loss: 3.6185, top1: 9.3750\n",
      "Epoch [6/60], Iter [583/633], LR: 0.005000, Loss: 3.5738, top1: 14.0625\n",
      "Epoch [6/60], Iter [584/633], LR: 0.005000, Loss: 3.5794, top1: 12.5000\n",
      "Epoch [6/60], Iter [585/633], LR: 0.005000, Loss: 3.5932, top1: 14.0625\n",
      "Epoch [6/60], Iter [586/633], LR: 0.005000, Loss: 3.6125, top1: 12.5000\n",
      "Epoch [6/60], Iter [587/633], LR: 0.005000, Loss: 3.5540, top1: 15.6250\n",
      "Epoch [6/60], Iter [588/633], LR: 0.005000, Loss: 3.6223, top1: 10.9375\n",
      "Epoch [6/60], Iter [589/633], LR: 0.005000, Loss: 3.5869, top1: 15.6250\n",
      "Epoch [6/60], Iter [590/633], LR: 0.005000, Loss: 3.5996, top1: 15.6250\n",
      "Epoch [6/60], Iter [591/633], LR: 0.005000, Loss: 3.5979, top1: 14.0625\n",
      "Epoch [6/60], Iter [592/633], LR: 0.005000, Loss: 3.5876, top1: 17.1875\n",
      "Epoch [6/60], Iter [593/633], LR: 0.005000, Loss: 3.6148, top1: 14.0625\n",
      "Epoch [6/60], Iter [594/633], LR: 0.005000, Loss: 3.5927, top1: 12.5000\n",
      "Epoch [6/60], Iter [595/633], LR: 0.005000, Loss: 3.6430, top1: 10.9375\n",
      "Epoch [6/60], Iter [596/633], LR: 0.005000, Loss: 3.6289, top1: 7.8125\n",
      "Epoch [6/60], Iter [597/633], LR: 0.005000, Loss: 3.6267, top1: 14.0625\n",
      "Epoch [6/60], Iter [598/633], LR: 0.005000, Loss: 3.6161, top1: 12.5000\n",
      "Epoch [6/60], Iter [599/633], LR: 0.005000, Loss: 3.6328, top1: 7.8125\n",
      "Epoch [6/60], Iter [600/633], LR: 0.005000, Loss: 3.6420, top1: 9.3750\n",
      "Epoch [6/60], Iter [601/633], LR: 0.005000, Loss: 3.6143, top1: 10.9375\n",
      "Epoch [6/60], Iter [602/633], LR: 0.005000, Loss: 3.5766, top1: 15.6250\n",
      "Epoch [6/60], Iter [603/633], LR: 0.005000, Loss: 3.5881, top1: 15.6250\n",
      "Epoch [6/60], Iter [604/633], LR: 0.005000, Loss: 3.5830, top1: 12.5000\n",
      "Epoch [6/60], Iter [605/633], LR: 0.005000, Loss: 3.5737, top1: 14.0625\n",
      "Epoch [6/60], Iter [606/633], LR: 0.005000, Loss: 3.6125, top1: 10.9375\n",
      "Epoch [6/60], Iter [607/633], LR: 0.005000, Loss: 3.6108, top1: 9.3750\n",
      "Epoch [6/60], Iter [608/633], LR: 0.005000, Loss: 3.5847, top1: 12.5000\n",
      "Epoch [6/60], Iter [609/633], LR: 0.005000, Loss: 3.6179, top1: 14.0625\n",
      "Epoch [6/60], Iter [610/633], LR: 0.005000, Loss: 3.5781, top1: 10.9375\n",
      "Epoch [6/60], Iter [611/633], LR: 0.005000, Loss: 3.5767, top1: 9.3750\n",
      "Epoch [6/60], Iter [612/633], LR: 0.005000, Loss: 3.5797, top1: 10.9375\n",
      "Epoch [6/60], Iter [613/633], LR: 0.005000, Loss: 3.5978, top1: 18.7500\n",
      "Epoch [6/60], Iter [614/633], LR: 0.005000, Loss: 3.6284, top1: 9.3750\n",
      "Epoch [6/60], Iter [615/633], LR: 0.005000, Loss: 3.5842, top1: 7.8125\n",
      "Epoch [6/60], Iter [616/633], LR: 0.005000, Loss: 3.5730, top1: 15.6250\n",
      "Epoch [6/60], Iter [617/633], LR: 0.005000, Loss: 3.6268, top1: 4.6875\n",
      "Epoch [6/60], Iter [618/633], LR: 0.005000, Loss: 3.6040, top1: 15.6250\n",
      "Epoch [6/60], Iter [619/633], LR: 0.005000, Loss: 3.5978, top1: 15.6250\n",
      "Epoch [6/60], Iter [620/633], LR: 0.005000, Loss: 3.6240, top1: 9.3750\n",
      "Epoch [6/60], Iter [621/633], LR: 0.005000, Loss: 3.5686, top1: 15.6250\n",
      "Epoch [6/60], Iter [622/633], LR: 0.005000, Loss: 3.5844, top1: 12.5000\n",
      "Epoch [6/60], Iter [623/633], LR: 0.005000, Loss: 3.6029, top1: 9.3750\n",
      "Epoch [6/60], Iter [624/633], LR: 0.005000, Loss: 3.6161, top1: 4.6875\n",
      "Epoch [6/60], Iter [625/633], LR: 0.005000, Loss: 3.5834, top1: 15.6250\n",
      "Epoch [6/60], Iter [626/633], LR: 0.005000, Loss: 3.5844, top1: 17.1875\n",
      "Epoch [6/60], Iter [627/633], LR: 0.005000, Loss: 3.6100, top1: 10.9375\n",
      "Epoch [6/60], Iter [628/633], LR: 0.005000, Loss: 3.5677, top1: 18.7500\n",
      "Epoch [6/60], Iter [629/633], LR: 0.005000, Loss: 3.6201, top1: 7.8125\n",
      "Epoch [6/60], Iter [630/633], LR: 0.005000, Loss: 3.6062, top1: 10.9375\n",
      "Epoch [6/60], Iter [631/633], LR: 0.005000, Loss: 3.6093, top1: 14.0625\n",
      "Epoch [6/60], Iter [632/633], LR: 0.005000, Loss: 3.6339, top1: 7.8125\n",
      "Epoch [6/60], Iter [633/633], LR: 0.005000, Loss: 3.6041, top1: 12.5000\n",
      "Epoch [6/60], Iter [634/633], LR: 0.005000, Loss: 3.5891, top1: 9.6774\n",
      "Epoch [6/60], Val_Loss: 3.6148, Val_top1: 11.6009, best_top1: 10.9050\n",
      "epoch time: 4.422588189442952 min\n",
      "Taking top1 snapshot...\n",
      "Epoch [7/60], Iter [1/633], LR: 0.005000, Loss: 3.5632, top1: 14.0625\n",
      "Epoch [7/60], Iter [2/633], LR: 0.005000, Loss: 3.6103, top1: 7.8125\n",
      "Epoch [7/60], Iter [3/633], LR: 0.005000, Loss: 3.6043, top1: 6.2500\n",
      "Epoch [7/60], Iter [4/633], LR: 0.005000, Loss: 3.5957, top1: 14.0625\n",
      "Epoch [7/60], Iter [5/633], LR: 0.005000, Loss: 3.6531, top1: 7.8125\n",
      "Epoch [7/60], Iter [6/633], LR: 0.005000, Loss: 3.6221, top1: 6.2500\n",
      "Epoch [7/60], Iter [7/633], LR: 0.005000, Loss: 3.6029, top1: 9.3750\n",
      "Epoch [7/60], Iter [8/633], LR: 0.005000, Loss: 3.5865, top1: 18.7500\n",
      "Epoch [7/60], Iter [9/633], LR: 0.005000, Loss: 3.6150, top1: 12.5000\n",
      "Epoch [7/60], Iter [10/633], LR: 0.005000, Loss: 3.6620, top1: 10.9375\n",
      "Epoch [7/60], Iter [11/633], LR: 0.005000, Loss: 3.5612, top1: 12.5000\n",
      "Epoch [7/60], Iter [12/633], LR: 0.005000, Loss: 3.6240, top1: 7.8125\n",
      "Epoch [7/60], Iter [13/633], LR: 0.005000, Loss: 3.6278, top1: 14.0625\n",
      "Epoch [7/60], Iter [14/633], LR: 0.005000, Loss: 3.6158, top1: 7.8125\n",
      "Epoch [7/60], Iter [15/633], LR: 0.005000, Loss: 3.6571, top1: 4.6875\n",
      "Epoch [7/60], Iter [16/633], LR: 0.005000, Loss: 3.5709, top1: 14.0625\n",
      "Epoch [7/60], Iter [17/633], LR: 0.005000, Loss: 3.5784, top1: 12.5000\n",
      "Epoch [7/60], Iter [18/633], LR: 0.005000, Loss: 3.5735, top1: 14.0625\n",
      "Epoch [7/60], Iter [19/633], LR: 0.005000, Loss: 3.5997, top1: 14.0625\n",
      "Epoch [7/60], Iter [20/633], LR: 0.005000, Loss: 3.6411, top1: 10.9375\n",
      "Epoch [7/60], Iter [21/633], LR: 0.005000, Loss: 3.6253, top1: 7.8125\n",
      "Epoch [7/60], Iter [22/633], LR: 0.005000, Loss: 3.6078, top1: 14.0625\n",
      "Epoch [7/60], Iter [23/633], LR: 0.005000, Loss: 3.6292, top1: 10.9375\n",
      "Epoch [7/60], Iter [24/633], LR: 0.005000, Loss: 3.6144, top1: 10.9375\n",
      "Epoch [7/60], Iter [25/633], LR: 0.005000, Loss: 3.5662, top1: 17.1875\n",
      "Epoch [7/60], Iter [26/633], LR: 0.005000, Loss: 3.6140, top1: 10.9375\n",
      "Epoch [7/60], Iter [27/633], LR: 0.005000, Loss: 3.6108, top1: 7.8125\n",
      "Epoch [7/60], Iter [28/633], LR: 0.005000, Loss: 3.6034, top1: 10.9375\n",
      "Epoch [7/60], Iter [29/633], LR: 0.005000, Loss: 3.6396, top1: 7.8125\n",
      "Epoch [7/60], Iter [30/633], LR: 0.005000, Loss: 3.6421, top1: 4.6875\n",
      "Epoch [7/60], Iter [31/633], LR: 0.005000, Loss: 3.6254, top1: 9.3750\n",
      "Epoch [7/60], Iter [32/633], LR: 0.005000, Loss: 3.6100, top1: 12.5000\n",
      "Epoch [7/60], Iter [33/633], LR: 0.005000, Loss: 3.5924, top1: 4.6875\n",
      "Epoch [7/60], Iter [34/633], LR: 0.005000, Loss: 3.6548, top1: 12.5000\n",
      "Epoch [7/60], Iter [35/633], LR: 0.005000, Loss: 3.5795, top1: 17.1875\n",
      "Epoch [7/60], Iter [36/633], LR: 0.005000, Loss: 3.6266, top1: 14.0625\n",
      "Epoch [7/60], Iter [37/633], LR: 0.005000, Loss: 3.6258, top1: 10.9375\n",
      "Epoch [7/60], Iter [38/633], LR: 0.005000, Loss: 3.6271, top1: 7.8125\n",
      "Epoch [7/60], Iter [39/633], LR: 0.005000, Loss: 3.6001, top1: 15.6250\n",
      "Epoch [7/60], Iter [40/633], LR: 0.005000, Loss: 3.5542, top1: 23.4375\n",
      "Epoch [7/60], Iter [41/633], LR: 0.005000, Loss: 3.6176, top1: 6.2500\n",
      "Epoch [7/60], Iter [42/633], LR: 0.005000, Loss: 3.6005, top1: 9.3750\n",
      "Epoch [7/60], Iter [43/633], LR: 0.005000, Loss: 3.6079, top1: 10.9375\n",
      "Epoch [7/60], Iter [44/633], LR: 0.005000, Loss: 3.6312, top1: 10.9375\n",
      "Epoch [7/60], Iter [45/633], LR: 0.005000, Loss: 3.6408, top1: 6.2500\n",
      "Epoch [7/60], Iter [46/633], LR: 0.005000, Loss: 3.6187, top1: 14.0625\n",
      "Epoch [7/60], Iter [47/633], LR: 0.005000, Loss: 3.5649, top1: 15.6250\n",
      "Epoch [7/60], Iter [48/633], LR: 0.005000, Loss: 3.6106, top1: 9.3750\n",
      "Epoch [7/60], Iter [49/633], LR: 0.005000, Loss: 3.5986, top1: 9.3750\n",
      "Epoch [7/60], Iter [50/633], LR: 0.005000, Loss: 3.6280, top1: 9.3750\n",
      "Epoch [7/60], Iter [51/633], LR: 0.005000, Loss: 3.6248, top1: 6.2500\n",
      "Epoch [7/60], Iter [52/633], LR: 0.005000, Loss: 3.6469, top1: 9.3750\n",
      "Epoch [7/60], Iter [53/633], LR: 0.005000, Loss: 3.6539, top1: 3.1250\n",
      "Epoch [7/60], Iter [54/633], LR: 0.005000, Loss: 3.6264, top1: 7.8125\n",
      "Epoch [7/60], Iter [55/633], LR: 0.005000, Loss: 3.5692, top1: 18.7500\n",
      "Epoch [7/60], Iter [56/633], LR: 0.005000, Loss: 3.6122, top1: 9.3750\n",
      "Epoch [7/60], Iter [57/633], LR: 0.005000, Loss: 3.6306, top1: 6.2500\n",
      "Epoch [7/60], Iter [58/633], LR: 0.005000, Loss: 3.6318, top1: 9.3750\n",
      "Epoch [7/60], Iter [59/633], LR: 0.005000, Loss: 3.6061, top1: 7.8125\n",
      "Epoch [7/60], Iter [60/633], LR: 0.005000, Loss: 3.5951, top1: 14.0625\n",
      "Epoch [7/60], Iter [61/633], LR: 0.005000, Loss: 3.6023, top1: 9.3750\n",
      "Epoch [7/60], Iter [62/633], LR: 0.005000, Loss: 3.5980, top1: 15.6250\n",
      "Epoch [7/60], Iter [63/633], LR: 0.005000, Loss: 3.6477, top1: 4.6875\n",
      "Epoch [7/60], Iter [64/633], LR: 0.005000, Loss: 3.6310, top1: 10.9375\n",
      "Epoch [7/60], Iter [65/633], LR: 0.005000, Loss: 3.6289, top1: 12.5000\n",
      "Epoch [7/60], Iter [66/633], LR: 0.005000, Loss: 3.5733, top1: 18.7500\n",
      "Epoch [7/60], Iter [67/633], LR: 0.005000, Loss: 3.6282, top1: 4.6875\n",
      "Epoch [7/60], Iter [68/633], LR: 0.005000, Loss: 3.5705, top1: 17.1875\n",
      "Epoch [7/60], Iter [69/633], LR: 0.005000, Loss: 3.6065, top1: 9.3750\n",
      "Epoch [7/60], Iter [70/633], LR: 0.005000, Loss: 3.6119, top1: 6.2500\n",
      "Epoch [7/60], Iter [71/633], LR: 0.005000, Loss: 3.5964, top1: 9.3750\n",
      "Epoch [7/60], Iter [72/633], LR: 0.005000, Loss: 3.5981, top1: 10.9375\n",
      "Epoch [7/60], Iter [73/633], LR: 0.005000, Loss: 3.5947, top1: 10.9375\n",
      "Epoch [7/60], Iter [74/633], LR: 0.005000, Loss: 3.6125, top1: 12.5000\n",
      "Epoch [7/60], Iter [75/633], LR: 0.005000, Loss: 3.6195, top1: 12.5000\n",
      "Epoch [7/60], Iter [76/633], LR: 0.005000, Loss: 3.6437, top1: 6.2500\n",
      "Epoch [7/60], Iter [77/633], LR: 0.005000, Loss: 3.5450, top1: 12.5000\n",
      "Epoch [7/60], Iter [78/633], LR: 0.005000, Loss: 3.5909, top1: 12.5000\n",
      "Epoch [7/60], Iter [79/633], LR: 0.005000, Loss: 3.5973, top1: 10.9375\n",
      "Epoch [7/60], Iter [80/633], LR: 0.005000, Loss: 3.6027, top1: 7.8125\n",
      "Epoch [7/60], Iter [81/633], LR: 0.005000, Loss: 3.6248, top1: 6.2500\n",
      "Epoch [7/60], Iter [82/633], LR: 0.005000, Loss: 3.5979, top1: 9.3750\n",
      "Epoch [7/60], Iter [83/633], LR: 0.005000, Loss: 3.5960, top1: 17.1875\n",
      "Epoch [7/60], Iter [84/633], LR: 0.005000, Loss: 3.6433, top1: 9.3750\n",
      "Epoch [7/60], Iter [85/633], LR: 0.005000, Loss: 3.5766, top1: 9.3750\n",
      "Epoch [7/60], Iter [86/633], LR: 0.005000, Loss: 3.5985, top1: 14.0625\n",
      "Epoch [7/60], Iter [87/633], LR: 0.005000, Loss: 3.6339, top1: 9.3750\n",
      "Epoch [7/60], Iter [88/633], LR: 0.005000, Loss: 3.6120, top1: 12.5000\n",
      "Epoch [7/60], Iter [89/633], LR: 0.005000, Loss: 3.6087, top1: 7.8125\n",
      "Epoch [7/60], Iter [90/633], LR: 0.005000, Loss: 3.5587, top1: 15.6250\n",
      "Epoch [7/60], Iter [91/633], LR: 0.005000, Loss: 3.5629, top1: 20.3125\n",
      "Epoch [7/60], Iter [92/633], LR: 0.005000, Loss: 3.5822, top1: 15.6250\n",
      "Epoch [7/60], Iter [93/633], LR: 0.005000, Loss: 3.5548, top1: 17.1875\n",
      "Epoch [7/60], Iter [94/633], LR: 0.005000, Loss: 3.5988, top1: 17.1875\n",
      "Epoch [7/60], Iter [95/633], LR: 0.005000, Loss: 3.5701, top1: 18.7500\n",
      "Epoch [7/60], Iter [96/633], LR: 0.005000, Loss: 3.5814, top1: 10.9375\n",
      "Epoch [7/60], Iter [97/633], LR: 0.005000, Loss: 3.5873, top1: 15.6250\n",
      "Epoch [7/60], Iter [98/633], LR: 0.005000, Loss: 3.6117, top1: 6.2500\n",
      "Epoch [7/60], Iter [99/633], LR: 0.005000, Loss: 3.5597, top1: 14.0625\n",
      "Epoch [7/60], Iter [100/633], LR: 0.005000, Loss: 3.6061, top1: 6.2500\n",
      "Epoch [7/60], Iter [101/633], LR: 0.005000, Loss: 3.5823, top1: 9.3750\n",
      "Epoch [7/60], Iter [102/633], LR: 0.005000, Loss: 3.5323, top1: 12.5000\n",
      "Epoch [7/60], Iter [103/633], LR: 0.005000, Loss: 3.5821, top1: 18.7500\n",
      "Epoch [7/60], Iter [104/633], LR: 0.005000, Loss: 3.6174, top1: 9.3750\n",
      "Epoch [7/60], Iter [105/633], LR: 0.005000, Loss: 3.5592, top1: 18.7500\n",
      "Epoch [7/60], Iter [106/633], LR: 0.005000, Loss: 3.5706, top1: 15.6250\n",
      "Epoch [7/60], Iter [107/633], LR: 0.005000, Loss: 3.5802, top1: 14.0625\n",
      "Epoch [7/60], Iter [108/633], LR: 0.005000, Loss: 3.5816, top1: 17.1875\n",
      "Epoch [7/60], Iter [109/633], LR: 0.005000, Loss: 3.6484, top1: 7.8125\n",
      "Epoch [7/60], Iter [110/633], LR: 0.005000, Loss: 3.6367, top1: 9.3750\n",
      "Epoch [7/60], Iter [111/633], LR: 0.005000, Loss: 3.5849, top1: 17.1875\n",
      "Epoch [7/60], Iter [112/633], LR: 0.005000, Loss: 3.6180, top1: 9.3750\n",
      "Epoch [7/60], Iter [113/633], LR: 0.005000, Loss: 3.6062, top1: 12.5000\n",
      "Epoch [7/60], Iter [114/633], LR: 0.005000, Loss: 3.5711, top1: 20.3125\n",
      "Epoch [7/60], Iter [115/633], LR: 0.005000, Loss: 3.5617, top1: 18.7500\n",
      "Epoch [7/60], Iter [116/633], LR: 0.005000, Loss: 3.5875, top1: 9.3750\n",
      "Epoch [7/60], Iter [117/633], LR: 0.005000, Loss: 3.5985, top1: 12.5000\n",
      "Epoch [7/60], Iter [118/633], LR: 0.005000, Loss: 3.6013, top1: 10.9375\n",
      "Epoch [7/60], Iter [119/633], LR: 0.005000, Loss: 3.5696, top1: 12.5000\n",
      "Epoch [7/60], Iter [120/633], LR: 0.005000, Loss: 3.5296, top1: 17.1875\n",
      "Epoch [7/60], Iter [121/633], LR: 0.005000, Loss: 3.5793, top1: 9.3750\n",
      "Epoch [7/60], Iter [122/633], LR: 0.005000, Loss: 3.5816, top1: 10.9375\n",
      "Epoch [7/60], Iter [123/633], LR: 0.005000, Loss: 3.6110, top1: 14.0625\n",
      "Epoch [7/60], Iter [124/633], LR: 0.005000, Loss: 3.5780, top1: 12.5000\n",
      "Epoch [7/60], Iter [125/633], LR: 0.005000, Loss: 3.5893, top1: 14.0625\n",
      "Epoch [7/60], Iter [126/633], LR: 0.005000, Loss: 3.5716, top1: 14.0625\n",
      "Epoch [7/60], Iter [127/633], LR: 0.005000, Loss: 3.6129, top1: 12.5000\n",
      "Epoch [7/60], Iter [128/633], LR: 0.005000, Loss: 3.6047, top1: 15.6250\n",
      "Epoch [7/60], Iter [129/633], LR: 0.005000, Loss: 3.6314, top1: 7.8125\n",
      "Epoch [7/60], Iter [130/633], LR: 0.005000, Loss: 3.6119, top1: 7.8125\n",
      "Epoch [7/60], Iter [131/633], LR: 0.005000, Loss: 3.6093, top1: 14.0625\n",
      "Epoch [7/60], Iter [132/633], LR: 0.005000, Loss: 3.5805, top1: 15.6250\n",
      "Epoch [7/60], Iter [133/633], LR: 0.005000, Loss: 3.5952, top1: 10.9375\n",
      "Epoch [7/60], Iter [134/633], LR: 0.005000, Loss: 3.5912, top1: 14.0625\n",
      "Epoch [7/60], Iter [135/633], LR: 0.005000, Loss: 3.5773, top1: 14.0625\n",
      "Epoch [7/60], Iter [136/633], LR: 0.005000, Loss: 3.6382, top1: 7.8125\n",
      "Epoch [7/60], Iter [137/633], LR: 0.005000, Loss: 3.6338, top1: 9.3750\n",
      "Epoch [7/60], Iter [138/633], LR: 0.005000, Loss: 3.6022, top1: 9.3750\n",
      "Epoch [7/60], Iter [139/633], LR: 0.005000, Loss: 3.6083, top1: 12.5000\n",
      "Epoch [7/60], Iter [140/633], LR: 0.005000, Loss: 3.6148, top1: 10.9375\n",
      "Epoch [7/60], Iter [141/633], LR: 0.005000, Loss: 3.6472, top1: 9.3750\n",
      "Epoch [7/60], Iter [142/633], LR: 0.005000, Loss: 3.6186, top1: 14.0625\n",
      "Epoch [7/60], Iter [143/633], LR: 0.005000, Loss: 3.5965, top1: 10.9375\n",
      "Epoch [7/60], Iter [144/633], LR: 0.005000, Loss: 3.6185, top1: 10.9375\n",
      "Epoch [7/60], Iter [145/633], LR: 0.005000, Loss: 3.5557, top1: 23.4375\n",
      "Epoch [7/60], Iter [146/633], LR: 0.005000, Loss: 3.5616, top1: 14.0625\n",
      "Epoch [7/60], Iter [147/633], LR: 0.005000, Loss: 3.6053, top1: 9.3750\n",
      "Epoch [7/60], Iter [148/633], LR: 0.005000, Loss: 3.5709, top1: 12.5000\n",
      "Epoch [7/60], Iter [149/633], LR: 0.005000, Loss: 3.6181, top1: 10.9375\n",
      "Epoch [7/60], Iter [150/633], LR: 0.005000, Loss: 3.6186, top1: 6.2500\n",
      "Epoch [7/60], Iter [151/633], LR: 0.005000, Loss: 3.5989, top1: 14.0625\n",
      "Epoch [7/60], Iter [152/633], LR: 0.005000, Loss: 3.5794, top1: 18.7500\n",
      "Epoch [7/60], Iter [153/633], LR: 0.005000, Loss: 3.5776, top1: 20.3125\n",
      "Epoch [7/60], Iter [154/633], LR: 0.005000, Loss: 3.6023, top1: 14.0625\n",
      "Epoch [7/60], Iter [155/633], LR: 0.005000, Loss: 3.5532, top1: 17.1875\n",
      "Epoch [7/60], Iter [156/633], LR: 0.005000, Loss: 3.5797, top1: 21.8750\n",
      "Epoch [7/60], Iter [157/633], LR: 0.005000, Loss: 3.6036, top1: 10.9375\n",
      "Epoch [7/60], Iter [158/633], LR: 0.005000, Loss: 3.5561, top1: 20.3125\n",
      "Epoch [7/60], Iter [159/633], LR: 0.005000, Loss: 3.6068, top1: 9.3750\n",
      "Epoch [7/60], Iter [160/633], LR: 0.005000, Loss: 3.6148, top1: 7.8125\n",
      "Epoch [7/60], Iter [161/633], LR: 0.005000, Loss: 3.6358, top1: 7.8125\n",
      "Epoch [7/60], Iter [162/633], LR: 0.005000, Loss: 3.5999, top1: 14.0625\n",
      "Epoch [7/60], Iter [163/633], LR: 0.005000, Loss: 3.6194, top1: 10.9375\n",
      "Epoch [7/60], Iter [164/633], LR: 0.005000, Loss: 3.5890, top1: 10.9375\n",
      "Epoch [7/60], Iter [165/633], LR: 0.005000, Loss: 3.6117, top1: 10.9375\n",
      "Epoch [7/60], Iter [166/633], LR: 0.005000, Loss: 3.6027, top1: 10.9375\n",
      "Epoch [7/60], Iter [167/633], LR: 0.005000, Loss: 3.5875, top1: 14.0625\n",
      "Epoch [7/60], Iter [168/633], LR: 0.005000, Loss: 3.6053, top1: 14.0625\n",
      "Epoch [7/60], Iter [169/633], LR: 0.005000, Loss: 3.6128, top1: 12.5000\n",
      "Epoch [7/60], Iter [170/633], LR: 0.005000, Loss: 3.5865, top1: 17.1875\n",
      "Epoch [7/60], Iter [171/633], LR: 0.005000, Loss: 3.6243, top1: 7.8125\n",
      "Epoch [7/60], Iter [172/633], LR: 0.005000, Loss: 3.6031, top1: 12.5000\n",
      "Epoch [7/60], Iter [173/633], LR: 0.005000, Loss: 3.6340, top1: 4.6875\n",
      "Epoch [7/60], Iter [174/633], LR: 0.005000, Loss: 3.6160, top1: 6.2500\n",
      "Epoch [7/60], Iter [175/633], LR: 0.005000, Loss: 3.5953, top1: 14.0625\n",
      "Epoch [7/60], Iter [176/633], LR: 0.005000, Loss: 3.5946, top1: 15.6250\n",
      "Epoch [7/60], Iter [177/633], LR: 0.005000, Loss: 3.5851, top1: 14.0625\n",
      "Epoch [7/60], Iter [178/633], LR: 0.005000, Loss: 3.5956, top1: 15.6250\n",
      "Epoch [7/60], Iter [179/633], LR: 0.005000, Loss: 3.5554, top1: 20.3125\n",
      "Epoch [7/60], Iter [180/633], LR: 0.005000, Loss: 3.6266, top1: 9.3750\n",
      "Epoch [7/60], Iter [181/633], LR: 0.005000, Loss: 3.5901, top1: 12.5000\n",
      "Epoch [7/60], Iter [182/633], LR: 0.005000, Loss: 3.5395, top1: 23.4375\n",
      "Epoch [7/60], Iter [183/633], LR: 0.005000, Loss: 3.5744, top1: 17.1875\n",
      "Epoch [7/60], Iter [184/633], LR: 0.005000, Loss: 3.5665, top1: 17.1875\n",
      "Epoch [7/60], Iter [185/633], LR: 0.005000, Loss: 3.6310, top1: 6.2500\n",
      "Epoch [7/60], Iter [186/633], LR: 0.005000, Loss: 3.5619, top1: 17.1875\n",
      "Epoch [7/60], Iter [187/633], LR: 0.005000, Loss: 3.5825, top1: 10.9375\n",
      "Epoch [7/60], Iter [188/633], LR: 0.005000, Loss: 3.5820, top1: 14.0625\n",
      "Epoch [7/60], Iter [189/633], LR: 0.005000, Loss: 3.5641, top1: 12.5000\n",
      "Epoch [7/60], Iter [190/633], LR: 0.005000, Loss: 3.5694, top1: 12.5000\n",
      "Epoch [7/60], Iter [191/633], LR: 0.005000, Loss: 3.5876, top1: 10.9375\n",
      "Epoch [7/60], Iter [192/633], LR: 0.005000, Loss: 3.6151, top1: 10.9375\n",
      "Epoch [7/60], Iter [193/633], LR: 0.005000, Loss: 3.6449, top1: 7.8125\n",
      "Epoch [7/60], Iter [194/633], LR: 0.005000, Loss: 3.6306, top1: 7.8125\n",
      "Epoch [7/60], Iter [195/633], LR: 0.005000, Loss: 3.5947, top1: 14.0625\n",
      "Epoch [7/60], Iter [196/633], LR: 0.005000, Loss: 3.6415, top1: 9.3750\n",
      "Epoch [7/60], Iter [197/633], LR: 0.005000, Loss: 3.6201, top1: 14.0625\n",
      "Epoch [7/60], Iter [198/633], LR: 0.005000, Loss: 3.6017, top1: 10.9375\n",
      "Epoch [7/60], Iter [199/633], LR: 0.005000, Loss: 3.5858, top1: 15.6250\n",
      "Epoch [7/60], Iter [200/633], LR: 0.005000, Loss: 3.5997, top1: 15.6250\n",
      "Epoch [7/60], Iter [201/633], LR: 0.005000, Loss: 3.5946, top1: 12.5000\n",
      "Epoch [7/60], Iter [202/633], LR: 0.005000, Loss: 3.6042, top1: 7.8125\n",
      "Epoch [7/60], Iter [203/633], LR: 0.005000, Loss: 3.6051, top1: 14.0625\n",
      "Epoch [7/60], Iter [204/633], LR: 0.005000, Loss: 3.6091, top1: 9.3750\n",
      "Epoch [7/60], Iter [205/633], LR: 0.005000, Loss: 3.5980, top1: 14.0625\n",
      "Epoch [7/60], Iter [206/633], LR: 0.005000, Loss: 3.6259, top1: 12.5000\n",
      "Epoch [7/60], Iter [207/633], LR: 0.005000, Loss: 3.6237, top1: 4.6875\n",
      "Epoch [7/60], Iter [208/633], LR: 0.005000, Loss: 3.5645, top1: 15.6250\n",
      "Epoch [7/60], Iter [209/633], LR: 0.005000, Loss: 3.6956, top1: 3.1250\n",
      "Epoch [7/60], Iter [210/633], LR: 0.005000, Loss: 3.5851, top1: 14.0625\n",
      "Epoch [7/60], Iter [211/633], LR: 0.005000, Loss: 3.5543, top1: 20.3125\n",
      "Epoch [7/60], Iter [212/633], LR: 0.005000, Loss: 3.5919, top1: 10.9375\n",
      "Epoch [7/60], Iter [213/633], LR: 0.005000, Loss: 3.6187, top1: 12.5000\n",
      "Epoch [7/60], Iter [214/633], LR: 0.005000, Loss: 3.6058, top1: 12.5000\n",
      "Epoch [7/60], Iter [215/633], LR: 0.005000, Loss: 3.5711, top1: 9.3750\n",
      "Epoch [7/60], Iter [216/633], LR: 0.005000, Loss: 3.5863, top1: 14.0625\n",
      "Epoch [7/60], Iter [217/633], LR: 0.005000, Loss: 3.6188, top1: 7.8125\n",
      "Epoch [7/60], Iter [218/633], LR: 0.005000, Loss: 3.5643, top1: 20.3125\n",
      "Epoch [7/60], Iter [219/633], LR: 0.005000, Loss: 3.6208, top1: 7.8125\n",
      "Epoch [7/60], Iter [220/633], LR: 0.005000, Loss: 3.6158, top1: 6.2500\n",
      "Epoch [7/60], Iter [221/633], LR: 0.005000, Loss: 3.6088, top1: 10.9375\n",
      "Epoch [7/60], Iter [222/633], LR: 0.005000, Loss: 3.5811, top1: 17.1875\n",
      "Epoch [7/60], Iter [223/633], LR: 0.005000, Loss: 3.5836, top1: 14.0625\n",
      "Epoch [7/60], Iter [224/633], LR: 0.005000, Loss: 3.6271, top1: 12.5000\n",
      "Epoch [7/60], Iter [225/633], LR: 0.005000, Loss: 3.5938, top1: 12.5000\n",
      "Epoch [7/60], Iter [226/633], LR: 0.005000, Loss: 3.6384, top1: 9.3750\n",
      "Epoch [7/60], Iter [227/633], LR: 0.005000, Loss: 3.5836, top1: 12.5000\n",
      "Epoch [7/60], Iter [228/633], LR: 0.005000, Loss: 3.6083, top1: 12.5000\n",
      "Epoch [7/60], Iter [229/633], LR: 0.005000, Loss: 3.6171, top1: 9.3750\n",
      "Epoch [7/60], Iter [230/633], LR: 0.005000, Loss: 3.5720, top1: 18.7500\n",
      "Epoch [7/60], Iter [231/633], LR: 0.005000, Loss: 3.5620, top1: 14.0625\n",
      "Epoch [7/60], Iter [232/633], LR: 0.005000, Loss: 3.6104, top1: 10.9375\n",
      "Epoch [7/60], Iter [233/633], LR: 0.005000, Loss: 3.6066, top1: 14.0625\n",
      "Epoch [7/60], Iter [234/633], LR: 0.005000, Loss: 3.5816, top1: 14.0625\n",
      "Epoch [7/60], Iter [235/633], LR: 0.005000, Loss: 3.6182, top1: 7.8125\n",
      "Epoch [7/60], Iter [236/633], LR: 0.005000, Loss: 3.5608, top1: 14.0625\n",
      "Epoch [7/60], Iter [237/633], LR: 0.005000, Loss: 3.5863, top1: 10.9375\n",
      "Epoch [7/60], Iter [238/633], LR: 0.005000, Loss: 3.6324, top1: 6.2500\n",
      "Epoch [7/60], Iter [239/633], LR: 0.005000, Loss: 3.5746, top1: 15.6250\n",
      "Epoch [7/60], Iter [240/633], LR: 0.005000, Loss: 3.5894, top1: 10.9375\n",
      "Epoch [7/60], Iter [241/633], LR: 0.005000, Loss: 3.5848, top1: 20.3125\n",
      "Epoch [7/60], Iter [242/633], LR: 0.005000, Loss: 3.5984, top1: 17.1875\n",
      "Epoch [7/60], Iter [243/633], LR: 0.005000, Loss: 3.6021, top1: 14.0625\n",
      "Epoch [7/60], Iter [244/633], LR: 0.005000, Loss: 3.6074, top1: 10.9375\n",
      "Epoch [7/60], Iter [245/633], LR: 0.005000, Loss: 3.6065, top1: 15.6250\n",
      "Epoch [7/60], Iter [246/633], LR: 0.005000, Loss: 3.6090, top1: 12.5000\n",
      "Epoch [7/60], Iter [247/633], LR: 0.005000, Loss: 3.6026, top1: 12.5000\n",
      "Epoch [7/60], Iter [248/633], LR: 0.005000, Loss: 3.5831, top1: 12.5000\n",
      "Epoch [7/60], Iter [249/633], LR: 0.005000, Loss: 3.6363, top1: 12.5000\n",
      "Epoch [7/60], Iter [250/633], LR: 0.005000, Loss: 3.5682, top1: 14.0625\n",
      "Epoch [7/60], Iter [251/633], LR: 0.005000, Loss: 3.6280, top1: 7.8125\n",
      "Epoch [7/60], Iter [252/633], LR: 0.005000, Loss: 3.5829, top1: 7.8125\n",
      "Epoch [7/60], Iter [253/633], LR: 0.005000, Loss: 3.6324, top1: 9.3750\n",
      "Epoch [7/60], Iter [254/633], LR: 0.005000, Loss: 3.6022, top1: 12.5000\n",
      "Epoch [7/60], Iter [255/633], LR: 0.005000, Loss: 3.5663, top1: 20.3125\n",
      "Epoch [7/60], Iter [256/633], LR: 0.005000, Loss: 3.5931, top1: 10.9375\n",
      "Epoch [7/60], Iter [257/633], LR: 0.005000, Loss: 3.6362, top1: 9.3750\n",
      "Epoch [7/60], Iter [258/633], LR: 0.005000, Loss: 3.6448, top1: 6.2500\n",
      "Epoch [7/60], Iter [259/633], LR: 0.005000, Loss: 3.6406, top1: 6.2500\n",
      "Epoch [7/60], Iter [260/633], LR: 0.005000, Loss: 3.5735, top1: 17.1875\n",
      "Epoch [7/60], Iter [261/633], LR: 0.005000, Loss: 3.6303, top1: 7.8125\n",
      "Epoch [7/60], Iter [262/633], LR: 0.005000, Loss: 3.6290, top1: 12.5000\n",
      "Epoch [7/60], Iter [263/633], LR: 0.005000, Loss: 3.6337, top1: 9.3750\n",
      "Epoch [7/60], Iter [264/633], LR: 0.005000, Loss: 3.5862, top1: 12.5000\n",
      "Epoch [7/60], Iter [265/633], LR: 0.005000, Loss: 3.6170, top1: 9.3750\n",
      "Epoch [7/60], Iter [266/633], LR: 0.005000, Loss: 3.5847, top1: 17.1875\n",
      "Epoch [7/60], Iter [267/633], LR: 0.005000, Loss: 3.6210, top1: 6.2500\n",
      "Epoch [7/60], Iter [268/633], LR: 0.005000, Loss: 3.5827, top1: 15.6250\n",
      "Epoch [7/60], Iter [269/633], LR: 0.005000, Loss: 3.6437, top1: 9.3750\n",
      "Epoch [7/60], Iter [270/633], LR: 0.005000, Loss: 3.6415, top1: 6.2500\n",
      "Epoch [7/60], Iter [271/633], LR: 0.005000, Loss: 3.6485, top1: 7.8125\n",
      "Epoch [7/60], Iter [272/633], LR: 0.005000, Loss: 3.5476, top1: 18.7500\n",
      "Epoch [7/60], Iter [273/633], LR: 0.005000, Loss: 3.5830, top1: 18.7500\n",
      "Epoch [7/60], Iter [274/633], LR: 0.005000, Loss: 3.5704, top1: 12.5000\n",
      "Epoch [7/60], Iter [275/633], LR: 0.005000, Loss: 3.5580, top1: 18.7500\n",
      "Epoch [7/60], Iter [276/633], LR: 0.005000, Loss: 3.6258, top1: 9.3750\n",
      "Epoch [7/60], Iter [277/633], LR: 0.005000, Loss: 3.6300, top1: 6.2500\n",
      "Epoch [7/60], Iter [278/633], LR: 0.005000, Loss: 3.5887, top1: 10.9375\n",
      "Epoch [7/60], Iter [279/633], LR: 0.005000, Loss: 3.6051, top1: 10.9375\n",
      "Epoch [7/60], Iter [280/633], LR: 0.005000, Loss: 3.5989, top1: 10.9375\n",
      "Epoch [7/60], Iter [281/633], LR: 0.005000, Loss: 3.6319, top1: 7.8125\n",
      "Epoch [7/60], Iter [282/633], LR: 0.005000, Loss: 3.6076, top1: 14.0625\n",
      "Epoch [7/60], Iter [283/633], LR: 0.005000, Loss: 3.6367, top1: 3.1250\n",
      "Epoch [7/60], Iter [284/633], LR: 0.005000, Loss: 3.5794, top1: 17.1875\n",
      "Epoch [7/60], Iter [285/633], LR: 0.005000, Loss: 3.6110, top1: 12.5000\n",
      "Epoch [7/60], Iter [286/633], LR: 0.005000, Loss: 3.5611, top1: 10.9375\n",
      "Epoch [7/60], Iter [287/633], LR: 0.005000, Loss: 3.5776, top1: 7.8125\n",
      "Epoch [7/60], Iter [288/633], LR: 0.005000, Loss: 3.6105, top1: 9.3750\n",
      "Epoch [7/60], Iter [289/633], LR: 0.005000, Loss: 3.5925, top1: 12.5000\n",
      "Epoch [7/60], Iter [290/633], LR: 0.005000, Loss: 3.6276, top1: 12.5000\n",
      "Epoch [7/60], Iter [291/633], LR: 0.005000, Loss: 3.5700, top1: 20.3125\n",
      "Epoch [7/60], Iter [292/633], LR: 0.005000, Loss: 3.6306, top1: 9.3750\n",
      "Epoch [7/60], Iter [293/633], LR: 0.005000, Loss: 3.6508, top1: 7.8125\n",
      "Epoch [7/60], Iter [294/633], LR: 0.005000, Loss: 3.5955, top1: 21.8750\n",
      "Epoch [7/60], Iter [295/633], LR: 0.005000, Loss: 3.5636, top1: 14.0625\n",
      "Epoch [7/60], Iter [296/633], LR: 0.005000, Loss: 3.6119, top1: 12.5000\n",
      "Epoch [7/60], Iter [297/633], LR: 0.005000, Loss: 3.5858, top1: 17.1875\n",
      "Epoch [7/60], Iter [298/633], LR: 0.005000, Loss: 3.5543, top1: 20.3125\n",
      "Epoch [7/60], Iter [299/633], LR: 0.005000, Loss: 3.6022, top1: 7.8125\n",
      "Epoch [7/60], Iter [300/633], LR: 0.005000, Loss: 3.6351, top1: 10.9375\n",
      "Epoch [7/60], Iter [301/633], LR: 0.005000, Loss: 3.6200, top1: 10.9375\n",
      "Epoch [7/60], Iter [302/633], LR: 0.005000, Loss: 3.5965, top1: 10.9375\n",
      "Epoch [7/60], Iter [303/633], LR: 0.005000, Loss: 3.5739, top1: 9.3750\n",
      "Epoch [7/60], Iter [304/633], LR: 0.005000, Loss: 3.5911, top1: 15.6250\n",
      "Epoch [7/60], Iter [305/633], LR: 0.005000, Loss: 3.5573, top1: 18.7500\n",
      "Epoch [7/60], Iter [306/633], LR: 0.005000, Loss: 3.5426, top1: 17.1875\n",
      "Epoch [7/60], Iter [307/633], LR: 0.005000, Loss: 3.5550, top1: 10.9375\n",
      "Epoch [7/60], Iter [308/633], LR: 0.005000, Loss: 3.5695, top1: 17.1875\n",
      "Epoch [7/60], Iter [309/633], LR: 0.005000, Loss: 3.6164, top1: 14.0625\n",
      "Epoch [7/60], Iter [310/633], LR: 0.005000, Loss: 3.5738, top1: 14.0625\n",
      "Epoch [7/60], Iter [311/633], LR: 0.005000, Loss: 3.5887, top1: 12.5000\n",
      "Epoch [7/60], Iter [312/633], LR: 0.005000, Loss: 3.6005, top1: 9.3750\n",
      "Epoch [7/60], Iter [313/633], LR: 0.005000, Loss: 3.5854, top1: 14.0625\n",
      "Epoch [7/60], Iter [314/633], LR: 0.005000, Loss: 3.5685, top1: 14.0625\n",
      "Epoch [7/60], Iter [315/633], LR: 0.005000, Loss: 3.6137, top1: 10.9375\n",
      "Epoch [7/60], Iter [316/633], LR: 0.005000, Loss: 3.6012, top1: 9.3750\n",
      "Epoch [7/60], Iter [317/633], LR: 0.005000, Loss: 3.6305, top1: 10.9375\n",
      "Epoch [7/60], Iter [318/633], LR: 0.005000, Loss: 3.5989, top1: 10.9375\n",
      "Epoch [7/60], Iter [319/633], LR: 0.005000, Loss: 3.6064, top1: 14.0625\n",
      "Epoch [7/60], Iter [320/633], LR: 0.005000, Loss: 3.6280, top1: 10.9375\n",
      "Epoch [7/60], Iter [321/633], LR: 0.005000, Loss: 3.5990, top1: 12.5000\n",
      "Epoch [7/60], Iter [322/633], LR: 0.005000, Loss: 3.5991, top1: 6.2500\n",
      "Epoch [7/60], Iter [323/633], LR: 0.005000, Loss: 3.5584, top1: 14.0625\n",
      "Epoch [7/60], Iter [324/633], LR: 0.005000, Loss: 3.5690, top1: 20.3125\n",
      "Epoch [7/60], Iter [325/633], LR: 0.005000, Loss: 3.5866, top1: 10.9375\n",
      "Epoch [7/60], Iter [326/633], LR: 0.005000, Loss: 3.5576, top1: 20.3125\n",
      "Epoch [7/60], Iter [327/633], LR: 0.005000, Loss: 3.5865, top1: 12.5000\n",
      "Epoch [7/60], Iter [328/633], LR: 0.005000, Loss: 3.5917, top1: 7.8125\n",
      "Epoch [7/60], Iter [329/633], LR: 0.005000, Loss: 3.5432, top1: 18.7500\n",
      "Epoch [7/60], Iter [330/633], LR: 0.005000, Loss: 3.5935, top1: 7.8125\n",
      "Epoch [7/60], Iter [331/633], LR: 0.005000, Loss: 3.5903, top1: 14.0625\n",
      "Epoch [7/60], Iter [332/633], LR: 0.005000, Loss: 3.6187, top1: 7.8125\n",
      "Epoch [7/60], Iter [333/633], LR: 0.005000, Loss: 3.6285, top1: 4.6875\n",
      "Epoch [7/60], Iter [334/633], LR: 0.005000, Loss: 3.5723, top1: 17.1875\n",
      "Epoch [7/60], Iter [335/633], LR: 0.005000, Loss: 3.5927, top1: 12.5000\n",
      "Epoch [7/60], Iter [336/633], LR: 0.005000, Loss: 3.6226, top1: 9.3750\n",
      "Epoch [7/60], Iter [337/633], LR: 0.005000, Loss: 3.6359, top1: 9.3750\n",
      "Epoch [7/60], Iter [338/633], LR: 0.005000, Loss: 3.6107, top1: 12.5000\n",
      "Epoch [7/60], Iter [339/633], LR: 0.005000, Loss: 3.5903, top1: 9.3750\n",
      "Epoch [7/60], Iter [340/633], LR: 0.005000, Loss: 3.6446, top1: 7.8125\n",
      "Epoch [7/60], Iter [341/633], LR: 0.005000, Loss: 3.5771, top1: 10.9375\n",
      "Epoch [7/60], Iter [342/633], LR: 0.005000, Loss: 3.5540, top1: 12.5000\n",
      "Epoch [7/60], Iter [343/633], LR: 0.005000, Loss: 3.5983, top1: 12.5000\n",
      "Epoch [7/60], Iter [344/633], LR: 0.005000, Loss: 3.5632, top1: 15.6250\n",
      "Epoch [7/60], Iter [345/633], LR: 0.005000, Loss: 3.5962, top1: 17.1875\n",
      "Epoch [7/60], Iter [346/633], LR: 0.005000, Loss: 3.5836, top1: 15.6250\n",
      "Epoch [7/60], Iter [347/633], LR: 0.005000, Loss: 3.5840, top1: 14.0625\n",
      "Epoch [7/60], Iter [348/633], LR: 0.005000, Loss: 3.5855, top1: 4.6875\n",
      "Epoch [7/60], Iter [349/633], LR: 0.005000, Loss: 3.5721, top1: 10.9375\n",
      "Epoch [7/60], Iter [350/633], LR: 0.005000, Loss: 3.6267, top1: 10.9375\n",
      "Epoch [7/60], Iter [351/633], LR: 0.005000, Loss: 3.6113, top1: 10.9375\n",
      "Epoch [7/60], Iter [352/633], LR: 0.005000, Loss: 3.5883, top1: 15.6250\n",
      "Epoch [7/60], Iter [353/633], LR: 0.005000, Loss: 3.5996, top1: 17.1875\n",
      "Epoch [7/60], Iter [354/633], LR: 0.005000, Loss: 3.6186, top1: 6.2500\n",
      "Epoch [7/60], Iter [355/633], LR: 0.005000, Loss: 3.6375, top1: 9.3750\n",
      "Epoch [7/60], Iter [356/633], LR: 0.005000, Loss: 3.5893, top1: 10.9375\n",
      "Epoch [7/60], Iter [357/633], LR: 0.005000, Loss: 3.5597, top1: 12.5000\n",
      "Epoch [7/60], Iter [358/633], LR: 0.005000, Loss: 3.6036, top1: 10.9375\n",
      "Epoch [7/60], Iter [359/633], LR: 0.005000, Loss: 3.6193, top1: 6.2500\n",
      "Epoch [7/60], Iter [360/633], LR: 0.005000, Loss: 3.5767, top1: 15.6250\n",
      "Epoch [7/60], Iter [361/633], LR: 0.005000, Loss: 3.6410, top1: 9.3750\n",
      "Epoch [7/60], Iter [362/633], LR: 0.005000, Loss: 3.5731, top1: 15.6250\n",
      "Epoch [7/60], Iter [363/633], LR: 0.005000, Loss: 3.5870, top1: 14.0625\n",
      "Epoch [7/60], Iter [364/633], LR: 0.005000, Loss: 3.5786, top1: 15.6250\n",
      "Epoch [7/60], Iter [365/633], LR: 0.005000, Loss: 3.5984, top1: 9.3750\n",
      "Epoch [7/60], Iter [366/633], LR: 0.005000, Loss: 3.6220, top1: 15.6250\n",
      "Epoch [7/60], Iter [367/633], LR: 0.005000, Loss: 3.6296, top1: 7.8125\n",
      "Epoch [7/60], Iter [368/633], LR: 0.005000, Loss: 3.6120, top1: 10.9375\n",
      "Epoch [7/60], Iter [369/633], LR: 0.005000, Loss: 3.5643, top1: 15.6250\n",
      "Epoch [7/60], Iter [370/633], LR: 0.005000, Loss: 3.5611, top1: 18.7500\n",
      "Epoch [7/60], Iter [371/633], LR: 0.005000, Loss: 3.6364, top1: 10.9375\n",
      "Epoch [7/60], Iter [372/633], LR: 0.005000, Loss: 3.6226, top1: 9.3750\n",
      "Epoch [7/60], Iter [373/633], LR: 0.005000, Loss: 3.6304, top1: 7.8125\n",
      "Epoch [7/60], Iter [374/633], LR: 0.005000, Loss: 3.5769, top1: 15.6250\n",
      "Epoch [7/60], Iter [375/633], LR: 0.005000, Loss: 3.6022, top1: 10.9375\n",
      "Epoch [7/60], Iter [376/633], LR: 0.005000, Loss: 3.5732, top1: 12.5000\n",
      "Epoch [7/60], Iter [377/633], LR: 0.005000, Loss: 3.6339, top1: 3.1250\n",
      "Epoch [7/60], Iter [378/633], LR: 0.005000, Loss: 3.5899, top1: 10.9375\n",
      "Epoch [7/60], Iter [379/633], LR: 0.005000, Loss: 3.6169, top1: 15.6250\n",
      "Epoch [7/60], Iter [380/633], LR: 0.005000, Loss: 3.5518, top1: 17.1875\n",
      "Epoch [7/60], Iter [381/633], LR: 0.005000, Loss: 3.5541, top1: 17.1875\n",
      "Epoch [7/60], Iter [382/633], LR: 0.005000, Loss: 3.5762, top1: 14.0625\n",
      "Epoch [7/60], Iter [383/633], LR: 0.005000, Loss: 3.5685, top1: 17.1875\n",
      "Epoch [7/60], Iter [384/633], LR: 0.005000, Loss: 3.6135, top1: 6.2500\n",
      "Epoch [7/60], Iter [385/633], LR: 0.005000, Loss: 3.5564, top1: 18.7500\n",
      "Epoch [7/60], Iter [386/633], LR: 0.005000, Loss: 3.5437, top1: 14.0625\n",
      "Epoch [7/60], Iter [387/633], LR: 0.005000, Loss: 3.5342, top1: 23.4375\n",
      "Epoch [7/60], Iter [388/633], LR: 0.005000, Loss: 3.5434, top1: 20.3125\n",
      "Epoch [7/60], Iter [389/633], LR: 0.005000, Loss: 3.5819, top1: 10.9375\n",
      "Epoch [7/60], Iter [390/633], LR: 0.005000, Loss: 3.5867, top1: 10.9375\n",
      "Epoch [7/60], Iter [391/633], LR: 0.005000, Loss: 3.6161, top1: 7.8125\n",
      "Epoch [7/60], Iter [392/633], LR: 0.005000, Loss: 3.5800, top1: 12.5000\n",
      "Epoch [7/60], Iter [393/633], LR: 0.005000, Loss: 3.6008, top1: 9.3750\n",
      "Epoch [7/60], Iter [394/633], LR: 0.005000, Loss: 3.5839, top1: 14.0625\n",
      "Epoch [7/60], Iter [395/633], LR: 0.005000, Loss: 3.5904, top1: 17.1875\n",
      "Epoch [7/60], Iter [396/633], LR: 0.005000, Loss: 3.6293, top1: 7.8125\n",
      "Epoch [7/60], Iter [397/633], LR: 0.005000, Loss: 3.5788, top1: 12.5000\n",
      "Epoch [7/60], Iter [398/633], LR: 0.005000, Loss: 3.6164, top1: 14.0625\n",
      "Epoch [7/60], Iter [399/633], LR: 0.005000, Loss: 3.6174, top1: 12.5000\n",
      "Epoch [7/60], Iter [400/633], LR: 0.005000, Loss: 3.5819, top1: 12.5000\n",
      "Epoch [7/60], Iter [401/633], LR: 0.005000, Loss: 3.5592, top1: 14.0625\n",
      "Epoch [7/60], Iter [402/633], LR: 0.005000, Loss: 3.5711, top1: 14.0625\n",
      "Epoch [7/60], Iter [403/633], LR: 0.005000, Loss: 3.6504, top1: 6.2500\n",
      "Epoch [7/60], Iter [404/633], LR: 0.005000, Loss: 3.6330, top1: 12.5000\n",
      "Epoch [7/60], Iter [405/633], LR: 0.005000, Loss: 3.6125, top1: 3.1250\n",
      "Epoch [7/60], Iter [406/633], LR: 0.005000, Loss: 3.5771, top1: 17.1875\n",
      "Epoch [7/60], Iter [407/633], LR: 0.005000, Loss: 3.5606, top1: 18.7500\n",
      "Epoch [7/60], Iter [408/633], LR: 0.005000, Loss: 3.5343, top1: 23.4375\n",
      "Epoch [7/60], Iter [409/633], LR: 0.005000, Loss: 3.5570, top1: 21.8750\n",
      "Epoch [7/60], Iter [410/633], LR: 0.005000, Loss: 3.5977, top1: 12.5000\n",
      "Epoch [7/60], Iter [411/633], LR: 0.005000, Loss: 3.5874, top1: 12.5000\n",
      "Epoch [7/60], Iter [412/633], LR: 0.005000, Loss: 3.6150, top1: 10.9375\n",
      "Epoch [7/60], Iter [413/633], LR: 0.005000, Loss: 3.6394, top1: 12.5000\n",
      "Epoch [7/60], Iter [414/633], LR: 0.005000, Loss: 3.5405, top1: 20.3125\n",
      "Epoch [7/60], Iter [415/633], LR: 0.005000, Loss: 3.5660, top1: 17.1875\n",
      "Epoch [7/60], Iter [416/633], LR: 0.005000, Loss: 3.5666, top1: 18.7500\n",
      "Epoch [7/60], Iter [417/633], LR: 0.005000, Loss: 3.5844, top1: 14.0625\n",
      "Epoch [7/60], Iter [418/633], LR: 0.005000, Loss: 3.5575, top1: 17.1875\n",
      "Epoch [7/60], Iter [419/633], LR: 0.005000, Loss: 3.6244, top1: 7.8125\n",
      "Epoch [7/60], Iter [420/633], LR: 0.005000, Loss: 3.6270, top1: 14.0625\n",
      "Epoch [7/60], Iter [421/633], LR: 0.005000, Loss: 3.6137, top1: 12.5000\n",
      "Epoch [7/60], Iter [422/633], LR: 0.005000, Loss: 3.6392, top1: 4.6875\n",
      "Epoch [7/60], Iter [423/633], LR: 0.005000, Loss: 3.6109, top1: 7.8125\n",
      "Epoch [7/60], Iter [424/633], LR: 0.005000, Loss: 3.6128, top1: 9.3750\n",
      "Epoch [7/60], Iter [425/633], LR: 0.005000, Loss: 3.6046, top1: 17.1875\n",
      "Epoch [7/60], Iter [426/633], LR: 0.005000, Loss: 3.5930, top1: 12.5000\n",
      "Epoch [7/60], Iter [427/633], LR: 0.005000, Loss: 3.6425, top1: 6.2500\n",
      "Epoch [7/60], Iter [428/633], LR: 0.005000, Loss: 3.6061, top1: 7.8125\n",
      "Epoch [7/60], Iter [429/633], LR: 0.005000, Loss: 3.5686, top1: 18.7500\n",
      "Epoch [7/60], Iter [430/633], LR: 0.005000, Loss: 3.6312, top1: 7.8125\n",
      "Epoch [7/60], Iter [431/633], LR: 0.005000, Loss: 3.6156, top1: 6.2500\n",
      "Epoch [7/60], Iter [432/633], LR: 0.005000, Loss: 3.6442, top1: 7.8125\n",
      "Epoch [7/60], Iter [433/633], LR: 0.005000, Loss: 3.5792, top1: 10.9375\n",
      "Epoch [7/60], Iter [434/633], LR: 0.005000, Loss: 3.5711, top1: 12.5000\n",
      "Epoch [7/60], Iter [435/633], LR: 0.005000, Loss: 3.5877, top1: 15.6250\n",
      "Epoch [7/60], Iter [436/633], LR: 0.005000, Loss: 3.6457, top1: 9.3750\n",
      "Epoch [7/60], Iter [437/633], LR: 0.005000, Loss: 3.6083, top1: 7.8125\n",
      "Epoch [7/60], Iter [438/633], LR: 0.005000, Loss: 3.6058, top1: 14.0625\n",
      "Epoch [7/60], Iter [439/633], LR: 0.005000, Loss: 3.6220, top1: 7.8125\n",
      "Epoch [7/60], Iter [440/633], LR: 0.005000, Loss: 3.5917, top1: 12.5000\n",
      "Epoch [7/60], Iter [441/633], LR: 0.005000, Loss: 3.6010, top1: 12.5000\n",
      "Epoch [7/60], Iter [442/633], LR: 0.005000, Loss: 3.5696, top1: 18.7500\n",
      "Epoch [7/60], Iter [443/633], LR: 0.005000, Loss: 3.6044, top1: 9.3750\n",
      "Epoch [7/60], Iter [444/633], LR: 0.005000, Loss: 3.5932, top1: 15.6250\n",
      "Epoch [7/60], Iter [445/633], LR: 0.005000, Loss: 3.5979, top1: 9.3750\n",
      "Epoch [7/60], Iter [446/633], LR: 0.005000, Loss: 3.6290, top1: 10.9375\n",
      "Epoch [7/60], Iter [447/633], LR: 0.005000, Loss: 3.6014, top1: 12.5000\n",
      "Epoch [7/60], Iter [448/633], LR: 0.005000, Loss: 3.6165, top1: 12.5000\n",
      "Epoch [7/60], Iter [449/633], LR: 0.005000, Loss: 3.6005, top1: 10.9375\n",
      "Epoch [7/60], Iter [450/633], LR: 0.005000, Loss: 3.6047, top1: 7.8125\n",
      "Epoch [7/60], Iter [451/633], LR: 0.005000, Loss: 3.6071, top1: 10.9375\n",
      "Epoch [7/60], Iter [452/633], LR: 0.005000, Loss: 3.5663, top1: 18.7500\n",
      "Epoch [7/60], Iter [453/633], LR: 0.005000, Loss: 3.5557, top1: 20.3125\n",
      "Epoch [7/60], Iter [454/633], LR: 0.005000, Loss: 3.6142, top1: 9.3750\n",
      "Epoch [7/60], Iter [455/633], LR: 0.005000, Loss: 3.5340, top1: 14.0625\n",
      "Epoch [7/60], Iter [456/633], LR: 0.005000, Loss: 3.6103, top1: 10.9375\n",
      "Epoch [7/60], Iter [457/633], LR: 0.005000, Loss: 3.5937, top1: 10.9375\n",
      "Epoch [7/60], Iter [458/633], LR: 0.005000, Loss: 3.5725, top1: 14.0625\n",
      "Epoch [7/60], Iter [459/633], LR: 0.005000, Loss: 3.6269, top1: 4.6875\n",
      "Epoch [7/60], Iter [460/633], LR: 0.005000, Loss: 3.6144, top1: 6.2500\n",
      "Epoch [7/60], Iter [461/633], LR: 0.005000, Loss: 3.6305, top1: 3.1250\n",
      "Epoch [7/60], Iter [462/633], LR: 0.005000, Loss: 3.5742, top1: 15.6250\n",
      "Epoch [7/60], Iter [463/633], LR: 0.005000, Loss: 3.5686, top1: 17.1875\n",
      "Epoch [7/60], Iter [464/633], LR: 0.005000, Loss: 3.6236, top1: 6.2500\n",
      "Epoch [7/60], Iter [465/633], LR: 0.005000, Loss: 3.6014, top1: 14.0625\n",
      "Epoch [7/60], Iter [466/633], LR: 0.005000, Loss: 3.5152, top1: 21.8750\n",
      "Epoch [7/60], Iter [467/633], LR: 0.005000, Loss: 3.6193, top1: 10.9375\n",
      "Epoch [7/60], Iter [468/633], LR: 0.005000, Loss: 3.5567, top1: 18.7500\n",
      "Epoch [7/60], Iter [469/633], LR: 0.005000, Loss: 3.5993, top1: 9.3750\n",
      "Epoch [7/60], Iter [470/633], LR: 0.005000, Loss: 3.6350, top1: 6.2500\n",
      "Epoch [7/60], Iter [471/633], LR: 0.005000, Loss: 3.5918, top1: 15.6250\n",
      "Epoch [7/60], Iter [472/633], LR: 0.005000, Loss: 3.6107, top1: 9.3750\n",
      "Epoch [7/60], Iter [473/633], LR: 0.005000, Loss: 3.5843, top1: 9.3750\n",
      "Epoch [7/60], Iter [474/633], LR: 0.005000, Loss: 3.6214, top1: 7.8125\n",
      "Epoch [7/60], Iter [475/633], LR: 0.005000, Loss: 3.5976, top1: 14.0625\n",
      "Epoch [7/60], Iter [476/633], LR: 0.005000, Loss: 3.5977, top1: 15.6250\n",
      "Epoch [7/60], Iter [477/633], LR: 0.005000, Loss: 3.6313, top1: 6.2500\n",
      "Epoch [7/60], Iter [478/633], LR: 0.005000, Loss: 3.6173, top1: 9.3750\n",
      "Epoch [7/60], Iter [479/633], LR: 0.005000, Loss: 3.5791, top1: 12.5000\n",
      "Epoch [7/60], Iter [480/633], LR: 0.005000, Loss: 3.6295, top1: 7.8125\n",
      "Epoch [7/60], Iter [481/633], LR: 0.005000, Loss: 3.5997, top1: 6.2500\n",
      "Epoch [7/60], Iter [482/633], LR: 0.005000, Loss: 3.5905, top1: 9.3750\n",
      "Epoch [7/60], Iter [483/633], LR: 0.005000, Loss: 3.6232, top1: 6.2500\n",
      "Epoch [7/60], Iter [484/633], LR: 0.005000, Loss: 3.6388, top1: 10.9375\n",
      "Epoch [7/60], Iter [485/633], LR: 0.005000, Loss: 3.6079, top1: 7.8125\n",
      "Epoch [7/60], Iter [486/633], LR: 0.005000, Loss: 3.6286, top1: 7.8125\n",
      "Epoch [7/60], Iter [487/633], LR: 0.005000, Loss: 3.5657, top1: 15.6250\n",
      "Epoch [7/60], Iter [488/633], LR: 0.005000, Loss: 3.5952, top1: 17.1875\n",
      "Epoch [7/60], Iter [489/633], LR: 0.005000, Loss: 3.5791, top1: 9.3750\n",
      "Epoch [7/60], Iter [490/633], LR: 0.005000, Loss: 3.5928, top1: 14.0625\n",
      "Epoch [7/60], Iter [491/633], LR: 0.005000, Loss: 3.5900, top1: 10.9375\n",
      "Epoch [7/60], Iter [492/633], LR: 0.005000, Loss: 3.6066, top1: 7.8125\n",
      "Epoch [7/60], Iter [493/633], LR: 0.005000, Loss: 3.6228, top1: 10.9375\n",
      "Epoch [7/60], Iter [494/633], LR: 0.005000, Loss: 3.5731, top1: 23.4375\n",
      "Epoch [7/60], Iter [495/633], LR: 0.005000, Loss: 3.5691, top1: 15.6250\n",
      "Epoch [7/60], Iter [496/633], LR: 0.005000, Loss: 3.5839, top1: 12.5000\n",
      "Epoch [7/60], Iter [497/633], LR: 0.005000, Loss: 3.5897, top1: 9.3750\n",
      "Epoch [7/60], Iter [498/633], LR: 0.005000, Loss: 3.5891, top1: 9.3750\n",
      "Epoch [7/60], Iter [499/633], LR: 0.005000, Loss: 3.6061, top1: 12.5000\n",
      "Epoch [7/60], Iter [500/633], LR: 0.005000, Loss: 3.5792, top1: 4.6875\n",
      "Epoch [7/60], Iter [501/633], LR: 0.005000, Loss: 3.6401, top1: 9.3750\n",
      "Epoch [7/60], Iter [502/633], LR: 0.005000, Loss: 3.5963, top1: 10.9375\n",
      "Epoch [7/60], Iter [503/633], LR: 0.005000, Loss: 3.5990, top1: 9.3750\n",
      "Epoch [7/60], Iter [504/633], LR: 0.005000, Loss: 3.5826, top1: 12.5000\n",
      "Epoch [7/60], Iter [505/633], LR: 0.005000, Loss: 3.5675, top1: 14.0625\n",
      "Epoch [7/60], Iter [506/633], LR: 0.005000, Loss: 3.6186, top1: 15.6250\n",
      "Epoch [7/60], Iter [507/633], LR: 0.005000, Loss: 3.5425, top1: 17.1875\n",
      "Epoch [7/60], Iter [508/633], LR: 0.005000, Loss: 3.6039, top1: 10.9375\n",
      "Epoch [7/60], Iter [509/633], LR: 0.005000, Loss: 3.6051, top1: 9.3750\n",
      "Epoch [7/60], Iter [510/633], LR: 0.005000, Loss: 3.6095, top1: 14.0625\n",
      "Epoch [7/60], Iter [511/633], LR: 0.005000, Loss: 3.5760, top1: 18.7500\n",
      "Epoch [7/60], Iter [512/633], LR: 0.005000, Loss: 3.5885, top1: 12.5000\n",
      "Epoch [7/60], Iter [513/633], LR: 0.005000, Loss: 3.5838, top1: 15.6250\n",
      "Epoch [7/60], Iter [514/633], LR: 0.005000, Loss: 3.5591, top1: 17.1875\n",
      "Epoch [7/60], Iter [515/633], LR: 0.005000, Loss: 3.5913, top1: 9.3750\n",
      "Epoch [7/60], Iter [516/633], LR: 0.005000, Loss: 3.6288, top1: 9.3750\n",
      "Epoch [7/60], Iter [517/633], LR: 0.005000, Loss: 3.6042, top1: 10.9375\n",
      "Epoch [7/60], Iter [518/633], LR: 0.005000, Loss: 3.5747, top1: 17.1875\n",
      "Epoch [7/60], Iter [519/633], LR: 0.005000, Loss: 3.6034, top1: 14.0625\n",
      "Epoch [7/60], Iter [520/633], LR: 0.005000, Loss: 3.5870, top1: 12.5000\n",
      "Epoch [7/60], Iter [521/633], LR: 0.005000, Loss: 3.6073, top1: 14.0625\n",
      "Epoch [7/60], Iter [522/633], LR: 0.005000, Loss: 3.6092, top1: 10.9375\n",
      "Epoch [7/60], Iter [523/633], LR: 0.005000, Loss: 3.5815, top1: 18.7500\n",
      "Epoch [7/60], Iter [524/633], LR: 0.005000, Loss: 3.5827, top1: 12.5000\n",
      "Epoch [7/60], Iter [525/633], LR: 0.005000, Loss: 3.6276, top1: 10.9375\n",
      "Epoch [7/60], Iter [526/633], LR: 0.005000, Loss: 3.6484, top1: 4.6875\n",
      "Epoch [7/60], Iter [527/633], LR: 0.005000, Loss: 3.6167, top1: 9.3750\n",
      "Epoch [7/60], Iter [528/633], LR: 0.005000, Loss: 3.5717, top1: 17.1875\n",
      "Epoch [7/60], Iter [529/633], LR: 0.005000, Loss: 3.5183, top1: 23.4375\n",
      "Epoch [7/60], Iter [530/633], LR: 0.005000, Loss: 3.6468, top1: 6.2500\n",
      "Epoch [7/60], Iter [531/633], LR: 0.005000, Loss: 3.5917, top1: 14.0625\n",
      "Epoch [7/60], Iter [532/633], LR: 0.005000, Loss: 3.5760, top1: 14.0625\n",
      "Epoch [7/60], Iter [533/633], LR: 0.005000, Loss: 3.5796, top1: 15.6250\n",
      "Epoch [7/60], Iter [534/633], LR: 0.005000, Loss: 3.5966, top1: 10.9375\n",
      "Epoch [7/60], Iter [535/633], LR: 0.005000, Loss: 3.5522, top1: 12.5000\n",
      "Epoch [7/60], Iter [536/633], LR: 0.005000, Loss: 3.5747, top1: 15.6250\n",
      "Epoch [7/60], Iter [537/633], LR: 0.005000, Loss: 3.5789, top1: 10.9375\n",
      "Epoch [7/60], Iter [538/633], LR: 0.005000, Loss: 3.5885, top1: 18.7500\n",
      "Epoch [7/60], Iter [539/633], LR: 0.005000, Loss: 3.5736, top1: 14.0625\n",
      "Epoch [7/60], Iter [540/633], LR: 0.005000, Loss: 3.5972, top1: 9.3750\n",
      "Epoch [7/60], Iter [541/633], LR: 0.005000, Loss: 3.6367, top1: 10.9375\n",
      "Epoch [7/60], Iter [542/633], LR: 0.005000, Loss: 3.6014, top1: 15.6250\n",
      "Epoch [7/60], Iter [543/633], LR: 0.005000, Loss: 3.5785, top1: 14.0625\n",
      "Epoch [7/60], Iter [544/633], LR: 0.005000, Loss: 3.6247, top1: 7.8125\n",
      "Epoch [7/60], Iter [545/633], LR: 0.005000, Loss: 3.5740, top1: 15.6250\n",
      "Epoch [7/60], Iter [546/633], LR: 0.005000, Loss: 3.6045, top1: 14.0625\n",
      "Epoch [7/60], Iter [547/633], LR: 0.005000, Loss: 3.5592, top1: 14.0625\n",
      "Epoch [7/60], Iter [548/633], LR: 0.005000, Loss: 3.5774, top1: 17.1875\n",
      "Epoch [7/60], Iter [549/633], LR: 0.005000, Loss: 3.5822, top1: 12.5000\n",
      "Epoch [7/60], Iter [550/633], LR: 0.005000, Loss: 3.5841, top1: 15.6250\n",
      "Epoch [7/60], Iter [551/633], LR: 0.005000, Loss: 3.5763, top1: 10.9375\n",
      "Epoch [7/60], Iter [552/633], LR: 0.005000, Loss: 3.6187, top1: 15.6250\n",
      "Epoch [7/60], Iter [553/633], LR: 0.005000, Loss: 3.5820, top1: 14.0625\n",
      "Epoch [7/60], Iter [554/633], LR: 0.005000, Loss: 3.6410, top1: 4.6875\n",
      "Epoch [7/60], Iter [555/633], LR: 0.005000, Loss: 3.5890, top1: 12.5000\n",
      "Epoch [7/60], Iter [556/633], LR: 0.005000, Loss: 3.5804, top1: 10.9375\n",
      "Epoch [7/60], Iter [557/633], LR: 0.005000, Loss: 3.6271, top1: 4.6875\n",
      "Epoch [7/60], Iter [558/633], LR: 0.005000, Loss: 3.6306, top1: 7.8125\n",
      "Epoch [7/60], Iter [559/633], LR: 0.005000, Loss: 3.6158, top1: 7.8125\n",
      "Epoch [7/60], Iter [560/633], LR: 0.005000, Loss: 3.5719, top1: 15.6250\n",
      "Epoch [7/60], Iter [561/633], LR: 0.005000, Loss: 3.5723, top1: 17.1875\n",
      "Epoch [7/60], Iter [562/633], LR: 0.005000, Loss: 3.5212, top1: 26.5625\n",
      "Epoch [7/60], Iter [563/633], LR: 0.005000, Loss: 3.5328, top1: 20.3125\n",
      "Epoch [7/60], Iter [564/633], LR: 0.005000, Loss: 3.5787, top1: 14.0625\n",
      "Epoch [7/60], Iter [565/633], LR: 0.005000, Loss: 3.6120, top1: 4.6875\n",
      "Epoch [7/60], Iter [566/633], LR: 0.005000, Loss: 3.6195, top1: 9.3750\n",
      "Epoch [7/60], Iter [567/633], LR: 0.005000, Loss: 3.6068, top1: 14.0625\n",
      "Epoch [7/60], Iter [568/633], LR: 0.005000, Loss: 3.5807, top1: 17.1875\n",
      "Epoch [7/60], Iter [569/633], LR: 0.005000, Loss: 3.5551, top1: 14.0625\n",
      "Epoch [7/60], Iter [570/633], LR: 0.005000, Loss: 3.6151, top1: 6.2500\n",
      "Epoch [7/60], Iter [571/633], LR: 0.005000, Loss: 3.5498, top1: 15.6250\n",
      "Epoch [7/60], Iter [572/633], LR: 0.005000, Loss: 3.6479, top1: 9.3750\n",
      "Epoch [7/60], Iter [573/633], LR: 0.005000, Loss: 3.6044, top1: 9.3750\n",
      "Epoch [7/60], Iter [574/633], LR: 0.005000, Loss: 3.6160, top1: 4.6875\n",
      "Epoch [7/60], Iter [575/633], LR: 0.005000, Loss: 3.6067, top1: 12.5000\n",
      "Epoch [7/60], Iter [576/633], LR: 0.005000, Loss: 3.5635, top1: 17.1875\n",
      "Epoch [7/60], Iter [577/633], LR: 0.005000, Loss: 3.5445, top1: 21.8750\n",
      "Epoch [7/60], Iter [578/633], LR: 0.005000, Loss: 3.6402, top1: 3.1250\n",
      "Epoch [7/60], Iter [579/633], LR: 0.005000, Loss: 3.6022, top1: 14.0625\n",
      "Epoch [7/60], Iter [580/633], LR: 0.005000, Loss: 3.6016, top1: 15.6250\n",
      "Epoch [7/60], Iter [581/633], LR: 0.005000, Loss: 3.5993, top1: 10.9375\n",
      "Epoch [7/60], Iter [582/633], LR: 0.005000, Loss: 3.5741, top1: 17.1875\n",
      "Epoch [7/60], Iter [583/633], LR: 0.005000, Loss: 3.6104, top1: 7.8125\n",
      "Epoch [7/60], Iter [584/633], LR: 0.005000, Loss: 3.5949, top1: 7.8125\n",
      "Epoch [7/60], Iter [585/633], LR: 0.005000, Loss: 3.6190, top1: 4.6875\n",
      "Epoch [7/60], Iter [586/633], LR: 0.005000, Loss: 3.6139, top1: 10.9375\n",
      "Epoch [7/60], Iter [587/633], LR: 0.005000, Loss: 3.6018, top1: 7.8125\n",
      "Epoch [7/60], Iter [588/633], LR: 0.005000, Loss: 3.5983, top1: 14.0625\n",
      "Epoch [7/60], Iter [589/633], LR: 0.005000, Loss: 3.5583, top1: 12.5000\n",
      "Epoch [7/60], Iter [590/633], LR: 0.005000, Loss: 3.5493, top1: 23.4375\n",
      "Epoch [7/60], Iter [591/633], LR: 0.005000, Loss: 3.5961, top1: 14.0625\n",
      "Epoch [7/60], Iter [592/633], LR: 0.005000, Loss: 3.5891, top1: 20.3125\n",
      "Epoch [7/60], Iter [593/633], LR: 0.005000, Loss: 3.5764, top1: 12.5000\n",
      "Epoch [7/60], Iter [594/633], LR: 0.005000, Loss: 3.6033, top1: 9.3750\n",
      "Epoch [7/60], Iter [595/633], LR: 0.005000, Loss: 3.5903, top1: 15.6250\n",
      "Epoch [7/60], Iter [596/633], LR: 0.005000, Loss: 3.6100, top1: 7.8125\n",
      "Epoch [7/60], Iter [597/633], LR: 0.005000, Loss: 3.6271, top1: 7.8125\n",
      "Epoch [7/60], Iter [598/633], LR: 0.005000, Loss: 3.5835, top1: 10.9375\n",
      "Epoch [7/60], Iter [599/633], LR: 0.005000, Loss: 3.6034, top1: 10.9375\n",
      "Epoch [7/60], Iter [600/633], LR: 0.005000, Loss: 3.6012, top1: 17.1875\n",
      "Epoch [7/60], Iter [601/633], LR: 0.005000, Loss: 3.5411, top1: 23.4375\n",
      "Epoch [7/60], Iter [602/633], LR: 0.005000, Loss: 3.6379, top1: 7.8125\n",
      "Epoch [7/60], Iter [603/633], LR: 0.005000, Loss: 3.5596, top1: 14.0625\n",
      "Epoch [7/60], Iter [604/633], LR: 0.005000, Loss: 3.5736, top1: 14.0625\n",
      "Epoch [7/60], Iter [605/633], LR: 0.005000, Loss: 3.5998, top1: 7.8125\n",
      "Epoch [7/60], Iter [606/633], LR: 0.005000, Loss: 3.6075, top1: 7.8125\n",
      "Epoch [7/60], Iter [607/633], LR: 0.005000, Loss: 3.5707, top1: 15.6250\n",
      "Epoch [7/60], Iter [608/633], LR: 0.005000, Loss: 3.5956, top1: 7.8125\n",
      "Epoch [7/60], Iter [609/633], LR: 0.005000, Loss: 3.6014, top1: 12.5000\n",
      "Epoch [7/60], Iter [610/633], LR: 0.005000, Loss: 3.5702, top1: 14.0625\n",
      "Epoch [7/60], Iter [611/633], LR: 0.005000, Loss: 3.5821, top1: 12.5000\n",
      "Epoch [7/60], Iter [612/633], LR: 0.005000, Loss: 3.5649, top1: 14.0625\n",
      "Epoch [7/60], Iter [613/633], LR: 0.005000, Loss: 3.5813, top1: 12.5000\n",
      "Epoch [7/60], Iter [614/633], LR: 0.005000, Loss: 3.5498, top1: 20.3125\n",
      "Epoch [7/60], Iter [615/633], LR: 0.005000, Loss: 3.5701, top1: 12.5000\n",
      "Epoch [7/60], Iter [616/633], LR: 0.005000, Loss: 3.5588, top1: 12.5000\n",
      "Epoch [7/60], Iter [617/633], LR: 0.005000, Loss: 3.6266, top1: 9.3750\n",
      "Epoch [7/60], Iter [618/633], LR: 0.005000, Loss: 3.6263, top1: 6.2500\n",
      "Epoch [7/60], Iter [619/633], LR: 0.005000, Loss: 3.5963, top1: 9.3750\n",
      "Epoch [7/60], Iter [620/633], LR: 0.005000, Loss: 3.6151, top1: 4.6875\n",
      "Epoch [7/60], Iter [621/633], LR: 0.005000, Loss: 3.5631, top1: 17.1875\n",
      "Epoch [7/60], Iter [622/633], LR: 0.005000, Loss: 3.6204, top1: 12.5000\n",
      "Epoch [7/60], Iter [623/633], LR: 0.005000, Loss: 3.5706, top1: 14.0625\n",
      "Epoch [7/60], Iter [624/633], LR: 0.005000, Loss: 3.5956, top1: 10.9375\n",
      "Epoch [7/60], Iter [625/633], LR: 0.005000, Loss: 3.5692, top1: 18.7500\n",
      "Epoch [7/60], Iter [626/633], LR: 0.005000, Loss: 3.6562, top1: 3.1250\n",
      "Epoch [7/60], Iter [627/633], LR: 0.005000, Loss: 3.5905, top1: 12.5000\n",
      "Epoch [7/60], Iter [628/633], LR: 0.005000, Loss: 3.6102, top1: 15.6250\n",
      "Epoch [7/60], Iter [629/633], LR: 0.005000, Loss: 3.5485, top1: 17.1875\n",
      "Epoch [7/60], Iter [630/633], LR: 0.005000, Loss: 3.5417, top1: 10.9375\n",
      "Epoch [7/60], Iter [631/633], LR: 0.005000, Loss: 3.6045, top1: 7.8125\n",
      "Epoch [7/60], Iter [632/633], LR: 0.005000, Loss: 3.6157, top1: 12.5000\n",
      "Epoch [7/60], Iter [633/633], LR: 0.005000, Loss: 3.6090, top1: 15.6250\n",
      "Epoch [7/60], Iter [634/633], LR: 0.005000, Loss: 3.5532, top1: 16.1290\n",
      "Epoch [7/60], Val_Loss: 3.5989, Val_top1: 12.3680, best_top1: 11.6009\n",
      "epoch time: 4.448514596621195 min\n",
      "Taking top1 snapshot...\n",
      "Epoch [8/60], Iter [1/633], LR: 0.005000, Loss: 3.5717, top1: 18.7500\n",
      "Epoch [8/60], Iter [2/633], LR: 0.005000, Loss: 3.6017, top1: 9.3750\n",
      "Epoch [8/60], Iter [3/633], LR: 0.005000, Loss: 3.5987, top1: 10.9375\n",
      "Epoch [8/60], Iter [4/633], LR: 0.005000, Loss: 3.5693, top1: 17.1875\n",
      "Epoch [8/60], Iter [5/633], LR: 0.005000, Loss: 3.6104, top1: 10.9375\n",
      "Epoch [8/60], Iter [6/633], LR: 0.005000, Loss: 3.6009, top1: 10.9375\n",
      "Epoch [8/60], Iter [7/633], LR: 0.005000, Loss: 3.6022, top1: 10.9375\n",
      "Epoch [8/60], Iter [8/633], LR: 0.005000, Loss: 3.5859, top1: 12.5000\n",
      "Epoch [8/60], Iter [9/633], LR: 0.005000, Loss: 3.5908, top1: 17.1875\n",
      "Epoch [8/60], Iter [10/633], LR: 0.005000, Loss: 3.5720, top1: 17.1875\n",
      "Epoch [8/60], Iter [11/633], LR: 0.005000, Loss: 3.6087, top1: 9.3750\n",
      "Epoch [8/60], Iter [12/633], LR: 0.005000, Loss: 3.6296, top1: 3.1250\n",
      "Epoch [8/60], Iter [13/633], LR: 0.005000, Loss: 3.6033, top1: 7.8125\n",
      "Epoch [8/60], Iter [14/633], LR: 0.005000, Loss: 3.6288, top1: 7.8125\n",
      "Epoch [8/60], Iter [15/633], LR: 0.005000, Loss: 3.5806, top1: 15.6250\n",
      "Epoch [8/60], Iter [16/633], LR: 0.005000, Loss: 3.6145, top1: 9.3750\n",
      "Epoch [8/60], Iter [17/633], LR: 0.005000, Loss: 3.6124, top1: 12.5000\n",
      "Epoch [8/60], Iter [18/633], LR: 0.005000, Loss: 3.5493, top1: 15.6250\n",
      "Epoch [8/60], Iter [19/633], LR: 0.005000, Loss: 3.6226, top1: 10.9375\n",
      "Epoch [8/60], Iter [20/633], LR: 0.005000, Loss: 3.5776, top1: 14.0625\n",
      "Epoch [8/60], Iter [21/633], LR: 0.005000, Loss: 3.6119, top1: 12.5000\n",
      "Epoch [8/60], Iter [22/633], LR: 0.005000, Loss: 3.5809, top1: 10.9375\n",
      "Epoch [8/60], Iter [23/633], LR: 0.005000, Loss: 3.5974, top1: 10.9375\n",
      "Epoch [8/60], Iter [24/633], LR: 0.005000, Loss: 3.6161, top1: 7.8125\n",
      "Epoch [8/60], Iter [25/633], LR: 0.005000, Loss: 3.5197, top1: 23.4375\n",
      "Epoch [8/60], Iter [26/633], LR: 0.005000, Loss: 3.6149, top1: 7.8125\n",
      "Epoch [8/60], Iter [27/633], LR: 0.005000, Loss: 3.5606, top1: 15.6250\n",
      "Epoch [8/60], Iter [28/633], LR: 0.005000, Loss: 3.5856, top1: 14.0625\n",
      "Epoch [8/60], Iter [29/633], LR: 0.005000, Loss: 3.5982, top1: 10.9375\n",
      "Epoch [8/60], Iter [30/633], LR: 0.005000, Loss: 3.6020, top1: 14.0625\n",
      "Epoch [8/60], Iter [31/633], LR: 0.005000, Loss: 3.5936, top1: 7.8125\n",
      "Epoch [8/60], Iter [32/633], LR: 0.005000, Loss: 3.6002, top1: 20.3125\n",
      "Epoch [8/60], Iter [33/633], LR: 0.005000, Loss: 3.5461, top1: 17.1875\n",
      "Epoch [8/60], Iter [34/633], LR: 0.005000, Loss: 3.5803, top1: 14.0625\n",
      "Epoch [8/60], Iter [35/633], LR: 0.005000, Loss: 3.6186, top1: 7.8125\n",
      "Epoch [8/60], Iter [36/633], LR: 0.005000, Loss: 3.5801, top1: 18.7500\n",
      "Epoch [8/60], Iter [37/633], LR: 0.005000, Loss: 3.6645, top1: 1.5625\n",
      "Epoch [8/60], Iter [38/633], LR: 0.005000, Loss: 3.5637, top1: 15.6250\n",
      "Epoch [8/60], Iter [39/633], LR: 0.005000, Loss: 3.5898, top1: 14.0625\n",
      "Epoch [8/60], Iter [40/633], LR: 0.005000, Loss: 3.6022, top1: 9.3750\n",
      "Epoch [8/60], Iter [41/633], LR: 0.005000, Loss: 3.5767, top1: 18.7500\n",
      "Epoch [8/60], Iter [42/633], LR: 0.005000, Loss: 3.6022, top1: 9.3750\n",
      "Epoch [8/60], Iter [43/633], LR: 0.005000, Loss: 3.6032, top1: 15.6250\n",
      "Epoch [8/60], Iter [44/633], LR: 0.005000, Loss: 3.5924, top1: 10.9375\n",
      "Epoch [8/60], Iter [45/633], LR: 0.005000, Loss: 3.6045, top1: 18.7500\n",
      "Epoch [8/60], Iter [46/633], LR: 0.005000, Loss: 3.6076, top1: 10.9375\n",
      "Epoch [8/60], Iter [47/633], LR: 0.005000, Loss: 3.6531, top1: 4.6875\n",
      "Epoch [8/60], Iter [48/633], LR: 0.005000, Loss: 3.6172, top1: 12.5000\n",
      "Epoch [8/60], Iter [49/633], LR: 0.005000, Loss: 3.5931, top1: 12.5000\n",
      "Epoch [8/60], Iter [50/633], LR: 0.005000, Loss: 3.6074, top1: 10.9375\n",
      "Epoch [8/60], Iter [51/633], LR: 0.005000, Loss: 3.5731, top1: 10.9375\n",
      "Epoch [8/60], Iter [52/633], LR: 0.005000, Loss: 3.5230, top1: 21.8750\n",
      "Epoch [8/60], Iter [53/633], LR: 0.005000, Loss: 3.5979, top1: 12.5000\n",
      "Epoch [8/60], Iter [54/633], LR: 0.005000, Loss: 3.5938, top1: 14.0625\n",
      "Epoch [8/60], Iter [55/633], LR: 0.005000, Loss: 3.5842, top1: 14.0625\n",
      "Epoch [8/60], Iter [56/633], LR: 0.005000, Loss: 3.5340, top1: 20.3125\n",
      "Epoch [8/60], Iter [57/633], LR: 0.005000, Loss: 3.5792, top1: 14.0625\n",
      "Epoch [8/60], Iter [58/633], LR: 0.005000, Loss: 3.5960, top1: 14.0625\n",
      "Epoch [8/60], Iter [59/633], LR: 0.005000, Loss: 3.5830, top1: 14.0625\n",
      "Epoch [8/60], Iter [60/633], LR: 0.005000, Loss: 3.5582, top1: 9.3750\n",
      "Epoch [8/60], Iter [61/633], LR: 0.005000, Loss: 3.6075, top1: 14.0625\n",
      "Epoch [8/60], Iter [62/633], LR: 0.005000, Loss: 3.5996, top1: 12.5000\n",
      "Epoch [8/60], Iter [63/633], LR: 0.005000, Loss: 3.6473, top1: 4.6875\n",
      "Epoch [8/60], Iter [64/633], LR: 0.005000, Loss: 3.5963, top1: 12.5000\n",
      "Epoch [8/60], Iter [65/633], LR: 0.005000, Loss: 3.5412, top1: 20.3125\n",
      "Epoch [8/60], Iter [66/633], LR: 0.005000, Loss: 3.6312, top1: 4.6875\n",
      "Epoch [8/60], Iter [67/633], LR: 0.005000, Loss: 3.5822, top1: 15.6250\n",
      "Epoch [8/60], Iter [68/633], LR: 0.005000, Loss: 3.5873, top1: 15.6250\n",
      "Epoch [8/60], Iter [69/633], LR: 0.005000, Loss: 3.5954, top1: 15.6250\n",
      "Epoch [8/60], Iter [70/633], LR: 0.005000, Loss: 3.5563, top1: 21.8750\n",
      "Epoch [8/60], Iter [71/633], LR: 0.005000, Loss: 3.5662, top1: 18.7500\n",
      "Epoch [8/60], Iter [72/633], LR: 0.005000, Loss: 3.6239, top1: 12.5000\n",
      "Epoch [8/60], Iter [73/633], LR: 0.005000, Loss: 3.6256, top1: 6.2500\n",
      "Epoch [8/60], Iter [74/633], LR: 0.005000, Loss: 3.5837, top1: 7.8125\n",
      "Epoch [8/60], Iter [75/633], LR: 0.005000, Loss: 3.5908, top1: 12.5000\n",
      "Epoch [8/60], Iter [76/633], LR: 0.005000, Loss: 3.5910, top1: 14.0625\n",
      "Epoch [8/60], Iter [77/633], LR: 0.005000, Loss: 3.6088, top1: 15.6250\n",
      "Epoch [8/60], Iter [78/633], LR: 0.005000, Loss: 3.6382, top1: 9.3750\n",
      "Epoch [8/60], Iter [79/633], LR: 0.005000, Loss: 3.6505, top1: 9.3750\n",
      "Epoch [8/60], Iter [80/633], LR: 0.005000, Loss: 3.5981, top1: 14.0625\n",
      "Epoch [8/60], Iter [81/633], LR: 0.005000, Loss: 3.5766, top1: 17.1875\n",
      "Epoch [8/60], Iter [82/633], LR: 0.005000, Loss: 3.6110, top1: 12.5000\n",
      "Epoch [8/60], Iter [83/633], LR: 0.005000, Loss: 3.5975, top1: 12.5000\n",
      "Epoch [8/60], Iter [84/633], LR: 0.005000, Loss: 3.6110, top1: 14.0625\n",
      "Epoch [8/60], Iter [85/633], LR: 0.005000, Loss: 3.6016, top1: 12.5000\n",
      "Epoch [8/60], Iter [86/633], LR: 0.005000, Loss: 3.5967, top1: 6.2500\n",
      "Epoch [8/60], Iter [87/633], LR: 0.005000, Loss: 3.5957, top1: 12.5000\n",
      "Epoch [8/60], Iter [88/633], LR: 0.005000, Loss: 3.6133, top1: 10.9375\n",
      "Epoch [8/60], Iter [89/633], LR: 0.005000, Loss: 3.5737, top1: 10.9375\n",
      "Epoch [8/60], Iter [90/633], LR: 0.005000, Loss: 3.6020, top1: 10.9375\n",
      "Epoch [8/60], Iter [91/633], LR: 0.005000, Loss: 3.5671, top1: 15.6250\n",
      "Epoch [8/60], Iter [92/633], LR: 0.005000, Loss: 3.5888, top1: 15.6250\n",
      "Epoch [8/60], Iter [93/633], LR: 0.005000, Loss: 3.5768, top1: 14.0625\n",
      "Epoch [8/60], Iter [94/633], LR: 0.005000, Loss: 3.5908, top1: 12.5000\n",
      "Epoch [8/60], Iter [95/633], LR: 0.005000, Loss: 3.6061, top1: 14.0625\n",
      "Epoch [8/60], Iter [96/633], LR: 0.005000, Loss: 3.5736, top1: 18.7500\n",
      "Epoch [8/60], Iter [97/633], LR: 0.005000, Loss: 3.6302, top1: 6.2500\n",
      "Epoch [8/60], Iter [98/633], LR: 0.005000, Loss: 3.5987, top1: 15.6250\n",
      "Epoch [8/60], Iter [99/633], LR: 0.005000, Loss: 3.6132, top1: 10.9375\n",
      "Epoch [8/60], Iter [100/633], LR: 0.005000, Loss: 3.6059, top1: 9.3750\n",
      "Epoch [8/60], Iter [101/633], LR: 0.005000, Loss: 3.5900, top1: 14.0625\n",
      "Epoch [8/60], Iter [102/633], LR: 0.005000, Loss: 3.6051, top1: 6.2500\n",
      "Epoch [8/60], Iter [103/633], LR: 0.005000, Loss: 3.6115, top1: 10.9375\n",
      "Epoch [8/60], Iter [104/633], LR: 0.005000, Loss: 3.5928, top1: 14.0625\n",
      "Epoch [8/60], Iter [105/633], LR: 0.005000, Loss: 3.5982, top1: 7.8125\n",
      "Epoch [8/60], Iter [106/633], LR: 0.005000, Loss: 3.5450, top1: 21.8750\n",
      "Epoch [8/60], Iter [107/633], LR: 0.005000, Loss: 3.6643, top1: 3.1250\n",
      "Epoch [8/60], Iter [108/633], LR: 0.005000, Loss: 3.5770, top1: 14.0625\n",
      "Epoch [8/60], Iter [109/633], LR: 0.005000, Loss: 3.6592, top1: 14.0625\n",
      "Epoch [8/60], Iter [110/633], LR: 0.005000, Loss: 3.5695, top1: 14.0625\n",
      "Epoch [8/60], Iter [111/633], LR: 0.005000, Loss: 3.5929, top1: 14.0625\n",
      "Epoch [8/60], Iter [112/633], LR: 0.005000, Loss: 3.6104, top1: 7.8125\n",
      "Epoch [8/60], Iter [113/633], LR: 0.005000, Loss: 3.5844, top1: 18.7500\n",
      "Epoch [8/60], Iter [114/633], LR: 0.005000, Loss: 3.5666, top1: 14.0625\n",
      "Epoch [8/60], Iter [115/633], LR: 0.005000, Loss: 3.6260, top1: 9.3750\n",
      "Epoch [8/60], Iter [116/633], LR: 0.005000, Loss: 3.5725, top1: 17.1875\n",
      "Epoch [8/60], Iter [117/633], LR: 0.005000, Loss: 3.6289, top1: 7.8125\n",
      "Epoch [8/60], Iter [118/633], LR: 0.005000, Loss: 3.6105, top1: 7.8125\n",
      "Epoch [8/60], Iter [119/633], LR: 0.005000, Loss: 3.6432, top1: 4.6875\n",
      "Epoch [8/60], Iter [120/633], LR: 0.005000, Loss: 3.5855, top1: 12.5000\n",
      "Epoch [8/60], Iter [121/633], LR: 0.005000, Loss: 3.6062, top1: 9.3750\n",
      "Epoch [8/60], Iter [122/633], LR: 0.005000, Loss: 3.5862, top1: 18.7500\n",
      "Epoch [8/60], Iter [123/633], LR: 0.005000, Loss: 3.6282, top1: 10.9375\n",
      "Epoch [8/60], Iter [124/633], LR: 0.005000, Loss: 3.6008, top1: 12.5000\n",
      "Epoch [8/60], Iter [125/633], LR: 0.005000, Loss: 3.6143, top1: 14.0625\n",
      "Epoch [8/60], Iter [126/633], LR: 0.005000, Loss: 3.5557, top1: 28.1250\n",
      "Epoch [8/60], Iter [127/633], LR: 0.005000, Loss: 3.5732, top1: 17.1875\n",
      "Epoch [8/60], Iter [128/633], LR: 0.005000, Loss: 3.6363, top1: 7.8125\n",
      "Epoch [8/60], Iter [129/633], LR: 0.005000, Loss: 3.6207, top1: 10.9375\n",
      "Epoch [8/60], Iter [130/633], LR: 0.005000, Loss: 3.5891, top1: 18.7500\n",
      "Epoch [8/60], Iter [131/633], LR: 0.005000, Loss: 3.5894, top1: 7.8125\n",
      "Epoch [8/60], Iter [132/633], LR: 0.005000, Loss: 3.6172, top1: 10.9375\n",
      "Epoch [8/60], Iter [133/633], LR: 0.005000, Loss: 3.6238, top1: 15.6250\n",
      "Epoch [8/60], Iter [134/633], LR: 0.005000, Loss: 3.5356, top1: 17.1875\n",
      "Epoch [8/60], Iter [135/633], LR: 0.005000, Loss: 3.6048, top1: 12.5000\n",
      "Epoch [8/60], Iter [136/633], LR: 0.005000, Loss: 3.6245, top1: 9.3750\n",
      "Epoch [8/60], Iter [137/633], LR: 0.005000, Loss: 3.5553, top1: 18.7500\n",
      "Epoch [8/60], Iter [138/633], LR: 0.005000, Loss: 3.6127, top1: 17.1875\n",
      "Epoch [8/60], Iter [139/633], LR: 0.005000, Loss: 3.5825, top1: 9.3750\n",
      "Epoch [8/60], Iter [140/633], LR: 0.005000, Loss: 3.6175, top1: 9.3750\n",
      "Epoch [8/60], Iter [141/633], LR: 0.005000, Loss: 3.6129, top1: 7.8125\n",
      "Epoch [8/60], Iter [142/633], LR: 0.005000, Loss: 3.6256, top1: 9.3750\n",
      "Epoch [8/60], Iter [143/633], LR: 0.005000, Loss: 3.5791, top1: 21.8750\n",
      "Epoch [8/60], Iter [144/633], LR: 0.005000, Loss: 3.5914, top1: 10.9375\n",
      "Epoch [8/60], Iter [145/633], LR: 0.005000, Loss: 3.5918, top1: 12.5000\n",
      "Epoch [8/60], Iter [146/633], LR: 0.005000, Loss: 3.5670, top1: 17.1875\n",
      "Epoch [8/60], Iter [147/633], LR: 0.005000, Loss: 3.6061, top1: 9.3750\n",
      "Epoch [8/60], Iter [148/633], LR: 0.005000, Loss: 3.6052, top1: 14.0625\n",
      "Epoch [8/60], Iter [149/633], LR: 0.005000, Loss: 3.5668, top1: 17.1875\n",
      "Epoch [8/60], Iter [150/633], LR: 0.005000, Loss: 3.6125, top1: 7.8125\n",
      "Epoch [8/60], Iter [151/633], LR: 0.005000, Loss: 3.6014, top1: 14.0625\n",
      "Epoch [8/60], Iter [152/633], LR: 0.005000, Loss: 3.5823, top1: 9.3750\n",
      "Epoch [8/60], Iter [153/633], LR: 0.005000, Loss: 3.5841, top1: 15.6250\n",
      "Epoch [8/60], Iter [154/633], LR: 0.005000, Loss: 3.5966, top1: 15.6250\n",
      "Epoch [8/60], Iter [155/633], LR: 0.005000, Loss: 3.5728, top1: 14.0625\n",
      "Epoch [8/60], Iter [156/633], LR: 0.005000, Loss: 3.5372, top1: 17.1875\n",
      "Epoch [8/60], Iter [157/633], LR: 0.005000, Loss: 3.5642, top1: 17.1875\n",
      "Epoch [8/60], Iter [158/633], LR: 0.005000, Loss: 3.5990, top1: 9.3750\n",
      "Epoch [8/60], Iter [159/633], LR: 0.005000, Loss: 3.6127, top1: 12.5000\n",
      "Epoch [8/60], Iter [160/633], LR: 0.005000, Loss: 3.6106, top1: 9.3750\n",
      "Epoch [8/60], Iter [161/633], LR: 0.005000, Loss: 3.6116, top1: 10.9375\n",
      "Epoch [8/60], Iter [162/633], LR: 0.005000, Loss: 3.5742, top1: 15.6250\n",
      "Epoch [8/60], Iter [163/633], LR: 0.005000, Loss: 3.5926, top1: 10.9375\n",
      "Epoch [8/60], Iter [164/633], LR: 0.005000, Loss: 3.5644, top1: 23.4375\n",
      "Epoch [8/60], Iter [165/633], LR: 0.005000, Loss: 3.5523, top1: 14.0625\n",
      "Epoch [8/60], Iter [166/633], LR: 0.005000, Loss: 3.6099, top1: 4.6875\n",
      "Epoch [8/60], Iter [167/633], LR: 0.005000, Loss: 3.6084, top1: 9.3750\n",
      "Epoch [8/60], Iter [168/633], LR: 0.005000, Loss: 3.6457, top1: 12.5000\n",
      "Epoch [8/60], Iter [169/633], LR: 0.005000, Loss: 3.6027, top1: 12.5000\n",
      "Epoch [8/60], Iter [170/633], LR: 0.005000, Loss: 3.5964, top1: 14.0625\n",
      "Epoch [8/60], Iter [171/633], LR: 0.005000, Loss: 3.5958, top1: 14.0625\n",
      "Epoch [8/60], Iter [172/633], LR: 0.005000, Loss: 3.5807, top1: 14.0625\n",
      "Epoch [8/60], Iter [173/633], LR: 0.005000, Loss: 3.6245, top1: 10.9375\n",
      "Epoch [8/60], Iter [174/633], LR: 0.005000, Loss: 3.5881, top1: 14.0625\n",
      "Epoch [8/60], Iter [175/633], LR: 0.005000, Loss: 3.6049, top1: 15.6250\n",
      "Epoch [8/60], Iter [176/633], LR: 0.005000, Loss: 3.6065, top1: 10.9375\n",
      "Epoch [8/60], Iter [177/633], LR: 0.005000, Loss: 3.6065, top1: 10.9375\n",
      "Epoch [8/60], Iter [178/633], LR: 0.005000, Loss: 3.6119, top1: 9.3750\n",
      "Epoch [8/60], Iter [179/633], LR: 0.005000, Loss: 3.5709, top1: 12.5000\n",
      "Epoch [8/60], Iter [180/633], LR: 0.005000, Loss: 3.5993, top1: 9.3750\n",
      "Epoch [8/60], Iter [181/633], LR: 0.005000, Loss: 3.6115, top1: 9.3750\n",
      "Epoch [8/60], Iter [182/633], LR: 0.005000, Loss: 3.6130, top1: 14.0625\n",
      "Epoch [8/60], Iter [183/633], LR: 0.005000, Loss: 3.6202, top1: 10.9375\n",
      "Epoch [8/60], Iter [184/633], LR: 0.005000, Loss: 3.5848, top1: 15.6250\n",
      "Epoch [8/60], Iter [185/633], LR: 0.005000, Loss: 3.6059, top1: 15.6250\n",
      "Epoch [8/60], Iter [186/633], LR: 0.005000, Loss: 3.5834, top1: 15.6250\n",
      "Epoch [8/60], Iter [187/633], LR: 0.005000, Loss: 3.6183, top1: 10.9375\n",
      "Epoch [8/60], Iter [188/633], LR: 0.005000, Loss: 3.6259, top1: 7.8125\n",
      "Epoch [8/60], Iter [189/633], LR: 0.005000, Loss: 3.6144, top1: 15.6250\n",
      "Epoch [8/60], Iter [190/633], LR: 0.005000, Loss: 3.6271, top1: 7.8125\n",
      "Epoch [8/60], Iter [191/633], LR: 0.005000, Loss: 3.6323, top1: 6.2500\n",
      "Epoch [8/60], Iter [192/633], LR: 0.005000, Loss: 3.5949, top1: 9.3750\n",
      "Epoch [8/60], Iter [193/633], LR: 0.005000, Loss: 3.6039, top1: 7.8125\n",
      "Epoch [8/60], Iter [194/633], LR: 0.005000, Loss: 3.5857, top1: 18.7500\n",
      "Epoch [8/60], Iter [195/633], LR: 0.005000, Loss: 3.6126, top1: 10.9375\n",
      "Epoch [8/60], Iter [196/633], LR: 0.005000, Loss: 3.6480, top1: 6.2500\n",
      "Epoch [8/60], Iter [197/633], LR: 0.005000, Loss: 3.5805, top1: 18.7500\n",
      "Epoch [8/60], Iter [198/633], LR: 0.005000, Loss: 3.5888, top1: 20.3125\n",
      "Epoch [8/60], Iter [199/633], LR: 0.005000, Loss: 3.6018, top1: 7.8125\n",
      "Epoch [8/60], Iter [200/633], LR: 0.005000, Loss: 3.6287, top1: 9.3750\n",
      "Epoch [8/60], Iter [201/633], LR: 0.005000, Loss: 3.6011, top1: 7.8125\n",
      "Epoch [8/60], Iter [202/633], LR: 0.005000, Loss: 3.5756, top1: 15.6250\n",
      "Epoch [8/60], Iter [203/633], LR: 0.005000, Loss: 3.5824, top1: 9.3750\n",
      "Epoch [8/60], Iter [204/633], LR: 0.005000, Loss: 3.6152, top1: 10.9375\n",
      "Epoch [8/60], Iter [205/633], LR: 0.005000, Loss: 3.5903, top1: 9.3750\n",
      "Epoch [8/60], Iter [206/633], LR: 0.005000, Loss: 3.5924, top1: 9.3750\n",
      "Epoch [8/60], Iter [207/633], LR: 0.005000, Loss: 3.5531, top1: 17.1875\n",
      "Epoch [8/60], Iter [208/633], LR: 0.005000, Loss: 3.5995, top1: 15.6250\n",
      "Epoch [8/60], Iter [209/633], LR: 0.005000, Loss: 3.6035, top1: 12.5000\n",
      "Epoch [8/60], Iter [210/633], LR: 0.005000, Loss: 3.5330, top1: 20.3125\n",
      "Epoch [8/60], Iter [211/633], LR: 0.005000, Loss: 3.5706, top1: 12.5000\n",
      "Epoch [8/60], Iter [212/633], LR: 0.005000, Loss: 3.5594, top1: 18.7500\n",
      "Epoch [8/60], Iter [213/633], LR: 0.005000, Loss: 3.5820, top1: 15.6250\n",
      "Epoch [8/60], Iter [214/633], LR: 0.005000, Loss: 3.5606, top1: 15.6250\n",
      "Epoch [8/60], Iter [215/633], LR: 0.005000, Loss: 3.5904, top1: 7.8125\n",
      "Epoch [8/60], Iter [216/633], LR: 0.005000, Loss: 3.5892, top1: 14.0625\n",
      "Epoch [8/60], Iter [217/633], LR: 0.005000, Loss: 3.5968, top1: 12.5000\n",
      "Epoch [8/60], Iter [218/633], LR: 0.005000, Loss: 3.5817, top1: 15.6250\n",
      "Epoch [8/60], Iter [219/633], LR: 0.005000, Loss: 3.6364, top1: 7.8125\n",
      "Epoch [8/60], Iter [220/633], LR: 0.005000, Loss: 3.5764, top1: 12.5000\n",
      "Epoch [8/60], Iter [221/633], LR: 0.005000, Loss: 3.5561, top1: 14.0625\n",
      "Epoch [8/60], Iter [222/633], LR: 0.005000, Loss: 3.5912, top1: 12.5000\n",
      "Epoch [8/60], Iter [223/633], LR: 0.005000, Loss: 3.6665, top1: 6.2500\n",
      "Epoch [8/60], Iter [224/633], LR: 0.005000, Loss: 3.6096, top1: 7.8125\n",
      "Epoch [8/60], Iter [225/633], LR: 0.005000, Loss: 3.5972, top1: 15.6250\n",
      "Epoch [8/60], Iter [226/633], LR: 0.005000, Loss: 3.6186, top1: 7.8125\n",
      "Epoch [8/60], Iter [227/633], LR: 0.005000, Loss: 3.5652, top1: 20.3125\n",
      "Epoch [8/60], Iter [228/633], LR: 0.005000, Loss: 3.6078, top1: 9.3750\n",
      "Epoch [8/60], Iter [229/633], LR: 0.005000, Loss: 3.5860, top1: 14.0625\n",
      "Epoch [8/60], Iter [230/633], LR: 0.005000, Loss: 3.5718, top1: 20.3125\n",
      "Epoch [8/60], Iter [231/633], LR: 0.005000, Loss: 3.5525, top1: 20.3125\n",
      "Epoch [8/60], Iter [232/633], LR: 0.005000, Loss: 3.6012, top1: 9.3750\n",
      "Epoch [8/60], Iter [233/633], LR: 0.005000, Loss: 3.6676, top1: 7.8125\n",
      "Epoch [8/60], Iter [234/633], LR: 0.005000, Loss: 3.6046, top1: 10.9375\n",
      "Epoch [8/60], Iter [235/633], LR: 0.005000, Loss: 3.6060, top1: 15.6250\n",
      "Epoch [8/60], Iter [236/633], LR: 0.005000, Loss: 3.5919, top1: 10.9375\n",
      "Epoch [8/60], Iter [237/633], LR: 0.005000, Loss: 3.6050, top1: 15.6250\n",
      "Epoch [8/60], Iter [238/633], LR: 0.005000, Loss: 3.6295, top1: 7.8125\n",
      "Epoch [8/60], Iter [239/633], LR: 0.005000, Loss: 3.5931, top1: 7.8125\n",
      "Epoch [8/60], Iter [240/633], LR: 0.005000, Loss: 3.6153, top1: 4.6875\n",
      "Epoch [8/60], Iter [241/633], LR: 0.005000, Loss: 3.6619, top1: 6.2500\n",
      "Epoch [8/60], Iter [242/633], LR: 0.005000, Loss: 3.5972, top1: 14.0625\n",
      "Epoch [8/60], Iter [243/633], LR: 0.005000, Loss: 3.5718, top1: 14.0625\n",
      "Epoch [8/60], Iter [244/633], LR: 0.005000, Loss: 3.6183, top1: 12.5000\n",
      "Epoch [8/60], Iter [245/633], LR: 0.005000, Loss: 3.6116, top1: 12.5000\n",
      "Epoch [8/60], Iter [246/633], LR: 0.005000, Loss: 3.5624, top1: 12.5000\n",
      "Epoch [8/60], Iter [247/633], LR: 0.005000, Loss: 3.6080, top1: 17.1875\n",
      "Epoch [8/60], Iter [248/633], LR: 0.005000, Loss: 3.5869, top1: 15.6250\n",
      "Epoch [8/60], Iter [249/633], LR: 0.005000, Loss: 3.6331, top1: 6.2500\n",
      "Epoch [8/60], Iter [250/633], LR: 0.005000, Loss: 3.5863, top1: 7.8125\n",
      "Epoch [8/60], Iter [251/633], LR: 0.005000, Loss: 3.5658, top1: 14.0625\n",
      "Epoch [8/60], Iter [252/633], LR: 0.005000, Loss: 3.5649, top1: 17.1875\n",
      "Epoch [8/60], Iter [253/633], LR: 0.005000, Loss: 3.5949, top1: 14.0625\n",
      "Epoch [8/60], Iter [254/633], LR: 0.005000, Loss: 3.6005, top1: 10.9375\n",
      "Epoch [8/60], Iter [255/633], LR: 0.005000, Loss: 3.6262, top1: 7.8125\n",
      "Epoch [8/60], Iter [256/633], LR: 0.005000, Loss: 3.6197, top1: 10.9375\n",
      "Epoch [8/60], Iter [257/633], LR: 0.005000, Loss: 3.6268, top1: 4.6875\n",
      "Epoch [8/60], Iter [258/633], LR: 0.005000, Loss: 3.5909, top1: 14.0625\n",
      "Epoch [8/60], Iter [259/633], LR: 0.005000, Loss: 3.5662, top1: 17.1875\n",
      "Epoch [8/60], Iter [260/633], LR: 0.005000, Loss: 3.6005, top1: 12.5000\n",
      "Epoch [8/60], Iter [261/633], LR: 0.005000, Loss: 3.6282, top1: 17.1875\n",
      "Epoch [8/60], Iter [262/633], LR: 0.005000, Loss: 3.5411, top1: 15.6250\n",
      "Epoch [8/60], Iter [263/633], LR: 0.005000, Loss: 3.5951, top1: 15.6250\n",
      "Epoch [8/60], Iter [264/633], LR: 0.005000, Loss: 3.5824, top1: 14.0625\n",
      "Epoch [8/60], Iter [265/633], LR: 0.005000, Loss: 3.5839, top1: 15.6250\n",
      "Epoch [8/60], Iter [266/633], LR: 0.005000, Loss: 3.5863, top1: 14.0625\n",
      "Epoch [8/60], Iter [267/633], LR: 0.005000, Loss: 3.6480, top1: 9.3750\n",
      "Epoch [8/60], Iter [268/633], LR: 0.005000, Loss: 3.5961, top1: 10.9375\n",
      "Epoch [8/60], Iter [269/633], LR: 0.005000, Loss: 3.5820, top1: 14.0625\n",
      "Epoch [8/60], Iter [270/633], LR: 0.005000, Loss: 3.5985, top1: 10.9375\n",
      "Epoch [8/60], Iter [271/633], LR: 0.005000, Loss: 3.6128, top1: 14.0625\n",
      "Epoch [8/60], Iter [272/633], LR: 0.005000, Loss: 3.5775, top1: 18.7500\n",
      "Epoch [8/60], Iter [273/633], LR: 0.005000, Loss: 3.5762, top1: 14.0625\n",
      "Epoch [8/60], Iter [274/633], LR: 0.005000, Loss: 3.5368, top1: 17.1875\n",
      "Epoch [8/60], Iter [275/633], LR: 0.005000, Loss: 3.5595, top1: 18.7500\n",
      "Epoch [8/60], Iter [276/633], LR: 0.005000, Loss: 3.6431, top1: 6.2500\n",
      "Epoch [8/60], Iter [277/633], LR: 0.005000, Loss: 3.6100, top1: 12.5000\n",
      "Epoch [8/60], Iter [278/633], LR: 0.005000, Loss: 3.6251, top1: 9.3750\n",
      "Epoch [8/60], Iter [279/633], LR: 0.005000, Loss: 3.6152, top1: 14.0625\n",
      "Epoch [8/60], Iter [280/633], LR: 0.005000, Loss: 3.6074, top1: 14.0625\n",
      "Epoch [8/60], Iter [281/633], LR: 0.005000, Loss: 3.5867, top1: 7.8125\n",
      "Epoch [8/60], Iter [282/633], LR: 0.005000, Loss: 3.6036, top1: 10.9375\n",
      "Epoch [8/60], Iter [283/633], LR: 0.005000, Loss: 3.5494, top1: 17.1875\n",
      "Epoch [8/60], Iter [284/633], LR: 0.005000, Loss: 3.5844, top1: 18.7500\n",
      "Epoch [8/60], Iter [285/633], LR: 0.005000, Loss: 3.6049, top1: 12.5000\n",
      "Epoch [8/60], Iter [286/633], LR: 0.005000, Loss: 3.6156, top1: 7.8125\n",
      "Epoch [8/60], Iter [287/633], LR: 0.005000, Loss: 3.5722, top1: 20.3125\n",
      "Epoch [8/60], Iter [288/633], LR: 0.005000, Loss: 3.6006, top1: 4.6875\n",
      "Epoch [8/60], Iter [289/633], LR: 0.005000, Loss: 3.5885, top1: 15.6250\n",
      "Epoch [8/60], Iter [290/633], LR: 0.005000, Loss: 3.5907, top1: 17.1875\n",
      "Epoch [8/60], Iter [291/633], LR: 0.005000, Loss: 3.5688, top1: 14.0625\n",
      "Epoch [8/60], Iter [292/633], LR: 0.005000, Loss: 3.5820, top1: 14.0625\n",
      "Epoch [8/60], Iter [293/633], LR: 0.005000, Loss: 3.5830, top1: 15.6250\n",
      "Epoch [8/60], Iter [294/633], LR: 0.005000, Loss: 3.5733, top1: 12.5000\n",
      "Epoch [8/60], Iter [295/633], LR: 0.005000, Loss: 3.5814, top1: 14.0625\n",
      "Epoch [8/60], Iter [296/633], LR: 0.005000, Loss: 3.5642, top1: 15.6250\n",
      "Epoch [8/60], Iter [297/633], LR: 0.005000, Loss: 3.6098, top1: 6.2500\n",
      "Epoch [8/60], Iter [298/633], LR: 0.005000, Loss: 3.5995, top1: 9.3750\n",
      "Epoch [8/60], Iter [299/633], LR: 0.005000, Loss: 3.5957, top1: 9.3750\n",
      "Epoch [8/60], Iter [300/633], LR: 0.005000, Loss: 3.5973, top1: 12.5000\n",
      "Epoch [8/60], Iter [301/633], LR: 0.005000, Loss: 3.6196, top1: 7.8125\n",
      "Epoch [8/60], Iter [302/633], LR: 0.005000, Loss: 3.6274, top1: 9.3750\n",
      "Epoch [8/60], Iter [303/633], LR: 0.005000, Loss: 3.5819, top1: 10.9375\n",
      "Epoch [8/60], Iter [304/633], LR: 0.005000, Loss: 3.5835, top1: 18.7500\n",
      "Epoch [8/60], Iter [305/633], LR: 0.005000, Loss: 3.6196, top1: 9.3750\n",
      "Epoch [8/60], Iter [306/633], LR: 0.005000, Loss: 3.5737, top1: 17.1875\n",
      "Epoch [8/60], Iter [307/633], LR: 0.005000, Loss: 3.5980, top1: 12.5000\n",
      "Epoch [8/60], Iter [308/633], LR: 0.005000, Loss: 3.5954, top1: 10.9375\n",
      "Epoch [8/60], Iter [309/633], LR: 0.005000, Loss: 3.5601, top1: 10.9375\n",
      "Epoch [8/60], Iter [310/633], LR: 0.005000, Loss: 3.5377, top1: 18.7500\n",
      "Epoch [8/60], Iter [311/633], LR: 0.005000, Loss: 3.6265, top1: 9.3750\n",
      "Epoch [8/60], Iter [312/633], LR: 0.005000, Loss: 3.5617, top1: 15.6250\n",
      "Epoch [8/60], Iter [313/633], LR: 0.005000, Loss: 3.5888, top1: 7.8125\n",
      "Epoch [8/60], Iter [314/633], LR: 0.005000, Loss: 3.6166, top1: 10.9375\n",
      "Epoch [8/60], Iter [315/633], LR: 0.005000, Loss: 3.6083, top1: 15.6250\n",
      "Epoch [8/60], Iter [316/633], LR: 0.005000, Loss: 3.5859, top1: 20.3125\n",
      "Epoch [8/60], Iter [317/633], LR: 0.005000, Loss: 3.5919, top1: 17.1875\n",
      "Epoch [8/60], Iter [318/633], LR: 0.005000, Loss: 3.5797, top1: 17.1875\n",
      "Epoch [8/60], Iter [319/633], LR: 0.005000, Loss: 3.6298, top1: 14.0625\n",
      "Epoch [8/60], Iter [320/633], LR: 0.005000, Loss: 3.6003, top1: 14.0625\n",
      "Epoch [8/60], Iter [321/633], LR: 0.005000, Loss: 3.6026, top1: 14.0625\n",
      "Epoch [8/60], Iter [322/633], LR: 0.005000, Loss: 3.5587, top1: 14.0625\n",
      "Epoch [8/60], Iter [323/633], LR: 0.005000, Loss: 3.5439, top1: 15.6250\n",
      "Epoch [8/60], Iter [324/633], LR: 0.005000, Loss: 3.6101, top1: 12.5000\n",
      "Epoch [8/60], Iter [325/633], LR: 0.005000, Loss: 3.6106, top1: 9.3750\n",
      "Epoch [8/60], Iter [326/633], LR: 0.005000, Loss: 3.5777, top1: 18.7500\n",
      "Epoch [8/60], Iter [327/633], LR: 0.005000, Loss: 3.6136, top1: 6.2500\n",
      "Epoch [8/60], Iter [328/633], LR: 0.005000, Loss: 3.5871, top1: 14.0625\n",
      "Epoch [8/60], Iter [329/633], LR: 0.005000, Loss: 3.5740, top1: 10.9375\n",
      "Epoch [8/60], Iter [330/633], LR: 0.005000, Loss: 3.6144, top1: 10.9375\n",
      "Epoch [8/60], Iter [331/633], LR: 0.005000, Loss: 3.5738, top1: 15.6250\n",
      "Epoch [8/60], Iter [332/633], LR: 0.005000, Loss: 3.6477, top1: 12.5000\n",
      "Epoch [8/60], Iter [333/633], LR: 0.005000, Loss: 3.5881, top1: 7.8125\n",
      "Epoch [8/60], Iter [334/633], LR: 0.005000, Loss: 3.5761, top1: 18.7500\n",
      "Epoch [8/60], Iter [335/633], LR: 0.005000, Loss: 3.5917, top1: 14.0625\n",
      "Epoch [8/60], Iter [336/633], LR: 0.005000, Loss: 3.5858, top1: 14.0625\n",
      "Epoch [8/60], Iter [337/633], LR: 0.005000, Loss: 3.6120, top1: 15.6250\n",
      "Epoch [8/60], Iter [338/633], LR: 0.005000, Loss: 3.5392, top1: 14.0625\n",
      "Epoch [8/60], Iter [339/633], LR: 0.005000, Loss: 3.5758, top1: 12.5000\n",
      "Epoch [8/60], Iter [340/633], LR: 0.005000, Loss: 3.5892, top1: 9.3750\n",
      "Epoch [8/60], Iter [341/633], LR: 0.005000, Loss: 3.5655, top1: 20.3125\n",
      "Epoch [8/60], Iter [342/633], LR: 0.005000, Loss: 3.5679, top1: 14.0625\n",
      "Epoch [8/60], Iter [343/633], LR: 0.005000, Loss: 3.5814, top1: 12.5000\n",
      "Epoch [8/60], Iter [344/633], LR: 0.005000, Loss: 3.6159, top1: 12.5000\n",
      "Epoch [8/60], Iter [345/633], LR: 0.005000, Loss: 3.5858, top1: 15.6250\n",
      "Epoch [8/60], Iter [346/633], LR: 0.005000, Loss: 3.5725, top1: 15.6250\n",
      "Epoch [8/60], Iter [347/633], LR: 0.005000, Loss: 3.5823, top1: 12.5000\n",
      "Epoch [8/60], Iter [348/633], LR: 0.005000, Loss: 3.6139, top1: 9.3750\n",
      "Epoch [8/60], Iter [349/633], LR: 0.005000, Loss: 3.6003, top1: 10.9375\n",
      "Epoch [8/60], Iter [350/633], LR: 0.005000, Loss: 3.5601, top1: 18.7500\n",
      "Epoch [8/60], Iter [351/633], LR: 0.005000, Loss: 3.6010, top1: 14.0625\n",
      "Epoch [8/60], Iter [352/633], LR: 0.005000, Loss: 3.6024, top1: 12.5000\n",
      "Epoch [8/60], Iter [353/633], LR: 0.005000, Loss: 3.6144, top1: 12.5000\n",
      "Epoch [8/60], Iter [354/633], LR: 0.005000, Loss: 3.6040, top1: 9.3750\n",
      "Epoch [8/60], Iter [355/633], LR: 0.005000, Loss: 3.6314, top1: 9.3750\n",
      "Epoch [8/60], Iter [356/633], LR: 0.005000, Loss: 3.5600, top1: 20.3125\n",
      "Epoch [8/60], Iter [357/633], LR: 0.005000, Loss: 3.5963, top1: 9.3750\n",
      "Epoch [8/60], Iter [358/633], LR: 0.005000, Loss: 3.5601, top1: 21.8750\n",
      "Epoch [8/60], Iter [359/633], LR: 0.005000, Loss: 3.5932, top1: 7.8125\n",
      "Epoch [8/60], Iter [360/633], LR: 0.005000, Loss: 3.5866, top1: 10.9375\n",
      "Epoch [8/60], Iter [361/633], LR: 0.005000, Loss: 3.5858, top1: 9.3750\n",
      "Epoch [8/60], Iter [362/633], LR: 0.005000, Loss: 3.5771, top1: 17.1875\n",
      "Epoch [8/60], Iter [363/633], LR: 0.005000, Loss: 3.5384, top1: 20.3125\n",
      "Epoch [8/60], Iter [364/633], LR: 0.005000, Loss: 3.5879, top1: 12.5000\n",
      "Epoch [8/60], Iter [365/633], LR: 0.005000, Loss: 3.6161, top1: 6.2500\n",
      "Epoch [8/60], Iter [366/633], LR: 0.005000, Loss: 3.5702, top1: 17.1875\n",
      "Epoch [8/60], Iter [367/633], LR: 0.005000, Loss: 3.5975, top1: 12.5000\n",
      "Epoch [8/60], Iter [368/633], LR: 0.005000, Loss: 3.5301, top1: 20.3125\n",
      "Epoch [8/60], Iter [369/633], LR: 0.005000, Loss: 3.5926, top1: 15.6250\n",
      "Epoch [8/60], Iter [370/633], LR: 0.005000, Loss: 3.5544, top1: 18.7500\n",
      "Epoch [8/60], Iter [371/633], LR: 0.005000, Loss: 3.5915, top1: 15.6250\n",
      "Epoch [8/60], Iter [372/633], LR: 0.005000, Loss: 3.6126, top1: 12.5000\n",
      "Epoch [8/60], Iter [373/633], LR: 0.005000, Loss: 3.5912, top1: 10.9375\n",
      "Epoch [8/60], Iter [374/633], LR: 0.005000, Loss: 3.6115, top1: 7.8125\n",
      "Epoch [8/60], Iter [375/633], LR: 0.005000, Loss: 3.6229, top1: 9.3750\n",
      "Epoch [8/60], Iter [376/633], LR: 0.005000, Loss: 3.5592, top1: 20.3125\n",
      "Epoch [8/60], Iter [377/633], LR: 0.005000, Loss: 3.6035, top1: 9.3750\n",
      "Epoch [8/60], Iter [378/633], LR: 0.005000, Loss: 3.5822, top1: 9.3750\n",
      "Epoch [8/60], Iter [379/633], LR: 0.005000, Loss: 3.5935, top1: 9.3750\n",
      "Epoch [8/60], Iter [380/633], LR: 0.005000, Loss: 3.5881, top1: 14.0625\n",
      "Epoch [8/60], Iter [381/633], LR: 0.005000, Loss: 3.6292, top1: 9.3750\n",
      "Epoch [8/60], Iter [382/633], LR: 0.005000, Loss: 3.5786, top1: 12.5000\n",
      "Epoch [8/60], Iter [383/633], LR: 0.005000, Loss: 3.6332, top1: 12.5000\n",
      "Epoch [8/60], Iter [384/633], LR: 0.005000, Loss: 3.6027, top1: 12.5000\n",
      "Epoch [8/60], Iter [385/633], LR: 0.005000, Loss: 3.6185, top1: 6.2500\n",
      "Epoch [8/60], Iter [386/633], LR: 0.005000, Loss: 3.6110, top1: 7.8125\n",
      "Epoch [8/60], Iter [387/633], LR: 0.005000, Loss: 3.5586, top1: 15.6250\n",
      "Epoch [8/60], Iter [388/633], LR: 0.005000, Loss: 3.5801, top1: 14.0625\n",
      "Epoch [8/60], Iter [389/633], LR: 0.005000, Loss: 3.6023, top1: 12.5000\n",
      "Epoch [8/60], Iter [390/633], LR: 0.005000, Loss: 3.6178, top1: 7.8125\n",
      "Epoch [8/60], Iter [391/633], LR: 0.005000, Loss: 3.6228, top1: 9.3750\n",
      "Epoch [8/60], Iter [392/633], LR: 0.005000, Loss: 3.6108, top1: 10.9375\n",
      "Epoch [8/60], Iter [393/633], LR: 0.005000, Loss: 3.5859, top1: 15.6250\n",
      "Epoch [8/60], Iter [394/633], LR: 0.005000, Loss: 3.5550, top1: 15.6250\n",
      "Epoch [8/60], Iter [395/633], LR: 0.005000, Loss: 3.5596, top1: 12.5000\n",
      "Epoch [8/60], Iter [396/633], LR: 0.005000, Loss: 3.5922, top1: 12.5000\n",
      "Epoch [8/60], Iter [397/633], LR: 0.005000, Loss: 3.5616, top1: 10.9375\n",
      "Epoch [8/60], Iter [398/633], LR: 0.005000, Loss: 3.6105, top1: 10.9375\n",
      "Epoch [8/60], Iter [399/633], LR: 0.005000, Loss: 3.6143, top1: 9.3750\n",
      "Epoch [8/60], Iter [400/633], LR: 0.005000, Loss: 3.5970, top1: 9.3750\n",
      "Epoch [8/60], Iter [401/633], LR: 0.005000, Loss: 3.5719, top1: 14.0625\n",
      "Epoch [8/60], Iter [402/633], LR: 0.005000, Loss: 3.6025, top1: 15.6250\n",
      "Epoch [8/60], Iter [403/633], LR: 0.005000, Loss: 3.6057, top1: 10.9375\n",
      "Epoch [8/60], Iter [404/633], LR: 0.005000, Loss: 3.6164, top1: 9.3750\n",
      "Epoch [8/60], Iter [405/633], LR: 0.005000, Loss: 3.6352, top1: 9.3750\n",
      "Epoch [8/60], Iter [406/633], LR: 0.005000, Loss: 3.5619, top1: 15.6250\n",
      "Epoch [8/60], Iter [407/633], LR: 0.005000, Loss: 3.5874, top1: 7.8125\n",
      "Epoch [8/60], Iter [408/633], LR: 0.005000, Loss: 3.5846, top1: 14.0625\n",
      "Epoch [8/60], Iter [409/633], LR: 0.005000, Loss: 3.6135, top1: 6.2500\n",
      "Epoch [8/60], Iter [410/633], LR: 0.005000, Loss: 3.5679, top1: 14.0625\n",
      "Epoch [8/60], Iter [411/633], LR: 0.005000, Loss: 3.5920, top1: 15.6250\n",
      "Epoch [8/60], Iter [412/633], LR: 0.005000, Loss: 3.5933, top1: 10.9375\n",
      "Epoch [8/60], Iter [413/633], LR: 0.005000, Loss: 3.6094, top1: 9.3750\n",
      "Epoch [8/60], Iter [414/633], LR: 0.005000, Loss: 3.5711, top1: 17.1875\n",
      "Epoch [8/60], Iter [415/633], LR: 0.005000, Loss: 3.5917, top1: 17.1875\n",
      "Epoch [8/60], Iter [416/633], LR: 0.005000, Loss: 3.5814, top1: 20.3125\n",
      "Epoch [8/60], Iter [417/633], LR: 0.005000, Loss: 3.6440, top1: 9.3750\n",
      "Epoch [8/60], Iter [418/633], LR: 0.005000, Loss: 3.5733, top1: 10.9375\n",
      "Epoch [8/60], Iter [419/633], LR: 0.005000, Loss: 3.6094, top1: 4.6875\n",
      "Epoch [8/60], Iter [420/633], LR: 0.005000, Loss: 3.5767, top1: 14.0625\n",
      "Epoch [8/60], Iter [421/633], LR: 0.005000, Loss: 3.5728, top1: 12.5000\n",
      "Epoch [8/60], Iter [422/633], LR: 0.005000, Loss: 3.5681, top1: 9.3750\n",
      "Epoch [8/60], Iter [423/633], LR: 0.005000, Loss: 3.5826, top1: 14.0625\n",
      "Epoch [8/60], Iter [424/633], LR: 0.005000, Loss: 3.5959, top1: 17.1875\n",
      "Epoch [8/60], Iter [425/633], LR: 0.005000, Loss: 3.6105, top1: 12.5000\n",
      "Epoch [8/60], Iter [426/633], LR: 0.005000, Loss: 3.5904, top1: 9.3750\n",
      "Epoch [8/60], Iter [427/633], LR: 0.005000, Loss: 3.5420, top1: 21.8750\n",
      "Epoch [8/60], Iter [428/633], LR: 0.005000, Loss: 3.6248, top1: 12.5000\n",
      "Epoch [8/60], Iter [429/633], LR: 0.005000, Loss: 3.5853, top1: 17.1875\n",
      "Epoch [8/60], Iter [430/633], LR: 0.005000, Loss: 3.5833, top1: 20.3125\n",
      "Epoch [8/60], Iter [431/633], LR: 0.005000, Loss: 3.6166, top1: 10.9375\n",
      "Epoch [8/60], Iter [432/633], LR: 0.005000, Loss: 3.5637, top1: 15.6250\n",
      "Epoch [8/60], Iter [433/633], LR: 0.005000, Loss: 3.5687, top1: 9.3750\n",
      "Epoch [8/60], Iter [434/633], LR: 0.005000, Loss: 3.5840, top1: 10.9375\n",
      "Epoch [8/60], Iter [435/633], LR: 0.005000, Loss: 3.5798, top1: 17.1875\n",
      "Epoch [8/60], Iter [436/633], LR: 0.005000, Loss: 3.6061, top1: 12.5000\n",
      "Epoch [8/60], Iter [437/633], LR: 0.005000, Loss: 3.6023, top1: 7.8125\n",
      "Epoch [8/60], Iter [438/633], LR: 0.005000, Loss: 3.5871, top1: 20.3125\n",
      "Epoch [8/60], Iter [439/633], LR: 0.005000, Loss: 3.5264, top1: 18.7500\n",
      "Epoch [8/60], Iter [440/633], LR: 0.005000, Loss: 3.6415, top1: 12.5000\n",
      "Epoch [8/60], Iter [441/633], LR: 0.005000, Loss: 3.6058, top1: 14.0625\n",
      "Epoch [8/60], Iter [442/633], LR: 0.005000, Loss: 3.5632, top1: 14.0625\n",
      "Epoch [8/60], Iter [443/633], LR: 0.005000, Loss: 3.5726, top1: 10.9375\n",
      "Epoch [8/60], Iter [444/633], LR: 0.005000, Loss: 3.5682, top1: 21.8750\n",
      "Epoch [8/60], Iter [445/633], LR: 0.005000, Loss: 3.6140, top1: 9.3750\n",
      "Epoch [8/60], Iter [446/633], LR: 0.005000, Loss: 3.5515, top1: 15.6250\n",
      "Epoch [8/60], Iter [447/633], LR: 0.005000, Loss: 3.6180, top1: 7.8125\n",
      "Epoch [8/60], Iter [448/633], LR: 0.005000, Loss: 3.6518, top1: 9.3750\n",
      "Epoch [8/60], Iter [449/633], LR: 0.005000, Loss: 3.5662, top1: 12.5000\n",
      "Epoch [8/60], Iter [450/633], LR: 0.005000, Loss: 3.6256, top1: 9.3750\n",
      "Epoch [8/60], Iter [451/633], LR: 0.005000, Loss: 3.5604, top1: 15.6250\n",
      "Epoch [8/60], Iter [452/633], LR: 0.005000, Loss: 3.6044, top1: 10.9375\n",
      "Epoch [8/60], Iter [453/633], LR: 0.005000, Loss: 3.5538, top1: 17.1875\n",
      "Epoch [8/60], Iter [454/633], LR: 0.005000, Loss: 3.5959, top1: 14.0625\n",
      "Epoch [8/60], Iter [455/633], LR: 0.005000, Loss: 3.5722, top1: 15.6250\n",
      "Epoch [8/60], Iter [456/633], LR: 0.005000, Loss: 3.6013, top1: 7.8125\n",
      "Epoch [8/60], Iter [457/633], LR: 0.005000, Loss: 3.5820, top1: 14.0625\n",
      "Epoch [8/60], Iter [458/633], LR: 0.005000, Loss: 3.5617, top1: 18.7500\n",
      "Epoch [8/60], Iter [459/633], LR: 0.005000, Loss: 3.5808, top1: 18.7500\n",
      "Epoch [8/60], Iter [460/633], LR: 0.005000, Loss: 3.6094, top1: 12.5000\n",
      "Epoch [8/60], Iter [461/633], LR: 0.005000, Loss: 3.5963, top1: 10.9375\n",
      "Epoch [8/60], Iter [462/633], LR: 0.005000, Loss: 3.5913, top1: 7.8125\n",
      "Epoch [8/60], Iter [463/633], LR: 0.005000, Loss: 3.6228, top1: 12.5000\n",
      "Epoch [8/60], Iter [464/633], LR: 0.005000, Loss: 3.5694, top1: 14.0625\n",
      "Epoch [8/60], Iter [465/633], LR: 0.005000, Loss: 3.5793, top1: 15.6250\n",
      "Epoch [8/60], Iter [466/633], LR: 0.005000, Loss: 3.5777, top1: 15.6250\n",
      "Epoch [8/60], Iter [467/633], LR: 0.005000, Loss: 3.6179, top1: 7.8125\n",
      "Epoch [8/60], Iter [468/633], LR: 0.005000, Loss: 3.6048, top1: 14.0625\n",
      "Epoch [8/60], Iter [469/633], LR: 0.005000, Loss: 3.6092, top1: 10.9375\n",
      "Epoch [8/60], Iter [470/633], LR: 0.005000, Loss: 3.6134, top1: 9.3750\n",
      "Epoch [8/60], Iter [471/633], LR: 0.005000, Loss: 3.5979, top1: 14.0625\n",
      "Epoch [8/60], Iter [472/633], LR: 0.005000, Loss: 3.5606, top1: 15.6250\n",
      "Epoch [8/60], Iter [473/633], LR: 0.005000, Loss: 3.5864, top1: 9.3750\n",
      "Epoch [8/60], Iter [474/633], LR: 0.005000, Loss: 3.6178, top1: 10.9375\n",
      "Epoch [8/60], Iter [475/633], LR: 0.005000, Loss: 3.5806, top1: 10.9375\n",
      "Epoch [8/60], Iter [476/633], LR: 0.005000, Loss: 3.5939, top1: 12.5000\n",
      "Epoch [8/60], Iter [477/633], LR: 0.005000, Loss: 3.6096, top1: 12.5000\n",
      "Epoch [8/60], Iter [478/633], LR: 0.005000, Loss: 3.5862, top1: 14.0625\n",
      "Epoch [8/60], Iter [479/633], LR: 0.005000, Loss: 3.6166, top1: 12.5000\n",
      "Epoch [8/60], Iter [480/633], LR: 0.005000, Loss: 3.6023, top1: 9.3750\n",
      "Epoch [8/60], Iter [481/633], LR: 0.005000, Loss: 3.5984, top1: 7.8125\n",
      "Epoch [8/60], Iter [482/633], LR: 0.005000, Loss: 3.5793, top1: 18.7500\n",
      "Epoch [8/60], Iter [483/633], LR: 0.005000, Loss: 3.5848, top1: 15.6250\n",
      "Epoch [8/60], Iter [484/633], LR: 0.005000, Loss: 3.6126, top1: 12.5000\n",
      "Epoch [8/60], Iter [485/633], LR: 0.005000, Loss: 3.6103, top1: 7.8125\n",
      "Epoch [8/60], Iter [486/633], LR: 0.005000, Loss: 3.6194, top1: 7.8125\n",
      "Epoch [8/60], Iter [487/633], LR: 0.005000, Loss: 3.5859, top1: 18.7500\n",
      "Epoch [8/60], Iter [488/633], LR: 0.005000, Loss: 3.5438, top1: 17.1875\n",
      "Epoch [8/60], Iter [489/633], LR: 0.005000, Loss: 3.5987, top1: 12.5000\n",
      "Epoch [8/60], Iter [490/633], LR: 0.005000, Loss: 3.6116, top1: 10.9375\n",
      "Epoch [8/60], Iter [491/633], LR: 0.005000, Loss: 3.5903, top1: 14.0625\n",
      "Epoch [8/60], Iter [492/633], LR: 0.005000, Loss: 3.6110, top1: 15.6250\n",
      "Epoch [8/60], Iter [493/633], LR: 0.005000, Loss: 3.6400, top1: 3.1250\n",
      "Epoch [8/60], Iter [494/633], LR: 0.005000, Loss: 3.6068, top1: 9.3750\n",
      "Epoch [8/60], Iter [495/633], LR: 0.005000, Loss: 3.5680, top1: 12.5000\n",
      "Epoch [8/60], Iter [496/633], LR: 0.005000, Loss: 3.5753, top1: 14.0625\n",
      "Epoch [8/60], Iter [497/633], LR: 0.005000, Loss: 3.6056, top1: 15.6250\n",
      "Epoch [8/60], Iter [498/633], LR: 0.005000, Loss: 3.6014, top1: 9.3750\n",
      "Epoch [8/60], Iter [499/633], LR: 0.005000, Loss: 3.6367, top1: 4.6875\n",
      "Epoch [8/60], Iter [500/633], LR: 0.005000, Loss: 3.5773, top1: 18.7500\n",
      "Epoch [8/60], Iter [501/633], LR: 0.005000, Loss: 3.5841, top1: 15.6250\n",
      "Epoch [8/60], Iter [502/633], LR: 0.005000, Loss: 3.5882, top1: 9.3750\n",
      "Epoch [8/60], Iter [503/633], LR: 0.005000, Loss: 3.6584, top1: 6.2500\n",
      "Epoch [8/60], Iter [504/633], LR: 0.005000, Loss: 3.5878, top1: 14.0625\n",
      "Epoch [8/60], Iter [505/633], LR: 0.005000, Loss: 3.5743, top1: 9.3750\n",
      "Epoch [8/60], Iter [506/633], LR: 0.005000, Loss: 3.6091, top1: 10.9375\n",
      "Epoch [8/60], Iter [507/633], LR: 0.005000, Loss: 3.5914, top1: 7.8125\n",
      "Epoch [8/60], Iter [508/633], LR: 0.005000, Loss: 3.5639, top1: 14.0625\n",
      "Epoch [8/60], Iter [509/633], LR: 0.005000, Loss: 3.5638, top1: 17.1875\n",
      "Epoch [8/60], Iter [510/633], LR: 0.005000, Loss: 3.5978, top1: 9.3750\n",
      "Epoch [8/60], Iter [511/633], LR: 0.005000, Loss: 3.6044, top1: 14.0625\n",
      "Epoch [8/60], Iter [512/633], LR: 0.005000, Loss: 3.6006, top1: 9.3750\n",
      "Epoch [8/60], Iter [513/633], LR: 0.005000, Loss: 3.5491, top1: 17.1875\n",
      "Epoch [8/60], Iter [514/633], LR: 0.005000, Loss: 3.5515, top1: 17.1875\n",
      "Epoch [8/60], Iter [515/633], LR: 0.005000, Loss: 3.6033, top1: 14.0625\n",
      "Epoch [8/60], Iter [516/633], LR: 0.005000, Loss: 3.5984, top1: 7.8125\n",
      "Epoch [8/60], Iter [517/633], LR: 0.005000, Loss: 3.6131, top1: 15.6250\n",
      "Epoch [8/60], Iter [518/633], LR: 0.005000, Loss: 3.6042, top1: 14.0625\n",
      "Epoch [8/60], Iter [519/633], LR: 0.005000, Loss: 3.6013, top1: 9.3750\n",
      "Epoch [8/60], Iter [520/633], LR: 0.005000, Loss: 3.5664, top1: 14.0625\n",
      "Epoch [8/60], Iter [521/633], LR: 0.005000, Loss: 3.5743, top1: 10.9375\n",
      "Epoch [8/60], Iter [522/633], LR: 0.005000, Loss: 3.6496, top1: 3.1250\n",
      "Epoch [8/60], Iter [523/633], LR: 0.005000, Loss: 3.5687, top1: 18.7500\n",
      "Epoch [8/60], Iter [524/633], LR: 0.005000, Loss: 3.5780, top1: 17.1875\n",
      "Epoch [8/60], Iter [525/633], LR: 0.005000, Loss: 3.6147, top1: 10.9375\n",
      "Epoch [8/60], Iter [526/633], LR: 0.005000, Loss: 3.6040, top1: 14.0625\n",
      "Epoch [8/60], Iter [527/633], LR: 0.005000, Loss: 3.6165, top1: 10.9375\n",
      "Epoch [8/60], Iter [528/633], LR: 0.005000, Loss: 3.5896, top1: 15.6250\n",
      "Epoch [8/60], Iter [529/633], LR: 0.005000, Loss: 3.5436, top1: 15.6250\n",
      "Epoch [8/60], Iter [530/633], LR: 0.005000, Loss: 3.6133, top1: 14.0625\n",
      "Epoch [8/60], Iter [531/633], LR: 0.005000, Loss: 3.5934, top1: 10.9375\n",
      "Epoch [8/60], Iter [532/633], LR: 0.005000, Loss: 3.5787, top1: 12.5000\n",
      "Epoch [8/60], Iter [533/633], LR: 0.005000, Loss: 3.5720, top1: 15.6250\n",
      "Epoch [8/60], Iter [534/633], LR: 0.005000, Loss: 3.5711, top1: 14.0625\n",
      "Epoch [8/60], Iter [535/633], LR: 0.005000, Loss: 3.6004, top1: 12.5000\n",
      "Epoch [8/60], Iter [536/633], LR: 0.005000, Loss: 3.6003, top1: 9.3750\n",
      "Epoch [8/60], Iter [537/633], LR: 0.005000, Loss: 3.5842, top1: 15.6250\n",
      "Epoch [8/60], Iter [538/633], LR: 0.005000, Loss: 3.5244, top1: 23.4375\n",
      "Epoch [8/60], Iter [539/633], LR: 0.005000, Loss: 3.5496, top1: 14.0625\n",
      "Epoch [8/60], Iter [540/633], LR: 0.005000, Loss: 3.6407, top1: 9.3750\n",
      "Epoch [8/60], Iter [541/633], LR: 0.005000, Loss: 3.6283, top1: 14.0625\n",
      "Epoch [8/60], Iter [542/633], LR: 0.005000, Loss: 3.6145, top1: 9.3750\n",
      "Epoch [8/60], Iter [543/633], LR: 0.005000, Loss: 3.6089, top1: 14.0625\n",
      "Epoch [8/60], Iter [544/633], LR: 0.005000, Loss: 3.6013, top1: 10.9375\n",
      "Epoch [8/60], Iter [545/633], LR: 0.005000, Loss: 3.6076, top1: 7.8125\n",
      "Epoch [8/60], Iter [546/633], LR: 0.005000, Loss: 3.5805, top1: 7.8125\n",
      "Epoch [8/60], Iter [547/633], LR: 0.005000, Loss: 3.6087, top1: 12.5000\n",
      "Epoch [8/60], Iter [548/633], LR: 0.005000, Loss: 3.6012, top1: 9.3750\n",
      "Epoch [8/60], Iter [549/633], LR: 0.005000, Loss: 3.6325, top1: 9.3750\n",
      "Epoch [8/60], Iter [550/633], LR: 0.005000, Loss: 3.5735, top1: 14.0625\n",
      "Epoch [8/60], Iter [551/633], LR: 0.005000, Loss: 3.6371, top1: 14.0625\n",
      "Epoch [8/60], Iter [552/633], LR: 0.005000, Loss: 3.6051, top1: 10.9375\n",
      "Epoch [8/60], Iter [553/633], LR: 0.005000, Loss: 3.5818, top1: 14.0625\n",
      "Epoch [8/60], Iter [554/633], LR: 0.005000, Loss: 3.5964, top1: 7.8125\n",
      "Epoch [8/60], Iter [555/633], LR: 0.005000, Loss: 3.5866, top1: 9.3750\n",
      "Epoch [8/60], Iter [556/633], LR: 0.005000, Loss: 3.6180, top1: 17.1875\n",
      "Epoch [8/60], Iter [557/633], LR: 0.005000, Loss: 3.6395, top1: 7.8125\n",
      "Epoch [8/60], Iter [558/633], LR: 0.005000, Loss: 3.5794, top1: 14.0625\n",
      "Epoch [8/60], Iter [559/633], LR: 0.005000, Loss: 3.6045, top1: 14.0625\n",
      "Epoch [8/60], Iter [560/633], LR: 0.005000, Loss: 3.6315, top1: 6.2500\n",
      "Epoch [8/60], Iter [561/633], LR: 0.005000, Loss: 3.6019, top1: 14.0625\n",
      "Epoch [8/60], Iter [562/633], LR: 0.005000, Loss: 3.5375, top1: 17.1875\n",
      "Epoch [8/60], Iter [563/633], LR: 0.005000, Loss: 3.5799, top1: 12.5000\n",
      "Epoch [8/60], Iter [564/633], LR: 0.005000, Loss: 3.5296, top1: 31.2500\n",
      "Epoch [8/60], Iter [565/633], LR: 0.005000, Loss: 3.5927, top1: 15.6250\n",
      "Epoch [8/60], Iter [566/633], LR: 0.005000, Loss: 3.5772, top1: 14.0625\n",
      "Epoch [8/60], Iter [567/633], LR: 0.005000, Loss: 3.6121, top1: 10.9375\n",
      "Epoch [8/60], Iter [568/633], LR: 0.005000, Loss: 3.5769, top1: 20.3125\n",
      "Epoch [8/60], Iter [569/633], LR: 0.005000, Loss: 3.5517, top1: 18.7500\n",
      "Epoch [8/60], Iter [570/633], LR: 0.005000, Loss: 3.5867, top1: 15.6250\n",
      "Epoch [8/60], Iter [571/633], LR: 0.005000, Loss: 3.6276, top1: 9.3750\n",
      "Epoch [8/60], Iter [572/633], LR: 0.005000, Loss: 3.6256, top1: 7.8125\n",
      "Epoch [8/60], Iter [573/633], LR: 0.005000, Loss: 3.6070, top1: 14.0625\n",
      "Epoch [8/60], Iter [574/633], LR: 0.005000, Loss: 3.5698, top1: 12.5000\n",
      "Epoch [8/60], Iter [575/633], LR: 0.005000, Loss: 3.5917, top1: 7.8125\n",
      "Epoch [8/60], Iter [576/633], LR: 0.005000, Loss: 3.6067, top1: 7.8125\n",
      "Epoch [8/60], Iter [577/633], LR: 0.005000, Loss: 3.5821, top1: 10.9375\n",
      "Epoch [8/60], Iter [578/633], LR: 0.005000, Loss: 3.5171, top1: 23.4375\n",
      "Epoch [8/60], Iter [579/633], LR: 0.005000, Loss: 3.6053, top1: 12.5000\n",
      "Epoch [8/60], Iter [580/633], LR: 0.005000, Loss: 3.5635, top1: 18.7500\n",
      "Epoch [8/60], Iter [581/633], LR: 0.005000, Loss: 3.5953, top1: 17.1875\n",
      "Epoch [8/60], Iter [582/633], LR: 0.005000, Loss: 3.5985, top1: 17.1875\n",
      "Epoch [8/60], Iter [583/633], LR: 0.005000, Loss: 3.5841, top1: 7.8125\n",
      "Epoch [8/60], Iter [584/633], LR: 0.005000, Loss: 3.6197, top1: 14.0625\n",
      "Epoch [8/60], Iter [585/633], LR: 0.005000, Loss: 3.5801, top1: 10.9375\n",
      "Epoch [8/60], Iter [586/633], LR: 0.005000, Loss: 3.5903, top1: 12.5000\n",
      "Epoch [8/60], Iter [587/633], LR: 0.005000, Loss: 3.6368, top1: 14.0625\n",
      "Epoch [8/60], Iter [588/633], LR: 0.005000, Loss: 3.6150, top1: 17.1875\n",
      "Epoch [8/60], Iter [589/633], LR: 0.005000, Loss: 3.5879, top1: 10.9375\n",
      "Epoch [8/60], Iter [590/633], LR: 0.005000, Loss: 3.5961, top1: 18.7500\n",
      "Epoch [8/60], Iter [591/633], LR: 0.005000, Loss: 3.6016, top1: 15.6250\n",
      "Epoch [8/60], Iter [592/633], LR: 0.005000, Loss: 3.5894, top1: 12.5000\n",
      "Epoch [8/60], Iter [593/633], LR: 0.005000, Loss: 3.6293, top1: 4.6875\n",
      "Epoch [8/60], Iter [594/633], LR: 0.005000, Loss: 3.6140, top1: 12.5000\n",
      "Epoch [8/60], Iter [595/633], LR: 0.005000, Loss: 3.6352, top1: 9.3750\n",
      "Epoch [8/60], Iter [596/633], LR: 0.005000, Loss: 3.6027, top1: 15.6250\n",
      "Epoch [8/60], Iter [597/633], LR: 0.005000, Loss: 3.6009, top1: 14.0625\n",
      "Epoch [8/60], Iter [598/633], LR: 0.005000, Loss: 3.5666, top1: 12.5000\n",
      "Epoch [8/60], Iter [599/633], LR: 0.005000, Loss: 3.6428, top1: 10.9375\n",
      "Epoch [8/60], Iter [600/633], LR: 0.005000, Loss: 3.5788, top1: 14.0625\n",
      "Epoch [8/60], Iter [601/633], LR: 0.005000, Loss: 3.6137, top1: 10.9375\n",
      "Epoch [8/60], Iter [602/633], LR: 0.005000, Loss: 3.5729, top1: 14.0625\n",
      "Epoch [8/60], Iter [603/633], LR: 0.005000, Loss: 3.6016, top1: 9.3750\n",
      "Epoch [8/60], Iter [604/633], LR: 0.005000, Loss: 3.5873, top1: 14.0625\n",
      "Epoch [8/60], Iter [605/633], LR: 0.005000, Loss: 3.5754, top1: 14.0625\n",
      "Epoch [8/60], Iter [606/633], LR: 0.005000, Loss: 3.5977, top1: 14.0625\n",
      "Epoch [8/60], Iter [607/633], LR: 0.005000, Loss: 3.6045, top1: 10.9375\n",
      "Epoch [8/60], Iter [608/633], LR: 0.005000, Loss: 3.6216, top1: 4.6875\n",
      "Epoch [8/60], Iter [609/633], LR: 0.005000, Loss: 3.6149, top1: 7.8125\n",
      "Epoch [8/60], Iter [610/633], LR: 0.005000, Loss: 3.6073, top1: 10.9375\n",
      "Epoch [8/60], Iter [611/633], LR: 0.005000, Loss: 3.5592, top1: 15.6250\n",
      "Epoch [8/60], Iter [612/633], LR: 0.005000, Loss: 3.5806, top1: 10.9375\n",
      "Epoch [8/60], Iter [613/633], LR: 0.005000, Loss: 3.5882, top1: 14.0625\n",
      "Epoch [8/60], Iter [614/633], LR: 0.005000, Loss: 3.6104, top1: 15.6250\n",
      "Epoch [8/60], Iter [615/633], LR: 0.005000, Loss: 3.5595, top1: 15.6250\n",
      "Epoch [8/60], Iter [616/633], LR: 0.005000, Loss: 3.6072, top1: 7.8125\n",
      "Epoch [8/60], Iter [617/633], LR: 0.005000, Loss: 3.5491, top1: 20.3125\n",
      "Epoch [8/60], Iter [618/633], LR: 0.005000, Loss: 3.5584, top1: 25.0000\n",
      "Epoch [8/60], Iter [619/633], LR: 0.005000, Loss: 3.5612, top1: 15.6250\n",
      "Epoch [8/60], Iter [620/633], LR: 0.005000, Loss: 3.5474, top1: 23.4375\n",
      "Epoch [8/60], Iter [621/633], LR: 0.005000, Loss: 3.6042, top1: 12.5000\n",
      "Epoch [8/60], Iter [622/633], LR: 0.005000, Loss: 3.6149, top1: 7.8125\n",
      "Epoch [8/60], Iter [623/633], LR: 0.005000, Loss: 3.5677, top1: 10.9375\n",
      "Epoch [8/60], Iter [624/633], LR: 0.005000, Loss: 3.5753, top1: 10.9375\n",
      "Epoch [8/60], Iter [625/633], LR: 0.005000, Loss: 3.5139, top1: 25.0000\n",
      "Epoch [8/60], Iter [626/633], LR: 0.005000, Loss: 3.6309, top1: 12.5000\n",
      "Epoch [8/60], Iter [627/633], LR: 0.005000, Loss: 3.6372, top1: 9.3750\n",
      "Epoch [8/60], Iter [628/633], LR: 0.005000, Loss: 3.5860, top1: 10.9375\n",
      "Epoch [8/60], Iter [629/633], LR: 0.005000, Loss: 3.5736, top1: 20.3125\n",
      "Epoch [8/60], Iter [630/633], LR: 0.005000, Loss: 3.6460, top1: 12.5000\n",
      "Epoch [8/60], Iter [631/633], LR: 0.005000, Loss: 3.5448, top1: 20.3125\n",
      "Epoch [8/60], Iter [632/633], LR: 0.005000, Loss: 3.5689, top1: 14.0625\n",
      "Epoch [8/60], Iter [633/633], LR: 0.005000, Loss: 3.5863, top1: 14.0625\n",
      "Epoch [8/60], Iter [634/633], LR: 0.005000, Loss: 3.5818, top1: 16.1290\n",
      "Epoch [8/60], Val_Loss: 3.5831, Val_top1: 13.2702, best_top1: 12.3680\n",
      "epoch time: 4.448542972405751 min\n",
      "Taking top1 snapshot...\n",
      "Epoch [9/60], Iter [1/633], LR: 0.005000, Loss: 3.6133, top1: 12.5000\n",
      "Epoch [9/60], Iter [2/633], LR: 0.005000, Loss: 3.5467, top1: 25.0000\n",
      "Epoch [9/60], Iter [3/633], LR: 0.005000, Loss: 3.6309, top1: 9.3750\n",
      "Epoch [9/60], Iter [4/633], LR: 0.005000, Loss: 3.5680, top1: 14.0625\n",
      "Epoch [9/60], Iter [5/633], LR: 0.005000, Loss: 3.5875, top1: 12.5000\n",
      "Epoch [9/60], Iter [6/633], LR: 0.005000, Loss: 3.5492, top1: 12.5000\n",
      "Epoch [9/60], Iter [7/633], LR: 0.005000, Loss: 3.6156, top1: 7.8125\n",
      "Epoch [9/60], Iter [8/633], LR: 0.005000, Loss: 3.5982, top1: 18.7500\n",
      "Epoch [9/60], Iter [9/633], LR: 0.005000, Loss: 3.6012, top1: 10.9375\n",
      "Epoch [9/60], Iter [10/633], LR: 0.005000, Loss: 3.5518, top1: 18.7500\n",
      "Epoch [9/60], Iter [11/633], LR: 0.005000, Loss: 3.6439, top1: 6.2500\n",
      "Epoch [9/60], Iter [12/633], LR: 0.005000, Loss: 3.6045, top1: 12.5000\n",
      "Epoch [9/60], Iter [13/633], LR: 0.005000, Loss: 3.6099, top1: 10.9375\n",
      "Epoch [9/60], Iter [14/633], LR: 0.005000, Loss: 3.5861, top1: 14.0625\n",
      "Epoch [9/60], Iter [15/633], LR: 0.005000, Loss: 3.6082, top1: 12.5000\n",
      "Epoch [9/60], Iter [16/633], LR: 0.005000, Loss: 3.5730, top1: 14.0625\n",
      "Epoch [9/60], Iter [17/633], LR: 0.005000, Loss: 3.5805, top1: 9.3750\n",
      "Epoch [9/60], Iter [18/633], LR: 0.005000, Loss: 3.5950, top1: 10.9375\n",
      "Epoch [9/60], Iter [19/633], LR: 0.005000, Loss: 3.5834, top1: 17.1875\n",
      "Epoch [9/60], Iter [20/633], LR: 0.005000, Loss: 3.5596, top1: 17.1875\n",
      "Epoch [9/60], Iter [21/633], LR: 0.005000, Loss: 3.6258, top1: 7.8125\n",
      "Epoch [9/60], Iter [22/633], LR: 0.005000, Loss: 3.5954, top1: 18.7500\n",
      "Epoch [9/60], Iter [23/633], LR: 0.005000, Loss: 3.6392, top1: 7.8125\n",
      "Epoch [9/60], Iter [24/633], LR: 0.005000, Loss: 3.6206, top1: 7.8125\n",
      "Epoch [9/60], Iter [25/633], LR: 0.005000, Loss: 3.5943, top1: 14.0625\n",
      "Epoch [9/60], Iter [26/633], LR: 0.005000, Loss: 3.5950, top1: 15.6250\n",
      "Epoch [9/60], Iter [27/633], LR: 0.005000, Loss: 3.6178, top1: 7.8125\n",
      "Epoch [9/60], Iter [28/633], LR: 0.005000, Loss: 3.5844, top1: 15.6250\n",
      "Epoch [9/60], Iter [29/633], LR: 0.005000, Loss: 3.5707, top1: 15.6250\n",
      "Epoch [9/60], Iter [30/633], LR: 0.005000, Loss: 3.6027, top1: 14.0625\n",
      "Epoch [9/60], Iter [31/633], LR: 0.005000, Loss: 3.5731, top1: 15.6250\n",
      "Epoch [9/60], Iter [32/633], LR: 0.005000, Loss: 3.5741, top1: 14.0625\n",
      "Epoch [9/60], Iter [33/633], LR: 0.005000, Loss: 3.5295, top1: 18.7500\n",
      "Epoch [9/60], Iter [34/633], LR: 0.005000, Loss: 3.5909, top1: 15.6250\n",
      "Epoch [9/60], Iter [35/633], LR: 0.005000, Loss: 3.6035, top1: 4.6875\n",
      "Epoch [9/60], Iter [36/633], LR: 0.005000, Loss: 3.5791, top1: 14.0625\n",
      "Epoch [9/60], Iter [37/633], LR: 0.005000, Loss: 3.6285, top1: 4.6875\n",
      "Epoch [9/60], Iter [38/633], LR: 0.005000, Loss: 3.5746, top1: 14.0625\n",
      "Epoch [9/60], Iter [39/633], LR: 0.005000, Loss: 3.5960, top1: 14.0625\n",
      "Epoch [9/60], Iter [40/633], LR: 0.005000, Loss: 3.5857, top1: 14.0625\n",
      "Epoch [9/60], Iter [41/633], LR: 0.005000, Loss: 3.6155, top1: 12.5000\n",
      "Epoch [9/60], Iter [42/633], LR: 0.005000, Loss: 3.6161, top1: 12.5000\n",
      "Epoch [9/60], Iter [43/633], LR: 0.005000, Loss: 3.5815, top1: 14.0625\n",
      "Epoch [9/60], Iter [44/633], LR: 0.005000, Loss: 3.6014, top1: 10.9375\n",
      "Epoch [9/60], Iter [45/633], LR: 0.005000, Loss: 3.5974, top1: 18.7500\n",
      "Epoch [9/60], Iter [46/633], LR: 0.005000, Loss: 3.6212, top1: 10.9375\n",
      "Epoch [9/60], Iter [47/633], LR: 0.005000, Loss: 3.5975, top1: 14.0625\n",
      "Epoch [9/60], Iter [48/633], LR: 0.005000, Loss: 3.6011, top1: 9.3750\n",
      "Epoch [9/60], Iter [49/633], LR: 0.005000, Loss: 3.5864, top1: 7.8125\n",
      "Epoch [9/60], Iter [50/633], LR: 0.005000, Loss: 3.5993, top1: 9.3750\n",
      "Epoch [9/60], Iter [51/633], LR: 0.005000, Loss: 3.5724, top1: 18.7500\n",
      "Epoch [9/60], Iter [52/633], LR: 0.005000, Loss: 3.5467, top1: 17.1875\n",
      "Epoch [9/60], Iter [53/633], LR: 0.005000, Loss: 3.5731, top1: 10.9375\n",
      "Epoch [9/60], Iter [54/633], LR: 0.005000, Loss: 3.6213, top1: 10.9375\n",
      "Epoch [9/60], Iter [55/633], LR: 0.005000, Loss: 3.5865, top1: 14.0625\n",
      "Epoch [9/60], Iter [56/633], LR: 0.005000, Loss: 3.5465, top1: 23.4375\n",
      "Epoch [9/60], Iter [57/633], LR: 0.005000, Loss: 3.5792, top1: 7.8125\n",
      "Epoch [9/60], Iter [58/633], LR: 0.005000, Loss: 3.6010, top1: 14.0625\n",
      "Epoch [9/60], Iter [59/633], LR: 0.005000, Loss: 3.6056, top1: 7.8125\n",
      "Epoch [9/60], Iter [60/633], LR: 0.005000, Loss: 3.5581, top1: 21.8750\n",
      "Epoch [9/60], Iter [61/633], LR: 0.005000, Loss: 3.5708, top1: 12.5000\n",
      "Epoch [9/60], Iter [62/633], LR: 0.005000, Loss: 3.5599, top1: 18.7500\n",
      "Epoch [9/60], Iter [63/633], LR: 0.005000, Loss: 3.5889, top1: 10.9375\n",
      "Epoch [9/60], Iter [64/633], LR: 0.005000, Loss: 3.5746, top1: 17.1875\n",
      "Epoch [9/60], Iter [65/633], LR: 0.005000, Loss: 3.5885, top1: 14.0625\n",
      "Epoch [9/60], Iter [66/633], LR: 0.005000, Loss: 3.5198, top1: 21.8750\n",
      "Epoch [9/60], Iter [67/633], LR: 0.005000, Loss: 3.5591, top1: 23.4375\n",
      "Epoch [9/60], Iter [68/633], LR: 0.005000, Loss: 3.5588, top1: 17.1875\n",
      "Epoch [9/60], Iter [69/633], LR: 0.005000, Loss: 3.5156, top1: 26.5625\n",
      "Epoch [9/60], Iter [70/633], LR: 0.005000, Loss: 3.6168, top1: 17.1875\n",
      "Epoch [9/60], Iter [71/633], LR: 0.005000, Loss: 3.5646, top1: 17.1875\n",
      "Epoch [9/60], Iter [72/633], LR: 0.005000, Loss: 3.6322, top1: 9.3750\n",
      "Epoch [9/60], Iter [73/633], LR: 0.005000, Loss: 3.5608, top1: 14.0625\n",
      "Epoch [9/60], Iter [74/633], LR: 0.005000, Loss: 3.5563, top1: 20.3125\n",
      "Epoch [9/60], Iter [75/633], LR: 0.005000, Loss: 3.6262, top1: 6.2500\n",
      "Epoch [9/60], Iter [76/633], LR: 0.005000, Loss: 3.5911, top1: 15.6250\n",
      "Epoch [9/60], Iter [77/633], LR: 0.005000, Loss: 3.5673, top1: 17.1875\n",
      "Epoch [9/60], Iter [78/633], LR: 0.005000, Loss: 3.5588, top1: 15.6250\n",
      "Epoch [9/60], Iter [79/633], LR: 0.005000, Loss: 3.5555, top1: 15.6250\n",
      "Epoch [9/60], Iter [80/633], LR: 0.005000, Loss: 3.5840, top1: 12.5000\n",
      "Epoch [9/60], Iter [81/633], LR: 0.005000, Loss: 3.6005, top1: 10.9375\n",
      "Epoch [9/60], Iter [82/633], LR: 0.005000, Loss: 3.5969, top1: 14.0625\n",
      "Epoch [9/60], Iter [83/633], LR: 0.005000, Loss: 3.5721, top1: 14.0625\n",
      "Epoch [9/60], Iter [84/633], LR: 0.005000, Loss: 3.5766, top1: 12.5000\n",
      "Epoch [9/60], Iter [85/633], LR: 0.005000, Loss: 3.5748, top1: 15.6250\n",
      "Epoch [9/60], Iter [86/633], LR: 0.005000, Loss: 3.5675, top1: 18.7500\n",
      "Epoch [9/60], Iter [87/633], LR: 0.005000, Loss: 3.5978, top1: 9.3750\n",
      "Epoch [9/60], Iter [88/633], LR: 0.005000, Loss: 3.5609, top1: 17.1875\n",
      "Epoch [9/60], Iter [89/633], LR: 0.005000, Loss: 3.6359, top1: 14.0625\n",
      "Epoch [9/60], Iter [90/633], LR: 0.005000, Loss: 3.6578, top1: 4.6875\n",
      "Epoch [9/60], Iter [91/633], LR: 0.005000, Loss: 3.5801, top1: 15.6250\n",
      "Epoch [9/60], Iter [92/633], LR: 0.005000, Loss: 3.5787, top1: 17.1875\n",
      "Epoch [9/60], Iter [93/633], LR: 0.005000, Loss: 3.5495, top1: 17.1875\n",
      "Epoch [9/60], Iter [94/633], LR: 0.005000, Loss: 3.6153, top1: 10.9375\n",
      "Epoch [9/60], Iter [95/633], LR: 0.005000, Loss: 3.5895, top1: 15.6250\n",
      "Epoch [9/60], Iter [96/633], LR: 0.005000, Loss: 3.6224, top1: 9.3750\n",
      "Epoch [9/60], Iter [97/633], LR: 0.005000, Loss: 3.5761, top1: 20.3125\n",
      "Epoch [9/60], Iter [98/633], LR: 0.005000, Loss: 3.5909, top1: 15.6250\n",
      "Epoch [9/60], Iter [99/633], LR: 0.005000, Loss: 3.5927, top1: 12.5000\n",
      "Epoch [9/60], Iter [100/633], LR: 0.005000, Loss: 3.6194, top1: 14.0625\n",
      "Epoch [9/60], Iter [101/633], LR: 0.005000, Loss: 3.6160, top1: 9.3750\n",
      "Epoch [9/60], Iter [102/633], LR: 0.005000, Loss: 3.5618, top1: 15.6250\n",
      "Epoch [9/60], Iter [103/633], LR: 0.005000, Loss: 3.5496, top1: 15.6250\n",
      "Epoch [9/60], Iter [104/633], LR: 0.005000, Loss: 3.6170, top1: 7.8125\n",
      "Epoch [9/60], Iter [105/633], LR: 0.005000, Loss: 3.6395, top1: 12.5000\n",
      "Epoch [9/60], Iter [106/633], LR: 0.005000, Loss: 3.6134, top1: 12.5000\n",
      "Epoch [9/60], Iter [107/633], LR: 0.005000, Loss: 3.5877, top1: 14.0625\n",
      "Epoch [9/60], Iter [108/633], LR: 0.005000, Loss: 3.5937, top1: 15.6250\n",
      "Epoch [9/60], Iter [109/633], LR: 0.005000, Loss: 3.5841, top1: 9.3750\n",
      "Epoch [9/60], Iter [110/633], LR: 0.005000, Loss: 3.5472, top1: 14.0625\n",
      "Epoch [9/60], Iter [111/633], LR: 0.005000, Loss: 3.6407, top1: 7.8125\n",
      "Epoch [9/60], Iter [112/633], LR: 0.005000, Loss: 3.5911, top1: 10.9375\n",
      "Epoch [9/60], Iter [113/633], LR: 0.005000, Loss: 3.5947, top1: 10.9375\n",
      "Epoch [9/60], Iter [114/633], LR: 0.005000, Loss: 3.5662, top1: 10.9375\n",
      "Epoch [9/60], Iter [115/633], LR: 0.005000, Loss: 3.5803, top1: 14.0625\n",
      "Epoch [9/60], Iter [116/633], LR: 0.005000, Loss: 3.6317, top1: 6.2500\n",
      "Epoch [9/60], Iter [117/633], LR: 0.005000, Loss: 3.5727, top1: 14.0625\n",
      "Epoch [9/60], Iter [118/633], LR: 0.005000, Loss: 3.6245, top1: 7.8125\n",
      "Epoch [9/60], Iter [119/633], LR: 0.005000, Loss: 3.5933, top1: 15.6250\n",
      "Epoch [9/60], Iter [120/633], LR: 0.005000, Loss: 3.6115, top1: 12.5000\n",
      "Epoch [9/60], Iter [121/633], LR: 0.005000, Loss: 3.5862, top1: 9.3750\n",
      "Epoch [9/60], Iter [122/633], LR: 0.005000, Loss: 3.6054, top1: 10.9375\n",
      "Epoch [9/60], Iter [123/633], LR: 0.005000, Loss: 3.5704, top1: 15.6250\n",
      "Epoch [9/60], Iter [124/633], LR: 0.005000, Loss: 3.5891, top1: 10.9375\n",
      "Epoch [9/60], Iter [125/633], LR: 0.005000, Loss: 3.5578, top1: 17.1875\n",
      "Epoch [9/60], Iter [126/633], LR: 0.005000, Loss: 3.5895, top1: 12.5000\n",
      "Epoch [9/60], Iter [127/633], LR: 0.005000, Loss: 3.5577, top1: 17.1875\n",
      "Epoch [9/60], Iter [128/633], LR: 0.005000, Loss: 3.6085, top1: 10.9375\n",
      "Epoch [9/60], Iter [129/633], LR: 0.005000, Loss: 3.5913, top1: 10.9375\n",
      "Epoch [9/60], Iter [130/633], LR: 0.005000, Loss: 3.5443, top1: 15.6250\n",
      "Epoch [9/60], Iter [131/633], LR: 0.005000, Loss: 3.5796, top1: 9.3750\n",
      "Epoch [9/60], Iter [132/633], LR: 0.005000, Loss: 3.5927, top1: 15.6250\n",
      "Epoch [9/60], Iter [133/633], LR: 0.005000, Loss: 3.6405, top1: 10.9375\n",
      "Epoch [9/60], Iter [134/633], LR: 0.005000, Loss: 3.5853, top1: 10.9375\n",
      "Epoch [9/60], Iter [135/633], LR: 0.005000, Loss: 3.5949, top1: 15.6250\n",
      "Epoch [9/60], Iter [136/633], LR: 0.005000, Loss: 3.5734, top1: 14.0625\n",
      "Epoch [9/60], Iter [137/633], LR: 0.005000, Loss: 3.5800, top1: 12.5000\n",
      "Epoch [9/60], Iter [138/633], LR: 0.005000, Loss: 3.6028, top1: 12.5000\n",
      "Epoch [9/60], Iter [139/633], LR: 0.005000, Loss: 3.5694, top1: 18.7500\n",
      "Epoch [9/60], Iter [140/633], LR: 0.005000, Loss: 3.5808, top1: 12.5000\n",
      "Epoch [9/60], Iter [141/633], LR: 0.005000, Loss: 3.6515, top1: 4.6875\n",
      "Epoch [9/60], Iter [142/633], LR: 0.005000, Loss: 3.6233, top1: 7.8125\n",
      "Epoch [9/60], Iter [143/633], LR: 0.005000, Loss: 3.6319, top1: 7.8125\n",
      "Epoch [9/60], Iter [144/633], LR: 0.005000, Loss: 3.5764, top1: 15.6250\n",
      "Epoch [9/60], Iter [145/633], LR: 0.005000, Loss: 3.6175, top1: 9.3750\n",
      "Epoch [9/60], Iter [146/633], LR: 0.005000, Loss: 3.6043, top1: 15.6250\n",
      "Epoch [9/60], Iter [147/633], LR: 0.005000, Loss: 3.5925, top1: 10.9375\n",
      "Epoch [9/60], Iter [148/633], LR: 0.005000, Loss: 3.5955, top1: 9.3750\n",
      "Epoch [9/60], Iter [149/633], LR: 0.005000, Loss: 3.5853, top1: 9.3750\n",
      "Epoch [9/60], Iter [150/633], LR: 0.005000, Loss: 3.5523, top1: 15.6250\n",
      "Epoch [9/60], Iter [151/633], LR: 0.005000, Loss: 3.5463, top1: 18.7500\n",
      "Epoch [9/60], Iter [152/633], LR: 0.005000, Loss: 3.5750, top1: 17.1875\n",
      "Epoch [9/60], Iter [153/633], LR: 0.005000, Loss: 3.5963, top1: 14.0625\n",
      "Epoch [9/60], Iter [154/633], LR: 0.005000, Loss: 3.5942, top1: 15.6250\n",
      "Epoch [9/60], Iter [155/633], LR: 0.005000, Loss: 3.5876, top1: 18.7500\n",
      "Epoch [9/60], Iter [156/633], LR: 0.005000, Loss: 3.6073, top1: 10.9375\n",
      "Epoch [9/60], Iter [157/633], LR: 0.005000, Loss: 3.5813, top1: 9.3750\n",
      "Epoch [9/60], Iter [158/633], LR: 0.005000, Loss: 3.5494, top1: 23.4375\n",
      "Epoch [9/60], Iter [159/633], LR: 0.005000, Loss: 3.6024, top1: 14.0625\n",
      "Epoch [9/60], Iter [160/633], LR: 0.005000, Loss: 3.5372, top1: 20.3125\n",
      "Epoch [9/60], Iter [161/633], LR: 0.005000, Loss: 3.5898, top1: 12.5000\n",
      "Epoch [9/60], Iter [162/633], LR: 0.005000, Loss: 3.5271, top1: 21.8750\n",
      "Epoch [9/60], Iter [163/633], LR: 0.005000, Loss: 3.5454, top1: 20.3125\n",
      "Epoch [9/60], Iter [164/633], LR: 0.005000, Loss: 3.5718, top1: 17.1875\n",
      "Epoch [9/60], Iter [165/633], LR: 0.005000, Loss: 3.5850, top1: 14.0625\n",
      "Epoch [9/60], Iter [166/633], LR: 0.005000, Loss: 3.6006, top1: 10.9375\n",
      "Epoch [9/60], Iter [167/633], LR: 0.005000, Loss: 3.6362, top1: 10.9375\n",
      "Epoch [9/60], Iter [168/633], LR: 0.005000, Loss: 3.6032, top1: 10.9375\n",
      "Epoch [9/60], Iter [169/633], LR: 0.005000, Loss: 3.5451, top1: 20.3125\n",
      "Epoch [9/60], Iter [170/633], LR: 0.005000, Loss: 3.5651, top1: 12.5000\n",
      "Epoch [9/60], Iter [171/633], LR: 0.005000, Loss: 3.5818, top1: 9.3750\n",
      "Epoch [9/60], Iter [172/633], LR: 0.005000, Loss: 3.5844, top1: 12.5000\n",
      "Epoch [9/60], Iter [173/633], LR: 0.005000, Loss: 3.6050, top1: 12.5000\n",
      "Epoch [9/60], Iter [174/633], LR: 0.005000, Loss: 3.5776, top1: 14.0625\n",
      "Epoch [9/60], Iter [175/633], LR: 0.005000, Loss: 3.6103, top1: 10.9375\n",
      "Epoch [9/60], Iter [176/633], LR: 0.005000, Loss: 3.6231, top1: 7.8125\n",
      "Epoch [9/60], Iter [177/633], LR: 0.005000, Loss: 3.5871, top1: 14.0625\n",
      "Epoch [9/60], Iter [178/633], LR: 0.005000, Loss: 3.5799, top1: 9.3750\n",
      "Epoch [9/60], Iter [179/633], LR: 0.005000, Loss: 3.6219, top1: 10.9375\n",
      "Epoch [9/60], Iter [180/633], LR: 0.005000, Loss: 3.5813, top1: 12.5000\n",
      "Epoch [9/60], Iter [181/633], LR: 0.005000, Loss: 3.5928, top1: 9.3750\n",
      "Epoch [9/60], Iter [182/633], LR: 0.005000, Loss: 3.6134, top1: 3.1250\n",
      "Epoch [9/60], Iter [183/633], LR: 0.005000, Loss: 3.5894, top1: 12.5000\n",
      "Epoch [9/60], Iter [184/633], LR: 0.005000, Loss: 3.6045, top1: 12.5000\n",
      "Epoch [9/60], Iter [185/633], LR: 0.005000, Loss: 3.5822, top1: 17.1875\n",
      "Epoch [9/60], Iter [186/633], LR: 0.005000, Loss: 3.5728, top1: 12.5000\n",
      "Epoch [9/60], Iter [187/633], LR: 0.005000, Loss: 3.5908, top1: 9.3750\n",
      "Epoch [9/60], Iter [188/633], LR: 0.005000, Loss: 3.6569, top1: 6.2500\n",
      "Epoch [9/60], Iter [189/633], LR: 0.005000, Loss: 3.5568, top1: 15.6250\n",
      "Epoch [9/60], Iter [190/633], LR: 0.005000, Loss: 3.5828, top1: 14.0625\n",
      "Epoch [9/60], Iter [191/633], LR: 0.005000, Loss: 3.6250, top1: 9.3750\n",
      "Epoch [9/60], Iter [192/633], LR: 0.005000, Loss: 3.5879, top1: 14.0625\n",
      "Epoch [9/60], Iter [193/633], LR: 0.005000, Loss: 3.6127, top1: 9.3750\n",
      "Epoch [9/60], Iter [194/633], LR: 0.005000, Loss: 3.6328, top1: 10.9375\n",
      "Epoch [9/60], Iter [195/633], LR: 0.005000, Loss: 3.5990, top1: 9.3750\n",
      "Epoch [9/60], Iter [196/633], LR: 0.005000, Loss: 3.5664, top1: 10.9375\n",
      "Epoch [9/60], Iter [197/633], LR: 0.005000, Loss: 3.5690, top1: 17.1875\n",
      "Epoch [9/60], Iter [198/633], LR: 0.005000, Loss: 3.5790, top1: 18.7500\n",
      "Epoch [9/60], Iter [199/633], LR: 0.005000, Loss: 3.5723, top1: 4.6875\n",
      "Epoch [9/60], Iter [200/633], LR: 0.005000, Loss: 3.6449, top1: 9.3750\n",
      "Epoch [9/60], Iter [201/633], LR: 0.005000, Loss: 3.5759, top1: 15.6250\n",
      "Epoch [9/60], Iter [202/633], LR: 0.005000, Loss: 3.6040, top1: 18.7500\n",
      "Epoch [9/60], Iter [203/633], LR: 0.005000, Loss: 3.6028, top1: 14.0625\n",
      "Epoch [9/60], Iter [204/633], LR: 0.005000, Loss: 3.5490, top1: 7.8125\n",
      "Epoch [9/60], Iter [205/633], LR: 0.005000, Loss: 3.6250, top1: 10.9375\n",
      "Epoch [9/60], Iter [206/633], LR: 0.005000, Loss: 3.6070, top1: 10.9375\n",
      "Epoch [9/60], Iter [207/633], LR: 0.005000, Loss: 3.6583, top1: 10.9375\n",
      "Epoch [9/60], Iter [208/633], LR: 0.005000, Loss: 3.5884, top1: 17.1875\n",
      "Epoch [9/60], Iter [209/633], LR: 0.005000, Loss: 3.5631, top1: 18.7500\n",
      "Epoch [9/60], Iter [210/633], LR: 0.005000, Loss: 3.6005, top1: 14.0625\n",
      "Epoch [9/60], Iter [211/633], LR: 0.005000, Loss: 3.6050, top1: 7.8125\n",
      "Epoch [9/60], Iter [212/633], LR: 0.005000, Loss: 3.6350, top1: 6.2500\n",
      "Epoch [9/60], Iter [213/633], LR: 0.005000, Loss: 3.6254, top1: 7.8125\n",
      "Epoch [9/60], Iter [214/633], LR: 0.005000, Loss: 3.5692, top1: 10.9375\n",
      "Epoch [9/60], Iter [215/633], LR: 0.005000, Loss: 3.5974, top1: 10.9375\n",
      "Epoch [9/60], Iter [216/633], LR: 0.005000, Loss: 3.5871, top1: 14.0625\n",
      "Epoch [9/60], Iter [217/633], LR: 0.005000, Loss: 3.5627, top1: 20.3125\n",
      "Epoch [9/60], Iter [218/633], LR: 0.005000, Loss: 3.6192, top1: 6.2500\n",
      "Epoch [9/60], Iter [219/633], LR: 0.005000, Loss: 3.5821, top1: 18.7500\n",
      "Epoch [9/60], Iter [220/633], LR: 0.005000, Loss: 3.5695, top1: 12.5000\n",
      "Epoch [9/60], Iter [221/633], LR: 0.005000, Loss: 3.5903, top1: 14.0625\n",
      "Epoch [9/60], Iter [222/633], LR: 0.005000, Loss: 3.5833, top1: 15.6250\n",
      "Epoch [9/60], Iter [223/633], LR: 0.005000, Loss: 3.6272, top1: 10.9375\n",
      "Epoch [9/60], Iter [224/633], LR: 0.005000, Loss: 3.5503, top1: 15.6250\n",
      "Epoch [9/60], Iter [225/633], LR: 0.005000, Loss: 3.6213, top1: 9.3750\n",
      "Epoch [9/60], Iter [226/633], LR: 0.005000, Loss: 3.5960, top1: 14.0625\n",
      "Epoch [9/60], Iter [227/633], LR: 0.005000, Loss: 3.5550, top1: 15.6250\n",
      "Epoch [9/60], Iter [228/633], LR: 0.005000, Loss: 3.6155, top1: 12.5000\n",
      "Epoch [9/60], Iter [229/633], LR: 0.005000, Loss: 3.5897, top1: 14.0625\n",
      "Epoch [9/60], Iter [230/633], LR: 0.005000, Loss: 3.5810, top1: 9.3750\n",
      "Epoch [9/60], Iter [231/633], LR: 0.005000, Loss: 3.5937, top1: 12.5000\n",
      "Epoch [9/60], Iter [232/633], LR: 0.005000, Loss: 3.5962, top1: 14.0625\n",
      "Epoch [9/60], Iter [233/633], LR: 0.005000, Loss: 3.5702, top1: 10.9375\n",
      "Epoch [9/60], Iter [234/633], LR: 0.005000, Loss: 3.6018, top1: 12.5000\n",
      "Epoch [9/60], Iter [235/633], LR: 0.005000, Loss: 3.5864, top1: 14.0625\n",
      "Epoch [9/60], Iter [236/633], LR: 0.005000, Loss: 3.5954, top1: 12.5000\n",
      "Epoch [9/60], Iter [237/633], LR: 0.005000, Loss: 3.5580, top1: 15.6250\n",
      "Epoch [9/60], Iter [238/633], LR: 0.005000, Loss: 3.5883, top1: 14.0625\n",
      "Epoch [9/60], Iter [239/633], LR: 0.005000, Loss: 3.5830, top1: 15.6250\n",
      "Epoch [9/60], Iter [240/633], LR: 0.005000, Loss: 3.5537, top1: 18.7500\n",
      "Epoch [9/60], Iter [241/633], LR: 0.005000, Loss: 3.6077, top1: 14.0625\n",
      "Epoch [9/60], Iter [242/633], LR: 0.005000, Loss: 3.5683, top1: 18.7500\n",
      "Epoch [9/60], Iter [243/633], LR: 0.005000, Loss: 3.5618, top1: 15.6250\n",
      "Epoch [9/60], Iter [244/633], LR: 0.005000, Loss: 3.6072, top1: 10.9375\n",
      "Epoch [9/60], Iter [245/633], LR: 0.005000, Loss: 3.5575, top1: 18.7500\n",
      "Epoch [9/60], Iter [246/633], LR: 0.005000, Loss: 3.6052, top1: 9.3750\n",
      "Epoch [9/60], Iter [247/633], LR: 0.005000, Loss: 3.5889, top1: 12.5000\n",
      "Epoch [9/60], Iter [248/633], LR: 0.005000, Loss: 3.5569, top1: 15.6250\n",
      "Epoch [9/60], Iter [249/633], LR: 0.005000, Loss: 3.6431, top1: 7.8125\n",
      "Epoch [9/60], Iter [250/633], LR: 0.005000, Loss: 3.5925, top1: 14.0625\n",
      "Epoch [9/60], Iter [251/633], LR: 0.005000, Loss: 3.5827, top1: 12.5000\n",
      "Epoch [9/60], Iter [252/633], LR: 0.005000, Loss: 3.5411, top1: 14.0625\n",
      "Epoch [9/60], Iter [253/633], LR: 0.005000, Loss: 3.5958, top1: 7.8125\n",
      "Epoch [9/60], Iter [254/633], LR: 0.005000, Loss: 3.5633, top1: 17.1875\n",
      "Epoch [9/60], Iter [255/633], LR: 0.005000, Loss: 3.6130, top1: 12.5000\n",
      "Epoch [9/60], Iter [256/633], LR: 0.005000, Loss: 3.5842, top1: 14.0625\n",
      "Epoch [9/60], Iter [257/633], LR: 0.005000, Loss: 3.5460, top1: 17.1875\n",
      "Epoch [9/60], Iter [258/633], LR: 0.005000, Loss: 3.6481, top1: 10.9375\n",
      "Epoch [9/60], Iter [259/633], LR: 0.005000, Loss: 3.5395, top1: 18.7500\n",
      "Epoch [9/60], Iter [260/633], LR: 0.005000, Loss: 3.6091, top1: 9.3750\n",
      "Epoch [9/60], Iter [261/633], LR: 0.005000, Loss: 3.5560, top1: 15.6250\n",
      "Epoch [9/60], Iter [262/633], LR: 0.005000, Loss: 3.6160, top1: 12.5000\n",
      "Epoch [9/60], Iter [263/633], LR: 0.005000, Loss: 3.6262, top1: 14.0625\n",
      "Epoch [9/60], Iter [264/633], LR: 0.005000, Loss: 3.5501, top1: 15.6250\n",
      "Epoch [9/60], Iter [265/633], LR: 0.005000, Loss: 3.5744, top1: 17.1875\n",
      "Epoch [9/60], Iter [266/633], LR: 0.005000, Loss: 3.5822, top1: 12.5000\n",
      "Epoch [9/60], Iter [267/633], LR: 0.005000, Loss: 3.5990, top1: 9.3750\n",
      "Epoch [9/60], Iter [268/633], LR: 0.005000, Loss: 3.5892, top1: 10.9375\n",
      "Epoch [9/60], Iter [269/633], LR: 0.005000, Loss: 3.5628, top1: 20.3125\n",
      "Epoch [9/60], Iter [270/633], LR: 0.005000, Loss: 3.6310, top1: 10.9375\n",
      "Epoch [9/60], Iter [271/633], LR: 0.005000, Loss: 3.5238, top1: 21.8750\n",
      "Epoch [9/60], Iter [272/633], LR: 0.005000, Loss: 3.6121, top1: 10.9375\n",
      "Epoch [9/60], Iter [273/633], LR: 0.005000, Loss: 3.6173, top1: 7.8125\n",
      "Epoch [9/60], Iter [274/633], LR: 0.005000, Loss: 3.5607, top1: 15.6250\n",
      "Epoch [9/60], Iter [275/633], LR: 0.005000, Loss: 3.5519, top1: 17.1875\n",
      "Epoch [9/60], Iter [276/633], LR: 0.005000, Loss: 3.5993, top1: 14.0625\n",
      "Epoch [9/60], Iter [277/633], LR: 0.005000, Loss: 3.6408, top1: 6.2500\n",
      "Epoch [9/60], Iter [278/633], LR: 0.005000, Loss: 3.6126, top1: 9.3750\n",
      "Epoch [9/60], Iter [279/633], LR: 0.005000, Loss: 3.5898, top1: 15.6250\n",
      "Epoch [9/60], Iter [280/633], LR: 0.005000, Loss: 3.5874, top1: 10.9375\n",
      "Epoch [9/60], Iter [281/633], LR: 0.005000, Loss: 3.5923, top1: 10.9375\n",
      "Epoch [9/60], Iter [282/633], LR: 0.005000, Loss: 3.5182, top1: 26.5625\n",
      "Epoch [9/60], Iter [283/633], LR: 0.005000, Loss: 3.5796, top1: 10.9375\n",
      "Epoch [9/60], Iter [284/633], LR: 0.005000, Loss: 3.6083, top1: 10.9375\n",
      "Epoch [9/60], Iter [285/633], LR: 0.005000, Loss: 3.5559, top1: 15.6250\n",
      "Epoch [9/60], Iter [286/633], LR: 0.005000, Loss: 3.5999, top1: 10.9375\n",
      "Epoch [9/60], Iter [287/633], LR: 0.005000, Loss: 3.5898, top1: 12.5000\n",
      "Epoch [9/60], Iter [288/633], LR: 0.005000, Loss: 3.5641, top1: 10.9375\n",
      "Epoch [9/60], Iter [289/633], LR: 0.005000, Loss: 3.5613, top1: 12.5000\n",
      "Epoch [9/60], Iter [290/633], LR: 0.005000, Loss: 3.5809, top1: 6.2500\n",
      "Epoch [9/60], Iter [291/633], LR: 0.005000, Loss: 3.6016, top1: 9.3750\n",
      "Epoch [9/60], Iter [292/633], LR: 0.005000, Loss: 3.6232, top1: 9.3750\n",
      "Epoch [9/60], Iter [293/633], LR: 0.005000, Loss: 3.5728, top1: 12.5000\n",
      "Epoch [9/60], Iter [294/633], LR: 0.005000, Loss: 3.6169, top1: 7.8125\n",
      "Epoch [9/60], Iter [295/633], LR: 0.005000, Loss: 3.5849, top1: 14.0625\n",
      "Epoch [9/60], Iter [296/633], LR: 0.005000, Loss: 3.5435, top1: 17.1875\n",
      "Epoch [9/60], Iter [297/633], LR: 0.005000, Loss: 3.5954, top1: 14.0625\n",
      "Epoch [9/60], Iter [298/633], LR: 0.005000, Loss: 3.5644, top1: 15.6250\n",
      "Epoch [9/60], Iter [299/633], LR: 0.005000, Loss: 3.6170, top1: 10.9375\n",
      "Epoch [9/60], Iter [300/633], LR: 0.005000, Loss: 3.5512, top1: 21.8750\n",
      "Epoch [9/60], Iter [301/633], LR: 0.005000, Loss: 3.5953, top1: 9.3750\n",
      "Epoch [9/60], Iter [302/633], LR: 0.005000, Loss: 3.5677, top1: 14.0625\n",
      "Epoch [9/60], Iter [303/633], LR: 0.005000, Loss: 3.5528, top1: 21.8750\n",
      "Epoch [9/60], Iter [304/633], LR: 0.005000, Loss: 3.5522, top1: 20.3125\n",
      "Epoch [9/60], Iter [305/633], LR: 0.005000, Loss: 3.5759, top1: 14.0625\n",
      "Epoch [9/60], Iter [306/633], LR: 0.005000, Loss: 3.5893, top1: 17.1875\n",
      "Epoch [9/60], Iter [307/633], LR: 0.005000, Loss: 3.5836, top1: 10.9375\n",
      "Epoch [9/60], Iter [308/633], LR: 0.005000, Loss: 3.5771, top1: 12.5000\n",
      "Epoch [9/60], Iter [309/633], LR: 0.005000, Loss: 3.5401, top1: 17.1875\n",
      "Epoch [9/60], Iter [310/633], LR: 0.005000, Loss: 3.5677, top1: 14.0625\n",
      "Epoch [9/60], Iter [311/633], LR: 0.005000, Loss: 3.5976, top1: 10.9375\n",
      "Epoch [9/60], Iter [312/633], LR: 0.005000, Loss: 3.5961, top1: 15.6250\n",
      "Epoch [9/60], Iter [313/633], LR: 0.005000, Loss: 3.6010, top1: 10.9375\n",
      "Epoch [9/60], Iter [314/633], LR: 0.005000, Loss: 3.6256, top1: 7.8125\n",
      "Epoch [9/60], Iter [315/633], LR: 0.005000, Loss: 3.6149, top1: 12.5000\n",
      "Epoch [9/60], Iter [316/633], LR: 0.005000, Loss: 3.5836, top1: 15.6250\n",
      "Epoch [9/60], Iter [317/633], LR: 0.005000, Loss: 3.5743, top1: 10.9375\n",
      "Epoch [9/60], Iter [318/633], LR: 0.005000, Loss: 3.5676, top1: 17.1875\n",
      "Epoch [9/60], Iter [319/633], LR: 0.005000, Loss: 3.6143, top1: 4.6875\n",
      "Epoch [9/60], Iter [320/633], LR: 0.005000, Loss: 3.6019, top1: 10.9375\n",
      "Epoch [9/60], Iter [321/633], LR: 0.005000, Loss: 3.5790, top1: 20.3125\n",
      "Epoch [9/60], Iter [322/633], LR: 0.005000, Loss: 3.5812, top1: 12.5000\n",
      "Epoch [9/60], Iter [323/633], LR: 0.005000, Loss: 3.5527, top1: 15.6250\n",
      "Epoch [9/60], Iter [324/633], LR: 0.005000, Loss: 3.5500, top1: 12.5000\n",
      "Epoch [9/60], Iter [325/633], LR: 0.005000, Loss: 3.6107, top1: 15.6250\n",
      "Epoch [9/60], Iter [326/633], LR: 0.005000, Loss: 3.5920, top1: 10.9375\n",
      "Epoch [9/60], Iter [327/633], LR: 0.005000, Loss: 3.6218, top1: 14.0625\n",
      "Epoch [9/60], Iter [328/633], LR: 0.005000, Loss: 3.5355, top1: 18.7500\n",
      "Epoch [9/60], Iter [329/633], LR: 0.005000, Loss: 3.5895, top1: 17.1875\n",
      "Epoch [9/60], Iter [330/633], LR: 0.005000, Loss: 3.6339, top1: 7.8125\n",
      "Epoch [9/60], Iter [331/633], LR: 0.005000, Loss: 3.5791, top1: 12.5000\n",
      "Epoch [9/60], Iter [332/633], LR: 0.005000, Loss: 3.5734, top1: 18.7500\n",
      "Epoch [9/60], Iter [333/633], LR: 0.005000, Loss: 3.6008, top1: 14.0625\n",
      "Epoch [9/60], Iter [334/633], LR: 0.005000, Loss: 3.5911, top1: 17.1875\n",
      "Epoch [9/60], Iter [335/633], LR: 0.005000, Loss: 3.5574, top1: 18.7500\n",
      "Epoch [9/60], Iter [336/633], LR: 0.005000, Loss: 3.6182, top1: 12.5000\n",
      "Epoch [9/60], Iter [337/633], LR: 0.005000, Loss: 3.6602, top1: 6.2500\n",
      "Epoch [9/60], Iter [338/633], LR: 0.005000, Loss: 3.6201, top1: 7.8125\n",
      "Epoch [9/60], Iter [339/633], LR: 0.005000, Loss: 3.5434, top1: 17.1875\n",
      "Epoch [9/60], Iter [340/633], LR: 0.005000, Loss: 3.6434, top1: 14.0625\n",
      "Epoch [9/60], Iter [341/633], LR: 0.005000, Loss: 3.6023, top1: 15.6250\n",
      "Epoch [9/60], Iter [342/633], LR: 0.005000, Loss: 3.5951, top1: 14.0625\n",
      "Epoch [9/60], Iter [343/633], LR: 0.005000, Loss: 3.5632, top1: 10.9375\n",
      "Epoch [9/60], Iter [344/633], LR: 0.005000, Loss: 3.5871, top1: 9.3750\n",
      "Epoch [9/60], Iter [345/633], LR: 0.005000, Loss: 3.5978, top1: 10.9375\n",
      "Epoch [9/60], Iter [346/633], LR: 0.005000, Loss: 3.5954, top1: 9.3750\n",
      "Epoch [9/60], Iter [347/633], LR: 0.005000, Loss: 3.6229, top1: 7.8125\n",
      "Epoch [9/60], Iter [348/633], LR: 0.005000, Loss: 3.6057, top1: 7.8125\n",
      "Epoch [9/60], Iter [349/633], LR: 0.005000, Loss: 3.6101, top1: 7.8125\n",
      "Epoch [9/60], Iter [350/633], LR: 0.005000, Loss: 3.6013, top1: 10.9375\n",
      "Epoch [9/60], Iter [351/633], LR: 0.005000, Loss: 3.5637, top1: 18.7500\n",
      "Epoch [9/60], Iter [352/633], LR: 0.005000, Loss: 3.5868, top1: 12.5000\n",
      "Epoch [9/60], Iter [353/633], LR: 0.005000, Loss: 3.6368, top1: 10.9375\n",
      "Epoch [9/60], Iter [354/633], LR: 0.005000, Loss: 3.6174, top1: 7.8125\n",
      "Epoch [9/60], Iter [355/633], LR: 0.005000, Loss: 3.6272, top1: 9.3750\n",
      "Epoch [9/60], Iter [356/633], LR: 0.005000, Loss: 3.6223, top1: 12.5000\n",
      "Epoch [9/60], Iter [357/633], LR: 0.005000, Loss: 3.6275, top1: 7.8125\n",
      "Epoch [9/60], Iter [358/633], LR: 0.005000, Loss: 3.5403, top1: 21.8750\n",
      "Epoch [9/60], Iter [359/633], LR: 0.005000, Loss: 3.5686, top1: 9.3750\n",
      "Epoch [9/60], Iter [360/633], LR: 0.005000, Loss: 3.6197, top1: 10.9375\n",
      "Epoch [9/60], Iter [361/633], LR: 0.005000, Loss: 3.5817, top1: 18.7500\n",
      "Epoch [9/60], Iter [362/633], LR: 0.005000, Loss: 3.5837, top1: 17.1875\n",
      "Epoch [9/60], Iter [363/633], LR: 0.005000, Loss: 3.5892, top1: 12.5000\n",
      "Epoch [9/60], Iter [364/633], LR: 0.005000, Loss: 3.5898, top1: 14.0625\n",
      "Epoch [9/60], Iter [365/633], LR: 0.005000, Loss: 3.5995, top1: 9.3750\n",
      "Epoch [9/60], Iter [366/633], LR: 0.005000, Loss: 3.5771, top1: 15.6250\n",
      "Epoch [9/60], Iter [367/633], LR: 0.005000, Loss: 3.5677, top1: 12.5000\n",
      "Epoch [9/60], Iter [368/633], LR: 0.005000, Loss: 3.5939, top1: 7.8125\n",
      "Epoch [9/60], Iter [369/633], LR: 0.005000, Loss: 3.5827, top1: 17.1875\n",
      "Epoch [9/60], Iter [370/633], LR: 0.005000, Loss: 3.6289, top1: 10.9375\n",
      "Epoch [9/60], Iter [371/633], LR: 0.005000, Loss: 3.6104, top1: 12.5000\n",
      "Epoch [9/60], Iter [372/633], LR: 0.005000, Loss: 3.5704, top1: 15.6250\n",
      "Epoch [9/60], Iter [373/633], LR: 0.005000, Loss: 3.5934, top1: 14.0625\n",
      "Epoch [9/60], Iter [374/633], LR: 0.005000, Loss: 3.6116, top1: 10.9375\n",
      "Epoch [9/60], Iter [375/633], LR: 0.005000, Loss: 3.6122, top1: 10.9375\n",
      "Epoch [9/60], Iter [376/633], LR: 0.005000, Loss: 3.5917, top1: 18.7500\n",
      "Epoch [9/60], Iter [377/633], LR: 0.005000, Loss: 3.6013, top1: 15.6250\n",
      "Epoch [9/60], Iter [378/633], LR: 0.005000, Loss: 3.6115, top1: 12.5000\n",
      "Epoch [9/60], Iter [379/633], LR: 0.005000, Loss: 3.5810, top1: 17.1875\n",
      "Epoch [9/60], Iter [380/633], LR: 0.005000, Loss: 3.6232, top1: 7.8125\n",
      "Epoch [9/60], Iter [381/633], LR: 0.005000, Loss: 3.5644, top1: 15.6250\n",
      "Epoch [9/60], Iter [382/633], LR: 0.005000, Loss: 3.5960, top1: 7.8125\n",
      "Epoch [9/60], Iter [383/633], LR: 0.005000, Loss: 3.6135, top1: 7.8125\n",
      "Epoch [9/60], Iter [384/633], LR: 0.005000, Loss: 3.5468, top1: 12.5000\n",
      "Epoch [9/60], Iter [385/633], LR: 0.005000, Loss: 3.5989, top1: 14.0625\n",
      "Epoch [9/60], Iter [386/633], LR: 0.005000, Loss: 3.5833, top1: 18.7500\n",
      "Epoch [9/60], Iter [387/633], LR: 0.005000, Loss: 3.5473, top1: 21.8750\n",
      "Epoch [9/60], Iter [388/633], LR: 0.005000, Loss: 3.5982, top1: 12.5000\n",
      "Epoch [9/60], Iter [389/633], LR: 0.005000, Loss: 3.5563, top1: 15.6250\n",
      "Epoch [9/60], Iter [390/633], LR: 0.005000, Loss: 3.6136, top1: 10.9375\n",
      "Epoch [9/60], Iter [391/633], LR: 0.005000, Loss: 3.6132, top1: 10.9375\n",
      "Epoch [9/60], Iter [392/633], LR: 0.005000, Loss: 3.6194, top1: 9.3750\n",
      "Epoch [9/60], Iter [393/633], LR: 0.005000, Loss: 3.6090, top1: 10.9375\n",
      "Epoch [9/60], Iter [394/633], LR: 0.005000, Loss: 3.6004, top1: 9.3750\n",
      "Epoch [9/60], Iter [395/633], LR: 0.005000, Loss: 3.6202, top1: 3.1250\n",
      "Epoch [9/60], Iter [396/633], LR: 0.005000, Loss: 3.5803, top1: 9.3750\n",
      "Epoch [9/60], Iter [397/633], LR: 0.005000, Loss: 3.6408, top1: 4.6875\n",
      "Epoch [9/60], Iter [398/633], LR: 0.005000, Loss: 3.5733, top1: 15.6250\n",
      "Epoch [9/60], Iter [399/633], LR: 0.005000, Loss: 3.5746, top1: 18.7500\n",
      "Epoch [9/60], Iter [400/633], LR: 0.005000, Loss: 3.5599, top1: 18.7500\n",
      "Epoch [9/60], Iter [401/633], LR: 0.005000, Loss: 3.5920, top1: 12.5000\n",
      "Epoch [9/60], Iter [402/633], LR: 0.005000, Loss: 3.5238, top1: 18.7500\n",
      "Epoch [9/60], Iter [403/633], LR: 0.005000, Loss: 3.5950, top1: 12.5000\n",
      "Epoch [9/60], Iter [404/633], LR: 0.005000, Loss: 3.5627, top1: 14.0625\n",
      "Epoch [9/60], Iter [405/633], LR: 0.005000, Loss: 3.5743, top1: 12.5000\n",
      "Epoch [9/60], Iter [406/633], LR: 0.005000, Loss: 3.5374, top1: 17.1875\n",
      "Epoch [9/60], Iter [407/633], LR: 0.005000, Loss: 3.5673, top1: 20.3125\n",
      "Epoch [9/60], Iter [408/633], LR: 0.005000, Loss: 3.5824, top1: 9.3750\n",
      "Epoch [9/60], Iter [409/633], LR: 0.005000, Loss: 3.5647, top1: 18.7500\n",
      "Epoch [9/60], Iter [410/633], LR: 0.005000, Loss: 3.5863, top1: 12.5000\n",
      "Epoch [9/60], Iter [411/633], LR: 0.005000, Loss: 3.5615, top1: 15.6250\n",
      "Epoch [9/60], Iter [412/633], LR: 0.005000, Loss: 3.6441, top1: 6.2500\n",
      "Epoch [9/60], Iter [413/633], LR: 0.005000, Loss: 3.6028, top1: 7.8125\n",
      "Epoch [9/60], Iter [414/633], LR: 0.005000, Loss: 3.5447, top1: 15.6250\n",
      "Epoch [9/60], Iter [415/633], LR: 0.005000, Loss: 3.5875, top1: 18.7500\n",
      "Epoch [9/60], Iter [416/633], LR: 0.005000, Loss: 3.6084, top1: 6.2500\n",
      "Epoch [9/60], Iter [417/633], LR: 0.005000, Loss: 3.5897, top1: 14.0625\n",
      "Epoch [9/60], Iter [418/633], LR: 0.005000, Loss: 3.5926, top1: 10.9375\n",
      "Epoch [9/60], Iter [419/633], LR: 0.005000, Loss: 3.5882, top1: 7.8125\n",
      "Epoch [9/60], Iter [420/633], LR: 0.005000, Loss: 3.5794, top1: 17.1875\n",
      "Epoch [9/60], Iter [421/633], LR: 0.005000, Loss: 3.5906, top1: 9.3750\n",
      "Epoch [9/60], Iter [422/633], LR: 0.005000, Loss: 3.6005, top1: 9.3750\n",
      "Epoch [9/60], Iter [423/633], LR: 0.005000, Loss: 3.6086, top1: 10.9375\n",
      "Epoch [9/60], Iter [424/633], LR: 0.005000, Loss: 3.5733, top1: 18.7500\n",
      "Epoch [9/60], Iter [425/633], LR: 0.005000, Loss: 3.6162, top1: 4.6875\n",
      "Epoch [9/60], Iter [426/633], LR: 0.005000, Loss: 3.5937, top1: 9.3750\n",
      "Epoch [9/60], Iter [427/633], LR: 0.005000, Loss: 3.5617, top1: 15.6250\n",
      "Epoch [9/60], Iter [428/633], LR: 0.005000, Loss: 3.6731, top1: 9.3750\n",
      "Epoch [9/60], Iter [429/633], LR: 0.005000, Loss: 3.6028, top1: 12.5000\n",
      "Epoch [9/60], Iter [430/633], LR: 0.005000, Loss: 3.5991, top1: 12.5000\n",
      "Epoch [9/60], Iter [431/633], LR: 0.005000, Loss: 3.5572, top1: 17.1875\n",
      "Epoch [9/60], Iter [432/633], LR: 0.005000, Loss: 3.5483, top1: 12.5000\n",
      "Epoch [9/60], Iter [433/633], LR: 0.005000, Loss: 3.5624, top1: 18.7500\n",
      "Epoch [9/60], Iter [434/633], LR: 0.005000, Loss: 3.5906, top1: 10.9375\n",
      "Epoch [9/60], Iter [435/633], LR: 0.005000, Loss: 3.6222, top1: 9.3750\n",
      "Epoch [9/60], Iter [436/633], LR: 0.005000, Loss: 3.5932, top1: 9.3750\n",
      "Epoch [9/60], Iter [437/633], LR: 0.005000, Loss: 3.5857, top1: 12.5000\n",
      "Epoch [9/60], Iter [438/633], LR: 0.005000, Loss: 3.5344, top1: 18.7500\n",
      "Epoch [9/60], Iter [439/633], LR: 0.005000, Loss: 3.5547, top1: 17.1875\n",
      "Epoch [9/60], Iter [440/633], LR: 0.005000, Loss: 3.5997, top1: 9.3750\n",
      "Epoch [9/60], Iter [441/633], LR: 0.005000, Loss: 3.5565, top1: 12.5000\n",
      "Epoch [9/60], Iter [442/633], LR: 0.005000, Loss: 3.5804, top1: 14.0625\n",
      "Epoch [9/60], Iter [443/633], LR: 0.005000, Loss: 3.5706, top1: 20.3125\n",
      "Epoch [9/60], Iter [444/633], LR: 0.005000, Loss: 3.5836, top1: 18.7500\n",
      "Epoch [9/60], Iter [445/633], LR: 0.005000, Loss: 3.6082, top1: 14.0625\n",
      "Epoch [9/60], Iter [446/633], LR: 0.005000, Loss: 3.6175, top1: 14.0625\n",
      "Epoch [9/60], Iter [447/633], LR: 0.005000, Loss: 3.5877, top1: 12.5000\n",
      "Epoch [9/60], Iter [448/633], LR: 0.005000, Loss: 3.6275, top1: 12.5000\n",
      "Epoch [9/60], Iter [449/633], LR: 0.005000, Loss: 3.6592, top1: 6.2500\n",
      "Epoch [9/60], Iter [450/633], LR: 0.005000, Loss: 3.6078, top1: 12.5000\n",
      "Epoch [9/60], Iter [451/633], LR: 0.005000, Loss: 3.6047, top1: 9.3750\n",
      "Epoch [9/60], Iter [452/633], LR: 0.005000, Loss: 3.5966, top1: 9.3750\n",
      "Epoch [9/60], Iter [453/633], LR: 0.005000, Loss: 3.5885, top1: 12.5000\n",
      "Epoch [9/60], Iter [454/633], LR: 0.005000, Loss: 3.6203, top1: 10.9375\n",
      "Epoch [9/60], Iter [455/633], LR: 0.005000, Loss: 3.5946, top1: 10.9375\n",
      "Epoch [9/60], Iter [456/633], LR: 0.005000, Loss: 3.5983, top1: 12.5000\n",
      "Epoch [9/60], Iter [457/633], LR: 0.005000, Loss: 3.6210, top1: 7.8125\n",
      "Epoch [9/60], Iter [458/633], LR: 0.005000, Loss: 3.5482, top1: 15.6250\n",
      "Epoch [9/60], Iter [459/633], LR: 0.005000, Loss: 3.6088, top1: 12.5000\n",
      "Epoch [9/60], Iter [460/633], LR: 0.005000, Loss: 3.5771, top1: 14.0625\n",
      "Epoch [9/60], Iter [461/633], LR: 0.005000, Loss: 3.6046, top1: 6.2500\n",
      "Epoch [9/60], Iter [462/633], LR: 0.005000, Loss: 3.6111, top1: 12.5000\n",
      "Epoch [9/60], Iter [463/633], LR: 0.005000, Loss: 3.6217, top1: 10.9375\n",
      "Epoch [9/60], Iter [464/633], LR: 0.005000, Loss: 3.5898, top1: 9.3750\n",
      "Epoch [9/60], Iter [465/633], LR: 0.005000, Loss: 3.5294, top1: 20.3125\n",
      "Epoch [9/60], Iter [466/633], LR: 0.005000, Loss: 3.6031, top1: 14.0625\n",
      "Epoch [9/60], Iter [467/633], LR: 0.005000, Loss: 3.5886, top1: 12.5000\n",
      "Epoch [9/60], Iter [468/633], LR: 0.005000, Loss: 3.5932, top1: 12.5000\n",
      "Epoch [9/60], Iter [469/633], LR: 0.005000, Loss: 3.5759, top1: 18.7500\n",
      "Epoch [9/60], Iter [470/633], LR: 0.005000, Loss: 3.6060, top1: 14.0625\n",
      "Epoch [9/60], Iter [471/633], LR: 0.005000, Loss: 3.6116, top1: 9.3750\n",
      "Epoch [9/60], Iter [472/633], LR: 0.005000, Loss: 3.6061, top1: 10.9375\n",
      "Epoch [9/60], Iter [473/633], LR: 0.005000, Loss: 3.5984, top1: 10.9375\n",
      "Epoch [9/60], Iter [474/633], LR: 0.005000, Loss: 3.5674, top1: 17.1875\n",
      "Epoch [9/60], Iter [475/633], LR: 0.005000, Loss: 3.5604, top1: 18.7500\n",
      "Epoch [9/60], Iter [476/633], LR: 0.005000, Loss: 3.6214, top1: 7.8125\n",
      "Epoch [9/60], Iter [477/633], LR: 0.005000, Loss: 3.6110, top1: 7.8125\n",
      "Epoch [9/60], Iter [478/633], LR: 0.005000, Loss: 3.6060, top1: 12.5000\n",
      "Epoch [9/60], Iter [479/633], LR: 0.005000, Loss: 3.5827, top1: 15.6250\n",
      "Epoch [9/60], Iter [480/633], LR: 0.005000, Loss: 3.5601, top1: 17.1875\n",
      "Epoch [9/60], Iter [481/633], LR: 0.005000, Loss: 3.4903, top1: 28.1250\n",
      "Epoch [9/60], Iter [482/633], LR: 0.005000, Loss: 3.6230, top1: 7.8125\n",
      "Epoch [9/60], Iter [483/633], LR: 0.005000, Loss: 3.5638, top1: 17.1875\n",
      "Epoch [9/60], Iter [484/633], LR: 0.005000, Loss: 3.5688, top1: 10.9375\n",
      "Epoch [9/60], Iter [485/633], LR: 0.005000, Loss: 3.5424, top1: 17.1875\n",
      "Epoch [9/60], Iter [486/633], LR: 0.005000, Loss: 3.6108, top1: 6.2500\n",
      "Epoch [9/60], Iter [487/633], LR: 0.005000, Loss: 3.5841, top1: 15.6250\n",
      "Epoch [9/60], Iter [488/633], LR: 0.005000, Loss: 3.5745, top1: 12.5000\n",
      "Epoch [9/60], Iter [489/633], LR: 0.005000, Loss: 3.6167, top1: 7.8125\n",
      "Epoch [9/60], Iter [490/633], LR: 0.005000, Loss: 3.5816, top1: 12.5000\n",
      "Epoch [9/60], Iter [491/633], LR: 0.005000, Loss: 3.5493, top1: 17.1875\n",
      "Epoch [9/60], Iter [492/633], LR: 0.005000, Loss: 3.5726, top1: 17.1875\n",
      "Epoch [9/60], Iter [493/633], LR: 0.005000, Loss: 3.6090, top1: 10.9375\n",
      "Epoch [9/60], Iter [494/633], LR: 0.005000, Loss: 3.6009, top1: 12.5000\n",
      "Epoch [9/60], Iter [495/633], LR: 0.005000, Loss: 3.6010, top1: 14.0625\n",
      "Epoch [9/60], Iter [496/633], LR: 0.005000, Loss: 3.5690, top1: 18.7500\n",
      "Epoch [9/60], Iter [497/633], LR: 0.005000, Loss: 3.5942, top1: 12.5000\n",
      "Epoch [9/60], Iter [498/633], LR: 0.005000, Loss: 3.5995, top1: 15.6250\n",
      "Epoch [9/60], Iter [499/633], LR: 0.005000, Loss: 3.6527, top1: 4.6875\n",
      "Epoch [9/60], Iter [500/633], LR: 0.005000, Loss: 3.5847, top1: 7.8125\n",
      "Epoch [9/60], Iter [501/633], LR: 0.005000, Loss: 3.5784, top1: 17.1875\n",
      "Epoch [9/60], Iter [502/633], LR: 0.005000, Loss: 3.5658, top1: 17.1875\n",
      "Epoch [9/60], Iter [503/633], LR: 0.005000, Loss: 3.5936, top1: 9.3750\n",
      "Epoch [9/60], Iter [504/633], LR: 0.005000, Loss: 3.6250, top1: 12.5000\n",
      "Epoch [9/60], Iter [505/633], LR: 0.005000, Loss: 3.6340, top1: 9.3750\n",
      "Epoch [9/60], Iter [506/633], LR: 0.005000, Loss: 3.5644, top1: 17.1875\n",
      "Epoch [9/60], Iter [507/633], LR: 0.005000, Loss: 3.5861, top1: 12.5000\n",
      "Epoch [9/60], Iter [508/633], LR: 0.005000, Loss: 3.5801, top1: 12.5000\n",
      "Epoch [9/60], Iter [509/633], LR: 0.005000, Loss: 3.5987, top1: 10.9375\n",
      "Epoch [9/60], Iter [510/633], LR: 0.005000, Loss: 3.6199, top1: 14.0625\n",
      "Epoch [9/60], Iter [511/633], LR: 0.005000, Loss: 3.6115, top1: 14.0625\n",
      "Epoch [9/60], Iter [512/633], LR: 0.005000, Loss: 3.5959, top1: 4.6875\n",
      "Epoch [9/60], Iter [513/633], LR: 0.005000, Loss: 3.5484, top1: 20.3125\n",
      "Epoch [9/60], Iter [514/633], LR: 0.005000, Loss: 3.6117, top1: 12.5000\n",
      "Epoch [9/60], Iter [515/633], LR: 0.005000, Loss: 3.5561, top1: 17.1875\n",
      "Epoch [9/60], Iter [516/633], LR: 0.005000, Loss: 3.5729, top1: 18.7500\n",
      "Epoch [9/60], Iter [517/633], LR: 0.005000, Loss: 3.5686, top1: 9.3750\n",
      "Epoch [9/60], Iter [518/633], LR: 0.005000, Loss: 3.5582, top1: 20.3125\n",
      "Epoch [9/60], Iter [519/633], LR: 0.005000, Loss: 3.6381, top1: 6.2500\n",
      "Epoch [9/60], Iter [520/633], LR: 0.005000, Loss: 3.6151, top1: 9.3750\n",
      "Epoch [9/60], Iter [521/633], LR: 0.005000, Loss: 3.5652, top1: 12.5000\n",
      "Epoch [9/60], Iter [522/633], LR: 0.005000, Loss: 3.5635, top1: 12.5000\n",
      "Epoch [9/60], Iter [523/633], LR: 0.005000, Loss: 3.5331, top1: 20.3125\n",
      "Epoch [9/60], Iter [524/633], LR: 0.005000, Loss: 3.5408, top1: 26.5625\n",
      "Epoch [9/60], Iter [525/633], LR: 0.005000, Loss: 3.6224, top1: 15.6250\n",
      "Epoch [9/60], Iter [526/633], LR: 0.005000, Loss: 3.5934, top1: 15.6250\n",
      "Epoch [9/60], Iter [527/633], LR: 0.005000, Loss: 3.5538, top1: 14.0625\n",
      "Epoch [9/60], Iter [528/633], LR: 0.005000, Loss: 3.5562, top1: 20.3125\n",
      "Epoch [9/60], Iter [529/633], LR: 0.005000, Loss: 3.5965, top1: 15.6250\n",
      "Epoch [9/60], Iter [530/633], LR: 0.005000, Loss: 3.5799, top1: 12.5000\n",
      "Epoch [9/60], Iter [531/633], LR: 0.005000, Loss: 3.5836, top1: 12.5000\n",
      "Epoch [9/60], Iter [532/633], LR: 0.005000, Loss: 3.6111, top1: 15.6250\n",
      "Epoch [9/60], Iter [533/633], LR: 0.005000, Loss: 3.6006, top1: 9.3750\n",
      "Epoch [9/60], Iter [534/633], LR: 0.005000, Loss: 3.5935, top1: 9.3750\n",
      "Epoch [9/60], Iter [535/633], LR: 0.005000, Loss: 3.6044, top1: 9.3750\n",
      "Epoch [9/60], Iter [536/633], LR: 0.005000, Loss: 3.6105, top1: 10.9375\n",
      "Epoch [9/60], Iter [537/633], LR: 0.005000, Loss: 3.5334, top1: 25.0000\n",
      "Epoch [9/60], Iter [538/633], LR: 0.005000, Loss: 3.5680, top1: 14.0625\n",
      "Epoch [9/60], Iter [539/633], LR: 0.005000, Loss: 3.6033, top1: 4.6875\n",
      "Epoch [9/60], Iter [540/633], LR: 0.005000, Loss: 3.6515, top1: 6.2500\n",
      "Epoch [9/60], Iter [541/633], LR: 0.005000, Loss: 3.6254, top1: 6.2500\n",
      "Epoch [9/60], Iter [542/633], LR: 0.005000, Loss: 3.5571, top1: 14.0625\n",
      "Epoch [9/60], Iter [543/633], LR: 0.005000, Loss: 3.5565, top1: 21.8750\n",
      "Epoch [9/60], Iter [544/633], LR: 0.005000, Loss: 3.5726, top1: 20.3125\n",
      "Epoch [9/60], Iter [545/633], LR: 0.005000, Loss: 3.5513, top1: 17.1875\n",
      "Epoch [9/60], Iter [546/633], LR: 0.005000, Loss: 3.6033, top1: 10.9375\n",
      "Epoch [9/60], Iter [547/633], LR: 0.005000, Loss: 3.5276, top1: 20.3125\n",
      "Epoch [9/60], Iter [548/633], LR: 0.005000, Loss: 3.5939, top1: 9.3750\n",
      "Epoch [9/60], Iter [549/633], LR: 0.005000, Loss: 3.5674, top1: 14.0625\n",
      "Epoch [9/60], Iter [550/633], LR: 0.005000, Loss: 3.5705, top1: 12.5000\n",
      "Epoch [9/60], Iter [551/633], LR: 0.005000, Loss: 3.6025, top1: 12.5000\n",
      "Epoch [9/60], Iter [552/633], LR: 0.005000, Loss: 3.6559, top1: 6.2500\n",
      "Epoch [9/60], Iter [553/633], LR: 0.005000, Loss: 3.5966, top1: 15.6250\n",
      "Epoch [9/60], Iter [554/633], LR: 0.005000, Loss: 3.5720, top1: 15.6250\n",
      "Epoch [9/60], Iter [555/633], LR: 0.005000, Loss: 3.6040, top1: 4.6875\n",
      "Epoch [9/60], Iter [556/633], LR: 0.005000, Loss: 3.5239, top1: 23.4375\n",
      "Epoch [9/60], Iter [557/633], LR: 0.005000, Loss: 3.5708, top1: 10.9375\n",
      "Epoch [9/60], Iter [558/633], LR: 0.005000, Loss: 3.5727, top1: 10.9375\n",
      "Epoch [9/60], Iter [559/633], LR: 0.005000, Loss: 3.6117, top1: 12.5000\n",
      "Epoch [9/60], Iter [560/633], LR: 0.005000, Loss: 3.5894, top1: 9.3750\n",
      "Epoch [9/60], Iter [561/633], LR: 0.005000, Loss: 3.5871, top1: 10.9375\n",
      "Epoch [9/60], Iter [562/633], LR: 0.005000, Loss: 3.6192, top1: 10.9375\n",
      "Epoch [9/60], Iter [563/633], LR: 0.005000, Loss: 3.5862, top1: 12.5000\n",
      "Epoch [9/60], Iter [564/633], LR: 0.005000, Loss: 3.5745, top1: 15.6250\n",
      "Epoch [9/60], Iter [565/633], LR: 0.005000, Loss: 3.6082, top1: 4.6875\n",
      "Epoch [9/60], Iter [566/633], LR: 0.005000, Loss: 3.5977, top1: 12.5000\n",
      "Epoch [9/60], Iter [567/633], LR: 0.005000, Loss: 3.6311, top1: 14.0625\n",
      "Epoch [9/60], Iter [568/633], LR: 0.005000, Loss: 3.6291, top1: 9.3750\n",
      "Epoch [9/60], Iter [569/633], LR: 0.005000, Loss: 3.5742, top1: 14.0625\n",
      "Epoch [9/60], Iter [570/633], LR: 0.005000, Loss: 3.6227, top1: 12.5000\n",
      "Epoch [9/60], Iter [571/633], LR: 0.005000, Loss: 3.6461, top1: 9.3750\n",
      "Epoch [9/60], Iter [572/633], LR: 0.005000, Loss: 3.6506, top1: 3.1250\n",
      "Epoch [9/60], Iter [573/633], LR: 0.005000, Loss: 3.5849, top1: 18.7500\n",
      "Epoch [9/60], Iter [574/633], LR: 0.005000, Loss: 3.6289, top1: 7.8125\n",
      "Epoch [9/60], Iter [575/633], LR: 0.005000, Loss: 3.5878, top1: 15.6250\n",
      "Epoch [9/60], Iter [576/633], LR: 0.005000, Loss: 3.5891, top1: 12.5000\n",
      "Epoch [9/60], Iter [577/633], LR: 0.005000, Loss: 3.6115, top1: 12.5000\n",
      "Epoch [9/60], Iter [578/633], LR: 0.005000, Loss: 3.6032, top1: 12.5000\n",
      "Epoch [9/60], Iter [579/633], LR: 0.005000, Loss: 3.5631, top1: 12.5000\n",
      "Epoch [9/60], Iter [580/633], LR: 0.005000, Loss: 3.5753, top1: 15.6250\n",
      "Epoch [9/60], Iter [581/633], LR: 0.005000, Loss: 3.5921, top1: 14.0625\n",
      "Epoch [9/60], Iter [582/633], LR: 0.005000, Loss: 3.5903, top1: 12.5000\n",
      "Epoch [9/60], Iter [583/633], LR: 0.005000, Loss: 3.5654, top1: 15.6250\n",
      "Epoch [9/60], Iter [584/633], LR: 0.005000, Loss: 3.5823, top1: 17.1875\n",
      "Epoch [9/60], Iter [585/633], LR: 0.005000, Loss: 3.5405, top1: 18.7500\n",
      "Epoch [9/60], Iter [586/633], LR: 0.005000, Loss: 3.5573, top1: 18.7500\n",
      "Epoch [9/60], Iter [587/633], LR: 0.005000, Loss: 3.5206, top1: 23.4375\n",
      "Epoch [9/60], Iter [588/633], LR: 0.005000, Loss: 3.5983, top1: 15.6250\n",
      "Epoch [9/60], Iter [589/633], LR: 0.005000, Loss: 3.5439, top1: 20.3125\n",
      "Epoch [9/60], Iter [590/633], LR: 0.005000, Loss: 3.5853, top1: 12.5000\n",
      "Epoch [9/60], Iter [591/633], LR: 0.005000, Loss: 3.5860, top1: 12.5000\n",
      "Epoch [9/60], Iter [592/633], LR: 0.005000, Loss: 3.5542, top1: 15.6250\n",
      "Epoch [9/60], Iter [593/633], LR: 0.005000, Loss: 3.6098, top1: 6.2500\n",
      "Epoch [9/60], Iter [594/633], LR: 0.005000, Loss: 3.6279, top1: 10.9375\n",
      "Epoch [9/60], Iter [595/633], LR: 0.005000, Loss: 3.6020, top1: 14.0625\n",
      "Epoch [9/60], Iter [596/633], LR: 0.005000, Loss: 3.6235, top1: 9.3750\n",
      "Epoch [9/60], Iter [597/633], LR: 0.005000, Loss: 3.5926, top1: 14.0625\n",
      "Epoch [9/60], Iter [598/633], LR: 0.005000, Loss: 3.5902, top1: 20.3125\n",
      "Epoch [9/60], Iter [599/633], LR: 0.005000, Loss: 3.5692, top1: 15.6250\n",
      "Epoch [9/60], Iter [600/633], LR: 0.005000, Loss: 3.6306, top1: 12.5000\n",
      "Epoch [9/60], Iter [601/633], LR: 0.005000, Loss: 3.5561, top1: 15.6250\n",
      "Epoch [9/60], Iter [602/633], LR: 0.005000, Loss: 3.6214, top1: 7.8125\n",
      "Epoch [9/60], Iter [603/633], LR: 0.005000, Loss: 3.5605, top1: 14.0625\n",
      "Epoch [9/60], Iter [604/633], LR: 0.005000, Loss: 3.5913, top1: 14.0625\n",
      "Epoch [9/60], Iter [605/633], LR: 0.005000, Loss: 3.6175, top1: 14.0625\n",
      "Epoch [9/60], Iter [606/633], LR: 0.005000, Loss: 3.5739, top1: 17.1875\n",
      "Epoch [9/60], Iter [607/633], LR: 0.005000, Loss: 3.5751, top1: 17.1875\n",
      "Epoch [9/60], Iter [608/633], LR: 0.005000, Loss: 3.6355, top1: 7.8125\n",
      "Epoch [9/60], Iter [609/633], LR: 0.005000, Loss: 3.5908, top1: 10.9375\n",
      "Epoch [9/60], Iter [610/633], LR: 0.005000, Loss: 3.5387, top1: 15.6250\n",
      "Epoch [9/60], Iter [611/633], LR: 0.005000, Loss: 3.5377, top1: 17.1875\n",
      "Epoch [9/60], Iter [612/633], LR: 0.005000, Loss: 3.5958, top1: 15.6250\n",
      "Epoch [9/60], Iter [613/633], LR: 0.005000, Loss: 3.6196, top1: 10.9375\n",
      "Epoch [9/60], Iter [614/633], LR: 0.005000, Loss: 3.6043, top1: 14.0625\n",
      "Epoch [9/60], Iter [615/633], LR: 0.005000, Loss: 3.5846, top1: 10.9375\n",
      "Epoch [9/60], Iter [616/633], LR: 0.005000, Loss: 3.5698, top1: 17.1875\n",
      "Epoch [9/60], Iter [617/633], LR: 0.005000, Loss: 3.5786, top1: 18.7500\n",
      "Epoch [9/60], Iter [618/633], LR: 0.005000, Loss: 3.5686, top1: 17.1875\n",
      "Epoch [9/60], Iter [619/633], LR: 0.005000, Loss: 3.5895, top1: 10.9375\n",
      "Epoch [9/60], Iter [620/633], LR: 0.005000, Loss: 3.6068, top1: 9.3750\n",
      "Epoch [9/60], Iter [621/633], LR: 0.005000, Loss: 3.6200, top1: 9.3750\n",
      "Epoch [9/60], Iter [622/633], LR: 0.005000, Loss: 3.5379, top1: 18.7500\n",
      "Epoch [9/60], Iter [623/633], LR: 0.005000, Loss: 3.6285, top1: 14.0625\n",
      "Epoch [9/60], Iter [624/633], LR: 0.005000, Loss: 3.5778, top1: 15.6250\n",
      "Epoch [9/60], Iter [625/633], LR: 0.005000, Loss: 3.6122, top1: 9.3750\n",
      "Epoch [9/60], Iter [626/633], LR: 0.005000, Loss: 3.6051, top1: 10.9375\n",
      "Epoch [9/60], Iter [627/633], LR: 0.005000, Loss: 3.5526, top1: 12.5000\n",
      "Epoch [9/60], Iter [628/633], LR: 0.005000, Loss: 3.5937, top1: 15.6250\n",
      "Epoch [9/60], Iter [629/633], LR: 0.005000, Loss: 3.5959, top1: 12.5000\n",
      "Epoch [9/60], Iter [630/633], LR: 0.005000, Loss: 3.5914, top1: 14.0625\n",
      "Epoch [9/60], Iter [631/633], LR: 0.005000, Loss: 3.6021, top1: 9.3750\n",
      "Epoch [9/60], Iter [632/633], LR: 0.005000, Loss: 3.5992, top1: 12.5000\n",
      "Epoch [9/60], Iter [633/633], LR: 0.005000, Loss: 3.6018, top1: 12.5000\n",
      "Epoch [9/60], Iter [634/633], LR: 0.005000, Loss: 3.5508, top1: 16.1290\n",
      "Epoch [9/60], Val_Loss: 3.5737, Val_top1: 14.6567, best_top1: 13.2702\n",
      "epoch time: 4.44250918229421 min\n",
      "Taking top1 snapshot...\n",
      "Epoch [10/60], Iter [1/633], LR: 0.005000, Loss: 3.5837, top1: 9.3750\n",
      "Epoch [10/60], Iter [2/633], LR: 0.005000, Loss: 3.5555, top1: 21.8750\n",
      "Epoch [10/60], Iter [3/633], LR: 0.005000, Loss: 3.6042, top1: 7.8125\n",
      "Epoch [10/60], Iter [4/633], LR: 0.005000, Loss: 3.6022, top1: 10.9375\n",
      "Epoch [10/60], Iter [5/633], LR: 0.005000, Loss: 3.5849, top1: 12.5000\n",
      "Epoch [10/60], Iter [6/633], LR: 0.005000, Loss: 3.6237, top1: 14.0625\n",
      "Epoch [10/60], Iter [7/633], LR: 0.005000, Loss: 3.6053, top1: 7.8125\n",
      "Epoch [10/60], Iter [8/633], LR: 0.005000, Loss: 3.5989, top1: 14.0625\n",
      "Epoch [10/60], Iter [9/633], LR: 0.005000, Loss: 3.6103, top1: 10.9375\n",
      "Epoch [10/60], Iter [10/633], LR: 0.005000, Loss: 3.6795, top1: 7.8125\n",
      "Epoch [10/60], Iter [11/633], LR: 0.005000, Loss: 3.5957, top1: 12.5000\n",
      "Epoch [10/60], Iter [12/633], LR: 0.005000, Loss: 3.5872, top1: 10.9375\n",
      "Epoch [10/60], Iter [13/633], LR: 0.005000, Loss: 3.5939, top1: 10.9375\n",
      "Epoch [10/60], Iter [14/633], LR: 0.005000, Loss: 3.6152, top1: 7.8125\n",
      "Epoch [10/60], Iter [15/633], LR: 0.005000, Loss: 3.5703, top1: 17.1875\n",
      "Epoch [10/60], Iter [16/633], LR: 0.005000, Loss: 3.6082, top1: 10.9375\n",
      "Epoch [10/60], Iter [17/633], LR: 0.005000, Loss: 3.5846, top1: 15.6250\n",
      "Epoch [10/60], Iter [18/633], LR: 0.005000, Loss: 3.5692, top1: 17.1875\n",
      "Epoch [10/60], Iter [19/633], LR: 0.005000, Loss: 3.5867, top1: 12.5000\n",
      "Epoch [10/60], Iter [20/633], LR: 0.005000, Loss: 3.5246, top1: 25.0000\n",
      "Epoch [10/60], Iter [21/633], LR: 0.005000, Loss: 3.5933, top1: 10.9375\n",
      "Epoch [10/60], Iter [22/633], LR: 0.005000, Loss: 3.6162, top1: 12.5000\n",
      "Epoch [10/60], Iter [23/633], LR: 0.005000, Loss: 3.5717, top1: 14.0625\n",
      "Epoch [10/60], Iter [24/633], LR: 0.005000, Loss: 3.6114, top1: 9.3750\n",
      "Epoch [10/60], Iter [25/633], LR: 0.005000, Loss: 3.5976, top1: 10.9375\n",
      "Epoch [10/60], Iter [26/633], LR: 0.005000, Loss: 3.6502, top1: 9.3750\n",
      "Epoch [10/60], Iter [27/633], LR: 0.005000, Loss: 3.6081, top1: 7.8125\n",
      "Epoch [10/60], Iter [28/633], LR: 0.005000, Loss: 3.5869, top1: 12.5000\n",
      "Epoch [10/60], Iter [29/633], LR: 0.005000, Loss: 3.6176, top1: 12.5000\n",
      "Epoch [10/60], Iter [30/633], LR: 0.005000, Loss: 3.5770, top1: 7.8125\n",
      "Epoch [10/60], Iter [31/633], LR: 0.005000, Loss: 3.6022, top1: 10.9375\n",
      "Epoch [10/60], Iter [32/633], LR: 0.005000, Loss: 3.5958, top1: 10.9375\n",
      "Epoch [10/60], Iter [33/633], LR: 0.005000, Loss: 3.5852, top1: 15.6250\n",
      "Epoch [10/60], Iter [34/633], LR: 0.005000, Loss: 3.5763, top1: 17.1875\n",
      "Epoch [10/60], Iter [35/633], LR: 0.005000, Loss: 3.5972, top1: 9.3750\n",
      "Epoch [10/60], Iter [36/633], LR: 0.005000, Loss: 3.5525, top1: 17.1875\n",
      "Epoch [10/60], Iter [37/633], LR: 0.005000, Loss: 3.6358, top1: 6.2500\n",
      "Epoch [10/60], Iter [38/633], LR: 0.005000, Loss: 3.5976, top1: 9.3750\n",
      "Epoch [10/60], Iter [39/633], LR: 0.005000, Loss: 3.5724, top1: 17.1875\n",
      "Epoch [10/60], Iter [40/633], LR: 0.005000, Loss: 3.5620, top1: 14.0625\n",
      "Epoch [10/60], Iter [41/633], LR: 0.005000, Loss: 3.5922, top1: 12.5000\n",
      "Epoch [10/60], Iter [42/633], LR: 0.005000, Loss: 3.5661, top1: 12.5000\n",
      "Epoch [10/60], Iter [43/633], LR: 0.005000, Loss: 3.6106, top1: 12.5000\n",
      "Epoch [10/60], Iter [44/633], LR: 0.005000, Loss: 3.5986, top1: 7.8125\n",
      "Epoch [10/60], Iter [45/633], LR: 0.005000, Loss: 3.5569, top1: 18.7500\n",
      "Epoch [10/60], Iter [46/633], LR: 0.005000, Loss: 3.6371, top1: 6.2500\n",
      "Epoch [10/60], Iter [47/633], LR: 0.005000, Loss: 3.5552, top1: 18.7500\n",
      "Epoch [10/60], Iter [48/633], LR: 0.005000, Loss: 3.5946, top1: 10.9375\n",
      "Epoch [10/60], Iter [49/633], LR: 0.005000, Loss: 3.5795, top1: 17.1875\n",
      "Epoch [10/60], Iter [50/633], LR: 0.005000, Loss: 3.5718, top1: 14.0625\n",
      "Epoch [10/60], Iter [51/633], LR: 0.005000, Loss: 3.5627, top1: 12.5000\n",
      "Epoch [10/60], Iter [52/633], LR: 0.005000, Loss: 3.5931, top1: 17.1875\n",
      "Epoch [10/60], Iter [53/633], LR: 0.005000, Loss: 3.6006, top1: 17.1875\n",
      "Epoch [10/60], Iter [54/633], LR: 0.005000, Loss: 3.6088, top1: 9.3750\n",
      "Epoch [10/60], Iter [55/633], LR: 0.005000, Loss: 3.5971, top1: 14.0625\n",
      "Epoch [10/60], Iter [56/633], LR: 0.005000, Loss: 3.5623, top1: 18.7500\n",
      "Epoch [10/60], Iter [57/633], LR: 0.005000, Loss: 3.6453, top1: 7.8125\n",
      "Epoch [10/60], Iter [58/633], LR: 0.005000, Loss: 3.6113, top1: 9.3750\n",
      "Epoch [10/60], Iter [59/633], LR: 0.005000, Loss: 3.5936, top1: 12.5000\n",
      "Epoch [10/60], Iter [60/633], LR: 0.005000, Loss: 3.5680, top1: 17.1875\n",
      "Epoch [10/60], Iter [61/633], LR: 0.005000, Loss: 3.5129, top1: 20.3125\n",
      "Epoch [10/60], Iter [62/633], LR: 0.005000, Loss: 3.5839, top1: 14.0625\n",
      "Epoch [10/60], Iter [63/633], LR: 0.005000, Loss: 3.6178, top1: 9.3750\n",
      "Epoch [10/60], Iter [64/633], LR: 0.005000, Loss: 3.5950, top1: 9.3750\n",
      "Epoch [10/60], Iter [65/633], LR: 0.005000, Loss: 3.5438, top1: 14.0625\n",
      "Epoch [10/60], Iter [66/633], LR: 0.005000, Loss: 3.5600, top1: 15.6250\n",
      "Epoch [10/60], Iter [67/633], LR: 0.005000, Loss: 3.5927, top1: 18.7500\n",
      "Epoch [10/60], Iter [68/633], LR: 0.005000, Loss: 3.6385, top1: 9.3750\n",
      "Epoch [10/60], Iter [69/633], LR: 0.005000, Loss: 3.5424, top1: 17.1875\n",
      "Epoch [10/60], Iter [70/633], LR: 0.005000, Loss: 3.5980, top1: 14.0625\n",
      "Epoch [10/60], Iter [71/633], LR: 0.005000, Loss: 3.6091, top1: 12.5000\n",
      "Epoch [10/60], Iter [72/633], LR: 0.005000, Loss: 3.5342, top1: 17.1875\n",
      "Epoch [10/60], Iter [73/633], LR: 0.005000, Loss: 3.5692, top1: 12.5000\n",
      "Epoch [10/60], Iter [74/633], LR: 0.005000, Loss: 3.6004, top1: 10.9375\n",
      "Epoch [10/60], Iter [75/633], LR: 0.005000, Loss: 3.6131, top1: 9.3750\n",
      "Epoch [10/60], Iter [76/633], LR: 0.005000, Loss: 3.6304, top1: 7.8125\n",
      "Epoch [10/60], Iter [77/633], LR: 0.005000, Loss: 3.5638, top1: 21.8750\n",
      "Epoch [10/60], Iter [78/633], LR: 0.005000, Loss: 3.5748, top1: 17.1875\n",
      "Epoch [10/60], Iter [79/633], LR: 0.005000, Loss: 3.5816, top1: 14.0625\n",
      "Epoch [10/60], Iter [80/633], LR: 0.005000, Loss: 3.5639, top1: 17.1875\n",
      "Epoch [10/60], Iter [81/633], LR: 0.005000, Loss: 3.5624, top1: 15.6250\n",
      "Epoch [10/60], Iter [82/633], LR: 0.005000, Loss: 3.5466, top1: 21.8750\n",
      "Epoch [10/60], Iter [83/633], LR: 0.005000, Loss: 3.5584, top1: 18.7500\n",
      "Epoch [10/60], Iter [84/633], LR: 0.005000, Loss: 3.5725, top1: 10.9375\n",
      "Epoch [10/60], Iter [85/633], LR: 0.005000, Loss: 3.5873, top1: 10.9375\n",
      "Epoch [10/60], Iter [86/633], LR: 0.005000, Loss: 3.5993, top1: 10.9375\n",
      "Epoch [10/60], Iter [87/633], LR: 0.005000, Loss: 3.6143, top1: 9.3750\n",
      "Epoch [10/60], Iter [88/633], LR: 0.005000, Loss: 3.5831, top1: 12.5000\n",
      "Epoch [10/60], Iter [89/633], LR: 0.005000, Loss: 3.6159, top1: 10.9375\n",
      "Epoch [10/60], Iter [90/633], LR: 0.005000, Loss: 3.5837, top1: 12.5000\n",
      "Epoch [10/60], Iter [91/633], LR: 0.005000, Loss: 3.6022, top1: 10.9375\n",
      "Epoch [10/60], Iter [92/633], LR: 0.005000, Loss: 3.5620, top1: 12.5000\n",
      "Epoch [10/60], Iter [93/633], LR: 0.005000, Loss: 3.5337, top1: 25.0000\n",
      "Epoch [10/60], Iter [94/633], LR: 0.005000, Loss: 3.5823, top1: 15.6250\n",
      "Epoch [10/60], Iter [95/633], LR: 0.005000, Loss: 3.5461, top1: 18.7500\n",
      "Epoch [10/60], Iter [96/633], LR: 0.005000, Loss: 3.5831, top1: 15.6250\n",
      "Epoch [10/60], Iter [97/633], LR: 0.005000, Loss: 3.6271, top1: 12.5000\n",
      "Epoch [10/60], Iter [98/633], LR: 0.005000, Loss: 3.5796, top1: 17.1875\n",
      "Epoch [10/60], Iter [99/633], LR: 0.005000, Loss: 3.5878, top1: 15.6250\n",
      "Epoch [10/60], Iter [100/633], LR: 0.005000, Loss: 3.6015, top1: 17.1875\n",
      "Epoch [10/60], Iter [101/633], LR: 0.005000, Loss: 3.5974, top1: 9.3750\n",
      "Epoch [10/60], Iter [102/633], LR: 0.005000, Loss: 3.5672, top1: 14.0625\n",
      "Epoch [10/60], Iter [103/633], LR: 0.005000, Loss: 3.5437, top1: 17.1875\n",
      "Epoch [10/60], Iter [104/633], LR: 0.005000, Loss: 3.6413, top1: 6.2500\n",
      "Epoch [10/60], Iter [105/633], LR: 0.005000, Loss: 3.5721, top1: 18.7500\n",
      "Epoch [10/60], Iter [106/633], LR: 0.005000, Loss: 3.5880, top1: 10.9375\n",
      "Epoch [10/60], Iter [107/633], LR: 0.005000, Loss: 3.5906, top1: 7.8125\n",
      "Epoch [10/60], Iter [108/633], LR: 0.005000, Loss: 3.5934, top1: 6.2500\n",
      "Epoch [10/60], Iter [109/633], LR: 0.005000, Loss: 3.6145, top1: 12.5000\n",
      "Epoch [10/60], Iter [110/633], LR: 0.005000, Loss: 3.6065, top1: 9.3750\n",
      "Epoch [10/60], Iter [111/633], LR: 0.005000, Loss: 3.5211, top1: 21.8750\n",
      "Epoch [10/60], Iter [112/633], LR: 0.005000, Loss: 3.6097, top1: 14.0625\n",
      "Epoch [10/60], Iter [113/633], LR: 0.005000, Loss: 3.6270, top1: 10.9375\n",
      "Epoch [10/60], Iter [114/633], LR: 0.005000, Loss: 3.5525, top1: 15.6250\n",
      "Epoch [10/60], Iter [115/633], LR: 0.005000, Loss: 3.5477, top1: 14.0625\n",
      "Epoch [10/60], Iter [116/633], LR: 0.005000, Loss: 3.5832, top1: 15.6250\n",
      "Epoch [10/60], Iter [117/633], LR: 0.005000, Loss: 3.5915, top1: 10.9375\n",
      "Epoch [10/60], Iter [118/633], LR: 0.005000, Loss: 3.5605, top1: 14.0625\n",
      "Epoch [10/60], Iter [119/633], LR: 0.005000, Loss: 3.6272, top1: 4.6875\n",
      "Epoch [10/60], Iter [120/633], LR: 0.005000, Loss: 3.5795, top1: 12.5000\n",
      "Epoch [10/60], Iter [121/633], LR: 0.005000, Loss: 3.5823, top1: 14.0625\n",
      "Epoch [10/60], Iter [122/633], LR: 0.005000, Loss: 3.5911, top1: 14.0625\n",
      "Epoch [10/60], Iter [123/633], LR: 0.005000, Loss: 3.5745, top1: 14.0625\n",
      "Epoch [10/60], Iter [124/633], LR: 0.005000, Loss: 3.5907, top1: 10.9375\n",
      "Epoch [10/60], Iter [125/633], LR: 0.005000, Loss: 3.5762, top1: 12.5000\n",
      "Epoch [10/60], Iter [126/633], LR: 0.005000, Loss: 3.6038, top1: 12.5000\n",
      "Epoch [10/60], Iter [127/633], LR: 0.005000, Loss: 3.6097, top1: 15.6250\n",
      "Epoch [10/60], Iter [128/633], LR: 0.005000, Loss: 3.6013, top1: 12.5000\n",
      "Epoch [10/60], Iter [129/633], LR: 0.005000, Loss: 3.5235, top1: 23.4375\n",
      "Epoch [10/60], Iter [130/633], LR: 0.005000, Loss: 3.5665, top1: 18.7500\n",
      "Epoch [10/60], Iter [131/633], LR: 0.005000, Loss: 3.5820, top1: 10.9375\n",
      "Epoch [10/60], Iter [132/633], LR: 0.005000, Loss: 3.6017, top1: 12.5000\n",
      "Epoch [10/60], Iter [133/633], LR: 0.005000, Loss: 3.5759, top1: 10.9375\n",
      "Epoch [10/60], Iter [134/633], LR: 0.005000, Loss: 3.5979, top1: 14.0625\n",
      "Epoch [10/60], Iter [135/633], LR: 0.005000, Loss: 3.5629, top1: 9.3750\n",
      "Epoch [10/60], Iter [136/633], LR: 0.005000, Loss: 3.5171, top1: 21.8750\n",
      "Epoch [10/60], Iter [137/633], LR: 0.005000, Loss: 3.6167, top1: 7.8125\n",
      "Epoch [10/60], Iter [138/633], LR: 0.005000, Loss: 3.5548, top1: 14.0625\n",
      "Epoch [10/60], Iter [139/633], LR: 0.005000, Loss: 3.5511, top1: 18.7500\n",
      "Epoch [10/60], Iter [140/633], LR: 0.005000, Loss: 3.6153, top1: 12.5000\n",
      "Epoch [10/60], Iter [141/633], LR: 0.005000, Loss: 3.5643, top1: 18.7500\n",
      "Epoch [10/60], Iter [142/633], LR: 0.005000, Loss: 3.5696, top1: 14.0625\n",
      "Epoch [10/60], Iter [143/633], LR: 0.005000, Loss: 3.6454, top1: 3.1250\n",
      "Epoch [10/60], Iter [144/633], LR: 0.005000, Loss: 3.5425, top1: 18.7500\n",
      "Epoch [10/60], Iter [145/633], LR: 0.005000, Loss: 3.5889, top1: 10.9375\n",
      "Epoch [10/60], Iter [146/633], LR: 0.005000, Loss: 3.6148, top1: 9.3750\n",
      "Epoch [10/60], Iter [147/633], LR: 0.005000, Loss: 3.5756, top1: 18.7500\n",
      "Epoch [10/60], Iter [148/633], LR: 0.005000, Loss: 3.5750, top1: 18.7500\n",
      "Epoch [10/60], Iter [149/633], LR: 0.005000, Loss: 3.6070, top1: 9.3750\n",
      "Epoch [10/60], Iter [150/633], LR: 0.005000, Loss: 3.6041, top1: 7.8125\n",
      "Epoch [10/60], Iter [151/633], LR: 0.005000, Loss: 3.5674, top1: 15.6250\n",
      "Epoch [10/60], Iter [152/633], LR: 0.005000, Loss: 3.6009, top1: 17.1875\n",
      "Epoch [10/60], Iter [153/633], LR: 0.005000, Loss: 3.6047, top1: 10.9375\n",
      "Epoch [10/60], Iter [154/633], LR: 0.005000, Loss: 3.5733, top1: 14.0625\n",
      "Epoch [10/60], Iter [155/633], LR: 0.005000, Loss: 3.5765, top1: 10.9375\n",
      "Epoch [10/60], Iter [156/633], LR: 0.005000, Loss: 3.5804, top1: 9.3750\n",
      "Epoch [10/60], Iter [157/633], LR: 0.005000, Loss: 3.6161, top1: 3.1250\n",
      "Epoch [10/60], Iter [158/633], LR: 0.005000, Loss: 3.5788, top1: 17.1875\n",
      "Epoch [10/60], Iter [159/633], LR: 0.005000, Loss: 3.5683, top1: 18.7500\n",
      "Epoch [10/60], Iter [160/633], LR: 0.005000, Loss: 3.5727, top1: 10.9375\n",
      "Epoch [10/60], Iter [161/633], LR: 0.005000, Loss: 3.6182, top1: 10.9375\n",
      "Epoch [10/60], Iter [162/633], LR: 0.005000, Loss: 3.6367, top1: 0.0000\n",
      "Epoch [10/60], Iter [163/633], LR: 0.005000, Loss: 3.5379, top1: 17.1875\n",
      "Epoch [10/60], Iter [164/633], LR: 0.005000, Loss: 3.6343, top1: 14.0625\n",
      "Epoch [10/60], Iter [165/633], LR: 0.005000, Loss: 3.5611, top1: 17.1875\n",
      "Epoch [10/60], Iter [166/633], LR: 0.005000, Loss: 3.5613, top1: 18.7500\n",
      "Epoch [10/60], Iter [167/633], LR: 0.005000, Loss: 3.5713, top1: 15.6250\n",
      "Epoch [10/60], Iter [168/633], LR: 0.005000, Loss: 3.5904, top1: 14.0625\n",
      "Epoch [10/60], Iter [169/633], LR: 0.005000, Loss: 3.5698, top1: 10.9375\n",
      "Epoch [10/60], Iter [170/633], LR: 0.005000, Loss: 3.5629, top1: 12.5000\n",
      "Epoch [10/60], Iter [171/633], LR: 0.005000, Loss: 3.6201, top1: 12.5000\n",
      "Epoch [10/60], Iter [172/633], LR: 0.005000, Loss: 3.5920, top1: 9.3750\n",
      "Epoch [10/60], Iter [173/633], LR: 0.005000, Loss: 3.5826, top1: 14.0625\n",
      "Epoch [10/60], Iter [174/633], LR: 0.005000, Loss: 3.5796, top1: 12.5000\n",
      "Epoch [10/60], Iter [175/633], LR: 0.005000, Loss: 3.5677, top1: 15.6250\n",
      "Epoch [10/60], Iter [176/633], LR: 0.005000, Loss: 3.6037, top1: 7.8125\n",
      "Epoch [10/60], Iter [177/633], LR: 0.005000, Loss: 3.5951, top1: 10.9375\n",
      "Epoch [10/60], Iter [178/633], LR: 0.005000, Loss: 3.5834, top1: 7.8125\n",
      "Epoch [10/60], Iter [179/633], LR: 0.005000, Loss: 3.6134, top1: 7.8125\n",
      "Epoch [10/60], Iter [180/633], LR: 0.005000, Loss: 3.5504, top1: 20.3125\n",
      "Epoch [10/60], Iter [181/633], LR: 0.005000, Loss: 3.5827, top1: 15.6250\n",
      "Epoch [10/60], Iter [182/633], LR: 0.005000, Loss: 3.5971, top1: 10.9375\n",
      "Epoch [10/60], Iter [183/633], LR: 0.005000, Loss: 3.5302, top1: 14.0625\n",
      "Epoch [10/60], Iter [184/633], LR: 0.005000, Loss: 3.5341, top1: 21.8750\n",
      "Epoch [10/60], Iter [185/633], LR: 0.005000, Loss: 3.5316, top1: 20.3125\n",
      "Epoch [10/60], Iter [186/633], LR: 0.005000, Loss: 3.5477, top1: 25.0000\n",
      "Epoch [10/60], Iter [187/633], LR: 0.005000, Loss: 3.5697, top1: 12.5000\n",
      "Epoch [10/60], Iter [188/633], LR: 0.005000, Loss: 3.5399, top1: 14.0625\n",
      "Epoch [10/60], Iter [189/633], LR: 0.005000, Loss: 3.5735, top1: 17.1875\n",
      "Epoch [10/60], Iter [190/633], LR: 0.005000, Loss: 3.5512, top1: 12.5000\n",
      "Epoch [10/60], Iter [191/633], LR: 0.005000, Loss: 3.6296, top1: 6.2500\n",
      "Epoch [10/60], Iter [192/633], LR: 0.005000, Loss: 3.5944, top1: 12.5000\n",
      "Epoch [10/60], Iter [193/633], LR: 0.005000, Loss: 3.6133, top1: 9.3750\n",
      "Epoch [10/60], Iter [194/633], LR: 0.005000, Loss: 3.5730, top1: 18.7500\n",
      "Epoch [10/60], Iter [195/633], LR: 0.005000, Loss: 3.5398, top1: 17.1875\n",
      "Epoch [10/60], Iter [196/633], LR: 0.005000, Loss: 3.6056, top1: 10.9375\n",
      "Epoch [10/60], Iter [197/633], LR: 0.005000, Loss: 3.6008, top1: 12.5000\n",
      "Epoch [10/60], Iter [198/633], LR: 0.005000, Loss: 3.5845, top1: 15.6250\n",
      "Epoch [10/60], Iter [199/633], LR: 0.005000, Loss: 3.6071, top1: 7.8125\n",
      "Epoch [10/60], Iter [200/633], LR: 0.005000, Loss: 3.5866, top1: 15.6250\n",
      "Epoch [10/60], Iter [201/633], LR: 0.005000, Loss: 3.5813, top1: 14.0625\n",
      "Epoch [10/60], Iter [202/633], LR: 0.005000, Loss: 3.5569, top1: 18.7500\n",
      "Epoch [10/60], Iter [203/633], LR: 0.005000, Loss: 3.5817, top1: 12.5000\n",
      "Epoch [10/60], Iter [204/633], LR: 0.005000, Loss: 3.6377, top1: 4.6875\n",
      "Epoch [10/60], Iter [205/633], LR: 0.005000, Loss: 3.6209, top1: 10.9375\n",
      "Epoch [10/60], Iter [206/633], LR: 0.005000, Loss: 3.5762, top1: 17.1875\n",
      "Epoch [10/60], Iter [207/633], LR: 0.005000, Loss: 3.5824, top1: 7.8125\n",
      "Epoch [10/60], Iter [208/633], LR: 0.005000, Loss: 3.5764, top1: 12.5000\n",
      "Epoch [10/60], Iter [209/633], LR: 0.005000, Loss: 3.5862, top1: 14.0625\n",
      "Epoch [10/60], Iter [210/633], LR: 0.005000, Loss: 3.5972, top1: 12.5000\n",
      "Epoch [10/60], Iter [211/633], LR: 0.005000, Loss: 3.5573, top1: 15.6250\n",
      "Epoch [10/60], Iter [212/633], LR: 0.005000, Loss: 3.5480, top1: 17.1875\n",
      "Epoch [10/60], Iter [213/633], LR: 0.005000, Loss: 3.5146, top1: 28.1250\n",
      "Epoch [10/60], Iter [214/633], LR: 0.005000, Loss: 3.5930, top1: 9.3750\n",
      "Epoch [10/60], Iter [215/633], LR: 0.005000, Loss: 3.5661, top1: 18.7500\n",
      "Epoch [10/60], Iter [216/633], LR: 0.005000, Loss: 3.6024, top1: 9.3750\n",
      "Epoch [10/60], Iter [217/633], LR: 0.005000, Loss: 3.5716, top1: 12.5000\n",
      "Epoch [10/60], Iter [218/633], LR: 0.005000, Loss: 3.6171, top1: 7.8125\n",
      "Epoch [10/60], Iter [219/633], LR: 0.005000, Loss: 3.6144, top1: 10.9375\n",
      "Epoch [10/60], Iter [220/633], LR: 0.005000, Loss: 3.5258, top1: 25.0000\n",
      "Epoch [10/60], Iter [221/633], LR: 0.005000, Loss: 3.5351, top1: 21.8750\n",
      "Epoch [10/60], Iter [222/633], LR: 0.005000, Loss: 3.5783, top1: 12.5000\n",
      "Epoch [10/60], Iter [223/633], LR: 0.005000, Loss: 3.6194, top1: 6.2500\n",
      "Epoch [10/60], Iter [224/633], LR: 0.005000, Loss: 3.5689, top1: 20.3125\n",
      "Epoch [10/60], Iter [225/633], LR: 0.005000, Loss: 3.5754, top1: 10.9375\n",
      "Epoch [10/60], Iter [226/633], LR: 0.005000, Loss: 3.5460, top1: 20.3125\n",
      "Epoch [10/60], Iter [227/633], LR: 0.005000, Loss: 3.5878, top1: 12.5000\n",
      "Epoch [10/60], Iter [228/633], LR: 0.005000, Loss: 3.6317, top1: 1.5625\n",
      "Epoch [10/60], Iter [229/633], LR: 0.005000, Loss: 3.5978, top1: 7.8125\n",
      "Epoch [10/60], Iter [230/633], LR: 0.005000, Loss: 3.6168, top1: 14.0625\n",
      "Epoch [10/60], Iter [231/633], LR: 0.005000, Loss: 3.5763, top1: 14.0625\n",
      "Epoch [10/60], Iter [232/633], LR: 0.005000, Loss: 3.5807, top1: 14.0625\n",
      "Epoch [10/60], Iter [233/633], LR: 0.005000, Loss: 3.6393, top1: 9.3750\n",
      "Epoch [10/60], Iter [234/633], LR: 0.005000, Loss: 3.6175, top1: 14.0625\n",
      "Epoch [10/60], Iter [235/633], LR: 0.005000, Loss: 3.5867, top1: 14.0625\n",
      "Epoch [10/60], Iter [236/633], LR: 0.005000, Loss: 3.6026, top1: 9.3750\n",
      "Epoch [10/60], Iter [237/633], LR: 0.005000, Loss: 3.5975, top1: 10.9375\n",
      "Epoch [10/60], Iter [238/633], LR: 0.005000, Loss: 3.6284, top1: 10.9375\n",
      "Epoch [10/60], Iter [239/633], LR: 0.005000, Loss: 3.5307, top1: 21.8750\n",
      "Epoch [10/60], Iter [240/633], LR: 0.005000, Loss: 3.6067, top1: 7.8125\n",
      "Epoch [10/60], Iter [241/633], LR: 0.005000, Loss: 3.6276, top1: 6.2500\n",
      "Epoch [10/60], Iter [242/633], LR: 0.005000, Loss: 3.5747, top1: 14.0625\n",
      "Epoch [10/60], Iter [243/633], LR: 0.005000, Loss: 3.5465, top1: 20.3125\n",
      "Epoch [10/60], Iter [244/633], LR: 0.005000, Loss: 3.6010, top1: 12.5000\n",
      "Epoch [10/60], Iter [245/633], LR: 0.005000, Loss: 3.5489, top1: 10.9375\n",
      "Epoch [10/60], Iter [246/633], LR: 0.005000, Loss: 3.6152, top1: 12.5000\n",
      "Epoch [10/60], Iter [247/633], LR: 0.005000, Loss: 3.5727, top1: 14.0625\n",
      "Epoch [10/60], Iter [248/633], LR: 0.005000, Loss: 3.5320, top1: 25.0000\n",
      "Epoch [10/60], Iter [249/633], LR: 0.005000, Loss: 3.6051, top1: 12.5000\n",
      "Epoch [10/60], Iter [250/633], LR: 0.005000, Loss: 3.5705, top1: 15.6250\n",
      "Epoch [10/60], Iter [251/633], LR: 0.005000, Loss: 3.5953, top1: 12.5000\n",
      "Epoch [10/60], Iter [252/633], LR: 0.005000, Loss: 3.6262, top1: 10.9375\n",
      "Epoch [10/60], Iter [253/633], LR: 0.005000, Loss: 3.6227, top1: 10.9375\n",
      "Epoch [10/60], Iter [254/633], LR: 0.005000, Loss: 3.5802, top1: 15.6250\n",
      "Epoch [10/60], Iter [255/633], LR: 0.005000, Loss: 3.6052, top1: 10.9375\n",
      "Epoch [10/60], Iter [256/633], LR: 0.005000, Loss: 3.6114, top1: 10.9375\n",
      "Epoch [10/60], Iter [257/633], LR: 0.005000, Loss: 3.5820, top1: 12.5000\n",
      "Epoch [10/60], Iter [258/633], LR: 0.005000, Loss: 3.5650, top1: 15.6250\n",
      "Epoch [10/60], Iter [259/633], LR: 0.005000, Loss: 3.5398, top1: 18.7500\n",
      "Epoch [10/60], Iter [260/633], LR: 0.005000, Loss: 3.5901, top1: 14.0625\n",
      "Epoch [10/60], Iter [261/633], LR: 0.005000, Loss: 3.5301, top1: 21.8750\n",
      "Epoch [10/60], Iter [262/633], LR: 0.005000, Loss: 3.6110, top1: 9.3750\n",
      "Epoch [10/60], Iter [263/633], LR: 0.005000, Loss: 3.5694, top1: 14.0625\n",
      "Epoch [10/60], Iter [264/633], LR: 0.005000, Loss: 3.6359, top1: 10.9375\n",
      "Epoch [10/60], Iter [265/633], LR: 0.005000, Loss: 3.5922, top1: 12.5000\n",
      "Epoch [10/60], Iter [266/633], LR: 0.005000, Loss: 3.5763, top1: 15.6250\n",
      "Epoch [10/60], Iter [267/633], LR: 0.005000, Loss: 3.5647, top1: 20.3125\n",
      "Epoch [10/60], Iter [268/633], LR: 0.005000, Loss: 3.5752, top1: 10.9375\n",
      "Epoch [10/60], Iter [269/633], LR: 0.005000, Loss: 3.5846, top1: 14.0625\n",
      "Epoch [10/60], Iter [270/633], LR: 0.005000, Loss: 3.5674, top1: 14.0625\n",
      "Epoch [10/60], Iter [271/633], LR: 0.005000, Loss: 3.5983, top1: 7.8125\n",
      "Epoch [10/60], Iter [272/633], LR: 0.005000, Loss: 3.5431, top1: 17.1875\n",
      "Epoch [10/60], Iter [273/633], LR: 0.005000, Loss: 3.6060, top1: 14.0625\n",
      "Epoch [10/60], Iter [274/633], LR: 0.005000, Loss: 3.5782, top1: 12.5000\n",
      "Epoch [10/60], Iter [275/633], LR: 0.005000, Loss: 3.5630, top1: 15.6250\n",
      "Epoch [10/60], Iter [276/633], LR: 0.005000, Loss: 3.6064, top1: 17.1875\n",
      "Epoch [10/60], Iter [277/633], LR: 0.005000, Loss: 3.5789, top1: 10.9375\n",
      "Epoch [10/60], Iter [278/633], LR: 0.005000, Loss: 3.5727, top1: 15.6250\n",
      "Epoch [10/60], Iter [279/633], LR: 0.005000, Loss: 3.5791, top1: 9.3750\n",
      "Epoch [10/60], Iter [280/633], LR: 0.005000, Loss: 3.6095, top1: 9.3750\n",
      "Epoch [10/60], Iter [281/633], LR: 0.005000, Loss: 3.6263, top1: 12.5000\n",
      "Epoch [10/60], Iter [282/633], LR: 0.005000, Loss: 3.5811, top1: 17.1875\n",
      "Epoch [10/60], Iter [283/633], LR: 0.005000, Loss: 3.5860, top1: 12.5000\n",
      "Epoch [10/60], Iter [284/633], LR: 0.005000, Loss: 3.5901, top1: 17.1875\n",
      "Epoch [10/60], Iter [285/633], LR: 0.005000, Loss: 3.6102, top1: 14.0625\n",
      "Epoch [10/60], Iter [286/633], LR: 0.005000, Loss: 3.5785, top1: 12.5000\n",
      "Epoch [10/60], Iter [287/633], LR: 0.005000, Loss: 3.5900, top1: 18.7500\n",
      "Epoch [10/60], Iter [288/633], LR: 0.005000, Loss: 3.5458, top1: 15.6250\n",
      "Epoch [10/60], Iter [289/633], LR: 0.005000, Loss: 3.5996, top1: 12.5000\n",
      "Epoch [10/60], Iter [290/633], LR: 0.005000, Loss: 3.6030, top1: 9.3750\n",
      "Epoch [10/60], Iter [291/633], LR: 0.005000, Loss: 3.5768, top1: 10.9375\n",
      "Epoch [10/60], Iter [292/633], LR: 0.005000, Loss: 3.5830, top1: 9.3750\n",
      "Epoch [10/60], Iter [293/633], LR: 0.005000, Loss: 3.5940, top1: 12.5000\n",
      "Epoch [10/60], Iter [294/633], LR: 0.005000, Loss: 3.5985, top1: 10.9375\n",
      "Epoch [10/60], Iter [295/633], LR: 0.005000, Loss: 3.5869, top1: 14.0625\n",
      "Epoch [10/60], Iter [296/633], LR: 0.005000, Loss: 3.5875, top1: 12.5000\n",
      "Epoch [10/60], Iter [297/633], LR: 0.005000, Loss: 3.6417, top1: 4.6875\n",
      "Epoch [10/60], Iter [298/633], LR: 0.005000, Loss: 3.6111, top1: 14.0625\n",
      "Epoch [10/60], Iter [299/633], LR: 0.005000, Loss: 3.6135, top1: 12.5000\n",
      "Epoch [10/60], Iter [300/633], LR: 0.005000, Loss: 3.5815, top1: 12.5000\n",
      "Epoch [10/60], Iter [301/633], LR: 0.005000, Loss: 3.6211, top1: 9.3750\n",
      "Epoch [10/60], Iter [302/633], LR: 0.005000, Loss: 3.5447, top1: 20.3125\n",
      "Epoch [10/60], Iter [303/633], LR: 0.005000, Loss: 3.6101, top1: 7.8125\n",
      "Epoch [10/60], Iter [304/633], LR: 0.005000, Loss: 3.5286, top1: 23.4375\n",
      "Epoch [10/60], Iter [305/633], LR: 0.005000, Loss: 3.5942, top1: 20.3125\n",
      "Epoch [10/60], Iter [306/633], LR: 0.005000, Loss: 3.5904, top1: 17.1875\n",
      "Epoch [10/60], Iter [307/633], LR: 0.005000, Loss: 3.5784, top1: 12.5000\n",
      "Epoch [10/60], Iter [308/633], LR: 0.005000, Loss: 3.6107, top1: 10.9375\n",
      "Epoch [10/60], Iter [309/633], LR: 0.005000, Loss: 3.6035, top1: 9.3750\n",
      "Epoch [10/60], Iter [310/633], LR: 0.005000, Loss: 3.5534, top1: 18.7500\n",
      "Epoch [10/60], Iter [311/633], LR: 0.005000, Loss: 3.5824, top1: 18.7500\n",
      "Epoch [10/60], Iter [312/633], LR: 0.005000, Loss: 3.5688, top1: 15.6250\n",
      "Epoch [10/60], Iter [313/633], LR: 0.005000, Loss: 3.5879, top1: 12.5000\n",
      "Epoch [10/60], Iter [314/633], LR: 0.005000, Loss: 3.6044, top1: 15.6250\n",
      "Epoch [10/60], Iter [315/633], LR: 0.005000, Loss: 3.5536, top1: 18.7500\n",
      "Epoch [10/60], Iter [316/633], LR: 0.005000, Loss: 3.5469, top1: 17.1875\n",
      "Epoch [10/60], Iter [317/633], LR: 0.005000, Loss: 3.5906, top1: 12.5000\n",
      "Epoch [10/60], Iter [318/633], LR: 0.005000, Loss: 3.5892, top1: 12.5000\n",
      "Epoch [10/60], Iter [319/633], LR: 0.005000, Loss: 3.5752, top1: 15.6250\n",
      "Epoch [10/60], Iter [320/633], LR: 0.005000, Loss: 3.5939, top1: 17.1875\n",
      "Epoch [10/60], Iter [321/633], LR: 0.005000, Loss: 3.5576, top1: 17.1875\n",
      "Epoch [10/60], Iter [322/633], LR: 0.005000, Loss: 3.6065, top1: 12.5000\n",
      "Epoch [10/60], Iter [323/633], LR: 0.005000, Loss: 3.5718, top1: 10.9375\n",
      "Epoch [10/60], Iter [324/633], LR: 0.005000, Loss: 3.5553, top1: 21.8750\n",
      "Epoch [10/60], Iter [325/633], LR: 0.005000, Loss: 3.5465, top1: 17.1875\n",
      "Epoch [10/60], Iter [326/633], LR: 0.005000, Loss: 3.5528, top1: 18.7500\n",
      "Epoch [10/60], Iter [327/633], LR: 0.005000, Loss: 3.5631, top1: 12.5000\n",
      "Epoch [10/60], Iter [328/633], LR: 0.005000, Loss: 3.5631, top1: 14.0625\n",
      "Epoch [10/60], Iter [329/633], LR: 0.005000, Loss: 3.6103, top1: 14.0625\n",
      "Epoch [10/60], Iter [330/633], LR: 0.005000, Loss: 3.5899, top1: 17.1875\n",
      "Epoch [10/60], Iter [331/633], LR: 0.005000, Loss: 3.6373, top1: 6.2500\n",
      "Epoch [10/60], Iter [332/633], LR: 0.005000, Loss: 3.5547, top1: 14.0625\n",
      "Epoch [10/60], Iter [333/633], LR: 0.005000, Loss: 3.5854, top1: 17.1875\n",
      "Epoch [10/60], Iter [334/633], LR: 0.005000, Loss: 3.5560, top1: 20.3125\n",
      "Epoch [10/60], Iter [335/633], LR: 0.005000, Loss: 3.5542, top1: 20.3125\n",
      "Epoch [10/60], Iter [336/633], LR: 0.005000, Loss: 3.5542, top1: 21.8750\n",
      "Epoch [10/60], Iter [337/633], LR: 0.005000, Loss: 3.5954, top1: 12.5000\n",
      "Epoch [10/60], Iter [338/633], LR: 0.005000, Loss: 3.5775, top1: 9.3750\n",
      "Epoch [10/60], Iter [339/633], LR: 0.005000, Loss: 3.5670, top1: 17.1875\n",
      "Epoch [10/60], Iter [340/633], LR: 0.005000, Loss: 3.5748, top1: 20.3125\n",
      "Epoch [10/60], Iter [341/633], LR: 0.005000, Loss: 3.5864, top1: 6.2500\n",
      "Epoch [10/60], Iter [342/633], LR: 0.005000, Loss: 3.5889, top1: 10.9375\n",
      "Epoch [10/60], Iter [343/633], LR: 0.005000, Loss: 3.5499, top1: 15.6250\n",
      "Epoch [10/60], Iter [344/633], LR: 0.005000, Loss: 3.5758, top1: 17.1875\n",
      "Epoch [10/60], Iter [345/633], LR: 0.005000, Loss: 3.6197, top1: 10.9375\n",
      "Epoch [10/60], Iter [346/633], LR: 0.005000, Loss: 3.5474, top1: 17.1875\n",
      "Epoch [10/60], Iter [347/633], LR: 0.005000, Loss: 3.5914, top1: 10.9375\n",
      "Epoch [10/60], Iter [348/633], LR: 0.005000, Loss: 3.5874, top1: 12.5000\n",
      "Epoch [10/60], Iter [349/633], LR: 0.005000, Loss: 3.6153, top1: 10.9375\n",
      "Epoch [10/60], Iter [350/633], LR: 0.005000, Loss: 3.6069, top1: 14.0625\n",
      "Epoch [10/60], Iter [351/633], LR: 0.005000, Loss: 3.5902, top1: 7.8125\n",
      "Epoch [10/60], Iter [352/633], LR: 0.005000, Loss: 3.5538, top1: 14.0625\n",
      "Epoch [10/60], Iter [353/633], LR: 0.005000, Loss: 3.6037, top1: 14.0625\n",
      "Epoch [10/60], Iter [354/633], LR: 0.005000, Loss: 3.6220, top1: 7.8125\n",
      "Epoch [10/60], Iter [355/633], LR: 0.005000, Loss: 3.6513, top1: 6.2500\n",
      "Epoch [10/60], Iter [356/633], LR: 0.005000, Loss: 3.5950, top1: 12.5000\n",
      "Epoch [10/60], Iter [357/633], LR: 0.005000, Loss: 3.5796, top1: 14.0625\n",
      "Epoch [10/60], Iter [358/633], LR: 0.005000, Loss: 3.5862, top1: 17.1875\n",
      "Epoch [10/60], Iter [359/633], LR: 0.005000, Loss: 3.5579, top1: 15.6250\n",
      "Epoch [10/60], Iter [360/633], LR: 0.005000, Loss: 3.5560, top1: 10.9375\n",
      "Epoch [10/60], Iter [361/633], LR: 0.005000, Loss: 3.5510, top1: 14.0625\n",
      "Epoch [10/60], Iter [362/633], LR: 0.005000, Loss: 3.5604, top1: 18.7500\n",
      "Epoch [10/60], Iter [363/633], LR: 0.005000, Loss: 3.5892, top1: 14.0625\n",
      "Epoch [10/60], Iter [364/633], LR: 0.005000, Loss: 3.5335, top1: 21.8750\n",
      "Epoch [10/60], Iter [365/633], LR: 0.005000, Loss: 3.5706, top1: 21.8750\n",
      "Epoch [10/60], Iter [366/633], LR: 0.005000, Loss: 3.5592, top1: 12.5000\n",
      "Epoch [10/60], Iter [367/633], LR: 0.005000, Loss: 3.5221, top1: 18.7500\n",
      "Epoch [10/60], Iter [368/633], LR: 0.005000, Loss: 3.5990, top1: 14.0625\n",
      "Epoch [10/60], Iter [369/633], LR: 0.005000, Loss: 3.5824, top1: 14.0625\n",
      "Epoch [10/60], Iter [370/633], LR: 0.005000, Loss: 3.5644, top1: 15.6250\n",
      "Epoch [10/60], Iter [371/633], LR: 0.005000, Loss: 3.5692, top1: 17.1875\n",
      "Epoch [10/60], Iter [372/633], LR: 0.005000, Loss: 3.5382, top1: 17.1875\n",
      "Epoch [10/60], Iter [373/633], LR: 0.005000, Loss: 3.5672, top1: 15.6250\n",
      "Epoch [10/60], Iter [374/633], LR: 0.005000, Loss: 3.5800, top1: 9.3750\n",
      "Epoch [10/60], Iter [375/633], LR: 0.005000, Loss: 3.5901, top1: 15.6250\n",
      "Epoch [10/60], Iter [376/633], LR: 0.005000, Loss: 3.5500, top1: 14.0625\n",
      "Epoch [10/60], Iter [377/633], LR: 0.005000, Loss: 3.6019, top1: 9.3750\n",
      "Epoch [10/60], Iter [378/633], LR: 0.005000, Loss: 3.5648, top1: 15.6250\n",
      "Epoch [10/60], Iter [379/633], LR: 0.005000, Loss: 3.6027, top1: 17.1875\n",
      "Epoch [10/60], Iter [380/633], LR: 0.005000, Loss: 3.5642, top1: 12.5000\n",
      "Epoch [10/60], Iter [381/633], LR: 0.005000, Loss: 3.5537, top1: 14.0625\n",
      "Epoch [10/60], Iter [382/633], LR: 0.005000, Loss: 3.5359, top1: 15.6250\n",
      "Epoch [10/60], Iter [383/633], LR: 0.005000, Loss: 3.5407, top1: 23.4375\n",
      "Epoch [10/60], Iter [384/633], LR: 0.005000, Loss: 3.5297, top1: 21.8750\n",
      "Epoch [10/60], Iter [385/633], LR: 0.005000, Loss: 3.5168, top1: 25.0000\n",
      "Epoch [10/60], Iter [386/633], LR: 0.005000, Loss: 3.5780, top1: 15.6250\n",
      "Epoch [10/60], Iter [387/633], LR: 0.005000, Loss: 3.6047, top1: 7.8125\n",
      "Epoch [10/60], Iter [388/633], LR: 0.005000, Loss: 3.5708, top1: 17.1875\n",
      "Epoch [10/60], Iter [389/633], LR: 0.005000, Loss: 3.5743, top1: 17.1875\n",
      "Epoch [10/60], Iter [390/633], LR: 0.005000, Loss: 3.5387, top1: 18.7500\n",
      "Epoch [10/60], Iter [391/633], LR: 0.005000, Loss: 3.5776, top1: 17.1875\n",
      "Epoch [10/60], Iter [392/633], LR: 0.005000, Loss: 3.6004, top1: 10.9375\n",
      "Epoch [10/60], Iter [393/633], LR: 0.005000, Loss: 3.5929, top1: 14.0625\n",
      "Epoch [10/60], Iter [394/633], LR: 0.005000, Loss: 3.5392, top1: 18.7500\n",
      "Epoch [10/60], Iter [395/633], LR: 0.005000, Loss: 3.6004, top1: 9.3750\n",
      "Epoch [10/60], Iter [396/633], LR: 0.005000, Loss: 3.5708, top1: 17.1875\n",
      "Epoch [10/60], Iter [397/633], LR: 0.005000, Loss: 3.5945, top1: 10.9375\n",
      "Epoch [10/60], Iter [398/633], LR: 0.005000, Loss: 3.6137, top1: 10.9375\n",
      "Epoch [10/60], Iter [399/633], LR: 0.005000, Loss: 3.5865, top1: 12.5000\n",
      "Epoch [10/60], Iter [400/633], LR: 0.005000, Loss: 3.6273, top1: 9.3750\n",
      "Epoch [10/60], Iter [401/633], LR: 0.005000, Loss: 3.5490, top1: 14.0625\n",
      "Epoch [10/60], Iter [402/633], LR: 0.005000, Loss: 3.5826, top1: 9.3750\n",
      "Epoch [10/60], Iter [403/633], LR: 0.005000, Loss: 3.6258, top1: 12.5000\n",
      "Epoch [10/60], Iter [404/633], LR: 0.005000, Loss: 3.5969, top1: 9.3750\n",
      "Epoch [10/60], Iter [405/633], LR: 0.005000, Loss: 3.5690, top1: 12.5000\n",
      "Epoch [10/60], Iter [406/633], LR: 0.005000, Loss: 3.5761, top1: 17.1875\n",
      "Epoch [10/60], Iter [407/633], LR: 0.005000, Loss: 3.5767, top1: 15.6250\n",
      "Epoch [10/60], Iter [408/633], LR: 0.005000, Loss: 3.5809, top1: 14.0625\n",
      "Epoch [10/60], Iter [409/633], LR: 0.005000, Loss: 3.6133, top1: 14.0625\n",
      "Epoch [10/60], Iter [410/633], LR: 0.005000, Loss: 3.5905, top1: 12.5000\n",
      "Epoch [10/60], Iter [411/633], LR: 0.005000, Loss: 3.6247, top1: 7.8125\n",
      "Epoch [10/60], Iter [412/633], LR: 0.005000, Loss: 3.5895, top1: 10.9375\n",
      "Epoch [10/60], Iter [413/633], LR: 0.005000, Loss: 3.5795, top1: 10.9375\n",
      "Epoch [10/60], Iter [414/633], LR: 0.005000, Loss: 3.5928, top1: 9.3750\n",
      "Epoch [10/60], Iter [415/633], LR: 0.005000, Loss: 3.6170, top1: 6.2500\n",
      "Epoch [10/60], Iter [416/633], LR: 0.005000, Loss: 3.6009, top1: 10.9375\n",
      "Epoch [10/60], Iter [417/633], LR: 0.005000, Loss: 3.5400, top1: 14.0625\n",
      "Epoch [10/60], Iter [418/633], LR: 0.005000, Loss: 3.6018, top1: 9.3750\n",
      "Epoch [10/60], Iter [419/633], LR: 0.005000, Loss: 3.5551, top1: 14.0625\n",
      "Epoch [10/60], Iter [420/633], LR: 0.005000, Loss: 3.5661, top1: 15.6250\n",
      "Epoch [10/60], Iter [421/633], LR: 0.005000, Loss: 3.5819, top1: 14.0625\n",
      "Epoch [10/60], Iter [422/633], LR: 0.005000, Loss: 3.5394, top1: 17.1875\n",
      "Epoch [10/60], Iter [423/633], LR: 0.005000, Loss: 3.5641, top1: 10.9375\n",
      "Epoch [10/60], Iter [424/633], LR: 0.005000, Loss: 3.5851, top1: 12.5000\n",
      "Epoch [10/60], Iter [425/633], LR: 0.005000, Loss: 3.6041, top1: 9.3750\n",
      "Epoch [10/60], Iter [426/633], LR: 0.005000, Loss: 3.5858, top1: 12.5000\n",
      "Epoch [10/60], Iter [427/633], LR: 0.005000, Loss: 3.5669, top1: 12.5000\n",
      "Epoch [10/60], Iter [428/633], LR: 0.005000, Loss: 3.5755, top1: 17.1875\n",
      "Epoch [10/60], Iter [429/633], LR: 0.005000, Loss: 3.5920, top1: 12.5000\n",
      "Epoch [10/60], Iter [430/633], LR: 0.005000, Loss: 3.5801, top1: 12.5000\n",
      "Epoch [10/60], Iter [431/633], LR: 0.005000, Loss: 3.5565, top1: 17.1875\n",
      "Epoch [10/60], Iter [432/633], LR: 0.005000, Loss: 3.5414, top1: 18.7500\n",
      "Epoch [10/60], Iter [433/633], LR: 0.005000, Loss: 3.6447, top1: 7.8125\n",
      "Epoch [10/60], Iter [434/633], LR: 0.005000, Loss: 3.5372, top1: 18.7500\n",
      "Epoch [10/60], Iter [435/633], LR: 0.005000, Loss: 3.5919, top1: 17.1875\n",
      "Epoch [10/60], Iter [436/633], LR: 0.005000, Loss: 3.5703, top1: 14.0625\n",
      "Epoch [10/60], Iter [437/633], LR: 0.005000, Loss: 3.5928, top1: 14.0625\n",
      "Epoch [10/60], Iter [438/633], LR: 0.005000, Loss: 3.6371, top1: 7.8125\n",
      "Epoch [10/60], Iter [439/633], LR: 0.005000, Loss: 3.5462, top1: 15.6250\n",
      "Epoch [10/60], Iter [440/633], LR: 0.005000, Loss: 3.6337, top1: 7.8125\n",
      "Epoch [10/60], Iter [441/633], LR: 0.005000, Loss: 3.6272, top1: 10.9375\n",
      "Epoch [10/60], Iter [442/633], LR: 0.005000, Loss: 3.6418, top1: 7.8125\n",
      "Epoch [10/60], Iter [443/633], LR: 0.005000, Loss: 3.6156, top1: 12.5000\n",
      "Epoch [10/60], Iter [444/633], LR: 0.005000, Loss: 3.5793, top1: 17.1875\n",
      "Epoch [10/60], Iter [445/633], LR: 0.005000, Loss: 3.5706, top1: 20.3125\n",
      "Epoch [10/60], Iter [446/633], LR: 0.005000, Loss: 3.5775, top1: 12.5000\n",
      "Epoch [10/60], Iter [447/633], LR: 0.005000, Loss: 3.5619, top1: 18.7500\n",
      "Epoch [10/60], Iter [448/633], LR: 0.005000, Loss: 3.5759, top1: 17.1875\n",
      "Epoch [10/60], Iter [449/633], LR: 0.005000, Loss: 3.5641, top1: 15.6250\n",
      "Epoch [10/60], Iter [450/633], LR: 0.005000, Loss: 3.5727, top1: 15.6250\n",
      "Epoch [10/60], Iter [451/633], LR: 0.005000, Loss: 3.6105, top1: 9.3750\n",
      "Epoch [10/60], Iter [452/633], LR: 0.005000, Loss: 3.5752, top1: 18.7500\n",
      "Epoch [10/60], Iter [453/633], LR: 0.005000, Loss: 3.6015, top1: 9.3750\n",
      "Epoch [10/60], Iter [454/633], LR: 0.005000, Loss: 3.6448, top1: 4.6875\n",
      "Epoch [10/60], Iter [455/633], LR: 0.005000, Loss: 3.5635, top1: 15.6250\n",
      "Epoch [10/60], Iter [456/633], LR: 0.005000, Loss: 3.5902, top1: 15.6250\n",
      "Epoch [10/60], Iter [457/633], LR: 0.005000, Loss: 3.6012, top1: 10.9375\n",
      "Epoch [10/60], Iter [458/633], LR: 0.005000, Loss: 3.5739, top1: 7.8125\n",
      "Epoch [10/60], Iter [459/633], LR: 0.005000, Loss: 3.5719, top1: 10.9375\n",
      "Epoch [10/60], Iter [460/633], LR: 0.005000, Loss: 3.5991, top1: 6.2500\n",
      "Epoch [10/60], Iter [461/633], LR: 0.005000, Loss: 3.5484, top1: 15.6250\n",
      "Epoch [10/60], Iter [462/633], LR: 0.005000, Loss: 3.6183, top1: 9.3750\n",
      "Epoch [10/60], Iter [463/633], LR: 0.005000, Loss: 3.5979, top1: 9.3750\n",
      "Epoch [10/60], Iter [464/633], LR: 0.005000, Loss: 3.5996, top1: 15.6250\n",
      "Epoch [10/60], Iter [465/633], LR: 0.005000, Loss: 3.5255, top1: 20.3125\n",
      "Epoch [10/60], Iter [466/633], LR: 0.005000, Loss: 3.5656, top1: 18.7500\n",
      "Epoch [10/60], Iter [467/633], LR: 0.005000, Loss: 3.6255, top1: 7.8125\n",
      "Epoch [10/60], Iter [468/633], LR: 0.005000, Loss: 3.5918, top1: 14.0625\n",
      "Epoch [10/60], Iter [469/633], LR: 0.005000, Loss: 3.5408, top1: 18.7500\n",
      "Epoch [10/60], Iter [470/633], LR: 0.005000, Loss: 3.5873, top1: 17.1875\n",
      "Epoch [10/60], Iter [471/633], LR: 0.005000, Loss: 3.5521, top1: 15.6250\n",
      "Epoch [10/60], Iter [472/633], LR: 0.005000, Loss: 3.5623, top1: 10.9375\n",
      "Epoch [10/60], Iter [473/633], LR: 0.005000, Loss: 3.5852, top1: 12.5000\n",
      "Epoch [10/60], Iter [474/633], LR: 0.005000, Loss: 3.6066, top1: 6.2500\n",
      "Epoch [10/60], Iter [475/633], LR: 0.005000, Loss: 3.6152, top1: 7.8125\n",
      "Epoch [10/60], Iter [476/633], LR: 0.005000, Loss: 3.5488, top1: 15.6250\n",
      "Epoch [10/60], Iter [477/633], LR: 0.005000, Loss: 3.5890, top1: 20.3125\n",
      "Epoch [10/60], Iter [478/633], LR: 0.005000, Loss: 3.5486, top1: 20.3125\n",
      "Epoch [10/60], Iter [479/633], LR: 0.005000, Loss: 3.5792, top1: 18.7500\n",
      "Epoch [10/60], Iter [480/633], LR: 0.005000, Loss: 3.6028, top1: 10.9375\n",
      "Epoch [10/60], Iter [481/633], LR: 0.005000, Loss: 3.4889, top1: 29.6875\n",
      "Epoch [10/60], Iter [482/633], LR: 0.005000, Loss: 3.6071, top1: 7.8125\n",
      "Epoch [10/60], Iter [483/633], LR: 0.005000, Loss: 3.5628, top1: 21.8750\n",
      "Epoch [10/60], Iter [484/633], LR: 0.005000, Loss: 3.6059, top1: 9.3750\n",
      "Epoch [10/60], Iter [485/633], LR: 0.005000, Loss: 3.5647, top1: 20.3125\n",
      "Epoch [10/60], Iter [486/633], LR: 0.005000, Loss: 3.6146, top1: 14.0625\n",
      "Epoch [10/60], Iter [487/633], LR: 0.005000, Loss: 3.5742, top1: 18.7500\n",
      "Epoch [10/60], Iter [488/633], LR: 0.005000, Loss: 3.5762, top1: 12.5000\n",
      "Epoch [10/60], Iter [489/633], LR: 0.005000, Loss: 3.5642, top1: 17.1875\n",
      "Epoch [10/60], Iter [490/633], LR: 0.005000, Loss: 3.5560, top1: 15.6250\n",
      "Epoch [10/60], Iter [491/633], LR: 0.005000, Loss: 3.5909, top1: 10.9375\n",
      "Epoch [10/60], Iter [492/633], LR: 0.005000, Loss: 3.6265, top1: 7.8125\n",
      "Epoch [10/60], Iter [493/633], LR: 0.005000, Loss: 3.5585, top1: 15.6250\n",
      "Epoch [10/60], Iter [494/633], LR: 0.005000, Loss: 3.5733, top1: 9.3750\n",
      "Epoch [10/60], Iter [495/633], LR: 0.005000, Loss: 3.5497, top1: 14.0625\n",
      "Epoch [10/60], Iter [496/633], LR: 0.005000, Loss: 3.5667, top1: 18.7500\n",
      "Epoch [10/60], Iter [497/633], LR: 0.005000, Loss: 3.6171, top1: 9.3750\n",
      "Epoch [10/60], Iter [498/633], LR: 0.005000, Loss: 3.5583, top1: 15.6250\n",
      "Epoch [10/60], Iter [499/633], LR: 0.005000, Loss: 3.6161, top1: 12.5000\n",
      "Epoch [10/60], Iter [500/633], LR: 0.005000, Loss: 3.5561, top1: 15.6250\n",
      "Epoch [10/60], Iter [501/633], LR: 0.005000, Loss: 3.5603, top1: 18.7500\n",
      "Epoch [10/60], Iter [502/633], LR: 0.005000, Loss: 3.5508, top1: 14.0625\n",
      "Epoch [10/60], Iter [503/633], LR: 0.005000, Loss: 3.5849, top1: 12.5000\n",
      "Epoch [10/60], Iter [504/633], LR: 0.005000, Loss: 3.6251, top1: 12.5000\n",
      "Epoch [10/60], Iter [505/633], LR: 0.005000, Loss: 3.5620, top1: 9.3750\n",
      "Epoch [10/60], Iter [506/633], LR: 0.005000, Loss: 3.6312, top1: 9.3750\n",
      "Epoch [10/60], Iter [507/633], LR: 0.005000, Loss: 3.5348, top1: 23.4375\n",
      "Epoch [10/60], Iter [508/633], LR: 0.005000, Loss: 3.6170, top1: 12.5000\n",
      "Epoch [10/60], Iter [509/633], LR: 0.005000, Loss: 3.5455, top1: 18.7500\n",
      "Epoch [10/60], Iter [510/633], LR: 0.005000, Loss: 3.5620, top1: 15.6250\n",
      "Epoch [10/60], Iter [511/633], LR: 0.005000, Loss: 3.5829, top1: 17.1875\n",
      "Epoch [10/60], Iter [512/633], LR: 0.005000, Loss: 3.5875, top1: 17.1875\n",
      "Epoch [10/60], Iter [513/633], LR: 0.005000, Loss: 3.5678, top1: 17.1875\n",
      "Epoch [10/60], Iter [514/633], LR: 0.005000, Loss: 3.5970, top1: 10.9375\n",
      "Epoch [10/60], Iter [515/633], LR: 0.005000, Loss: 3.5939, top1: 10.9375\n",
      "Epoch [10/60], Iter [516/633], LR: 0.005000, Loss: 3.5042, top1: 26.5625\n",
      "Epoch [10/60], Iter [517/633], LR: 0.005000, Loss: 3.6100, top1: 7.8125\n",
      "Epoch [10/60], Iter [518/633], LR: 0.005000, Loss: 3.5997, top1: 12.5000\n",
      "Epoch [10/60], Iter [519/633], LR: 0.005000, Loss: 3.5934, top1: 12.5000\n",
      "Epoch [10/60], Iter [520/633], LR: 0.005000, Loss: 3.6325, top1: 7.8125\n",
      "Epoch [10/60], Iter [521/633], LR: 0.005000, Loss: 3.6086, top1: 10.9375\n",
      "Epoch [10/60], Iter [522/633], LR: 0.005000, Loss: 3.5938, top1: 7.8125\n",
      "Epoch [10/60], Iter [523/633], LR: 0.005000, Loss: 3.5785, top1: 10.9375\n",
      "Epoch [10/60], Iter [524/633], LR: 0.005000, Loss: 3.5688, top1: 18.7500\n",
      "Epoch [10/60], Iter [525/633], LR: 0.005000, Loss: 3.5620, top1: 18.7500\n",
      "Epoch [10/60], Iter [526/633], LR: 0.005000, Loss: 3.5746, top1: 14.0625\n",
      "Epoch [10/60], Iter [527/633], LR: 0.005000, Loss: 3.5913, top1: 7.8125\n",
      "Epoch [10/60], Iter [528/633], LR: 0.005000, Loss: 3.5732, top1: 14.0625\n",
      "Epoch [10/60], Iter [529/633], LR: 0.005000, Loss: 3.6055, top1: 14.0625\n",
      "Epoch [10/60], Iter [530/633], LR: 0.005000, Loss: 3.5610, top1: 18.7500\n",
      "Epoch [10/60], Iter [531/633], LR: 0.005000, Loss: 3.6241, top1: 6.2500\n",
      "Epoch [10/60], Iter [532/633], LR: 0.005000, Loss: 3.5619, top1: 10.9375\n",
      "Epoch [10/60], Iter [533/633], LR: 0.005000, Loss: 3.5948, top1: 14.0625\n",
      "Epoch [10/60], Iter [534/633], LR: 0.005000, Loss: 3.6387, top1: 10.9375\n",
      "Epoch [10/60], Iter [535/633], LR: 0.005000, Loss: 3.5614, top1: 17.1875\n",
      "Epoch [10/60], Iter [536/633], LR: 0.005000, Loss: 3.6258, top1: 9.3750\n",
      "Epoch [10/60], Iter [537/633], LR: 0.005000, Loss: 3.5732, top1: 9.3750\n",
      "Epoch [10/60], Iter [538/633], LR: 0.005000, Loss: 3.6039, top1: 10.9375\n",
      "Epoch [10/60], Iter [539/633], LR: 0.005000, Loss: 3.5720, top1: 17.1875\n",
      "Epoch [10/60], Iter [540/633], LR: 0.005000, Loss: 3.5659, top1: 14.0625\n",
      "Epoch [10/60], Iter [541/633], LR: 0.005000, Loss: 3.5685, top1: 17.1875\n",
      "Epoch [10/60], Iter [542/633], LR: 0.005000, Loss: 3.5444, top1: 20.3125\n",
      "Epoch [10/60], Iter [543/633], LR: 0.005000, Loss: 3.5797, top1: 17.1875\n",
      "Epoch [10/60], Iter [544/633], LR: 0.005000, Loss: 3.6128, top1: 12.5000\n",
      "Epoch [10/60], Iter [545/633], LR: 0.005000, Loss: 3.5775, top1: 12.5000\n",
      "Epoch [10/60], Iter [546/633], LR: 0.005000, Loss: 3.5507, top1: 15.6250\n",
      "Epoch [10/60], Iter [547/633], LR: 0.005000, Loss: 3.5446, top1: 14.0625\n",
      "Epoch [10/60], Iter [548/633], LR: 0.005000, Loss: 3.5701, top1: 17.1875\n",
      "Epoch [10/60], Iter [549/633], LR: 0.005000, Loss: 3.5484, top1: 18.7500\n",
      "Epoch [10/60], Iter [550/633], LR: 0.005000, Loss: 3.5091, top1: 23.4375\n",
      "Epoch [10/60], Iter [551/633], LR: 0.005000, Loss: 3.6138, top1: 14.0625\n",
      "Epoch [10/60], Iter [552/633], LR: 0.005000, Loss: 3.6027, top1: 9.3750\n",
      "Epoch [10/60], Iter [553/633], LR: 0.005000, Loss: 3.5887, top1: 14.0625\n",
      "Epoch [10/60], Iter [554/633], LR: 0.005000, Loss: 3.5509, top1: 20.3125\n",
      "Epoch [10/60], Iter [555/633], LR: 0.005000, Loss: 3.5557, top1: 12.5000\n",
      "Epoch [10/60], Iter [556/633], LR: 0.005000, Loss: 3.5606, top1: 20.3125\n",
      "Epoch [10/60], Iter [557/633], LR: 0.005000, Loss: 3.5955, top1: 14.0625\n",
      "Epoch [10/60], Iter [558/633], LR: 0.005000, Loss: 3.5729, top1: 14.0625\n",
      "Epoch [10/60], Iter [559/633], LR: 0.005000, Loss: 3.5809, top1: 17.1875\n",
      "Epoch [10/60], Iter [560/633], LR: 0.005000, Loss: 3.5900, top1: 14.0625\n",
      "Epoch [10/60], Iter [561/633], LR: 0.005000, Loss: 3.5729, top1: 18.7500\n",
      "Epoch [10/60], Iter [562/633], LR: 0.005000, Loss: 3.5937, top1: 10.9375\n",
      "Epoch [10/60], Iter [563/633], LR: 0.005000, Loss: 3.6017, top1: 12.5000\n",
      "Epoch [10/60], Iter [564/633], LR: 0.005000, Loss: 3.5446, top1: 21.8750\n",
      "Epoch [10/60], Iter [565/633], LR: 0.005000, Loss: 3.5459, top1: 17.1875\n",
      "Epoch [10/60], Iter [566/633], LR: 0.005000, Loss: 3.5926, top1: 12.5000\n",
      "Epoch [10/60], Iter [567/633], LR: 0.005000, Loss: 3.6434, top1: 4.6875\n",
      "Epoch [10/60], Iter [568/633], LR: 0.005000, Loss: 3.6197, top1: 9.3750\n",
      "Epoch [10/60], Iter [569/633], LR: 0.005000, Loss: 3.6080, top1: 9.3750\n",
      "Epoch [10/60], Iter [570/633], LR: 0.005000, Loss: 3.5853, top1: 6.2500\n",
      "Epoch [10/60], Iter [571/633], LR: 0.005000, Loss: 3.5854, top1: 12.5000\n",
      "Epoch [10/60], Iter [572/633], LR: 0.005000, Loss: 3.5765, top1: 10.9375\n",
      "Epoch [10/60], Iter [573/633], LR: 0.005000, Loss: 3.5934, top1: 14.0625\n",
      "Epoch [10/60], Iter [574/633], LR: 0.005000, Loss: 3.5957, top1: 12.5000\n",
      "Epoch [10/60], Iter [575/633], LR: 0.005000, Loss: 3.5596, top1: 17.1875\n",
      "Epoch [10/60], Iter [576/633], LR: 0.005000, Loss: 3.5704, top1: 18.7500\n",
      "Epoch [10/60], Iter [577/633], LR: 0.005000, Loss: 3.5989, top1: 14.0625\n",
      "Epoch [10/60], Iter [578/633], LR: 0.005000, Loss: 3.6175, top1: 7.8125\n",
      "Epoch [10/60], Iter [579/633], LR: 0.005000, Loss: 3.5626, top1: 21.8750\n",
      "Epoch [10/60], Iter [580/633], LR: 0.005000, Loss: 3.5564, top1: 15.6250\n",
      "Epoch [10/60], Iter [581/633], LR: 0.005000, Loss: 3.5902, top1: 12.5000\n",
      "Epoch [10/60], Iter [582/633], LR: 0.005000, Loss: 3.5875, top1: 7.8125\n",
      "Epoch [10/60], Iter [583/633], LR: 0.005000, Loss: 3.6221, top1: 4.6875\n",
      "Epoch [10/60], Iter [584/633], LR: 0.005000, Loss: 3.6009, top1: 12.5000\n",
      "Epoch [10/60], Iter [585/633], LR: 0.005000, Loss: 3.6180, top1: 12.5000\n",
      "Epoch [10/60], Iter [586/633], LR: 0.005000, Loss: 3.5916, top1: 14.0625\n",
      "Epoch [10/60], Iter [587/633], LR: 0.005000, Loss: 3.5833, top1: 15.6250\n",
      "Epoch [10/60], Iter [588/633], LR: 0.005000, Loss: 3.5873, top1: 12.5000\n",
      "Epoch [10/60], Iter [589/633], LR: 0.005000, Loss: 3.5959, top1: 14.0625\n",
      "Epoch [10/60], Iter [590/633], LR: 0.005000, Loss: 3.6420, top1: 4.6875\n",
      "Epoch [10/60], Iter [591/633], LR: 0.005000, Loss: 3.5920, top1: 6.2500\n",
      "Epoch [10/60], Iter [592/633], LR: 0.005000, Loss: 3.6198, top1: 6.2500\n",
      "Epoch [10/60], Iter [593/633], LR: 0.005000, Loss: 3.6006, top1: 15.6250\n",
      "Epoch [10/60], Iter [594/633], LR: 0.005000, Loss: 3.6098, top1: 9.3750\n",
      "Epoch [10/60], Iter [595/633], LR: 0.005000, Loss: 3.5943, top1: 15.6250\n",
      "Epoch [10/60], Iter [596/633], LR: 0.005000, Loss: 3.6079, top1: 10.9375\n",
      "Epoch [10/60], Iter [597/633], LR: 0.005000, Loss: 3.5973, top1: 14.0625\n",
      "Epoch [10/60], Iter [598/633], LR: 0.005000, Loss: 3.5903, top1: 10.9375\n",
      "Epoch [10/60], Iter [599/633], LR: 0.005000, Loss: 3.6002, top1: 7.8125\n",
      "Epoch [10/60], Iter [600/633], LR: 0.005000, Loss: 3.5726, top1: 12.5000\n",
      "Epoch [10/60], Iter [601/633], LR: 0.005000, Loss: 3.5801, top1: 14.0625\n",
      "Epoch [10/60], Iter [602/633], LR: 0.005000, Loss: 3.6177, top1: 10.9375\n",
      "Epoch [10/60], Iter [603/633], LR: 0.005000, Loss: 3.5965, top1: 20.3125\n",
      "Epoch [10/60], Iter [604/633], LR: 0.005000, Loss: 3.5659, top1: 14.0625\n",
      "Epoch [10/60], Iter [605/633], LR: 0.005000, Loss: 3.5632, top1: 9.3750\n",
      "Epoch [10/60], Iter [606/633], LR: 0.005000, Loss: 3.6006, top1: 20.3125\n",
      "Epoch [10/60], Iter [607/633], LR: 0.005000, Loss: 3.6233, top1: 7.8125\n",
      "Epoch [10/60], Iter [608/633], LR: 0.005000, Loss: 3.5876, top1: 14.0625\n",
      "Epoch [10/60], Iter [609/633], LR: 0.005000, Loss: 3.5877, top1: 10.9375\n",
      "Epoch [10/60], Iter [610/633], LR: 0.005000, Loss: 3.6133, top1: 9.3750\n",
      "Epoch [10/60], Iter [611/633], LR: 0.005000, Loss: 3.6021, top1: 7.8125\n",
      "Epoch [10/60], Iter [612/633], LR: 0.005000, Loss: 3.6008, top1: 12.5000\n",
      "Epoch [10/60], Iter [613/633], LR: 0.005000, Loss: 3.5904, top1: 17.1875\n",
      "Epoch [10/60], Iter [614/633], LR: 0.005000, Loss: 3.5679, top1: 20.3125\n",
      "Epoch [10/60], Iter [615/633], LR: 0.005000, Loss: 3.6259, top1: 3.1250\n",
      "Epoch [10/60], Iter [616/633], LR: 0.005000, Loss: 3.5787, top1: 14.0625\n",
      "Epoch [10/60], Iter [617/633], LR: 0.005000, Loss: 3.6005, top1: 9.3750\n",
      "Epoch [10/60], Iter [618/633], LR: 0.005000, Loss: 3.5852, top1: 18.7500\n",
      "Epoch [10/60], Iter [619/633], LR: 0.005000, Loss: 3.5626, top1: 18.7500\n",
      "Epoch [10/60], Iter [620/633], LR: 0.005000, Loss: 3.5831, top1: 15.6250\n",
      "Epoch [10/60], Iter [621/633], LR: 0.005000, Loss: 3.5592, top1: 17.1875\n",
      "Epoch [10/60], Iter [622/633], LR: 0.005000, Loss: 3.5668, top1: 17.1875\n",
      "Epoch [10/60], Iter [623/633], LR: 0.005000, Loss: 3.6063, top1: 10.9375\n",
      "Epoch [10/60], Iter [624/633], LR: 0.005000, Loss: 3.6276, top1: 10.9375\n",
      "Epoch [10/60], Iter [625/633], LR: 0.005000, Loss: 3.5824, top1: 12.5000\n",
      "Epoch [10/60], Iter [626/633], LR: 0.005000, Loss: 3.6151, top1: 10.9375\n",
      "Epoch [10/60], Iter [627/633], LR: 0.005000, Loss: 3.6083, top1: 7.8125\n",
      "Epoch [10/60], Iter [628/633], LR: 0.005000, Loss: 3.6032, top1: 7.8125\n",
      "Epoch [10/60], Iter [629/633], LR: 0.005000, Loss: 3.5562, top1: 18.7500\n",
      "Epoch [10/60], Iter [630/633], LR: 0.005000, Loss: 3.5643, top1: 17.1875\n",
      "Epoch [10/60], Iter [631/633], LR: 0.005000, Loss: 3.5871, top1: 12.5000\n",
      "Epoch [10/60], Iter [632/633], LR: 0.005000, Loss: 3.6288, top1: 4.6875\n",
      "Epoch [10/60], Iter [633/633], LR: 0.005000, Loss: 3.6164, top1: 10.9375\n",
      "Epoch [10/60], Iter [634/633], LR: 0.005000, Loss: 3.6123, top1: 17.7419\n",
      "Epoch [10/60], Val_Loss: 3.5980, Val_top1: 13.2702, best_top1: 14.6567\n",
      "epoch time: 4.408077275753021 min\n",
      "Epoch [11/60], Iter [1/633], LR: 0.005000, Loss: 3.6167, top1: 12.5000\n",
      "Epoch [11/60], Iter [2/633], LR: 0.005000, Loss: 3.6207, top1: 7.8125\n",
      "Epoch [11/60], Iter [3/633], LR: 0.005000, Loss: 3.6156, top1: 9.3750\n",
      "Epoch [11/60], Iter [4/633], LR: 0.005000, Loss: 3.6069, top1: 10.9375\n",
      "Epoch [11/60], Iter [5/633], LR: 0.005000, Loss: 3.6369, top1: 7.8125\n",
      "Epoch [11/60], Iter [6/633], LR: 0.005000, Loss: 3.5818, top1: 15.6250\n",
      "Epoch [11/60], Iter [7/633], LR: 0.005000, Loss: 3.5364, top1: 14.0625\n",
      "Epoch [11/60], Iter [8/633], LR: 0.005000, Loss: 3.5772, top1: 10.9375\n",
      "Epoch [11/60], Iter [9/633], LR: 0.005000, Loss: 3.5706, top1: 14.0625\n",
      "Epoch [11/60], Iter [10/633], LR: 0.005000, Loss: 3.5516, top1: 17.1875\n",
      "Epoch [11/60], Iter [11/633], LR: 0.005000, Loss: 3.6292, top1: 12.5000\n",
      "Epoch [11/60], Iter [12/633], LR: 0.005000, Loss: 3.5794, top1: 14.0625\n",
      "Epoch [11/60], Iter [13/633], LR: 0.005000, Loss: 3.5127, top1: 28.1250\n",
      "Epoch [11/60], Iter [14/633], LR: 0.005000, Loss: 3.6019, top1: 12.5000\n",
      "Epoch [11/60], Iter [15/633], LR: 0.005000, Loss: 3.5884, top1: 9.3750\n",
      "Epoch [11/60], Iter [16/633], LR: 0.005000, Loss: 3.5784, top1: 20.3125\n",
      "Epoch [11/60], Iter [17/633], LR: 0.005000, Loss: 3.5663, top1: 17.1875\n",
      "Epoch [11/60], Iter [18/633], LR: 0.005000, Loss: 3.5837, top1: 12.5000\n",
      "Epoch [11/60], Iter [19/633], LR: 0.005000, Loss: 3.5866, top1: 14.0625\n",
      "Epoch [11/60], Iter [20/633], LR: 0.005000, Loss: 3.5727, top1: 17.1875\n",
      "Epoch [11/60], Iter [21/633], LR: 0.005000, Loss: 3.5708, top1: 15.6250\n",
      "Epoch [11/60], Iter [22/633], LR: 0.005000, Loss: 3.6133, top1: 14.0625\n",
      "Epoch [11/60], Iter [23/633], LR: 0.005000, Loss: 3.6182, top1: 12.5000\n",
      "Epoch [11/60], Iter [24/633], LR: 0.005000, Loss: 3.6037, top1: 10.9375\n",
      "Epoch [11/60], Iter [25/633], LR: 0.005000, Loss: 3.5766, top1: 12.5000\n",
      "Epoch [11/60], Iter [26/633], LR: 0.005000, Loss: 3.5895, top1: 14.0625\n",
      "Epoch [11/60], Iter [27/633], LR: 0.005000, Loss: 3.5527, top1: 18.7500\n",
      "Epoch [11/60], Iter [28/633], LR: 0.005000, Loss: 3.5928, top1: 10.9375\n",
      "Epoch [11/60], Iter [29/633], LR: 0.005000, Loss: 3.5532, top1: 20.3125\n",
      "Epoch [11/60], Iter [30/633], LR: 0.005000, Loss: 3.5989, top1: 10.9375\n",
      "Epoch [11/60], Iter [31/633], LR: 0.005000, Loss: 3.6436, top1: 6.2500\n",
      "Epoch [11/60], Iter [32/633], LR: 0.005000, Loss: 3.4975, top1: 25.0000\n",
      "Epoch [11/60], Iter [33/633], LR: 0.005000, Loss: 3.6342, top1: 4.6875\n",
      "Epoch [11/60], Iter [34/633], LR: 0.005000, Loss: 3.5424, top1: 14.0625\n",
      "Epoch [11/60], Iter [35/633], LR: 0.005000, Loss: 3.5773, top1: 18.7500\n",
      "Epoch [11/60], Iter [36/633], LR: 0.005000, Loss: 3.6426, top1: 4.6875\n",
      "Epoch [11/60], Iter [37/633], LR: 0.005000, Loss: 3.5414, top1: 21.8750\n",
      "Epoch [11/60], Iter [38/633], LR: 0.005000, Loss: 3.5622, top1: 17.1875\n",
      "Epoch [11/60], Iter [39/633], LR: 0.005000, Loss: 3.5597, top1: 18.7500\n",
      "Epoch [11/60], Iter [40/633], LR: 0.005000, Loss: 3.5955, top1: 14.0625\n",
      "Epoch [11/60], Iter [41/633], LR: 0.005000, Loss: 3.6033, top1: 7.8125\n",
      "Epoch [11/60], Iter [42/633], LR: 0.005000, Loss: 3.5944, top1: 14.0625\n",
      "Epoch [11/60], Iter [43/633], LR: 0.005000, Loss: 3.5984, top1: 15.6250\n",
      "Epoch [11/60], Iter [44/633], LR: 0.005000, Loss: 3.5887, top1: 12.5000\n",
      "Epoch [11/60], Iter [45/633], LR: 0.005000, Loss: 3.5440, top1: 15.6250\n",
      "Epoch [11/60], Iter [46/633], LR: 0.005000, Loss: 3.6137, top1: 14.0625\n",
      "Epoch [11/60], Iter [47/633], LR: 0.005000, Loss: 3.6012, top1: 14.0625\n",
      "Epoch [11/60], Iter [48/633], LR: 0.005000, Loss: 3.5606, top1: 18.7500\n",
      "Epoch [11/60], Iter [49/633], LR: 0.005000, Loss: 3.5489, top1: 17.1875\n",
      "Epoch [11/60], Iter [50/633], LR: 0.005000, Loss: 3.5785, top1: 10.9375\n",
      "Epoch [11/60], Iter [51/633], LR: 0.005000, Loss: 3.5845, top1: 10.9375\n",
      "Epoch [11/60], Iter [52/633], LR: 0.005000, Loss: 3.6170, top1: 7.8125\n",
      "Epoch [11/60], Iter [53/633], LR: 0.005000, Loss: 3.5994, top1: 14.0625\n",
      "Epoch [11/60], Iter [54/633], LR: 0.005000, Loss: 3.5821, top1: 17.1875\n",
      "Epoch [11/60], Iter [55/633], LR: 0.005000, Loss: 3.5843, top1: 9.3750\n",
      "Epoch [11/60], Iter [56/633], LR: 0.005000, Loss: 3.5607, top1: 17.1875\n",
      "Epoch [11/60], Iter [57/633], LR: 0.005000, Loss: 3.5784, top1: 15.6250\n",
      "Epoch [11/60], Iter [58/633], LR: 0.005000, Loss: 3.5657, top1: 18.7500\n",
      "Epoch [11/60], Iter [59/633], LR: 0.005000, Loss: 3.5567, top1: 17.1875\n",
      "Epoch [11/60], Iter [60/633], LR: 0.005000, Loss: 3.6003, top1: 12.5000\n",
      "Epoch [11/60], Iter [61/633], LR: 0.005000, Loss: 3.6072, top1: 9.3750\n",
      "Epoch [11/60], Iter [62/633], LR: 0.005000, Loss: 3.6398, top1: 4.6875\n",
      "Epoch [11/60], Iter [63/633], LR: 0.005000, Loss: 3.6343, top1: 7.8125\n",
      "Epoch [11/60], Iter [64/633], LR: 0.005000, Loss: 3.5829, top1: 18.7500\n",
      "Epoch [11/60], Iter [65/633], LR: 0.005000, Loss: 3.5591, top1: 25.0000\n",
      "Epoch [11/60], Iter [66/633], LR: 0.005000, Loss: 3.6258, top1: 10.9375\n",
      "Epoch [11/60], Iter [67/633], LR: 0.005000, Loss: 3.5902, top1: 14.0625\n",
      "Epoch [11/60], Iter [68/633], LR: 0.005000, Loss: 3.5890, top1: 10.9375\n",
      "Epoch [11/60], Iter [69/633], LR: 0.005000, Loss: 3.5865, top1: 17.1875\n",
      "Epoch [11/60], Iter [70/633], LR: 0.005000, Loss: 3.6089, top1: 9.3750\n",
      "Epoch [11/60], Iter [71/633], LR: 0.005000, Loss: 3.5785, top1: 15.6250\n",
      "Epoch [11/60], Iter [72/633], LR: 0.005000, Loss: 3.5960, top1: 9.3750\n",
      "Epoch [11/60], Iter [73/633], LR: 0.005000, Loss: 3.5654, top1: 15.6250\n",
      "Epoch [11/60], Iter [74/633], LR: 0.005000, Loss: 3.5725, top1: 15.6250\n",
      "Epoch [11/60], Iter [75/633], LR: 0.005000, Loss: 3.5457, top1: 14.0625\n",
      "Epoch [11/60], Iter [76/633], LR: 0.005000, Loss: 3.5774, top1: 12.5000\n",
      "Epoch [11/60], Iter [77/633], LR: 0.005000, Loss: 3.5614, top1: 15.6250\n",
      "Epoch [11/60], Iter [78/633], LR: 0.005000, Loss: 3.5507, top1: 21.8750\n",
      "Epoch [11/60], Iter [79/633], LR: 0.005000, Loss: 3.6128, top1: 10.9375\n",
      "Epoch [11/60], Iter [80/633], LR: 0.005000, Loss: 3.5389, top1: 21.8750\n",
      "Epoch [11/60], Iter [81/633], LR: 0.005000, Loss: 3.5939, top1: 14.0625\n",
      "Epoch [11/60], Iter [82/633], LR: 0.005000, Loss: 3.5836, top1: 9.3750\n",
      "Epoch [11/60], Iter [83/633], LR: 0.005000, Loss: 3.6003, top1: 14.0625\n",
      "Epoch [11/60], Iter [84/633], LR: 0.005000, Loss: 3.5906, top1: 12.5000\n",
      "Epoch [11/60], Iter [85/633], LR: 0.005000, Loss: 3.5170, top1: 15.6250\n",
      "Epoch [11/60], Iter [86/633], LR: 0.005000, Loss: 3.5253, top1: 20.3125\n",
      "Epoch [11/60], Iter [87/633], LR: 0.005000, Loss: 3.5813, top1: 12.5000\n",
      "Epoch [11/60], Iter [88/633], LR: 0.005000, Loss: 3.5452, top1: 18.7500\n",
      "Epoch [11/60], Iter [89/633], LR: 0.005000, Loss: 3.6117, top1: 7.8125\n",
      "Epoch [11/60], Iter [90/633], LR: 0.005000, Loss: 3.5448, top1: 15.6250\n",
      "Epoch [11/60], Iter [91/633], LR: 0.005000, Loss: 3.5449, top1: 18.7500\n",
      "Epoch [11/60], Iter [92/633], LR: 0.005000, Loss: 3.5700, top1: 12.5000\n",
      "Epoch [11/60], Iter [93/633], LR: 0.005000, Loss: 3.5778, top1: 14.0625\n",
      "Epoch [11/60], Iter [94/633], LR: 0.005000, Loss: 3.6265, top1: 9.3750\n",
      "Epoch [11/60], Iter [95/633], LR: 0.005000, Loss: 3.5973, top1: 10.9375\n",
      "Epoch [11/60], Iter [96/633], LR: 0.005000, Loss: 3.5678, top1: 17.1875\n",
      "Epoch [11/60], Iter [97/633], LR: 0.005000, Loss: 3.5458, top1: 17.1875\n",
      "Epoch [11/60], Iter [98/633], LR: 0.005000, Loss: 3.6045, top1: 18.7500\n",
      "Epoch [11/60], Iter [99/633], LR: 0.005000, Loss: 3.5560, top1: 17.1875\n",
      "Epoch [11/60], Iter [100/633], LR: 0.005000, Loss: 3.5475, top1: 25.0000\n",
      "Epoch [11/60], Iter [101/633], LR: 0.005000, Loss: 3.5601, top1: 14.0625\n",
      "Epoch [11/60], Iter [102/633], LR: 0.005000, Loss: 3.5544, top1: 14.0625\n",
      "Epoch [11/60], Iter [103/633], LR: 0.005000, Loss: 3.5317, top1: 26.5625\n",
      "Epoch [11/60], Iter [104/633], LR: 0.005000, Loss: 3.5825, top1: 12.5000\n",
      "Epoch [11/60], Iter [105/633], LR: 0.005000, Loss: 3.5478, top1: 15.6250\n",
      "Epoch [11/60], Iter [106/633], LR: 0.005000, Loss: 3.5942, top1: 10.9375\n",
      "Epoch [11/60], Iter [107/633], LR: 0.005000, Loss: 3.5679, top1: 15.6250\n",
      "Epoch [11/60], Iter [108/633], LR: 0.005000, Loss: 3.5757, top1: 10.9375\n",
      "Epoch [11/60], Iter [109/633], LR: 0.005000, Loss: 3.6280, top1: 7.8125\n",
      "Epoch [11/60], Iter [110/633], LR: 0.005000, Loss: 3.5605, top1: 20.3125\n",
      "Epoch [11/60], Iter [111/633], LR: 0.005000, Loss: 3.6238, top1: 10.9375\n",
      "Epoch [11/60], Iter [112/633], LR: 0.005000, Loss: 3.5514, top1: 17.1875\n",
      "Epoch [11/60], Iter [113/633], LR: 0.005000, Loss: 3.5807, top1: 9.3750\n",
      "Epoch [11/60], Iter [114/633], LR: 0.005000, Loss: 3.5655, top1: 12.5000\n",
      "Epoch [11/60], Iter [115/633], LR: 0.005000, Loss: 3.5731, top1: 10.9375\n",
      "Epoch [11/60], Iter [116/633], LR: 0.005000, Loss: 3.5942, top1: 7.8125\n",
      "Epoch [11/60], Iter [117/633], LR: 0.005000, Loss: 3.5571, top1: 15.6250\n",
      "Epoch [11/60], Iter [118/633], LR: 0.005000, Loss: 3.5806, top1: 14.0625\n",
      "Epoch [11/60], Iter [119/633], LR: 0.005000, Loss: 3.5668, top1: 15.6250\n",
      "Epoch [11/60], Iter [120/633], LR: 0.005000, Loss: 3.5633, top1: 21.8750\n",
      "Epoch [11/60], Iter [121/633], LR: 0.005000, Loss: 3.5721, top1: 12.5000\n",
      "Epoch [11/60], Iter [122/633], LR: 0.005000, Loss: 3.6013, top1: 6.2500\n",
      "Epoch [11/60], Iter [123/633], LR: 0.005000, Loss: 3.6086, top1: 12.5000\n",
      "Epoch [11/60], Iter [124/633], LR: 0.005000, Loss: 3.5648, top1: 15.6250\n",
      "Epoch [11/60], Iter [125/633], LR: 0.005000, Loss: 3.5838, top1: 14.0625\n",
      "Epoch [11/60], Iter [126/633], LR: 0.005000, Loss: 3.6009, top1: 9.3750\n",
      "Epoch [11/60], Iter [127/633], LR: 0.005000, Loss: 3.5142, top1: 18.7500\n",
      "Epoch [11/60], Iter [128/633], LR: 0.005000, Loss: 3.5754, top1: 15.6250\n",
      "Epoch [11/60], Iter [129/633], LR: 0.005000, Loss: 3.6116, top1: 10.9375\n",
      "Epoch [11/60], Iter [130/633], LR: 0.005000, Loss: 3.5687, top1: 15.6250\n",
      "Epoch [11/60], Iter [131/633], LR: 0.005000, Loss: 3.5995, top1: 15.6250\n",
      "Epoch [11/60], Iter [132/633], LR: 0.005000, Loss: 3.5736, top1: 14.0625\n",
      "Epoch [11/60], Iter [133/633], LR: 0.005000, Loss: 3.5466, top1: 17.1875\n",
      "Epoch [11/60], Iter [134/633], LR: 0.005000, Loss: 3.5694, top1: 14.0625\n",
      "Epoch [11/60], Iter [135/633], LR: 0.005000, Loss: 3.5608, top1: 12.5000\n",
      "Epoch [11/60], Iter [136/633], LR: 0.005000, Loss: 3.5879, top1: 10.9375\n",
      "Epoch [11/60], Iter [137/633], LR: 0.005000, Loss: 3.6189, top1: 6.2500\n",
      "Epoch [11/60], Iter [138/633], LR: 0.005000, Loss: 3.5961, top1: 10.9375\n",
      "Epoch [11/60], Iter [139/633], LR: 0.005000, Loss: 3.5769, top1: 14.0625\n",
      "Epoch [11/60], Iter [140/633], LR: 0.005000, Loss: 3.5532, top1: 17.1875\n",
      "Epoch [11/60], Iter [141/633], LR: 0.005000, Loss: 3.6125, top1: 10.9375\n",
      "Epoch [11/60], Iter [142/633], LR: 0.005000, Loss: 3.6594, top1: 6.2500\n",
      "Epoch [11/60], Iter [143/633], LR: 0.005000, Loss: 3.6525, top1: 7.8125\n",
      "Epoch [11/60], Iter [144/633], LR: 0.005000, Loss: 3.5724, top1: 15.6250\n",
      "Epoch [11/60], Iter [145/633], LR: 0.005000, Loss: 3.5576, top1: 18.7500\n",
      "Epoch [11/60], Iter [146/633], LR: 0.005000, Loss: 3.5636, top1: 12.5000\n",
      "Epoch [11/60], Iter [147/633], LR: 0.005000, Loss: 3.6281, top1: 3.1250\n",
      "Epoch [11/60], Iter [148/633], LR: 0.005000, Loss: 3.5314, top1: 20.3125\n",
      "Epoch [11/60], Iter [149/633], LR: 0.005000, Loss: 3.6751, top1: 3.1250\n",
      "Epoch [11/60], Iter [150/633], LR: 0.005000, Loss: 3.5649, top1: 17.1875\n",
      "Epoch [11/60], Iter [151/633], LR: 0.005000, Loss: 3.5859, top1: 7.8125\n",
      "Epoch [11/60], Iter [152/633], LR: 0.005000, Loss: 3.5754, top1: 9.3750\n",
      "Epoch [11/60], Iter [153/633], LR: 0.005000, Loss: 3.6286, top1: 7.8125\n",
      "Epoch [11/60], Iter [154/633], LR: 0.005000, Loss: 3.6136, top1: 14.0625\n",
      "Epoch [11/60], Iter [155/633], LR: 0.005000, Loss: 3.5806, top1: 10.9375\n",
      "Epoch [11/60], Iter [156/633], LR: 0.005000, Loss: 3.5709, top1: 18.7500\n",
      "Epoch [11/60], Iter [157/633], LR: 0.005000, Loss: 3.5417, top1: 20.3125\n",
      "Epoch [11/60], Iter [158/633], LR: 0.005000, Loss: 3.5830, top1: 10.9375\n",
      "Epoch [11/60], Iter [159/633], LR: 0.005000, Loss: 3.5950, top1: 6.2500\n",
      "Epoch [11/60], Iter [160/633], LR: 0.005000, Loss: 3.5108, top1: 23.4375\n",
      "Epoch [11/60], Iter [161/633], LR: 0.005000, Loss: 3.5798, top1: 14.0625\n",
      "Epoch [11/60], Iter [162/633], LR: 0.005000, Loss: 3.6003, top1: 10.9375\n",
      "Epoch [11/60], Iter [163/633], LR: 0.005000, Loss: 3.5951, top1: 6.2500\n",
      "Epoch [11/60], Iter [164/633], LR: 0.005000, Loss: 3.5381, top1: 17.1875\n",
      "Epoch [11/60], Iter [165/633], LR: 0.005000, Loss: 3.5662, top1: 15.6250\n",
      "Epoch [11/60], Iter [166/633], LR: 0.005000, Loss: 3.6091, top1: 10.9375\n",
      "Epoch [11/60], Iter [167/633], LR: 0.005000, Loss: 3.5847, top1: 17.1875\n",
      "Epoch [11/60], Iter [168/633], LR: 0.005000, Loss: 3.6196, top1: 7.8125\n",
      "Epoch [11/60], Iter [169/633], LR: 0.005000, Loss: 3.6107, top1: 17.1875\n",
      "Epoch [11/60], Iter [170/633], LR: 0.005000, Loss: 3.5564, top1: 15.6250\n",
      "Epoch [11/60], Iter [171/633], LR: 0.005000, Loss: 3.6115, top1: 14.0625\n",
      "Epoch [11/60], Iter [172/633], LR: 0.005000, Loss: 3.6485, top1: 4.6875\n",
      "Epoch [11/60], Iter [173/633], LR: 0.005000, Loss: 3.5447, top1: 20.3125\n",
      "Epoch [11/60], Iter [174/633], LR: 0.005000, Loss: 3.5959, top1: 10.9375\n",
      "Epoch [11/60], Iter [175/633], LR: 0.005000, Loss: 3.5567, top1: 14.0625\n",
      "Epoch [11/60], Iter [176/633], LR: 0.005000, Loss: 3.6311, top1: 14.0625\n",
      "Epoch [11/60], Iter [177/633], LR: 0.005000, Loss: 3.5669, top1: 10.9375\n",
      "Epoch [11/60], Iter [178/633], LR: 0.005000, Loss: 3.5757, top1: 10.9375\n",
      "Epoch [11/60], Iter [179/633], LR: 0.005000, Loss: 3.5869, top1: 12.5000\n",
      "Epoch [11/60], Iter [180/633], LR: 0.005000, Loss: 3.5969, top1: 12.5000\n",
      "Epoch [11/60], Iter [181/633], LR: 0.005000, Loss: 3.5473, top1: 21.8750\n",
      "Epoch [11/60], Iter [182/633], LR: 0.005000, Loss: 3.5816, top1: 14.0625\n",
      "Epoch [11/60], Iter [183/633], LR: 0.005000, Loss: 3.5616, top1: 17.1875\n",
      "Epoch [11/60], Iter [184/633], LR: 0.005000, Loss: 3.6340, top1: 9.3750\n",
      "Epoch [11/60], Iter [185/633], LR: 0.005000, Loss: 3.5867, top1: 12.5000\n",
      "Epoch [11/60], Iter [186/633], LR: 0.005000, Loss: 3.5406, top1: 15.6250\n",
      "Epoch [11/60], Iter [187/633], LR: 0.005000, Loss: 3.5855, top1: 7.8125\n",
      "Epoch [11/60], Iter [188/633], LR: 0.005000, Loss: 3.5674, top1: 18.7500\n",
      "Epoch [11/60], Iter [189/633], LR: 0.005000, Loss: 3.5626, top1: 17.1875\n",
      "Epoch [11/60], Iter [190/633], LR: 0.005000, Loss: 3.5993, top1: 15.6250\n",
      "Epoch [11/60], Iter [191/633], LR: 0.005000, Loss: 3.5875, top1: 12.5000\n",
      "Epoch [11/60], Iter [192/633], LR: 0.005000, Loss: 3.6184, top1: 10.9375\n",
      "Epoch [11/60], Iter [193/633], LR: 0.005000, Loss: 3.5928, top1: 10.9375\n",
      "Epoch [11/60], Iter [194/633], LR: 0.005000, Loss: 3.5557, top1: 15.6250\n",
      "Epoch [11/60], Iter [195/633], LR: 0.005000, Loss: 3.5450, top1: 14.0625\n",
      "Epoch [11/60], Iter [196/633], LR: 0.005000, Loss: 3.6113, top1: 9.3750\n",
      "Epoch [11/60], Iter [197/633], LR: 0.005000, Loss: 3.5675, top1: 17.1875\n",
      "Epoch [11/60], Iter [198/633], LR: 0.005000, Loss: 3.5916, top1: 10.9375\n",
      "Epoch [11/60], Iter [199/633], LR: 0.005000, Loss: 3.6083, top1: 10.9375\n",
      "Epoch [11/60], Iter [200/633], LR: 0.005000, Loss: 3.5850, top1: 14.0625\n",
      "Epoch [11/60], Iter [201/633], LR: 0.005000, Loss: 3.6105, top1: 4.6875\n",
      "Epoch [11/60], Iter [202/633], LR: 0.005000, Loss: 3.6304, top1: 9.3750\n",
      "Epoch [11/60], Iter [203/633], LR: 0.005000, Loss: 3.5690, top1: 10.9375\n",
      "Epoch [11/60], Iter [204/633], LR: 0.005000, Loss: 3.5761, top1: 14.0625\n",
      "Epoch [11/60], Iter [205/633], LR: 0.005000, Loss: 3.6075, top1: 9.3750\n",
      "Epoch [11/60], Iter [206/633], LR: 0.005000, Loss: 3.6017, top1: 9.3750\n",
      "Epoch [11/60], Iter [207/633], LR: 0.005000, Loss: 3.6039, top1: 9.3750\n",
      "Epoch [11/60], Iter [208/633], LR: 0.005000, Loss: 3.6404, top1: 7.8125\n",
      "Epoch [11/60], Iter [209/633], LR: 0.005000, Loss: 3.5950, top1: 14.0625\n",
      "Epoch [11/60], Iter [210/633], LR: 0.005000, Loss: 3.6059, top1: 10.9375\n",
      "Epoch [11/60], Iter [211/633], LR: 0.005000, Loss: 3.6008, top1: 9.3750\n",
      "Epoch [11/60], Iter [212/633], LR: 0.005000, Loss: 3.5564, top1: 12.5000\n",
      "Epoch [11/60], Iter [213/633], LR: 0.005000, Loss: 3.5926, top1: 12.5000\n",
      "Epoch [11/60], Iter [214/633], LR: 0.005000, Loss: 3.5769, top1: 15.6250\n",
      "Epoch [11/60], Iter [215/633], LR: 0.005000, Loss: 3.6180, top1: 6.2500\n",
      "Epoch [11/60], Iter [216/633], LR: 0.005000, Loss: 3.5886, top1: 12.5000\n",
      "Epoch [11/60], Iter [217/633], LR: 0.005000, Loss: 3.5826, top1: 14.0625\n",
      "Epoch [11/60], Iter [218/633], LR: 0.005000, Loss: 3.5976, top1: 12.5000\n",
      "Epoch [11/60], Iter [219/633], LR: 0.005000, Loss: 3.5988, top1: 12.5000\n",
      "Epoch [11/60], Iter [220/633], LR: 0.005000, Loss: 3.5975, top1: 10.9375\n",
      "Epoch [11/60], Iter [221/633], LR: 0.005000, Loss: 3.5818, top1: 14.0625\n",
      "Epoch [11/60], Iter [222/633], LR: 0.005000, Loss: 3.5255, top1: 12.5000\n",
      "Epoch [11/60], Iter [223/633], LR: 0.005000, Loss: 3.5752, top1: 14.0625\n",
      "Epoch [11/60], Iter [224/633], LR: 0.005000, Loss: 3.5764, top1: 18.7500\n",
      "Epoch [11/60], Iter [225/633], LR: 0.005000, Loss: 3.5748, top1: 12.5000\n",
      "Epoch [11/60], Iter [226/633], LR: 0.005000, Loss: 3.5642, top1: 9.3750\n",
      "Epoch [11/60], Iter [227/633], LR: 0.005000, Loss: 3.5846, top1: 14.0625\n",
      "Epoch [11/60], Iter [228/633], LR: 0.005000, Loss: 3.5952, top1: 14.0625\n",
      "Epoch [11/60], Iter [229/633], LR: 0.005000, Loss: 3.5539, top1: 15.6250\n",
      "Epoch [11/60], Iter [230/633], LR: 0.005000, Loss: 3.5784, top1: 12.5000\n",
      "Epoch [11/60], Iter [231/633], LR: 0.005000, Loss: 3.5488, top1: 20.3125\n",
      "Epoch [11/60], Iter [232/633], LR: 0.005000, Loss: 3.5487, top1: 15.6250\n",
      "Epoch [11/60], Iter [233/633], LR: 0.005000, Loss: 3.5839, top1: 14.0625\n",
      "Epoch [11/60], Iter [234/633], LR: 0.005000, Loss: 3.6174, top1: 10.9375\n",
      "Epoch [11/60], Iter [235/633], LR: 0.005000, Loss: 3.6186, top1: 10.9375\n",
      "Epoch [11/60], Iter [236/633], LR: 0.005000, Loss: 3.5428, top1: 15.6250\n",
      "Epoch [11/60], Iter [237/633], LR: 0.005000, Loss: 3.5527, top1: 18.7500\n",
      "Epoch [11/60], Iter [238/633], LR: 0.005000, Loss: 3.5408, top1: 20.3125\n",
      "Epoch [11/60], Iter [239/633], LR: 0.005000, Loss: 3.5995, top1: 7.8125\n",
      "Epoch [11/60], Iter [240/633], LR: 0.005000, Loss: 3.5688, top1: 14.0625\n",
      "Epoch [11/60], Iter [241/633], LR: 0.005000, Loss: 3.5320, top1: 14.0625\n",
      "Epoch [11/60], Iter [242/633], LR: 0.005000, Loss: 3.5736, top1: 14.0625\n",
      "Epoch [11/60], Iter [243/633], LR: 0.005000, Loss: 3.5389, top1: 21.8750\n",
      "Epoch [11/60], Iter [244/633], LR: 0.005000, Loss: 3.6243, top1: 10.9375\n",
      "Epoch [11/60], Iter [245/633], LR: 0.005000, Loss: 3.6051, top1: 7.8125\n",
      "Epoch [11/60], Iter [246/633], LR: 0.005000, Loss: 3.5840, top1: 14.0625\n",
      "Epoch [11/60], Iter [247/633], LR: 0.005000, Loss: 3.5595, top1: 14.0625\n",
      "Epoch [11/60], Iter [248/633], LR: 0.005000, Loss: 3.5593, top1: 21.8750\n",
      "Epoch [11/60], Iter [249/633], LR: 0.005000, Loss: 3.6001, top1: 12.5000\n",
      "Epoch [11/60], Iter [250/633], LR: 0.005000, Loss: 3.5395, top1: 23.4375\n",
      "Epoch [11/60], Iter [251/633], LR: 0.005000, Loss: 3.6154, top1: 7.8125\n",
      "Epoch [11/60], Iter [252/633], LR: 0.005000, Loss: 3.5777, top1: 14.0625\n",
      "Epoch [11/60], Iter [253/633], LR: 0.005000, Loss: 3.6030, top1: 12.5000\n",
      "Epoch [11/60], Iter [254/633], LR: 0.005000, Loss: 3.6011, top1: 10.9375\n",
      "Epoch [11/60], Iter [255/633], LR: 0.005000, Loss: 3.6330, top1: 10.9375\n",
      "Epoch [11/60], Iter [256/633], LR: 0.005000, Loss: 3.5873, top1: 15.6250\n",
      "Epoch [11/60], Iter [257/633], LR: 0.005000, Loss: 3.5674, top1: 17.1875\n",
      "Epoch [11/60], Iter [258/633], LR: 0.005000, Loss: 3.5622, top1: 18.7500\n",
      "Epoch [11/60], Iter [259/633], LR: 0.005000, Loss: 3.6074, top1: 12.5000\n",
      "Epoch [11/60], Iter [260/633], LR: 0.005000, Loss: 3.6121, top1: 6.2500\n",
      "Epoch [11/60], Iter [261/633], LR: 0.005000, Loss: 3.5901, top1: 12.5000\n",
      "Epoch [11/60], Iter [262/633], LR: 0.005000, Loss: 3.5901, top1: 12.5000\n",
      "Epoch [11/60], Iter [263/633], LR: 0.005000, Loss: 3.5320, top1: 20.3125\n",
      "Epoch [11/60], Iter [264/633], LR: 0.005000, Loss: 3.5216, top1: 21.8750\n",
      "Epoch [11/60], Iter [265/633], LR: 0.005000, Loss: 3.6103, top1: 10.9375\n",
      "Epoch [11/60], Iter [266/633], LR: 0.005000, Loss: 3.5828, top1: 20.3125\n",
      "Epoch [11/60], Iter [267/633], LR: 0.005000, Loss: 3.5942, top1: 10.9375\n",
      "Epoch [11/60], Iter [268/633], LR: 0.005000, Loss: 3.5894, top1: 14.0625\n",
      "Epoch [11/60], Iter [269/633], LR: 0.005000, Loss: 3.6164, top1: 14.0625\n",
      "Epoch [11/60], Iter [270/633], LR: 0.005000, Loss: 3.5767, top1: 15.6250\n",
      "Epoch [11/60], Iter [271/633], LR: 0.005000, Loss: 3.4861, top1: 26.5625\n",
      "Epoch [11/60], Iter [272/633], LR: 0.005000, Loss: 3.6303, top1: 9.3750\n",
      "Epoch [11/60], Iter [273/633], LR: 0.005000, Loss: 3.6342, top1: 10.9375\n",
      "Epoch [11/60], Iter [274/633], LR: 0.005000, Loss: 3.6091, top1: 9.3750\n",
      "Epoch [11/60], Iter [275/633], LR: 0.005000, Loss: 3.6114, top1: 9.3750\n",
      "Epoch [11/60], Iter [276/633], LR: 0.005000, Loss: 3.5180, top1: 21.8750\n",
      "Epoch [11/60], Iter [277/633], LR: 0.005000, Loss: 3.5900, top1: 10.9375\n",
      "Epoch [11/60], Iter [278/633], LR: 0.005000, Loss: 3.5477, top1: 12.5000\n",
      "Epoch [11/60], Iter [279/633], LR: 0.005000, Loss: 3.5859, top1: 12.5000\n",
      "Epoch [11/60], Iter [280/633], LR: 0.005000, Loss: 3.5883, top1: 18.7500\n",
      "Epoch [11/60], Iter [281/633], LR: 0.005000, Loss: 3.6272, top1: 9.3750\n",
      "Epoch [11/60], Iter [282/633], LR: 0.005000, Loss: 3.5572, top1: 14.0625\n",
      "Epoch [11/60], Iter [283/633], LR: 0.005000, Loss: 3.5525, top1: 14.0625\n",
      "Epoch [11/60], Iter [284/633], LR: 0.005000, Loss: 3.5701, top1: 17.1875\n",
      "Epoch [11/60], Iter [285/633], LR: 0.005000, Loss: 3.5856, top1: 14.0625\n",
      "Epoch [11/60], Iter [286/633], LR: 0.005000, Loss: 3.5265, top1: 17.1875\n",
      "Epoch [11/60], Iter [287/633], LR: 0.005000, Loss: 3.6066, top1: 7.8125\n",
      "Epoch [11/60], Iter [288/633], LR: 0.005000, Loss: 3.6260, top1: 10.9375\n",
      "Epoch [11/60], Iter [289/633], LR: 0.005000, Loss: 3.6001, top1: 9.3750\n",
      "Epoch [11/60], Iter [290/633], LR: 0.005000, Loss: 3.6095, top1: 7.8125\n",
      "Epoch [11/60], Iter [291/633], LR: 0.005000, Loss: 3.5741, top1: 12.5000\n",
      "Epoch [11/60], Iter [292/633], LR: 0.005000, Loss: 3.5806, top1: 12.5000\n",
      "Epoch [11/60], Iter [293/633], LR: 0.005000, Loss: 3.5907, top1: 10.9375\n",
      "Epoch [11/60], Iter [294/633], LR: 0.005000, Loss: 3.5454, top1: 21.8750\n",
      "Epoch [11/60], Iter [295/633], LR: 0.005000, Loss: 3.5976, top1: 15.6250\n",
      "Epoch [11/60], Iter [296/633], LR: 0.005000, Loss: 3.5674, top1: 10.9375\n",
      "Epoch [11/60], Iter [297/633], LR: 0.005000, Loss: 3.5644, top1: 18.7500\n",
      "Epoch [11/60], Iter [298/633], LR: 0.005000, Loss: 3.5716, top1: 12.5000\n",
      "Epoch [11/60], Iter [299/633], LR: 0.005000, Loss: 3.5020, top1: 25.0000\n",
      "Epoch [11/60], Iter [300/633], LR: 0.005000, Loss: 3.6007, top1: 10.9375\n",
      "Epoch [11/60], Iter [301/633], LR: 0.005000, Loss: 3.5220, top1: 20.3125\n",
      "Epoch [11/60], Iter [302/633], LR: 0.005000, Loss: 3.5885, top1: 9.3750\n",
      "Epoch [11/60], Iter [303/633], LR: 0.005000, Loss: 3.6115, top1: 7.8125\n",
      "Epoch [11/60], Iter [304/633], LR: 0.005000, Loss: 3.5532, top1: 14.0625\n",
      "Epoch [11/60], Iter [305/633], LR: 0.005000, Loss: 3.5633, top1: 14.0625\n",
      "Epoch [11/60], Iter [306/633], LR: 0.005000, Loss: 3.5912, top1: 14.0625\n",
      "Epoch [11/60], Iter [307/633], LR: 0.005000, Loss: 3.5965, top1: 14.0625\n",
      "Epoch [11/60], Iter [308/633], LR: 0.005000, Loss: 3.5455, top1: 20.3125\n",
      "Epoch [11/60], Iter [309/633], LR: 0.005000, Loss: 3.6023, top1: 7.8125\n",
      "Epoch [11/60], Iter [310/633], LR: 0.005000, Loss: 3.5671, top1: 20.3125\n",
      "Epoch [11/60], Iter [311/633], LR: 0.005000, Loss: 3.5418, top1: 12.5000\n",
      "Epoch [11/60], Iter [312/633], LR: 0.005000, Loss: 3.5642, top1: 17.1875\n",
      "Epoch [11/60], Iter [313/633], LR: 0.005000, Loss: 3.5472, top1: 21.8750\n",
      "Epoch [11/60], Iter [314/633], LR: 0.005000, Loss: 3.6162, top1: 9.3750\n",
      "Epoch [11/60], Iter [315/633], LR: 0.005000, Loss: 3.5482, top1: 15.6250\n",
      "Epoch [11/60], Iter [316/633], LR: 0.005000, Loss: 3.5569, top1: 21.8750\n",
      "Epoch [11/60], Iter [317/633], LR: 0.005000, Loss: 3.5794, top1: 14.0625\n",
      "Epoch [11/60], Iter [318/633], LR: 0.005000, Loss: 3.5910, top1: 14.0625\n",
      "Epoch [11/60], Iter [319/633], LR: 0.005000, Loss: 3.5637, top1: 14.0625\n",
      "Epoch [11/60], Iter [320/633], LR: 0.005000, Loss: 3.5773, top1: 10.9375\n",
      "Epoch [11/60], Iter [321/633], LR: 0.005000, Loss: 3.5692, top1: 10.9375\n",
      "Epoch [11/60], Iter [322/633], LR: 0.005000, Loss: 3.6063, top1: 10.9375\n",
      "Epoch [11/60], Iter [323/633], LR: 0.005000, Loss: 3.5587, top1: 18.7500\n",
      "Epoch [11/60], Iter [324/633], LR: 0.005000, Loss: 3.5574, top1: 15.6250\n",
      "Epoch [11/60], Iter [325/633], LR: 0.005000, Loss: 3.5976, top1: 10.9375\n",
      "Epoch [11/60], Iter [326/633], LR: 0.005000, Loss: 3.6167, top1: 7.8125\n",
      "Epoch [11/60], Iter [327/633], LR: 0.005000, Loss: 3.5639, top1: 18.7500\n",
      "Epoch [11/60], Iter [328/633], LR: 0.005000, Loss: 3.6054, top1: 12.5000\n",
      "Epoch [11/60], Iter [329/633], LR: 0.005000, Loss: 3.5570, top1: 17.1875\n",
      "Epoch [11/60], Iter [330/633], LR: 0.005000, Loss: 3.6185, top1: 10.9375\n",
      "Epoch [11/60], Iter [331/633], LR: 0.005000, Loss: 3.5492, top1: 17.1875\n",
      "Epoch [11/60], Iter [332/633], LR: 0.005000, Loss: 3.5629, top1: 15.6250\n",
      "Epoch [11/60], Iter [333/633], LR: 0.005000, Loss: 3.5603, top1: 17.1875\n",
      "Epoch [11/60], Iter [334/633], LR: 0.005000, Loss: 3.6131, top1: 7.8125\n",
      "Epoch [11/60], Iter [335/633], LR: 0.005000, Loss: 3.5713, top1: 12.5000\n",
      "Epoch [11/60], Iter [336/633], LR: 0.005000, Loss: 3.5699, top1: 15.6250\n",
      "Epoch [11/60], Iter [337/633], LR: 0.005000, Loss: 3.5716, top1: 14.0625\n",
      "Epoch [11/60], Iter [338/633], LR: 0.005000, Loss: 3.5744, top1: 20.3125\n",
      "Epoch [11/60], Iter [339/633], LR: 0.005000, Loss: 3.5866, top1: 18.7500\n",
      "Epoch [11/60], Iter [340/633], LR: 0.005000, Loss: 3.5270, top1: 21.8750\n",
      "Epoch [11/60], Iter [341/633], LR: 0.005000, Loss: 3.6092, top1: 10.9375\n",
      "Epoch [11/60], Iter [342/633], LR: 0.005000, Loss: 3.5674, top1: 17.1875\n",
      "Epoch [11/60], Iter [343/633], LR: 0.005000, Loss: 3.5017, top1: 29.6875\n",
      "Epoch [11/60], Iter [344/633], LR: 0.005000, Loss: 3.5806, top1: 7.8125\n",
      "Epoch [11/60], Iter [345/633], LR: 0.005000, Loss: 3.5489, top1: 14.0625\n",
      "Epoch [11/60], Iter [346/633], LR: 0.005000, Loss: 3.5434, top1: 20.3125\n",
      "Epoch [11/60], Iter [347/633], LR: 0.005000, Loss: 3.5753, top1: 17.1875\n",
      "Epoch [11/60], Iter [348/633], LR: 0.005000, Loss: 3.6197, top1: 10.9375\n",
      "Epoch [11/60], Iter [349/633], LR: 0.005000, Loss: 3.6169, top1: 10.9375\n",
      "Epoch [11/60], Iter [350/633], LR: 0.005000, Loss: 3.6119, top1: 12.5000\n",
      "Epoch [11/60], Iter [351/633], LR: 0.005000, Loss: 3.6021, top1: 10.9375\n",
      "Epoch [11/60], Iter [352/633], LR: 0.005000, Loss: 3.5450, top1: 18.7500\n",
      "Epoch [11/60], Iter [353/633], LR: 0.005000, Loss: 3.6031, top1: 10.9375\n",
      "Epoch [11/60], Iter [354/633], LR: 0.005000, Loss: 3.5867, top1: 10.9375\n",
      "Epoch [11/60], Iter [355/633], LR: 0.005000, Loss: 3.5557, top1: 18.7500\n",
      "Epoch [11/60], Iter [356/633], LR: 0.005000, Loss: 3.5851, top1: 6.2500\n",
      "Epoch [11/60], Iter [357/633], LR: 0.005000, Loss: 3.5736, top1: 12.5000\n",
      "Epoch [11/60], Iter [358/633], LR: 0.005000, Loss: 3.5934, top1: 12.5000\n",
      "Epoch [11/60], Iter [359/633], LR: 0.005000, Loss: 3.5986, top1: 12.5000\n",
      "Epoch [11/60], Iter [360/633], LR: 0.005000, Loss: 3.6060, top1: 12.5000\n",
      "Epoch [11/60], Iter [361/633], LR: 0.005000, Loss: 3.5617, top1: 14.0625\n",
      "Epoch [11/60], Iter [362/633], LR: 0.005000, Loss: 3.5989, top1: 12.5000\n",
      "Epoch [11/60], Iter [363/633], LR: 0.005000, Loss: 3.5802, top1: 17.1875\n",
      "Epoch [11/60], Iter [364/633], LR: 0.005000, Loss: 3.5395, top1: 15.6250\n",
      "Epoch [11/60], Iter [365/633], LR: 0.005000, Loss: 3.5945, top1: 9.3750\n",
      "Epoch [11/60], Iter [366/633], LR: 0.005000, Loss: 3.5888, top1: 12.5000\n",
      "Epoch [11/60], Iter [367/633], LR: 0.005000, Loss: 3.5666, top1: 15.6250\n",
      "Epoch [11/60], Iter [368/633], LR: 0.005000, Loss: 3.5411, top1: 15.6250\n",
      "Epoch [11/60], Iter [369/633], LR: 0.005000, Loss: 3.5438, top1: 14.0625\n",
      "Epoch [11/60], Iter [370/633], LR: 0.005000, Loss: 3.5812, top1: 15.6250\n",
      "Epoch [11/60], Iter [371/633], LR: 0.005000, Loss: 3.5665, top1: 14.0625\n",
      "Epoch [11/60], Iter [372/633], LR: 0.005000, Loss: 3.6019, top1: 10.9375\n",
      "Epoch [11/60], Iter [373/633], LR: 0.005000, Loss: 3.5884, top1: 15.6250\n",
      "Epoch [11/60], Iter [374/633], LR: 0.005000, Loss: 3.6146, top1: 3.1250\n",
      "Epoch [11/60], Iter [375/633], LR: 0.005000, Loss: 3.5364, top1: 17.1875\n",
      "Epoch [11/60], Iter [376/633], LR: 0.005000, Loss: 3.5748, top1: 14.0625\n",
      "Epoch [11/60], Iter [377/633], LR: 0.005000, Loss: 3.5813, top1: 10.9375\n",
      "Epoch [11/60], Iter [378/633], LR: 0.005000, Loss: 3.5762, top1: 15.6250\n",
      "Epoch [11/60], Iter [379/633], LR: 0.005000, Loss: 3.5815, top1: 14.0625\n",
      "Epoch [11/60], Iter [380/633], LR: 0.005000, Loss: 3.5631, top1: 18.7500\n",
      "Epoch [11/60], Iter [381/633], LR: 0.005000, Loss: 3.5530, top1: 17.1875\n",
      "Epoch [11/60], Iter [382/633], LR: 0.005000, Loss: 3.6603, top1: 6.2500\n",
      "Epoch [11/60], Iter [383/633], LR: 0.005000, Loss: 3.5577, top1: 17.1875\n",
      "Epoch [11/60], Iter [384/633], LR: 0.005000, Loss: 3.5993, top1: 15.6250\n",
      "Epoch [11/60], Iter [385/633], LR: 0.005000, Loss: 3.5660, top1: 14.0625\n",
      "Epoch [11/60], Iter [386/633], LR: 0.005000, Loss: 3.5623, top1: 12.5000\n",
      "Epoch [11/60], Iter [387/633], LR: 0.005000, Loss: 3.5233, top1: 20.3125\n",
      "Epoch [11/60], Iter [388/633], LR: 0.005000, Loss: 3.5567, top1: 15.6250\n",
      "Epoch [11/60], Iter [389/633], LR: 0.005000, Loss: 3.5566, top1: 18.7500\n",
      "Epoch [11/60], Iter [390/633], LR: 0.005000, Loss: 3.5494, top1: 20.3125\n",
      "Epoch [11/60], Iter [391/633], LR: 0.005000, Loss: 3.6187, top1: 7.8125\n",
      "Epoch [11/60], Iter [392/633], LR: 0.005000, Loss: 3.5652, top1: 14.0625\n",
      "Epoch [11/60], Iter [393/633], LR: 0.005000, Loss: 3.5793, top1: 7.8125\n",
      "Epoch [11/60], Iter [394/633], LR: 0.005000, Loss: 3.5849, top1: 17.1875\n",
      "Epoch [11/60], Iter [395/633], LR: 0.005000, Loss: 3.6168, top1: 6.2500\n",
      "Epoch [11/60], Iter [396/633], LR: 0.005000, Loss: 3.6145, top1: 10.9375\n",
      "Epoch [11/60], Iter [397/633], LR: 0.005000, Loss: 3.6014, top1: 12.5000\n",
      "Epoch [11/60], Iter [398/633], LR: 0.005000, Loss: 3.5845, top1: 12.5000\n",
      "Epoch [11/60], Iter [399/633], LR: 0.005000, Loss: 3.5791, top1: 10.9375\n",
      "Epoch [11/60], Iter [400/633], LR: 0.005000, Loss: 3.5588, top1: 14.0625\n",
      "Epoch [11/60], Iter [401/633], LR: 0.005000, Loss: 3.6112, top1: 14.0625\n",
      "Epoch [11/60], Iter [402/633], LR: 0.005000, Loss: 3.6019, top1: 10.9375\n",
      "Epoch [11/60], Iter [403/633], LR: 0.005000, Loss: 3.5570, top1: 15.6250\n",
      "Epoch [11/60], Iter [404/633], LR: 0.005000, Loss: 3.5174, top1: 18.7500\n",
      "Epoch [11/60], Iter [405/633], LR: 0.005000, Loss: 3.5742, top1: 12.5000\n",
      "Epoch [11/60], Iter [406/633], LR: 0.005000, Loss: 3.5798, top1: 15.6250\n",
      "Epoch [11/60], Iter [407/633], LR: 0.005000, Loss: 3.6104, top1: 14.0625\n",
      "Epoch [11/60], Iter [408/633], LR: 0.005000, Loss: 3.5409, top1: 9.3750\n",
      "Epoch [11/60], Iter [409/633], LR: 0.005000, Loss: 3.5942, top1: 12.5000\n",
      "Epoch [11/60], Iter [410/633], LR: 0.005000, Loss: 3.5949, top1: 12.5000\n",
      "Epoch [11/60], Iter [411/633], LR: 0.005000, Loss: 3.5139, top1: 21.8750\n",
      "Epoch [11/60], Iter [412/633], LR: 0.005000, Loss: 3.5227, top1: 23.4375\n",
      "Epoch [11/60], Iter [413/633], LR: 0.005000, Loss: 3.6190, top1: 9.3750\n",
      "Epoch [11/60], Iter [414/633], LR: 0.005000, Loss: 3.5696, top1: 14.0625\n",
      "Epoch [11/60], Iter [415/633], LR: 0.005000, Loss: 3.5480, top1: 25.0000\n",
      "Epoch [11/60], Iter [416/633], LR: 0.005000, Loss: 3.5296, top1: 26.5625\n",
      "Epoch [11/60], Iter [417/633], LR: 0.005000, Loss: 3.6223, top1: 6.2500\n",
      "Epoch [11/60], Iter [418/633], LR: 0.005000, Loss: 3.5761, top1: 12.5000\n",
      "Epoch [11/60], Iter [419/633], LR: 0.005000, Loss: 3.5780, top1: 15.6250\n",
      "Epoch [11/60], Iter [420/633], LR: 0.005000, Loss: 3.5674, top1: 14.0625\n",
      "Epoch [11/60], Iter [421/633], LR: 0.005000, Loss: 3.5875, top1: 10.9375\n",
      "Epoch [11/60], Iter [422/633], LR: 0.005000, Loss: 3.5539, top1: 15.6250\n",
      "Epoch [11/60], Iter [423/633], LR: 0.005000, Loss: 3.5859, top1: 18.7500\n",
      "Epoch [11/60], Iter [424/633], LR: 0.005000, Loss: 3.5813, top1: 17.1875\n",
      "Epoch [11/60], Iter [425/633], LR: 0.005000, Loss: 3.6086, top1: 9.3750\n",
      "Epoch [11/60], Iter [426/633], LR: 0.005000, Loss: 3.5904, top1: 15.6250\n",
      "Epoch [11/60], Iter [427/633], LR: 0.005000, Loss: 3.5619, top1: 18.7500\n",
      "Epoch [11/60], Iter [428/633], LR: 0.005000, Loss: 3.5190, top1: 25.0000\n",
      "Epoch [11/60], Iter [429/633], LR: 0.005000, Loss: 3.5588, top1: 12.5000\n",
      "Epoch [11/60], Iter [430/633], LR: 0.005000, Loss: 3.5924, top1: 12.5000\n",
      "Epoch [11/60], Iter [431/633], LR: 0.005000, Loss: 3.6364, top1: 12.5000\n",
      "Epoch [11/60], Iter [432/633], LR: 0.005000, Loss: 3.5623, top1: 10.9375\n",
      "Epoch [11/60], Iter [433/633], LR: 0.005000, Loss: 3.5928, top1: 12.5000\n",
      "Epoch [11/60], Iter [434/633], LR: 0.005000, Loss: 3.5563, top1: 18.7500\n",
      "Epoch [11/60], Iter [435/633], LR: 0.005000, Loss: 3.6065, top1: 10.9375\n",
      "Epoch [11/60], Iter [436/633], LR: 0.005000, Loss: 3.6003, top1: 9.3750\n",
      "Epoch [11/60], Iter [437/633], LR: 0.005000, Loss: 3.6323, top1: 9.3750\n",
      "Epoch [11/60], Iter [438/633], LR: 0.005000, Loss: 3.5858, top1: 10.9375\n",
      "Epoch [11/60], Iter [439/633], LR: 0.005000, Loss: 3.5869, top1: 9.3750\n",
      "Epoch [11/60], Iter [440/633], LR: 0.005000, Loss: 3.5661, top1: 15.6250\n",
      "Epoch [11/60], Iter [441/633], LR: 0.005000, Loss: 3.5756, top1: 17.1875\n",
      "Epoch [11/60], Iter [442/633], LR: 0.005000, Loss: 3.5971, top1: 12.5000\n",
      "Epoch [11/60], Iter [443/633], LR: 0.005000, Loss: 3.5298, top1: 14.0625\n",
      "Epoch [11/60], Iter [444/633], LR: 0.005000, Loss: 3.6117, top1: 7.8125\n",
      "Epoch [11/60], Iter [445/633], LR: 0.005000, Loss: 3.6111, top1: 7.8125\n",
      "Epoch [11/60], Iter [446/633], LR: 0.005000, Loss: 3.5638, top1: 12.5000\n",
      "Epoch [11/60], Iter [447/633], LR: 0.005000, Loss: 3.5374, top1: 17.1875\n",
      "Epoch [11/60], Iter [448/633], LR: 0.005000, Loss: 3.5385, top1: 20.3125\n",
      "Epoch [11/60], Iter [449/633], LR: 0.005000, Loss: 3.5591, top1: 15.6250\n",
      "Epoch [11/60], Iter [450/633], LR: 0.005000, Loss: 3.5365, top1: 23.4375\n",
      "Epoch [11/60], Iter [451/633], LR: 0.005000, Loss: 3.5725, top1: 15.6250\n",
      "Epoch [11/60], Iter [452/633], LR: 0.005000, Loss: 3.5911, top1: 14.0625\n",
      "Epoch [11/60], Iter [453/633], LR: 0.005000, Loss: 3.6297, top1: 10.9375\n",
      "Epoch [11/60], Iter [454/633], LR: 0.005000, Loss: 3.6059, top1: 10.9375\n",
      "Epoch [11/60], Iter [455/633], LR: 0.005000, Loss: 3.5689, top1: 18.7500\n",
      "Epoch [11/60], Iter [456/633], LR: 0.005000, Loss: 3.6189, top1: 9.3750\n",
      "Epoch [11/60], Iter [457/633], LR: 0.005000, Loss: 3.5443, top1: 18.7500\n",
      "Epoch [11/60], Iter [458/633], LR: 0.005000, Loss: 3.5667, top1: 12.5000\n",
      "Epoch [11/60], Iter [459/633], LR: 0.005000, Loss: 3.6005, top1: 14.0625\n",
      "Epoch [11/60], Iter [460/633], LR: 0.005000, Loss: 3.5343, top1: 26.5625\n",
      "Epoch [11/60], Iter [461/633], LR: 0.005000, Loss: 3.5853, top1: 12.5000\n",
      "Epoch [11/60], Iter [462/633], LR: 0.005000, Loss: 3.5304, top1: 20.3125\n",
      "Epoch [11/60], Iter [463/633], LR: 0.005000, Loss: 3.6011, top1: 17.1875\n",
      "Epoch [11/60], Iter [464/633], LR: 0.005000, Loss: 3.5611, top1: 14.0625\n",
      "Epoch [11/60], Iter [465/633], LR: 0.005000, Loss: 3.5637, top1: 17.1875\n",
      "Epoch [11/60], Iter [466/633], LR: 0.005000, Loss: 3.5987, top1: 9.3750\n",
      "Epoch [11/60], Iter [467/633], LR: 0.005000, Loss: 3.5953, top1: 9.3750\n",
      "Epoch [11/60], Iter [468/633], LR: 0.005000, Loss: 3.5864, top1: 12.5000\n",
      "Epoch [11/60], Iter [469/633], LR: 0.005000, Loss: 3.6285, top1: 9.3750\n",
      "Epoch [11/60], Iter [470/633], LR: 0.005000, Loss: 3.5812, top1: 15.6250\n",
      "Epoch [11/60], Iter [471/633], LR: 0.005000, Loss: 3.5738, top1: 10.9375\n",
      "Epoch [11/60], Iter [472/633], LR: 0.005000, Loss: 3.5907, top1: 7.8125\n",
      "Epoch [11/60], Iter [473/633], LR: 0.005000, Loss: 3.5579, top1: 20.3125\n",
      "Epoch [11/60], Iter [474/633], LR: 0.005000, Loss: 3.5929, top1: 10.9375\n",
      "Epoch [11/60], Iter [475/633], LR: 0.005000, Loss: 3.5060, top1: 21.8750\n",
      "Epoch [11/60], Iter [476/633], LR: 0.005000, Loss: 3.6066, top1: 7.8125\n",
      "Epoch [11/60], Iter [477/633], LR: 0.005000, Loss: 3.6265, top1: 7.8125\n",
      "Epoch [11/60], Iter [478/633], LR: 0.005000, Loss: 3.5655, top1: 7.8125\n",
      "Epoch [11/60], Iter [479/633], LR: 0.005000, Loss: 3.6223, top1: 10.9375\n",
      "Epoch [11/60], Iter [480/633], LR: 0.005000, Loss: 3.5708, top1: 12.5000\n",
      "Epoch [11/60], Iter [481/633], LR: 0.005000, Loss: 3.5850, top1: 18.7500\n",
      "Epoch [11/60], Iter [482/633], LR: 0.005000, Loss: 3.5657, top1: 15.6250\n",
      "Epoch [11/60], Iter [483/633], LR: 0.005000, Loss: 3.5315, top1: 20.3125\n",
      "Epoch [11/60], Iter [484/633], LR: 0.005000, Loss: 3.5605, top1: 15.6250\n",
      "Epoch [11/60], Iter [485/633], LR: 0.005000, Loss: 3.5470, top1: 18.7500\n",
      "Epoch [11/60], Iter [486/633], LR: 0.005000, Loss: 3.5803, top1: 10.9375\n",
      "Epoch [11/60], Iter [487/633], LR: 0.005000, Loss: 3.5764, top1: 15.6250\n",
      "Epoch [11/60], Iter [488/633], LR: 0.005000, Loss: 3.5282, top1: 26.5625\n",
      "Epoch [11/60], Iter [489/633], LR: 0.005000, Loss: 3.6002, top1: 15.6250\n",
      "Epoch [11/60], Iter [490/633], LR: 0.005000, Loss: 3.6182, top1: 7.8125\n",
      "Epoch [11/60], Iter [491/633], LR: 0.005000, Loss: 3.6281, top1: 10.9375\n",
      "Epoch [11/60], Iter [492/633], LR: 0.005000, Loss: 3.5751, top1: 12.5000\n",
      "Epoch [11/60], Iter [493/633], LR: 0.005000, Loss: 3.5580, top1: 17.1875\n",
      "Epoch [11/60], Iter [494/633], LR: 0.005000, Loss: 3.6484, top1: 6.2500\n",
      "Epoch [11/60], Iter [495/633], LR: 0.005000, Loss: 3.6001, top1: 17.1875\n",
      "Epoch [11/60], Iter [496/633], LR: 0.005000, Loss: 3.5510, top1: 17.1875\n",
      "Epoch [11/60], Iter [497/633], LR: 0.005000, Loss: 3.5885, top1: 15.6250\n",
      "Epoch [11/60], Iter [498/633], LR: 0.005000, Loss: 3.5963, top1: 9.3750\n",
      "Epoch [11/60], Iter [499/633], LR: 0.005000, Loss: 3.6052, top1: 3.1250\n",
      "Epoch [11/60], Iter [500/633], LR: 0.005000, Loss: 3.5915, top1: 9.3750\n",
      "Epoch [11/60], Iter [501/633], LR: 0.005000, Loss: 3.5707, top1: 18.7500\n",
      "Epoch [11/60], Iter [502/633], LR: 0.005000, Loss: 3.6121, top1: 9.3750\n",
      "Epoch [11/60], Iter [503/633], LR: 0.005000, Loss: 3.5901, top1: 10.9375\n",
      "Epoch [11/60], Iter [504/633], LR: 0.005000, Loss: 3.5677, top1: 17.1875\n",
      "Epoch [11/60], Iter [505/633], LR: 0.005000, Loss: 3.5391, top1: 18.7500\n",
      "Epoch [11/60], Iter [506/633], LR: 0.005000, Loss: 3.6512, top1: 4.6875\n",
      "Epoch [11/60], Iter [507/633], LR: 0.005000, Loss: 3.5474, top1: 17.1875\n",
      "Epoch [11/60], Iter [508/633], LR: 0.005000, Loss: 3.5804, top1: 17.1875\n",
      "Epoch [11/60], Iter [509/633], LR: 0.005000, Loss: 3.6078, top1: 9.3750\n",
      "Epoch [11/60], Iter [510/633], LR: 0.005000, Loss: 3.5376, top1: 23.4375\n",
      "Epoch [11/60], Iter [511/633], LR: 0.005000, Loss: 3.5708, top1: 17.1875\n",
      "Epoch [11/60], Iter [512/633], LR: 0.005000, Loss: 3.5769, top1: 10.9375\n",
      "Epoch [11/60], Iter [513/633], LR: 0.005000, Loss: 3.5934, top1: 15.6250\n",
      "Epoch [11/60], Iter [514/633], LR: 0.005000, Loss: 3.6148, top1: 9.3750\n",
      "Epoch [11/60], Iter [515/633], LR: 0.005000, Loss: 3.5485, top1: 12.5000\n",
      "Epoch [11/60], Iter [516/633], LR: 0.005000, Loss: 3.5492, top1: 17.1875\n",
      "Epoch [11/60], Iter [517/633], LR: 0.005000, Loss: 3.5972, top1: 6.2500\n",
      "Epoch [11/60], Iter [518/633], LR: 0.005000, Loss: 3.5348, top1: 17.1875\n",
      "Epoch [11/60], Iter [519/633], LR: 0.005000, Loss: 3.6116, top1: 15.6250\n",
      "Epoch [11/60], Iter [520/633], LR: 0.005000, Loss: 3.6021, top1: 12.5000\n",
      "Epoch [11/60], Iter [521/633], LR: 0.005000, Loss: 3.5990, top1: 14.0625\n",
      "Epoch [11/60], Iter [522/633], LR: 0.005000, Loss: 3.5726, top1: 14.0625\n",
      "Epoch [11/60], Iter [523/633], LR: 0.005000, Loss: 3.5781, top1: 15.6250\n",
      "Epoch [11/60], Iter [524/633], LR: 0.005000, Loss: 3.5256, top1: 18.7500\n",
      "Epoch [11/60], Iter [525/633], LR: 0.005000, Loss: 3.5777, top1: 18.7500\n",
      "Epoch [11/60], Iter [526/633], LR: 0.005000, Loss: 3.6079, top1: 7.8125\n",
      "Epoch [11/60], Iter [527/633], LR: 0.005000, Loss: 3.5784, top1: 6.2500\n",
      "Epoch [11/60], Iter [528/633], LR: 0.005000, Loss: 3.5842, top1: 15.6250\n",
      "Epoch [11/60], Iter [529/633], LR: 0.005000, Loss: 3.6160, top1: 10.9375\n",
      "Epoch [11/60], Iter [530/633], LR: 0.005000, Loss: 3.5871, top1: 9.3750\n",
      "Epoch [11/60], Iter [531/633], LR: 0.005000, Loss: 3.5757, top1: 14.0625\n",
      "Epoch [11/60], Iter [532/633], LR: 0.005000, Loss: 3.5603, top1: 17.1875\n",
      "Epoch [11/60], Iter [533/633], LR: 0.005000, Loss: 3.5742, top1: 15.6250\n",
      "Epoch [11/60], Iter [534/633], LR: 0.005000, Loss: 3.6121, top1: 14.0625\n",
      "Epoch [11/60], Iter [535/633], LR: 0.005000, Loss: 3.5236, top1: 18.7500\n",
      "Epoch [11/60], Iter [536/633], LR: 0.005000, Loss: 3.5165, top1: 20.3125\n",
      "Epoch [11/60], Iter [537/633], LR: 0.005000, Loss: 3.6187, top1: 6.2500\n",
      "Epoch [11/60], Iter [538/633], LR: 0.005000, Loss: 3.5883, top1: 10.9375\n",
      "Epoch [11/60], Iter [539/633], LR: 0.005000, Loss: 3.5886, top1: 14.0625\n",
      "Epoch [11/60], Iter [540/633], LR: 0.005000, Loss: 3.5957, top1: 10.9375\n",
      "Epoch [11/60], Iter [541/633], LR: 0.005000, Loss: 3.5686, top1: 14.0625\n",
      "Epoch [11/60], Iter [542/633], LR: 0.005000, Loss: 3.6214, top1: 9.3750\n",
      "Epoch [11/60], Iter [543/633], LR: 0.005000, Loss: 3.5664, top1: 17.1875\n",
      "Epoch [11/60], Iter [544/633], LR: 0.005000, Loss: 3.6009, top1: 12.5000\n",
      "Epoch [11/60], Iter [545/633], LR: 0.005000, Loss: 3.6008, top1: 7.8125\n",
      "Epoch [11/60], Iter [546/633], LR: 0.005000, Loss: 3.5823, top1: 10.9375\n",
      "Epoch [11/60], Iter [547/633], LR: 0.005000, Loss: 3.5401, top1: 21.8750\n",
      "Epoch [11/60], Iter [548/633], LR: 0.005000, Loss: 3.5976, top1: 15.6250\n",
      "Epoch [11/60], Iter [549/633], LR: 0.005000, Loss: 3.5731, top1: 17.1875\n",
      "Epoch [11/60], Iter [550/633], LR: 0.005000, Loss: 3.5648, top1: 18.7500\n",
      "Epoch [11/60], Iter [551/633], LR: 0.005000, Loss: 3.5399, top1: 18.7500\n",
      "Epoch [11/60], Iter [552/633], LR: 0.005000, Loss: 3.5915, top1: 12.5000\n",
      "Epoch [11/60], Iter [553/633], LR: 0.005000, Loss: 3.6118, top1: 6.2500\n",
      "Epoch [11/60], Iter [554/633], LR: 0.005000, Loss: 3.6197, top1: 7.8125\n",
      "Epoch [11/60], Iter [555/633], LR: 0.005000, Loss: 3.5457, top1: 18.7500\n",
      "Epoch [11/60], Iter [556/633], LR: 0.005000, Loss: 3.6162, top1: 9.3750\n",
      "Epoch [11/60], Iter [557/633], LR: 0.005000, Loss: 3.5480, top1: 18.7500\n",
      "Epoch [11/60], Iter [558/633], LR: 0.005000, Loss: 3.6240, top1: 4.6875\n",
      "Epoch [11/60], Iter [559/633], LR: 0.005000, Loss: 3.5626, top1: 17.1875\n",
      "Epoch [11/60], Iter [560/633], LR: 0.005000, Loss: 3.5640, top1: 14.0625\n",
      "Epoch [11/60], Iter [561/633], LR: 0.005000, Loss: 3.6304, top1: 6.2500\n",
      "Epoch [11/60], Iter [562/633], LR: 0.005000, Loss: 3.5561, top1: 20.3125\n",
      "Epoch [11/60], Iter [563/633], LR: 0.005000, Loss: 3.5937, top1: 14.0625\n",
      "Epoch [11/60], Iter [564/633], LR: 0.005000, Loss: 3.5888, top1: 10.9375\n",
      "Epoch [11/60], Iter [565/633], LR: 0.005000, Loss: 3.6035, top1: 15.6250\n",
      "Epoch [11/60], Iter [566/633], LR: 0.005000, Loss: 3.5627, top1: 14.0625\n",
      "Epoch [11/60], Iter [567/633], LR: 0.005000, Loss: 3.5859, top1: 9.3750\n",
      "Epoch [11/60], Iter [568/633], LR: 0.005000, Loss: 3.5161, top1: 25.0000\n",
      "Epoch [11/60], Iter [569/633], LR: 0.005000, Loss: 3.5744, top1: 10.9375\n",
      "Epoch [11/60], Iter [570/633], LR: 0.005000, Loss: 3.5390, top1: 20.3125\n",
      "Epoch [11/60], Iter [571/633], LR: 0.005000, Loss: 3.5274, top1: 15.6250\n",
      "Epoch [11/60], Iter [572/633], LR: 0.005000, Loss: 3.6049, top1: 9.3750\n",
      "Epoch [11/60], Iter [573/633], LR: 0.005000, Loss: 3.6339, top1: 4.6875\n",
      "Epoch [11/60], Iter [574/633], LR: 0.005000, Loss: 3.5487, top1: 20.3125\n",
      "Epoch [11/60], Iter [575/633], LR: 0.005000, Loss: 3.5559, top1: 12.5000\n",
      "Epoch [11/60], Iter [576/633], LR: 0.005000, Loss: 3.5485, top1: 21.8750\n",
      "Epoch [11/60], Iter [577/633], LR: 0.005000, Loss: 3.5935, top1: 12.5000\n",
      "Epoch [11/60], Iter [578/633], LR: 0.005000, Loss: 3.5499, top1: 15.6250\n",
      "Epoch [11/60], Iter [579/633], LR: 0.005000, Loss: 3.5336, top1: 20.3125\n",
      "Epoch [11/60], Iter [580/633], LR: 0.005000, Loss: 3.5724, top1: 17.1875\n",
      "Epoch [11/60], Iter [581/633], LR: 0.005000, Loss: 3.6106, top1: 14.0625\n",
      "Epoch [11/60], Iter [582/633], LR: 0.005000, Loss: 3.5975, top1: 14.0625\n",
      "Epoch [11/60], Iter [583/633], LR: 0.005000, Loss: 3.6190, top1: 4.6875\n",
      "Epoch [11/60], Iter [584/633], LR: 0.005000, Loss: 3.5508, top1: 15.6250\n",
      "Epoch [11/60], Iter [585/633], LR: 0.005000, Loss: 3.6140, top1: 12.5000\n",
      "Epoch [11/60], Iter [586/633], LR: 0.005000, Loss: 3.5567, top1: 14.0625\n",
      "Epoch [11/60], Iter [587/633], LR: 0.005000, Loss: 3.5878, top1: 18.7500\n",
      "Epoch [11/60], Iter [588/633], LR: 0.005000, Loss: 3.5606, top1: 18.7500\n",
      "Epoch [11/60], Iter [589/633], LR: 0.005000, Loss: 3.5686, top1: 17.1875\n",
      "Epoch [11/60], Iter [590/633], LR: 0.005000, Loss: 3.5651, top1: 15.6250\n",
      "Epoch [11/60], Iter [591/633], LR: 0.005000, Loss: 3.5742, top1: 12.5000\n",
      "Epoch [11/60], Iter [592/633], LR: 0.005000, Loss: 3.5909, top1: 10.9375\n",
      "Epoch [11/60], Iter [593/633], LR: 0.005000, Loss: 3.5686, top1: 21.8750\n",
      "Epoch [11/60], Iter [594/633], LR: 0.005000, Loss: 3.5424, top1: 14.0625\n",
      "Epoch [11/60], Iter [595/633], LR: 0.005000, Loss: 3.6549, top1: 6.2500\n",
      "Epoch [11/60], Iter [596/633], LR: 0.005000, Loss: 3.5476, top1: 15.6250\n",
      "Epoch [11/60], Iter [597/633], LR: 0.005000, Loss: 3.6138, top1: 7.8125\n",
      "Epoch [11/60], Iter [598/633], LR: 0.005000, Loss: 3.5742, top1: 12.5000\n",
      "Epoch [11/60], Iter [599/633], LR: 0.005000, Loss: 3.5611, top1: 15.6250\n",
      "Epoch [11/60], Iter [600/633], LR: 0.005000, Loss: 3.5656, top1: 14.0625\n",
      "Epoch [11/60], Iter [601/633], LR: 0.005000, Loss: 3.6073, top1: 10.9375\n",
      "Epoch [11/60], Iter [602/633], LR: 0.005000, Loss: 3.5419, top1: 20.3125\n",
      "Epoch [11/60], Iter [603/633], LR: 0.005000, Loss: 3.5750, top1: 14.0625\n",
      "Epoch [11/60], Iter [604/633], LR: 0.005000, Loss: 3.5142, top1: 23.4375\n",
      "Epoch [11/60], Iter [605/633], LR: 0.005000, Loss: 3.5847, top1: 9.3750\n",
      "Epoch [11/60], Iter [606/633], LR: 0.005000, Loss: 3.5850, top1: 9.3750\n",
      "Epoch [11/60], Iter [607/633], LR: 0.005000, Loss: 3.5750, top1: 14.0625\n",
      "Epoch [11/60], Iter [608/633], LR: 0.005000, Loss: 3.5858, top1: 12.5000\n",
      "Epoch [11/60], Iter [609/633], LR: 0.005000, Loss: 3.5822, top1: 18.7500\n",
      "Epoch [11/60], Iter [610/633], LR: 0.005000, Loss: 3.6258, top1: 4.6875\n",
      "Epoch [11/60], Iter [611/633], LR: 0.005000, Loss: 3.5855, top1: 12.5000\n",
      "Epoch [11/60], Iter [612/633], LR: 0.005000, Loss: 3.6157, top1: 12.5000\n",
      "Epoch [11/60], Iter [613/633], LR: 0.005000, Loss: 3.5197, top1: 18.7500\n",
      "Epoch [11/60], Iter [614/633], LR: 0.005000, Loss: 3.6069, top1: 4.6875\n",
      "Epoch [11/60], Iter [615/633], LR: 0.005000, Loss: 3.5231, top1: 23.4375\n",
      "Epoch [11/60], Iter [616/633], LR: 0.005000, Loss: 3.5503, top1: 14.0625\n",
      "Epoch [11/60], Iter [617/633], LR: 0.005000, Loss: 3.5728, top1: 10.9375\n",
      "Epoch [11/60], Iter [618/633], LR: 0.005000, Loss: 3.5805, top1: 10.9375\n",
      "Epoch [11/60], Iter [619/633], LR: 0.005000, Loss: 3.5873, top1: 10.9375\n",
      "Epoch [11/60], Iter [620/633], LR: 0.005000, Loss: 3.5807, top1: 12.5000\n",
      "Epoch [11/60], Iter [621/633], LR: 0.005000, Loss: 3.5727, top1: 14.0625\n",
      "Epoch [11/60], Iter [622/633], LR: 0.005000, Loss: 3.5933, top1: 15.6250\n",
      "Epoch [11/60], Iter [623/633], LR: 0.005000, Loss: 3.5817, top1: 15.6250\n",
      "Epoch [11/60], Iter [624/633], LR: 0.005000, Loss: 3.6266, top1: 4.6875\n",
      "Epoch [11/60], Iter [625/633], LR: 0.005000, Loss: 3.5981, top1: 10.9375\n",
      "Epoch [11/60], Iter [626/633], LR: 0.005000, Loss: 3.5144, top1: 25.0000\n",
      "Epoch [11/60], Iter [627/633], LR: 0.005000, Loss: 3.6348, top1: 7.8125\n",
      "Epoch [11/60], Iter [628/633], LR: 0.005000, Loss: 3.5560, top1: 10.9375\n",
      "Epoch [11/60], Iter [629/633], LR: 0.005000, Loss: 3.5513, top1: 17.1875\n",
      "Epoch [11/60], Iter [630/633], LR: 0.005000, Loss: 3.5923, top1: 10.9375\n",
      "Epoch [11/60], Iter [631/633], LR: 0.005000, Loss: 3.5885, top1: 10.9375\n",
      "Epoch [11/60], Iter [632/633], LR: 0.005000, Loss: 3.5243, top1: 21.8750\n",
      "Epoch [11/60], Iter [633/633], LR: 0.005000, Loss: 3.5642, top1: 17.1875\n",
      "Epoch [11/60], Iter [634/633], LR: 0.005000, Loss: 3.5485, top1: 16.1290\n",
      "Epoch [11/60], Val_Loss: 3.5837, Val_top1: 12.9181, best_top1: 14.6567\n",
      "epoch time: 4.414024392763774 min\n",
      "Epoch [12/60], Iter [1/633], LR: 0.005000, Loss: 3.5455, top1: 20.3125\n",
      "Epoch [12/60], Iter [2/633], LR: 0.005000, Loss: 3.5977, top1: 14.0625\n",
      "Epoch [12/60], Iter [3/633], LR: 0.005000, Loss: 3.5645, top1: 17.1875\n",
      "Epoch [12/60], Iter [4/633], LR: 0.005000, Loss: 3.5678, top1: 18.7500\n",
      "Epoch [12/60], Iter [5/633], LR: 0.005000, Loss: 3.5519, top1: 17.1875\n",
      "Epoch [12/60], Iter [6/633], LR: 0.005000, Loss: 3.6109, top1: 9.3750\n",
      "Epoch [12/60], Iter [7/633], LR: 0.005000, Loss: 3.5344, top1: 18.7500\n",
      "Epoch [12/60], Iter [8/633], LR: 0.005000, Loss: 3.6010, top1: 10.9375\n",
      "Epoch [12/60], Iter [9/633], LR: 0.005000, Loss: 3.6078, top1: 9.3750\n",
      "Epoch [12/60], Iter [10/633], LR: 0.005000, Loss: 3.5418, top1: 18.7500\n",
      "Epoch [12/60], Iter [11/633], LR: 0.005000, Loss: 3.5252, top1: 21.8750\n",
      "Epoch [12/60], Iter [12/633], LR: 0.005000, Loss: 3.5824, top1: 12.5000\n",
      "Epoch [12/60], Iter [13/633], LR: 0.005000, Loss: 3.5259, top1: 29.6875\n",
      "Epoch [12/60], Iter [14/633], LR: 0.005000, Loss: 3.6263, top1: 10.9375\n",
      "Epoch [12/60], Iter [15/633], LR: 0.005000, Loss: 3.6236, top1: 4.6875\n",
      "Epoch [12/60], Iter [16/633], LR: 0.005000, Loss: 3.5560, top1: 12.5000\n",
      "Epoch [12/60], Iter [17/633], LR: 0.005000, Loss: 3.5742, top1: 12.5000\n",
      "Epoch [12/60], Iter [18/633], LR: 0.005000, Loss: 3.5330, top1: 17.1875\n",
      "Epoch [12/60], Iter [19/633], LR: 0.005000, Loss: 3.5971, top1: 12.5000\n",
      "Epoch [12/60], Iter [20/633], LR: 0.005000, Loss: 3.5975, top1: 18.7500\n",
      "Epoch [12/60], Iter [21/633], LR: 0.005000, Loss: 3.5592, top1: 12.5000\n",
      "Epoch [12/60], Iter [22/633], LR: 0.005000, Loss: 3.6066, top1: 6.2500\n",
      "Epoch [12/60], Iter [23/633], LR: 0.005000, Loss: 3.5621, top1: 12.5000\n",
      "Epoch [12/60], Iter [24/633], LR: 0.005000, Loss: 3.5741, top1: 10.9375\n",
      "Epoch [12/60], Iter [25/633], LR: 0.005000, Loss: 3.5928, top1: 9.3750\n",
      "Epoch [12/60], Iter [26/633], LR: 0.005000, Loss: 3.5828, top1: 15.6250\n",
      "Epoch [12/60], Iter [27/633], LR: 0.005000, Loss: 3.5958, top1: 18.7500\n",
      "Epoch [12/60], Iter [28/633], LR: 0.005000, Loss: 3.5963, top1: 14.0625\n",
      "Epoch [12/60], Iter [29/633], LR: 0.005000, Loss: 3.6264, top1: 10.9375\n",
      "Epoch [12/60], Iter [30/633], LR: 0.005000, Loss: 3.5844, top1: 7.8125\n",
      "Epoch [12/60], Iter [31/633], LR: 0.005000, Loss: 3.5350, top1: 23.4375\n",
      "Epoch [12/60], Iter [32/633], LR: 0.005000, Loss: 3.5981, top1: 12.5000\n",
      "Epoch [12/60], Iter [33/633], LR: 0.005000, Loss: 3.5575, top1: 20.3125\n",
      "Epoch [12/60], Iter [34/633], LR: 0.005000, Loss: 3.5982, top1: 12.5000\n",
      "Epoch [12/60], Iter [35/633], LR: 0.005000, Loss: 3.6209, top1: 9.3750\n",
      "Epoch [12/60], Iter [36/633], LR: 0.005000, Loss: 3.5490, top1: 15.6250\n",
      "Epoch [12/60], Iter [37/633], LR: 0.005000, Loss: 3.6025, top1: 10.9375\n",
      "Epoch [12/60], Iter [38/633], LR: 0.005000, Loss: 3.5957, top1: 12.5000\n",
      "Epoch [12/60], Iter [39/633], LR: 0.005000, Loss: 3.5488, top1: 15.6250\n",
      "Epoch [12/60], Iter [40/633], LR: 0.005000, Loss: 3.6222, top1: 7.8125\n",
      "Epoch [12/60], Iter [41/633], LR: 0.005000, Loss: 3.6111, top1: 9.3750\n",
      "Epoch [12/60], Iter [42/633], LR: 0.005000, Loss: 3.5800, top1: 10.9375\n",
      "Epoch [12/60], Iter [43/633], LR: 0.005000, Loss: 3.5141, top1: 28.1250\n",
      "Epoch [12/60], Iter [44/633], LR: 0.005000, Loss: 3.5854, top1: 14.0625\n",
      "Epoch [12/60], Iter [45/633], LR: 0.005000, Loss: 3.5814, top1: 12.5000\n",
      "Epoch [12/60], Iter [46/633], LR: 0.005000, Loss: 3.6096, top1: 7.8125\n",
      "Epoch [12/60], Iter [47/633], LR: 0.005000, Loss: 3.6063, top1: 9.3750\n",
      "Epoch [12/60], Iter [48/633], LR: 0.005000, Loss: 3.5772, top1: 14.0625\n",
      "Epoch [12/60], Iter [49/633], LR: 0.005000, Loss: 3.5681, top1: 15.6250\n",
      "Epoch [12/60], Iter [50/633], LR: 0.005000, Loss: 3.6265, top1: 10.9375\n",
      "Epoch [12/60], Iter [51/633], LR: 0.005000, Loss: 3.5681, top1: 14.0625\n",
      "Epoch [12/60], Iter [52/633], LR: 0.005000, Loss: 3.6029, top1: 12.5000\n",
      "Epoch [12/60], Iter [53/633], LR: 0.005000, Loss: 3.5755, top1: 10.9375\n",
      "Epoch [12/60], Iter [54/633], LR: 0.005000, Loss: 3.5650, top1: 21.8750\n",
      "Epoch [12/60], Iter [55/633], LR: 0.005000, Loss: 3.5546, top1: 9.3750\n",
      "Epoch [12/60], Iter [56/633], LR: 0.005000, Loss: 3.5474, top1: 17.1875\n",
      "Epoch [12/60], Iter [57/633], LR: 0.005000, Loss: 3.5761, top1: 15.6250\n",
      "Epoch [12/60], Iter [58/633], LR: 0.005000, Loss: 3.5868, top1: 12.5000\n",
      "Epoch [12/60], Iter [59/633], LR: 0.005000, Loss: 3.5779, top1: 15.6250\n",
      "Epoch [12/60], Iter [60/633], LR: 0.005000, Loss: 3.5825, top1: 15.6250\n",
      "Epoch [12/60], Iter [61/633], LR: 0.005000, Loss: 3.5976, top1: 14.0625\n",
      "Epoch [12/60], Iter [62/633], LR: 0.005000, Loss: 3.5670, top1: 12.5000\n",
      "Epoch [12/60], Iter [63/633], LR: 0.005000, Loss: 3.6381, top1: 14.0625\n",
      "Epoch [12/60], Iter [64/633], LR: 0.005000, Loss: 3.5964, top1: 15.6250\n",
      "Epoch [12/60], Iter [65/633], LR: 0.005000, Loss: 3.5681, top1: 20.3125\n",
      "Epoch [12/60], Iter [66/633], LR: 0.005000, Loss: 3.6165, top1: 9.3750\n",
      "Epoch [12/60], Iter [67/633], LR: 0.005000, Loss: 3.5534, top1: 18.7500\n",
      "Epoch [12/60], Iter [68/633], LR: 0.005000, Loss: 3.5847, top1: 12.5000\n",
      "Epoch [12/60], Iter [69/633], LR: 0.005000, Loss: 3.6276, top1: 14.0625\n",
      "Epoch [12/60], Iter [70/633], LR: 0.005000, Loss: 3.6111, top1: 12.5000\n",
      "Epoch [12/60], Iter [71/633], LR: 0.005000, Loss: 3.5672, top1: 20.3125\n",
      "Epoch [12/60], Iter [72/633], LR: 0.005000, Loss: 3.5985, top1: 14.0625\n",
      "Epoch [12/60], Iter [73/633], LR: 0.005000, Loss: 3.5867, top1: 12.5000\n",
      "Epoch [12/60], Iter [74/633], LR: 0.005000, Loss: 3.5571, top1: 20.3125\n",
      "Epoch [12/60], Iter [75/633], LR: 0.005000, Loss: 3.5374, top1: 14.0625\n",
      "Epoch [12/60], Iter [76/633], LR: 0.005000, Loss: 3.6069, top1: 12.5000\n",
      "Epoch [12/60], Iter [77/633], LR: 0.005000, Loss: 3.5899, top1: 14.0625\n",
      "Epoch [12/60], Iter [78/633], LR: 0.005000, Loss: 3.5828, top1: 12.5000\n",
      "Epoch [12/60], Iter [79/633], LR: 0.005000, Loss: 3.6041, top1: 14.0625\n",
      "Epoch [12/60], Iter [80/633], LR: 0.005000, Loss: 3.5763, top1: 12.5000\n",
      "Epoch [12/60], Iter [81/633], LR: 0.005000, Loss: 3.6175, top1: 9.3750\n",
      "Epoch [12/60], Iter [82/633], LR: 0.005000, Loss: 3.5315, top1: 25.0000\n",
      "Epoch [12/60], Iter [83/633], LR: 0.005000, Loss: 3.5981, top1: 14.0625\n",
      "Epoch [12/60], Iter [84/633], LR: 0.005000, Loss: 3.6123, top1: 14.0625\n",
      "Epoch [12/60], Iter [85/633], LR: 0.005000, Loss: 3.5377, top1: 17.1875\n",
      "Epoch [12/60], Iter [86/633], LR: 0.005000, Loss: 3.5664, top1: 15.6250\n",
      "Epoch [12/60], Iter [87/633], LR: 0.005000, Loss: 3.5371, top1: 15.6250\n",
      "Epoch [12/60], Iter [88/633], LR: 0.005000, Loss: 3.5791, top1: 15.6250\n",
      "Epoch [12/60], Iter [89/633], LR: 0.005000, Loss: 3.5553, top1: 25.0000\n",
      "Epoch [12/60], Iter [90/633], LR: 0.005000, Loss: 3.5902, top1: 14.0625\n",
      "Epoch [12/60], Iter [91/633], LR: 0.005000, Loss: 3.5660, top1: 14.0625\n",
      "Epoch [12/60], Iter [92/633], LR: 0.005000, Loss: 3.5482, top1: 18.7500\n",
      "Epoch [12/60], Iter [93/633], LR: 0.005000, Loss: 3.5532, top1: 17.1875\n",
      "Epoch [12/60], Iter [94/633], LR: 0.005000, Loss: 3.5576, top1: 14.0625\n",
      "Epoch [12/60], Iter [95/633], LR: 0.005000, Loss: 3.5387, top1: 15.6250\n",
      "Epoch [12/60], Iter [96/633], LR: 0.005000, Loss: 3.5685, top1: 12.5000\n",
      "Epoch [12/60], Iter [97/633], LR: 0.005000, Loss: 3.5887, top1: 14.0625\n",
      "Epoch [12/60], Iter [98/633], LR: 0.005000, Loss: 3.5763, top1: 14.0625\n",
      "Epoch [12/60], Iter [99/633], LR: 0.005000, Loss: 3.5948, top1: 14.0625\n",
      "Epoch [12/60], Iter [100/633], LR: 0.005000, Loss: 3.5645, top1: 18.7500\n",
      "Epoch [12/60], Iter [101/633], LR: 0.005000, Loss: 3.5869, top1: 9.3750\n",
      "Epoch [12/60], Iter [102/633], LR: 0.005000, Loss: 3.5254, top1: 18.7500\n",
      "Epoch [12/60], Iter [103/633], LR: 0.005000, Loss: 3.5845, top1: 9.3750\n",
      "Epoch [12/60], Iter [104/633], LR: 0.005000, Loss: 3.6151, top1: 14.0625\n",
      "Epoch [12/60], Iter [105/633], LR: 0.005000, Loss: 3.6220, top1: 10.9375\n",
      "Epoch [12/60], Iter [106/633], LR: 0.005000, Loss: 3.6140, top1: 12.5000\n",
      "Epoch [12/60], Iter [107/633], LR: 0.005000, Loss: 3.6441, top1: 6.2500\n",
      "Epoch [12/60], Iter [108/633], LR: 0.005000, Loss: 3.5959, top1: 12.5000\n",
      "Epoch [12/60], Iter [109/633], LR: 0.005000, Loss: 3.5927, top1: 10.9375\n",
      "Epoch [12/60], Iter [110/633], LR: 0.005000, Loss: 3.5887, top1: 9.3750\n",
      "Epoch [12/60], Iter [111/633], LR: 0.005000, Loss: 3.5453, top1: 18.7500\n",
      "Epoch [12/60], Iter [112/633], LR: 0.005000, Loss: 3.6162, top1: 9.3750\n",
      "Epoch [12/60], Iter [113/633], LR: 0.005000, Loss: 3.6028, top1: 7.8125\n",
      "Epoch [12/60], Iter [114/633], LR: 0.005000, Loss: 3.5531, top1: 15.6250\n",
      "Epoch [12/60], Iter [115/633], LR: 0.005000, Loss: 3.5946, top1: 17.1875\n",
      "Epoch [12/60], Iter [116/633], LR: 0.005000, Loss: 3.5285, top1: 20.3125\n",
      "Epoch [12/60], Iter [117/633], LR: 0.005000, Loss: 3.5651, top1: 14.0625\n",
      "Epoch [12/60], Iter [118/633], LR: 0.005000, Loss: 3.6009, top1: 12.5000\n",
      "Epoch [12/60], Iter [119/633], LR: 0.005000, Loss: 3.5702, top1: 10.9375\n",
      "Epoch [12/60], Iter [120/633], LR: 0.005000, Loss: 3.5942, top1: 12.5000\n",
      "Epoch [12/60], Iter [121/633], LR: 0.005000, Loss: 3.5614, top1: 15.6250\n",
      "Epoch [12/60], Iter [122/633], LR: 0.005000, Loss: 3.6168, top1: 7.8125\n",
      "Epoch [12/60], Iter [123/633], LR: 0.005000, Loss: 3.5828, top1: 12.5000\n",
      "Epoch [12/60], Iter [124/633], LR: 0.005000, Loss: 3.5372, top1: 17.1875\n",
      "Epoch [12/60], Iter [125/633], LR: 0.005000, Loss: 3.5327, top1: 25.0000\n",
      "Epoch [12/60], Iter [126/633], LR: 0.005000, Loss: 3.5748, top1: 14.0625\n",
      "Epoch [12/60], Iter [127/633], LR: 0.005000, Loss: 3.5566, top1: 15.6250\n",
      "Epoch [12/60], Iter [128/633], LR: 0.005000, Loss: 3.5467, top1: 17.1875\n",
      "Epoch [12/60], Iter [129/633], LR: 0.005000, Loss: 3.5667, top1: 12.5000\n",
      "Epoch [12/60], Iter [130/633], LR: 0.005000, Loss: 3.5816, top1: 14.0625\n",
      "Epoch [12/60], Iter [131/633], LR: 0.005000, Loss: 3.6089, top1: 12.5000\n",
      "Epoch [12/60], Iter [132/633], LR: 0.005000, Loss: 3.6058, top1: 9.3750\n",
      "Epoch [12/60], Iter [133/633], LR: 0.005000, Loss: 3.5680, top1: 17.1875\n",
      "Epoch [12/60], Iter [134/633], LR: 0.005000, Loss: 3.5576, top1: 12.5000\n",
      "Epoch [12/60], Iter [135/633], LR: 0.005000, Loss: 3.5728, top1: 18.7500\n",
      "Epoch [12/60], Iter [136/633], LR: 0.005000, Loss: 3.6020, top1: 7.8125\n",
      "Epoch [12/60], Iter [137/633], LR: 0.005000, Loss: 3.5798, top1: 12.5000\n",
      "Epoch [12/60], Iter [138/633], LR: 0.005000, Loss: 3.6293, top1: 7.8125\n",
      "Epoch [12/60], Iter [139/633], LR: 0.005000, Loss: 3.5870, top1: 14.0625\n",
      "Epoch [12/60], Iter [140/633], LR: 0.005000, Loss: 3.6051, top1: 17.1875\n",
      "Epoch [12/60], Iter [141/633], LR: 0.005000, Loss: 3.5254, top1: 23.4375\n",
      "Epoch [12/60], Iter [142/633], LR: 0.005000, Loss: 3.6062, top1: 10.9375\n",
      "Epoch [12/60], Iter [143/633], LR: 0.005000, Loss: 3.5720, top1: 23.4375\n",
      "Epoch [12/60], Iter [144/633], LR: 0.005000, Loss: 3.5972, top1: 14.0625\n",
      "Epoch [12/60], Iter [145/633], LR: 0.005000, Loss: 3.5031, top1: 20.3125\n",
      "Epoch [12/60], Iter [146/633], LR: 0.005000, Loss: 3.5975, top1: 12.5000\n",
      "Epoch [12/60], Iter [147/633], LR: 0.005000, Loss: 3.5979, top1: 12.5000\n",
      "Epoch [12/60], Iter [148/633], LR: 0.005000, Loss: 3.5791, top1: 7.8125\n",
      "Epoch [12/60], Iter [149/633], LR: 0.005000, Loss: 3.6459, top1: 4.6875\n",
      "Epoch [12/60], Iter [150/633], LR: 0.005000, Loss: 3.5274, top1: 17.1875\n",
      "Epoch [12/60], Iter [151/633], LR: 0.005000, Loss: 3.5291, top1: 17.1875\n",
      "Epoch [12/60], Iter [152/633], LR: 0.005000, Loss: 3.6119, top1: 12.5000\n",
      "Epoch [12/60], Iter [153/633], LR: 0.005000, Loss: 3.5945, top1: 4.6875\n",
      "Epoch [12/60], Iter [154/633], LR: 0.005000, Loss: 3.5656, top1: 15.6250\n",
      "Epoch [12/60], Iter [155/633], LR: 0.005000, Loss: 3.5774, top1: 15.6250\n",
      "Epoch [12/60], Iter [156/633], LR: 0.005000, Loss: 3.5858, top1: 14.0625\n",
      "Epoch [12/60], Iter [157/633], LR: 0.005000, Loss: 3.5531, top1: 12.5000\n",
      "Epoch [12/60], Iter [158/633], LR: 0.005000, Loss: 3.5556, top1: 15.6250\n",
      "Epoch [12/60], Iter [159/633], LR: 0.005000, Loss: 3.5528, top1: 17.1875\n",
      "Epoch [12/60], Iter [160/633], LR: 0.005000, Loss: 3.5649, top1: 15.6250\n",
      "Epoch [12/60], Iter [161/633], LR: 0.005000, Loss: 3.5906, top1: 15.6250\n",
      "Epoch [12/60], Iter [162/633], LR: 0.005000, Loss: 3.5896, top1: 12.5000\n",
      "Epoch [12/60], Iter [163/633], LR: 0.005000, Loss: 3.6189, top1: 7.8125\n",
      "Epoch [12/60], Iter [164/633], LR: 0.005000, Loss: 3.6487, top1: 3.1250\n",
      "Epoch [12/60], Iter [165/633], LR: 0.005000, Loss: 3.5723, top1: 15.6250\n",
      "Epoch [12/60], Iter [166/633], LR: 0.005000, Loss: 3.5590, top1: 17.1875\n",
      "Epoch [12/60], Iter [167/633], LR: 0.005000, Loss: 3.5844, top1: 12.5000\n",
      "Epoch [12/60], Iter [168/633], LR: 0.005000, Loss: 3.5549, top1: 25.0000\n",
      "Epoch [12/60], Iter [169/633], LR: 0.005000, Loss: 3.5939, top1: 7.8125\n",
      "Epoch [12/60], Iter [170/633], LR: 0.005000, Loss: 3.5509, top1: 15.6250\n",
      "Epoch [12/60], Iter [171/633], LR: 0.005000, Loss: 3.5333, top1: 17.1875\n",
      "Epoch [12/60], Iter [172/633], LR: 0.005000, Loss: 3.5617, top1: 14.0625\n",
      "Epoch [12/60], Iter [173/633], LR: 0.005000, Loss: 3.6061, top1: 15.6250\n",
      "Epoch [12/60], Iter [174/633], LR: 0.005000, Loss: 3.6026, top1: 14.0625\n",
      "Epoch [12/60], Iter [175/633], LR: 0.005000, Loss: 3.5838, top1: 14.0625\n",
      "Epoch [12/60], Iter [176/633], LR: 0.005000, Loss: 3.5828, top1: 12.5000\n",
      "Epoch [12/60], Iter [177/633], LR: 0.005000, Loss: 3.5630, top1: 14.0625\n",
      "Epoch [12/60], Iter [178/633], LR: 0.005000, Loss: 3.5805, top1: 18.7500\n",
      "Epoch [12/60], Iter [179/633], LR: 0.005000, Loss: 3.6015, top1: 6.2500\n",
      "Epoch [12/60], Iter [180/633], LR: 0.005000, Loss: 3.5845, top1: 12.5000\n",
      "Epoch [12/60], Iter [181/633], LR: 0.005000, Loss: 3.6127, top1: 9.3750\n",
      "Epoch [12/60], Iter [182/633], LR: 0.005000, Loss: 3.5835, top1: 12.5000\n",
      "Epoch [12/60], Iter [183/633], LR: 0.005000, Loss: 3.5525, top1: 18.7500\n",
      "Epoch [12/60], Iter [184/633], LR: 0.005000, Loss: 3.5742, top1: 17.1875\n",
      "Epoch [12/60], Iter [185/633], LR: 0.005000, Loss: 3.5863, top1: 10.9375\n",
      "Epoch [12/60], Iter [186/633], LR: 0.005000, Loss: 3.5822, top1: 15.6250\n",
      "Epoch [12/60], Iter [187/633], LR: 0.005000, Loss: 3.5313, top1: 25.0000\n",
      "Epoch [12/60], Iter [188/633], LR: 0.005000, Loss: 3.5724, top1: 18.7500\n",
      "Epoch [12/60], Iter [189/633], LR: 0.005000, Loss: 3.6048, top1: 12.5000\n",
      "Epoch [12/60], Iter [190/633], LR: 0.005000, Loss: 3.5095, top1: 26.5625\n",
      "Epoch [12/60], Iter [191/633], LR: 0.005000, Loss: 3.5669, top1: 12.5000\n",
      "Epoch [12/60], Iter [192/633], LR: 0.005000, Loss: 3.5673, top1: 14.0625\n",
      "Epoch [12/60], Iter [193/633], LR: 0.005000, Loss: 3.5243, top1: 28.1250\n",
      "Epoch [12/60], Iter [194/633], LR: 0.005000, Loss: 3.6066, top1: 7.8125\n",
      "Epoch [12/60], Iter [195/633], LR: 0.005000, Loss: 3.5848, top1: 15.6250\n",
      "Epoch [12/60], Iter [196/633], LR: 0.005000, Loss: 3.5846, top1: 20.3125\n",
      "Epoch [12/60], Iter [197/633], LR: 0.005000, Loss: 3.6427, top1: 10.9375\n",
      "Epoch [12/60], Iter [198/633], LR: 0.005000, Loss: 3.5689, top1: 12.5000\n",
      "Epoch [12/60], Iter [199/633], LR: 0.005000, Loss: 3.5770, top1: 15.6250\n",
      "Epoch [12/60], Iter [200/633], LR: 0.005000, Loss: 3.6002, top1: 9.3750\n",
      "Epoch [12/60], Iter [201/633], LR: 0.005000, Loss: 3.5528, top1: 17.1875\n",
      "Epoch [12/60], Iter [202/633], LR: 0.005000, Loss: 3.5565, top1: 14.0625\n",
      "Epoch [12/60], Iter [203/633], LR: 0.005000, Loss: 3.5765, top1: 18.7500\n",
      "Epoch [12/60], Iter [204/633], LR: 0.005000, Loss: 3.5760, top1: 14.0625\n",
      "Epoch [12/60], Iter [205/633], LR: 0.005000, Loss: 3.5170, top1: 26.5625\n",
      "Epoch [12/60], Iter [206/633], LR: 0.005000, Loss: 3.5708, top1: 20.3125\n",
      "Epoch [12/60], Iter [207/633], LR: 0.005000, Loss: 3.5675, top1: 14.0625\n",
      "Epoch [12/60], Iter [208/633], LR: 0.005000, Loss: 3.5712, top1: 12.5000\n",
      "Epoch [12/60], Iter [209/633], LR: 0.005000, Loss: 3.5682, top1: 14.0625\n",
      "Epoch [12/60], Iter [210/633], LR: 0.005000, Loss: 3.5887, top1: 14.0625\n",
      "Epoch [12/60], Iter [211/633], LR: 0.005000, Loss: 3.5622, top1: 21.8750\n",
      "Epoch [12/60], Iter [212/633], LR: 0.005000, Loss: 3.5743, top1: 17.1875\n",
      "Epoch [12/60], Iter [213/633], LR: 0.005000, Loss: 3.5274, top1: 18.7500\n",
      "Epoch [12/60], Iter [214/633], LR: 0.005000, Loss: 3.5555, top1: 15.6250\n",
      "Epoch [12/60], Iter [215/633], LR: 0.005000, Loss: 3.5661, top1: 17.1875\n",
      "Epoch [12/60], Iter [216/633], LR: 0.005000, Loss: 3.5285, top1: 20.3125\n",
      "Epoch [12/60], Iter [217/633], LR: 0.005000, Loss: 3.5997, top1: 12.5000\n",
      "Epoch [12/60], Iter [218/633], LR: 0.005000, Loss: 3.5584, top1: 17.1875\n",
      "Epoch [12/60], Iter [219/633], LR: 0.005000, Loss: 3.5422, top1: 25.0000\n",
      "Epoch [12/60], Iter [220/633], LR: 0.005000, Loss: 3.5988, top1: 10.9375\n",
      "Epoch [12/60], Iter [221/633], LR: 0.005000, Loss: 3.5520, top1: 20.3125\n",
      "Epoch [12/60], Iter [222/633], LR: 0.005000, Loss: 3.5816, top1: 15.6250\n",
      "Epoch [12/60], Iter [223/633], LR: 0.005000, Loss: 3.6544, top1: 9.3750\n",
      "Epoch [12/60], Iter [224/633], LR: 0.005000, Loss: 3.5832, top1: 10.9375\n",
      "Epoch [12/60], Iter [225/633], LR: 0.005000, Loss: 3.5615, top1: 14.0625\n",
      "Epoch [12/60], Iter [226/633], LR: 0.005000, Loss: 3.5644, top1: 15.6250\n",
      "Epoch [12/60], Iter [227/633], LR: 0.005000, Loss: 3.5813, top1: 9.3750\n",
      "Epoch [12/60], Iter [228/633], LR: 0.005000, Loss: 3.6253, top1: 9.3750\n",
      "Epoch [12/60], Iter [229/633], LR: 0.005000, Loss: 3.5616, top1: 20.3125\n",
      "Epoch [12/60], Iter [230/633], LR: 0.005000, Loss: 3.5969, top1: 12.5000\n",
      "Epoch [12/60], Iter [231/633], LR: 0.005000, Loss: 3.5605, top1: 14.0625\n",
      "Epoch [12/60], Iter [232/633], LR: 0.005000, Loss: 3.6011, top1: 10.9375\n",
      "Epoch [12/60], Iter [233/633], LR: 0.005000, Loss: 3.6192, top1: 9.3750\n",
      "Epoch [12/60], Iter [234/633], LR: 0.005000, Loss: 3.6040, top1: 12.5000\n",
      "Epoch [12/60], Iter [235/633], LR: 0.005000, Loss: 3.5858, top1: 17.1875\n",
      "Epoch [12/60], Iter [236/633], LR: 0.005000, Loss: 3.5993, top1: 9.3750\n",
      "Epoch [12/60], Iter [237/633], LR: 0.005000, Loss: 3.6210, top1: 9.3750\n",
      "Epoch [12/60], Iter [238/633], LR: 0.005000, Loss: 3.5784, top1: 14.0625\n",
      "Epoch [12/60], Iter [239/633], LR: 0.005000, Loss: 3.5671, top1: 17.1875\n",
      "Epoch [12/60], Iter [240/633], LR: 0.005000, Loss: 3.5526, top1: 20.3125\n",
      "Epoch [12/60], Iter [241/633], LR: 0.005000, Loss: 3.5622, top1: 21.8750\n",
      "Epoch [12/60], Iter [242/633], LR: 0.005000, Loss: 3.5646, top1: 14.0625\n",
      "Epoch [12/60], Iter [243/633], LR: 0.005000, Loss: 3.5732, top1: 12.5000\n",
      "Epoch [12/60], Iter [244/633], LR: 0.005000, Loss: 3.5640, top1: 18.7500\n",
      "Epoch [12/60], Iter [245/633], LR: 0.005000, Loss: 3.5898, top1: 12.5000\n",
      "Epoch [12/60], Iter [246/633], LR: 0.005000, Loss: 3.5817, top1: 20.3125\n",
      "Epoch [12/60], Iter [247/633], LR: 0.005000, Loss: 3.5595, top1: 14.0625\n",
      "Epoch [12/60], Iter [248/633], LR: 0.005000, Loss: 3.5591, top1: 20.3125\n",
      "Epoch [12/60], Iter [249/633], LR: 0.005000, Loss: 3.6266, top1: 4.6875\n",
      "Epoch [12/60], Iter [250/633], LR: 0.005000, Loss: 3.5623, top1: 12.5000\n",
      "Epoch [12/60], Iter [251/633], LR: 0.005000, Loss: 3.5666, top1: 12.5000\n",
      "Epoch [12/60], Iter [252/633], LR: 0.005000, Loss: 3.6089, top1: 14.0625\n",
      "Epoch [12/60], Iter [253/633], LR: 0.005000, Loss: 3.5172, top1: 25.0000\n",
      "Epoch [12/60], Iter [254/633], LR: 0.005000, Loss: 3.5320, top1: 15.6250\n",
      "Epoch [12/60], Iter [255/633], LR: 0.005000, Loss: 3.5085, top1: 21.8750\n",
      "Epoch [12/60], Iter [256/633], LR: 0.005000, Loss: 3.5176, top1: 20.3125\n",
      "Epoch [12/60], Iter [257/633], LR: 0.005000, Loss: 3.5533, top1: 18.7500\n",
      "Epoch [12/60], Iter [258/633], LR: 0.005000, Loss: 3.5564, top1: 18.7500\n",
      "Epoch [12/60], Iter [259/633], LR: 0.005000, Loss: 3.5758, top1: 12.5000\n",
      "Epoch [12/60], Iter [260/633], LR: 0.005000, Loss: 3.5779, top1: 10.9375\n",
      "Epoch [12/60], Iter [261/633], LR: 0.005000, Loss: 3.5862, top1: 12.5000\n",
      "Epoch [12/60], Iter [262/633], LR: 0.005000, Loss: 3.5897, top1: 17.1875\n",
      "Epoch [12/60], Iter [263/633], LR: 0.005000, Loss: 3.5891, top1: 9.3750\n",
      "Epoch [12/60], Iter [264/633], LR: 0.005000, Loss: 3.5773, top1: 14.0625\n",
      "Epoch [12/60], Iter [265/633], LR: 0.005000, Loss: 3.5665, top1: 14.0625\n",
      "Epoch [12/60], Iter [266/633], LR: 0.005000, Loss: 3.5966, top1: 12.5000\n",
      "Epoch [12/60], Iter [267/633], LR: 0.005000, Loss: 3.5685, top1: 15.6250\n",
      "Epoch [12/60], Iter [268/633], LR: 0.005000, Loss: 3.6091, top1: 7.8125\n",
      "Epoch [12/60], Iter [269/633], LR: 0.005000, Loss: 3.5810, top1: 14.0625\n",
      "Epoch [12/60], Iter [270/633], LR: 0.005000, Loss: 3.5585, top1: 12.5000\n",
      "Epoch [12/60], Iter [271/633], LR: 0.005000, Loss: 3.5649, top1: 17.1875\n",
      "Epoch [12/60], Iter [272/633], LR: 0.005000, Loss: 3.6097, top1: 14.0625\n",
      "Epoch [12/60], Iter [273/633], LR: 0.005000, Loss: 3.5991, top1: 7.8125\n",
      "Epoch [12/60], Iter [274/633], LR: 0.005000, Loss: 3.5781, top1: 15.6250\n",
      "Epoch [12/60], Iter [275/633], LR: 0.005000, Loss: 3.5694, top1: 15.6250\n",
      "Epoch [12/60], Iter [276/633], LR: 0.005000, Loss: 3.5683, top1: 17.1875\n",
      "Epoch [12/60], Iter [277/633], LR: 0.005000, Loss: 3.5667, top1: 14.0625\n",
      "Epoch [12/60], Iter [278/633], LR: 0.005000, Loss: 3.5873, top1: 10.9375\n",
      "Epoch [12/60], Iter [279/633], LR: 0.005000, Loss: 3.6231, top1: 14.0625\n",
      "Epoch [12/60], Iter [280/633], LR: 0.005000, Loss: 3.6315, top1: 7.8125\n",
      "Epoch [12/60], Iter [281/633], LR: 0.005000, Loss: 3.5763, top1: 15.6250\n",
      "Epoch [12/60], Iter [282/633], LR: 0.005000, Loss: 3.5523, top1: 17.1875\n",
      "Epoch [12/60], Iter [283/633], LR: 0.005000, Loss: 3.5950, top1: 10.9375\n",
      "Epoch [12/60], Iter [284/633], LR: 0.005000, Loss: 3.6386, top1: 7.8125\n",
      "Epoch [12/60], Iter [285/633], LR: 0.005000, Loss: 3.5389, top1: 21.8750\n",
      "Epoch [12/60], Iter [286/633], LR: 0.005000, Loss: 3.6046, top1: 12.5000\n",
      "Epoch [12/60], Iter [287/633], LR: 0.005000, Loss: 3.5933, top1: 15.6250\n",
      "Epoch [12/60], Iter [288/633], LR: 0.005000, Loss: 3.5739, top1: 14.0625\n",
      "Epoch [12/60], Iter [289/633], LR: 0.005000, Loss: 3.5109, top1: 18.7500\n",
      "Epoch [12/60], Iter [290/633], LR: 0.005000, Loss: 3.5744, top1: 20.3125\n",
      "Epoch [12/60], Iter [291/633], LR: 0.005000, Loss: 3.5751, top1: 12.5000\n",
      "Epoch [12/60], Iter [292/633], LR: 0.005000, Loss: 3.5503, top1: 15.6250\n",
      "Epoch [12/60], Iter [293/633], LR: 0.005000, Loss: 3.5908, top1: 10.9375\n",
      "Epoch [12/60], Iter [294/633], LR: 0.005000, Loss: 3.5654, top1: 15.6250\n",
      "Epoch [12/60], Iter [295/633], LR: 0.005000, Loss: 3.5181, top1: 20.3125\n",
      "Epoch [12/60], Iter [296/633], LR: 0.005000, Loss: 3.5640, top1: 15.6250\n",
      "Epoch [12/60], Iter [297/633], LR: 0.005000, Loss: 3.5917, top1: 12.5000\n",
      "Epoch [12/60], Iter [298/633], LR: 0.005000, Loss: 3.5231, top1: 18.7500\n",
      "Epoch [12/60], Iter [299/633], LR: 0.005000, Loss: 3.5583, top1: 10.9375\n",
      "Epoch [12/60], Iter [300/633], LR: 0.005000, Loss: 3.6017, top1: 10.9375\n",
      "Epoch [12/60], Iter [301/633], LR: 0.005000, Loss: 3.5720, top1: 7.8125\n",
      "Epoch [12/60], Iter [302/633], LR: 0.005000, Loss: 3.6105, top1: 7.8125\n",
      "Epoch [12/60], Iter [303/633], LR: 0.005000, Loss: 3.5828, top1: 15.6250\n",
      "Epoch [12/60], Iter [304/633], LR: 0.005000, Loss: 3.5642, top1: 12.5000\n",
      "Epoch [12/60], Iter [305/633], LR: 0.005000, Loss: 3.6054, top1: 7.8125\n",
      "Epoch [12/60], Iter [306/633], LR: 0.005000, Loss: 3.5159, top1: 20.3125\n",
      "Epoch [12/60], Iter [307/633], LR: 0.005000, Loss: 3.6085, top1: 9.3750\n",
      "Epoch [12/60], Iter [308/633], LR: 0.005000, Loss: 3.6156, top1: 9.3750\n",
      "Epoch [12/60], Iter [309/633], LR: 0.005000, Loss: 3.5699, top1: 14.0625\n",
      "Epoch [12/60], Iter [310/633], LR: 0.005000, Loss: 3.5077, top1: 21.8750\n",
      "Epoch [12/60], Iter [311/633], LR: 0.005000, Loss: 3.5742, top1: 17.1875\n",
      "Epoch [12/60], Iter [312/633], LR: 0.005000, Loss: 3.5772, top1: 18.7500\n",
      "Epoch [12/60], Iter [313/633], LR: 0.005000, Loss: 3.5774, top1: 20.3125\n",
      "Epoch [12/60], Iter [314/633], LR: 0.005000, Loss: 3.5821, top1: 10.9375\n",
      "Epoch [12/60], Iter [315/633], LR: 0.005000, Loss: 3.5802, top1: 17.1875\n",
      "Epoch [12/60], Iter [316/633], LR: 0.005000, Loss: 3.5814, top1: 10.9375\n",
      "Epoch [12/60], Iter [317/633], LR: 0.005000, Loss: 3.5521, top1: 15.6250\n",
      "Epoch [12/60], Iter [318/633], LR: 0.005000, Loss: 3.5940, top1: 12.5000\n",
      "Epoch [12/60], Iter [319/633], LR: 0.005000, Loss: 3.5779, top1: 15.6250\n",
      "Epoch [12/60], Iter [320/633], LR: 0.005000, Loss: 3.6046, top1: 12.5000\n",
      "Epoch [12/60], Iter [321/633], LR: 0.005000, Loss: 3.5789, top1: 14.0625\n",
      "Epoch [12/60], Iter [322/633], LR: 0.005000, Loss: 3.6214, top1: 12.5000\n",
      "Epoch [12/60], Iter [323/633], LR: 0.005000, Loss: 3.5976, top1: 9.3750\n",
      "Epoch [12/60], Iter [324/633], LR: 0.005000, Loss: 3.5630, top1: 17.1875\n",
      "Epoch [12/60], Iter [325/633], LR: 0.005000, Loss: 3.5809, top1: 14.0625\n",
      "Epoch [12/60], Iter [326/633], LR: 0.005000, Loss: 3.6018, top1: 10.9375\n",
      "Epoch [12/60], Iter [327/633], LR: 0.005000, Loss: 3.5705, top1: 12.5000\n",
      "Epoch [12/60], Iter [328/633], LR: 0.005000, Loss: 3.6097, top1: 9.3750\n",
      "Epoch [12/60], Iter [329/633], LR: 0.005000, Loss: 3.5912, top1: 18.7500\n",
      "Epoch [12/60], Iter [330/633], LR: 0.005000, Loss: 3.5725, top1: 10.9375\n",
      "Epoch [12/60], Iter [331/633], LR: 0.005000, Loss: 3.5619, top1: 17.1875\n",
      "Epoch [12/60], Iter [332/633], LR: 0.005000, Loss: 3.5715, top1: 15.6250\n",
      "Epoch [12/60], Iter [333/633], LR: 0.005000, Loss: 3.5982, top1: 12.5000\n",
      "Epoch [12/60], Iter [334/633], LR: 0.005000, Loss: 3.5902, top1: 14.0625\n",
      "Epoch [12/60], Iter [335/633], LR: 0.005000, Loss: 3.5687, top1: 15.6250\n",
      "Epoch [12/60], Iter [336/633], LR: 0.005000, Loss: 3.6086, top1: 14.0625\n",
      "Epoch [12/60], Iter [337/633], LR: 0.005000, Loss: 3.5358, top1: 17.1875\n",
      "Epoch [12/60], Iter [338/633], LR: 0.005000, Loss: 3.6126, top1: 14.0625\n",
      "Epoch [12/60], Iter [339/633], LR: 0.005000, Loss: 3.6062, top1: 9.3750\n",
      "Epoch [12/60], Iter [340/633], LR: 0.005000, Loss: 3.5390, top1: 20.3125\n",
      "Epoch [12/60], Iter [341/633], LR: 0.005000, Loss: 3.5325, top1: 20.3125\n",
      "Epoch [12/60], Iter [342/633], LR: 0.005000, Loss: 3.5998, top1: 12.5000\n",
      "Epoch [12/60], Iter [343/633], LR: 0.005000, Loss: 3.6061, top1: 14.0625\n",
      "Epoch [12/60], Iter [344/633], LR: 0.005000, Loss: 3.6060, top1: 4.6875\n",
      "Epoch [12/60], Iter [345/633], LR: 0.005000, Loss: 3.5993, top1: 10.9375\n",
      "Epoch [12/60], Iter [346/633], LR: 0.005000, Loss: 3.5672, top1: 10.9375\n",
      "Epoch [12/60], Iter [347/633], LR: 0.005000, Loss: 3.5915, top1: 17.1875\n",
      "Epoch [12/60], Iter [348/633], LR: 0.005000, Loss: 3.5563, top1: 17.1875\n",
      "Epoch [12/60], Iter [349/633], LR: 0.005000, Loss: 3.5871, top1: 10.9375\n",
      "Epoch [12/60], Iter [350/633], LR: 0.005000, Loss: 3.6027, top1: 14.0625\n",
      "Epoch [12/60], Iter [351/633], LR: 0.005000, Loss: 3.5603, top1: 14.0625\n",
      "Epoch [12/60], Iter [352/633], LR: 0.005000, Loss: 3.5755, top1: 15.6250\n",
      "Epoch [12/60], Iter [353/633], LR: 0.005000, Loss: 3.5616, top1: 14.0625\n",
      "Epoch [12/60], Iter [354/633], LR: 0.005000, Loss: 3.5614, top1: 15.6250\n",
      "Epoch [12/60], Iter [355/633], LR: 0.005000, Loss: 3.5676, top1: 18.7500\n",
      "Epoch [12/60], Iter [356/633], LR: 0.005000, Loss: 3.5562, top1: 15.6250\n",
      "Epoch [12/60], Iter [357/633], LR: 0.005000, Loss: 3.5278, top1: 17.1875\n",
      "Epoch [12/60], Iter [358/633], LR: 0.005000, Loss: 3.5380, top1: 17.1875\n",
      "Epoch [12/60], Iter [359/633], LR: 0.005000, Loss: 3.5811, top1: 9.3750\n",
      "Epoch [12/60], Iter [360/633], LR: 0.005000, Loss: 3.6326, top1: 7.8125\n",
      "Epoch [12/60], Iter [361/633], LR: 0.005000, Loss: 3.6129, top1: 10.9375\n",
      "Epoch [12/60], Iter [362/633], LR: 0.005000, Loss: 3.6155, top1: 10.9375\n",
      "Epoch [12/60], Iter [363/633], LR: 0.005000, Loss: 3.5365, top1: 17.1875\n",
      "Epoch [12/60], Iter [364/633], LR: 0.005000, Loss: 3.5660, top1: 18.7500\n",
      "Epoch [12/60], Iter [365/633], LR: 0.005000, Loss: 3.5571, top1: 10.9375\n",
      "Epoch [12/60], Iter [366/633], LR: 0.005000, Loss: 3.5900, top1: 12.5000\n",
      "Epoch [12/60], Iter [367/633], LR: 0.005000, Loss: 3.5466, top1: 12.5000\n",
      "Epoch [12/60], Iter [368/633], LR: 0.005000, Loss: 3.5931, top1: 7.8125\n",
      "Epoch [12/60], Iter [369/633], LR: 0.005000, Loss: 3.5501, top1: 17.1875\n",
      "Epoch [12/60], Iter [370/633], LR: 0.005000, Loss: 3.5608, top1: 14.0625\n",
      "Epoch [12/60], Iter [371/633], LR: 0.005000, Loss: 3.5420, top1: 17.1875\n",
      "Epoch [12/60], Iter [372/633], LR: 0.005000, Loss: 3.5620, top1: 18.7500\n",
      "Epoch [12/60], Iter [373/633], LR: 0.005000, Loss: 3.5455, top1: 14.0625\n",
      "Epoch [12/60], Iter [374/633], LR: 0.005000, Loss: 3.5886, top1: 15.6250\n",
      "Epoch [12/60], Iter [375/633], LR: 0.005000, Loss: 3.5434, top1: 21.8750\n",
      "Epoch [12/60], Iter [376/633], LR: 0.005000, Loss: 3.5722, top1: 14.0625\n",
      "Epoch [12/60], Iter [377/633], LR: 0.005000, Loss: 3.5786, top1: 15.6250\n",
      "Epoch [12/60], Iter [378/633], LR: 0.005000, Loss: 3.5836, top1: 14.0625\n",
      "Epoch [12/60], Iter [379/633], LR: 0.005000, Loss: 3.5570, top1: 15.6250\n",
      "Epoch [12/60], Iter [380/633], LR: 0.005000, Loss: 3.5481, top1: 23.4375\n",
      "Epoch [12/60], Iter [381/633], LR: 0.005000, Loss: 3.5868, top1: 12.5000\n",
      "Epoch [12/60], Iter [382/633], LR: 0.005000, Loss: 3.5857, top1: 17.1875\n",
      "Epoch [12/60], Iter [383/633], LR: 0.005000, Loss: 3.5494, top1: 12.5000\n",
      "Epoch [12/60], Iter [384/633], LR: 0.005000, Loss: 3.5881, top1: 10.9375\n",
      "Epoch [12/60], Iter [385/633], LR: 0.005000, Loss: 3.5755, top1: 17.1875\n",
      "Epoch [12/60], Iter [386/633], LR: 0.005000, Loss: 3.6017, top1: 12.5000\n",
      "Epoch [12/60], Iter [387/633], LR: 0.005000, Loss: 3.5443, top1: 17.1875\n",
      "Epoch [12/60], Iter [388/633], LR: 0.005000, Loss: 3.5208, top1: 20.3125\n",
      "Epoch [12/60], Iter [389/633], LR: 0.005000, Loss: 3.5678, top1: 9.3750\n",
      "Epoch [12/60], Iter [390/633], LR: 0.005000, Loss: 3.5410, top1: 10.9375\n",
      "Epoch [12/60], Iter [391/633], LR: 0.005000, Loss: 3.5917, top1: 9.3750\n",
      "Epoch [12/60], Iter [392/633], LR: 0.005000, Loss: 3.5813, top1: 12.5000\n",
      "Epoch [12/60], Iter [393/633], LR: 0.005000, Loss: 3.6153, top1: 10.9375\n",
      "Epoch [12/60], Iter [394/633], LR: 0.005000, Loss: 3.6402, top1: 6.2500\n",
      "Epoch [12/60], Iter [395/633], LR: 0.005000, Loss: 3.6148, top1: 6.2500\n",
      "Epoch [12/60], Iter [396/633], LR: 0.005000, Loss: 3.5557, top1: 17.1875\n",
      "Epoch [12/60], Iter [397/633], LR: 0.005000, Loss: 3.5584, top1: 15.6250\n",
      "Epoch [12/60], Iter [398/633], LR: 0.005000, Loss: 3.6076, top1: 12.5000\n",
      "Epoch [12/60], Iter [399/633], LR: 0.005000, Loss: 3.5589, top1: 17.1875\n",
      "Epoch [12/60], Iter [400/633], LR: 0.005000, Loss: 3.5782, top1: 18.7500\n",
      "Epoch [12/60], Iter [401/633], LR: 0.005000, Loss: 3.5453, top1: 17.1875\n",
      "Epoch [12/60], Iter [402/633], LR: 0.005000, Loss: 3.5722, top1: 14.0625\n",
      "Epoch [12/60], Iter [403/633], LR: 0.005000, Loss: 3.5611, top1: 17.1875\n",
      "Epoch [12/60], Iter [404/633], LR: 0.005000, Loss: 3.5921, top1: 12.5000\n",
      "Epoch [12/60], Iter [405/633], LR: 0.005000, Loss: 3.5825, top1: 14.0625\n",
      "Epoch [12/60], Iter [406/633], LR: 0.005000, Loss: 3.6165, top1: 9.3750\n",
      "Epoch [12/60], Iter [407/633], LR: 0.005000, Loss: 3.5784, top1: 14.0625\n",
      "Epoch [12/60], Iter [408/633], LR: 0.005000, Loss: 3.6023, top1: 10.9375\n",
      "Epoch [12/60], Iter [409/633], LR: 0.005000, Loss: 3.5189, top1: 20.3125\n",
      "Epoch [12/60], Iter [410/633], LR: 0.005000, Loss: 3.5728, top1: 10.9375\n",
      "Epoch [12/60], Iter [411/633], LR: 0.005000, Loss: 3.5695, top1: 17.1875\n",
      "Epoch [12/60], Iter [412/633], LR: 0.005000, Loss: 3.5585, top1: 17.1875\n",
      "Epoch [12/60], Iter [413/633], LR: 0.005000, Loss: 3.6187, top1: 10.9375\n",
      "Epoch [12/60], Iter [414/633], LR: 0.005000, Loss: 3.5807, top1: 15.6250\n",
      "Epoch [12/60], Iter [415/633], LR: 0.005000, Loss: 3.5632, top1: 12.5000\n",
      "Epoch [12/60], Iter [416/633], LR: 0.005000, Loss: 3.5766, top1: 20.3125\n",
      "Epoch [12/60], Iter [417/633], LR: 0.005000, Loss: 3.5930, top1: 15.6250\n",
      "Epoch [12/60], Iter [418/633], LR: 0.005000, Loss: 3.5439, top1: 17.1875\n",
      "Epoch [12/60], Iter [419/633], LR: 0.005000, Loss: 3.5641, top1: 18.7500\n",
      "Epoch [12/60], Iter [420/633], LR: 0.005000, Loss: 3.5707, top1: 15.6250\n",
      "Epoch [12/60], Iter [421/633], LR: 0.005000, Loss: 3.5970, top1: 9.3750\n",
      "Epoch [12/60], Iter [422/633], LR: 0.005000, Loss: 3.5901, top1: 7.8125\n",
      "Epoch [12/60], Iter [423/633], LR: 0.005000, Loss: 3.5233, top1: 28.1250\n",
      "Epoch [12/60], Iter [424/633], LR: 0.005000, Loss: 3.5740, top1: 14.0625\n",
      "Epoch [12/60], Iter [425/633], LR: 0.005000, Loss: 3.6017, top1: 12.5000\n",
      "Epoch [12/60], Iter [426/633], LR: 0.005000, Loss: 3.5537, top1: 12.5000\n",
      "Epoch [12/60], Iter [427/633], LR: 0.005000, Loss: 3.5657, top1: 18.7500\n",
      "Epoch [12/60], Iter [428/633], LR: 0.005000, Loss: 3.5590, top1: 14.0625\n",
      "Epoch [12/60], Iter [429/633], LR: 0.005000, Loss: 3.6007, top1: 14.0625\n",
      "Epoch [12/60], Iter [430/633], LR: 0.005000, Loss: 3.5542, top1: 17.1875\n",
      "Epoch [12/60], Iter [431/633], LR: 0.005000, Loss: 3.5619, top1: 12.5000\n",
      "Epoch [12/60], Iter [432/633], LR: 0.005000, Loss: 3.5383, top1: 17.1875\n",
      "Epoch [12/60], Iter [433/633], LR: 0.005000, Loss: 3.6126, top1: 6.2500\n",
      "Epoch [12/60], Iter [434/633], LR: 0.005000, Loss: 3.5194, top1: 25.0000\n",
      "Epoch [12/60], Iter [435/633], LR: 0.005000, Loss: 3.6227, top1: 4.6875\n",
      "Epoch [12/60], Iter [436/633], LR: 0.005000, Loss: 3.6049, top1: 9.3750\n",
      "Epoch [12/60], Iter [437/633], LR: 0.005000, Loss: 3.5646, top1: 15.6250\n",
      "Epoch [12/60], Iter [438/633], LR: 0.005000, Loss: 3.5808, top1: 10.9375\n",
      "Epoch [12/60], Iter [439/633], LR: 0.005000, Loss: 3.5744, top1: 12.5000\n",
      "Epoch [12/60], Iter [440/633], LR: 0.005000, Loss: 3.5840, top1: 10.9375\n",
      "Epoch [12/60], Iter [441/633], LR: 0.005000, Loss: 3.6107, top1: 9.3750\n",
      "Epoch [12/60], Iter [442/633], LR: 0.005000, Loss: 3.6171, top1: 7.8125\n",
      "Epoch [12/60], Iter [443/633], LR: 0.005000, Loss: 3.5869, top1: 20.3125\n",
      "Epoch [12/60], Iter [444/633], LR: 0.005000, Loss: 3.5253, top1: 17.1875\n",
      "Epoch [12/60], Iter [445/633], LR: 0.005000, Loss: 3.5725, top1: 10.9375\n",
      "Epoch [12/60], Iter [446/633], LR: 0.005000, Loss: 3.5699, top1: 10.9375\n",
      "Epoch [12/60], Iter [447/633], LR: 0.005000, Loss: 3.5828, top1: 9.3750\n",
      "Epoch [12/60], Iter [448/633], LR: 0.005000, Loss: 3.5807, top1: 10.9375\n",
      "Epoch [12/60], Iter [449/633], LR: 0.005000, Loss: 3.5990, top1: 10.9375\n",
      "Epoch [12/60], Iter [450/633], LR: 0.005000, Loss: 3.5478, top1: 18.7500\n",
      "Epoch [12/60], Iter [451/633], LR: 0.005000, Loss: 3.5755, top1: 12.5000\n",
      "Epoch [12/60], Iter [452/633], LR: 0.005000, Loss: 3.5697, top1: 14.0625\n",
      "Epoch [12/60], Iter [453/633], LR: 0.005000, Loss: 3.6006, top1: 14.0625\n",
      "Epoch [12/60], Iter [454/633], LR: 0.005000, Loss: 3.5499, top1: 12.5000\n",
      "Epoch [12/60], Iter [455/633], LR: 0.005000, Loss: 3.5290, top1: 14.0625\n",
      "Epoch [12/60], Iter [456/633], LR: 0.005000, Loss: 3.5543, top1: 18.7500\n",
      "Epoch [12/60], Iter [457/633], LR: 0.005000, Loss: 3.5224, top1: 17.1875\n",
      "Epoch [12/60], Iter [458/633], LR: 0.005000, Loss: 3.5902, top1: 14.0625\n",
      "Epoch [12/60], Iter [459/633], LR: 0.005000, Loss: 3.5610, top1: 17.1875\n",
      "Epoch [12/60], Iter [460/633], LR: 0.005000, Loss: 3.5413, top1: 21.8750\n",
      "Epoch [12/60], Iter [461/633], LR: 0.005000, Loss: 3.5612, top1: 17.1875\n",
      "Epoch [12/60], Iter [462/633], LR: 0.005000, Loss: 3.5815, top1: 17.1875\n",
      "Epoch [12/60], Iter [463/633], LR: 0.005000, Loss: 3.5483, top1: 15.6250\n",
      "Epoch [12/60], Iter [464/633], LR: 0.005000, Loss: 3.5560, top1: 15.6250\n",
      "Epoch [12/60], Iter [465/633], LR: 0.005000, Loss: 3.5949, top1: 10.9375\n",
      "Epoch [12/60], Iter [466/633], LR: 0.005000, Loss: 3.5745, top1: 14.0625\n",
      "Epoch [12/60], Iter [467/633], LR: 0.005000, Loss: 3.5321, top1: 23.4375\n",
      "Epoch [12/60], Iter [468/633], LR: 0.005000, Loss: 3.5322, top1: 17.1875\n",
      "Epoch [12/60], Iter [469/633], LR: 0.005000, Loss: 3.5860, top1: 10.9375\n",
      "Epoch [12/60], Iter [470/633], LR: 0.005000, Loss: 3.5863, top1: 14.0625\n",
      "Epoch [12/60], Iter [471/633], LR: 0.005000, Loss: 3.6166, top1: 10.9375\n",
      "Epoch [12/60], Iter [472/633], LR: 0.005000, Loss: 3.5299, top1: 18.7500\n",
      "Epoch [12/60], Iter [473/633], LR: 0.005000, Loss: 3.5682, top1: 14.0625\n",
      "Epoch [12/60], Iter [474/633], LR: 0.005000, Loss: 3.6004, top1: 14.0625\n",
      "Epoch [12/60], Iter [475/633], LR: 0.005000, Loss: 3.6294, top1: 6.2500\n",
      "Epoch [12/60], Iter [476/633], LR: 0.005000, Loss: 3.5561, top1: 15.6250\n",
      "Epoch [12/60], Iter [477/633], LR: 0.005000, Loss: 3.6243, top1: 12.5000\n",
      "Epoch [12/60], Iter [478/633], LR: 0.005000, Loss: 3.5159, top1: 21.8750\n",
      "Epoch [12/60], Iter [479/633], LR: 0.005000, Loss: 3.5783, top1: 18.7500\n",
      "Epoch [12/60], Iter [480/633], LR: 0.005000, Loss: 3.5560, top1: 18.7500\n",
      "Epoch [12/60], Iter [481/633], LR: 0.005000, Loss: 3.5945, top1: 7.8125\n",
      "Epoch [12/60], Iter [482/633], LR: 0.005000, Loss: 3.5420, top1: 18.7500\n",
      "Epoch [12/60], Iter [483/633], LR: 0.005000, Loss: 3.5500, top1: 20.3125\n",
      "Epoch [12/60], Iter [484/633], LR: 0.005000, Loss: 3.5197, top1: 21.8750\n",
      "Epoch [12/60], Iter [485/633], LR: 0.005000, Loss: 3.5588, top1: 15.6250\n",
      "Epoch [12/60], Iter [486/633], LR: 0.005000, Loss: 3.5504, top1: 25.0000\n",
      "Epoch [12/60], Iter [487/633], LR: 0.005000, Loss: 3.5259, top1: 18.7500\n",
      "Epoch [12/60], Iter [488/633], LR: 0.005000, Loss: 3.5827, top1: 14.0625\n",
      "Epoch [12/60], Iter [489/633], LR: 0.005000, Loss: 3.5681, top1: 15.6250\n",
      "Epoch [12/60], Iter [490/633], LR: 0.005000, Loss: 3.5949, top1: 14.0625\n",
      "Epoch [12/60], Iter [491/633], LR: 0.005000, Loss: 3.5446, top1: 15.6250\n",
      "Epoch [12/60], Iter [492/633], LR: 0.005000, Loss: 3.5480, top1: 26.5625\n",
      "Epoch [12/60], Iter [493/633], LR: 0.005000, Loss: 3.6073, top1: 9.3750\n",
      "Epoch [12/60], Iter [494/633], LR: 0.005000, Loss: 3.6124, top1: 7.8125\n",
      "Epoch [12/60], Iter [495/633], LR: 0.005000, Loss: 3.5907, top1: 17.1875\n",
      "Epoch [12/60], Iter [496/633], LR: 0.005000, Loss: 3.5785, top1: 10.9375\n",
      "Epoch [12/60], Iter [497/633], LR: 0.005000, Loss: 3.5712, top1: 12.5000\n",
      "Epoch [12/60], Iter [498/633], LR: 0.005000, Loss: 3.5647, top1: 14.0625\n",
      "Epoch [12/60], Iter [499/633], LR: 0.005000, Loss: 3.5983, top1: 17.1875\n",
      "Epoch [12/60], Iter [500/633], LR: 0.005000, Loss: 3.5734, top1: 9.3750\n",
      "Epoch [12/60], Iter [501/633], LR: 0.005000, Loss: 3.5961, top1: 12.5000\n",
      "Epoch [12/60], Iter [502/633], LR: 0.005000, Loss: 3.5664, top1: 15.6250\n",
      "Epoch [12/60], Iter [503/633], LR: 0.005000, Loss: 3.5689, top1: 20.3125\n",
      "Epoch [12/60], Iter [504/633], LR: 0.005000, Loss: 3.5815, top1: 14.0625\n",
      "Epoch [12/60], Iter [505/633], LR: 0.005000, Loss: 3.6303, top1: 4.6875\n",
      "Epoch [12/60], Iter [506/633], LR: 0.005000, Loss: 3.5631, top1: 14.0625\n",
      "Epoch [12/60], Iter [507/633], LR: 0.005000, Loss: 3.5372, top1: 25.0000\n",
      "Epoch [12/60], Iter [508/633], LR: 0.005000, Loss: 3.6010, top1: 10.9375\n",
      "Epoch [12/60], Iter [509/633], LR: 0.005000, Loss: 3.5784, top1: 14.0625\n",
      "Epoch [12/60], Iter [510/633], LR: 0.005000, Loss: 3.5115, top1: 26.5625\n",
      "Epoch [12/60], Iter [511/633], LR: 0.005000, Loss: 3.5863, top1: 14.0625\n",
      "Epoch [12/60], Iter [512/633], LR: 0.005000, Loss: 3.5449, top1: 12.5000\n",
      "Epoch [12/60], Iter [513/633], LR: 0.005000, Loss: 3.5545, top1: 14.0625\n",
      "Epoch [12/60], Iter [514/633], LR: 0.005000, Loss: 3.5367, top1: 18.7500\n",
      "Epoch [12/60], Iter [515/633], LR: 0.005000, Loss: 3.5973, top1: 10.9375\n",
      "Epoch [12/60], Iter [516/633], LR: 0.005000, Loss: 3.5895, top1: 14.0625\n",
      "Epoch [12/60], Iter [517/633], LR: 0.005000, Loss: 3.5932, top1: 10.9375\n",
      "Epoch [12/60], Iter [518/633], LR: 0.005000, Loss: 3.5881, top1: 12.5000\n",
      "Epoch [12/60], Iter [519/633], LR: 0.005000, Loss: 3.5286, top1: 17.1875\n",
      "Epoch [12/60], Iter [520/633], LR: 0.005000, Loss: 3.5410, top1: 23.4375\n",
      "Epoch [12/60], Iter [521/633], LR: 0.005000, Loss: 3.5847, top1: 15.6250\n",
      "Epoch [12/60], Iter [522/633], LR: 0.005000, Loss: 3.5728, top1: 15.6250\n",
      "Epoch [12/60], Iter [523/633], LR: 0.005000, Loss: 3.5906, top1: 17.1875\n",
      "Epoch [12/60], Iter [524/633], LR: 0.005000, Loss: 3.5494, top1: 18.7500\n",
      "Epoch [12/60], Iter [525/633], LR: 0.005000, Loss: 3.5910, top1: 7.8125\n",
      "Epoch [12/60], Iter [526/633], LR: 0.005000, Loss: 3.5645, top1: 14.0625\n",
      "Epoch [12/60], Iter [527/633], LR: 0.005000, Loss: 3.6331, top1: 7.8125\n",
      "Epoch [12/60], Iter [528/633], LR: 0.005000, Loss: 3.5741, top1: 10.9375\n",
      "Epoch [12/60], Iter [529/633], LR: 0.005000, Loss: 3.5976, top1: 18.7500\n",
      "Epoch [12/60], Iter [530/633], LR: 0.005000, Loss: 3.5682, top1: 12.5000\n",
      "Epoch [12/60], Iter [531/633], LR: 0.005000, Loss: 3.5477, top1: 20.3125\n",
      "Epoch [12/60], Iter [532/633], LR: 0.005000, Loss: 3.5497, top1: 14.0625\n",
      "Epoch [12/60], Iter [533/633], LR: 0.005000, Loss: 3.5701, top1: 14.0625\n",
      "Epoch [12/60], Iter [534/633], LR: 0.005000, Loss: 3.6034, top1: 14.0625\n",
      "Epoch [12/60], Iter [535/633], LR: 0.005000, Loss: 3.5842, top1: 14.0625\n",
      "Epoch [12/60], Iter [536/633], LR: 0.005000, Loss: 3.5460, top1: 18.7500\n",
      "Epoch [12/60], Iter [537/633], LR: 0.005000, Loss: 3.5556, top1: 14.0625\n",
      "Epoch [12/60], Iter [538/633], LR: 0.005000, Loss: 3.5709, top1: 7.8125\n",
      "Epoch [12/60], Iter [539/633], LR: 0.005000, Loss: 3.5892, top1: 9.3750\n",
      "Epoch [12/60], Iter [540/633], LR: 0.005000, Loss: 3.5307, top1: 20.3125\n",
      "Epoch [12/60], Iter [541/633], LR: 0.005000, Loss: 3.5696, top1: 14.0625\n",
      "Epoch [12/60], Iter [542/633], LR: 0.005000, Loss: 3.5673, top1: 17.1875\n",
      "Epoch [12/60], Iter [543/633], LR: 0.005000, Loss: 3.6025, top1: 9.3750\n",
      "Epoch [12/60], Iter [544/633], LR: 0.005000, Loss: 3.5894, top1: 12.5000\n",
      "Epoch [12/60], Iter [545/633], LR: 0.005000, Loss: 3.5573, top1: 15.6250\n",
      "Epoch [12/60], Iter [546/633], LR: 0.005000, Loss: 3.5993, top1: 9.3750\n",
      "Epoch [12/60], Iter [547/633], LR: 0.005000, Loss: 3.5143, top1: 20.3125\n",
      "Epoch [12/60], Iter [548/633], LR: 0.005000, Loss: 3.5821, top1: 10.9375\n",
      "Epoch [12/60], Iter [549/633], LR: 0.005000, Loss: 3.5510, top1: 15.6250\n",
      "Epoch [12/60], Iter [550/633], LR: 0.005000, Loss: 3.6123, top1: 7.8125\n",
      "Epoch [12/60], Iter [551/633], LR: 0.005000, Loss: 3.5392, top1: 18.7500\n",
      "Epoch [12/60], Iter [552/633], LR: 0.005000, Loss: 3.5780, top1: 14.0625\n",
      "Epoch [12/60], Iter [553/633], LR: 0.005000, Loss: 3.6010, top1: 12.5000\n",
      "Epoch [12/60], Iter [554/633], LR: 0.005000, Loss: 3.6212, top1: 6.2500\n",
      "Epoch [12/60], Iter [555/633], LR: 0.005000, Loss: 3.6134, top1: 14.0625\n",
      "Epoch [12/60], Iter [556/633], LR: 0.005000, Loss: 3.5741, top1: 18.7500\n",
      "Epoch [12/60], Iter [557/633], LR: 0.005000, Loss: 3.5569, top1: 15.6250\n",
      "Epoch [12/60], Iter [558/633], LR: 0.005000, Loss: 3.6063, top1: 10.9375\n",
      "Epoch [12/60], Iter [559/633], LR: 0.005000, Loss: 3.4777, top1: 25.0000\n",
      "Epoch [12/60], Iter [560/633], LR: 0.005000, Loss: 3.5723, top1: 15.6250\n",
      "Epoch [12/60], Iter [561/633], LR: 0.005000, Loss: 3.5908, top1: 18.7500\n",
      "Epoch [12/60], Iter [562/633], LR: 0.005000, Loss: 3.5412, top1: 14.0625\n",
      "Epoch [12/60], Iter [563/633], LR: 0.005000, Loss: 3.5227, top1: 17.1875\n",
      "Epoch [12/60], Iter [564/633], LR: 0.005000, Loss: 3.5772, top1: 14.0625\n",
      "Epoch [12/60], Iter [565/633], LR: 0.005000, Loss: 3.5549, top1: 18.7500\n",
      "Epoch [12/60], Iter [566/633], LR: 0.005000, Loss: 3.5731, top1: 12.5000\n",
      "Epoch [12/60], Iter [567/633], LR: 0.005000, Loss: 3.5632, top1: 17.1875\n",
      "Epoch [12/60], Iter [568/633], LR: 0.005000, Loss: 3.5949, top1: 7.8125\n",
      "Epoch [12/60], Iter [569/633], LR: 0.005000, Loss: 3.5483, top1: 15.6250\n",
      "Epoch [12/60], Iter [570/633], LR: 0.005000, Loss: 3.6278, top1: 9.3750\n",
      "Epoch [12/60], Iter [571/633], LR: 0.005000, Loss: 3.6228, top1: 7.8125\n",
      "Epoch [12/60], Iter [572/633], LR: 0.005000, Loss: 3.5419, top1: 17.1875\n",
      "Epoch [12/60], Iter [573/633], LR: 0.005000, Loss: 3.5484, top1: 17.1875\n",
      "Epoch [12/60], Iter [574/633], LR: 0.005000, Loss: 3.5881, top1: 4.6875\n",
      "Epoch [12/60], Iter [575/633], LR: 0.005000, Loss: 3.5951, top1: 10.9375\n",
      "Epoch [12/60], Iter [576/633], LR: 0.005000, Loss: 3.5158, top1: 18.7500\n",
      "Epoch [12/60], Iter [577/633], LR: 0.005000, Loss: 3.5482, top1: 12.5000\n",
      "Epoch [12/60], Iter [578/633], LR: 0.005000, Loss: 3.6206, top1: 10.9375\n",
      "Epoch [12/60], Iter [579/633], LR: 0.005000, Loss: 3.5943, top1: 7.8125\n",
      "Epoch [12/60], Iter [580/633], LR: 0.005000, Loss: 3.5709, top1: 18.7500\n",
      "Epoch [12/60], Iter [581/633], LR: 0.005000, Loss: 3.5522, top1: 18.7500\n",
      "Epoch [12/60], Iter [582/633], LR: 0.005000, Loss: 3.5085, top1: 29.6875\n",
      "Epoch [12/60], Iter [583/633], LR: 0.005000, Loss: 3.5702, top1: 17.1875\n",
      "Epoch [12/60], Iter [584/633], LR: 0.005000, Loss: 3.5468, top1: 14.0625\n",
      "Epoch [12/60], Iter [585/633], LR: 0.005000, Loss: 3.5642, top1: 18.7500\n",
      "Epoch [12/60], Iter [586/633], LR: 0.005000, Loss: 3.5752, top1: 15.6250\n",
      "Epoch [12/60], Iter [587/633], LR: 0.005000, Loss: 3.5041, top1: 29.6875\n",
      "Epoch [12/60], Iter [588/633], LR: 0.005000, Loss: 3.5777, top1: 17.1875\n",
      "Epoch [12/60], Iter [589/633], LR: 0.005000, Loss: 3.5816, top1: 14.0625\n",
      "Epoch [12/60], Iter [590/633], LR: 0.005000, Loss: 3.5875, top1: 15.6250\n",
      "Epoch [12/60], Iter [591/633], LR: 0.005000, Loss: 3.5356, top1: 17.1875\n",
      "Epoch [12/60], Iter [592/633], LR: 0.005000, Loss: 3.5653, top1: 12.5000\n",
      "Epoch [12/60], Iter [593/633], LR: 0.005000, Loss: 3.5888, top1: 17.1875\n",
      "Epoch [12/60], Iter [594/633], LR: 0.005000, Loss: 3.6468, top1: 6.2500\n",
      "Epoch [12/60], Iter [595/633], LR: 0.005000, Loss: 3.5584, top1: 20.3125\n",
      "Epoch [12/60], Iter [596/633], LR: 0.005000, Loss: 3.5507, top1: 20.3125\n",
      "Epoch [12/60], Iter [597/633], LR: 0.005000, Loss: 3.5845, top1: 10.9375\n",
      "Epoch [12/60], Iter [598/633], LR: 0.005000, Loss: 3.6051, top1: 9.3750\n",
      "Epoch [12/60], Iter [599/633], LR: 0.005000, Loss: 3.5092, top1: 21.8750\n",
      "Epoch [12/60], Iter [600/633], LR: 0.005000, Loss: 3.5618, top1: 15.6250\n",
      "Epoch [12/60], Iter [601/633], LR: 0.005000, Loss: 3.5838, top1: 9.3750\n",
      "Epoch [12/60], Iter [602/633], LR: 0.005000, Loss: 3.5480, top1: 17.1875\n",
      "Epoch [12/60], Iter [603/633], LR: 0.005000, Loss: 3.5478, top1: 23.4375\n",
      "Epoch [12/60], Iter [604/633], LR: 0.005000, Loss: 3.5883, top1: 14.0625\n",
      "Epoch [12/60], Iter [605/633], LR: 0.005000, Loss: 3.5494, top1: 14.0625\n",
      "Epoch [12/60], Iter [606/633], LR: 0.005000, Loss: 3.5888, top1: 9.3750\n",
      "Epoch [12/60], Iter [607/633], LR: 0.005000, Loss: 3.5870, top1: 14.0625\n",
      "Epoch [12/60], Iter [608/633], LR: 0.005000, Loss: 3.5981, top1: 9.3750\n",
      "Epoch [12/60], Iter [609/633], LR: 0.005000, Loss: 3.4701, top1: 28.1250\n",
      "Epoch [12/60], Iter [610/633], LR: 0.005000, Loss: 3.5772, top1: 17.1875\n",
      "Epoch [12/60], Iter [611/633], LR: 0.005000, Loss: 3.5999, top1: 9.3750\n",
      "Epoch [12/60], Iter [612/633], LR: 0.005000, Loss: 3.6117, top1: 10.9375\n",
      "Epoch [12/60], Iter [613/633], LR: 0.005000, Loss: 3.5561, top1: 10.9375\n",
      "Epoch [12/60], Iter [614/633], LR: 0.005000, Loss: 3.6058, top1: 9.3750\n",
      "Epoch [12/60], Iter [615/633], LR: 0.005000, Loss: 3.5755, top1: 14.0625\n",
      "Epoch [12/60], Iter [616/633], LR: 0.005000, Loss: 3.5837, top1: 14.0625\n",
      "Epoch [12/60], Iter [617/633], LR: 0.005000, Loss: 3.5862, top1: 9.3750\n",
      "Epoch [12/60], Iter [618/633], LR: 0.005000, Loss: 3.5442, top1: 21.8750\n",
      "Epoch [12/60], Iter [619/633], LR: 0.005000, Loss: 3.5345, top1: 17.1875\n",
      "Epoch [12/60], Iter [620/633], LR: 0.005000, Loss: 3.6284, top1: 10.9375\n",
      "Epoch [12/60], Iter [621/633], LR: 0.005000, Loss: 3.6126, top1: 7.8125\n",
      "Epoch [12/60], Iter [622/633], LR: 0.005000, Loss: 3.5803, top1: 15.6250\n",
      "Epoch [12/60], Iter [623/633], LR: 0.005000, Loss: 3.5488, top1: 20.3125\n",
      "Epoch [12/60], Iter [624/633], LR: 0.005000, Loss: 3.5882, top1: 14.0625\n",
      "Epoch [12/60], Iter [625/633], LR: 0.005000, Loss: 3.5348, top1: 14.0625\n",
      "Epoch [12/60], Iter [626/633], LR: 0.005000, Loss: 3.5691, top1: 18.7500\n",
      "Epoch [12/60], Iter [627/633], LR: 0.005000, Loss: 3.5647, top1: 17.1875\n",
      "Epoch [12/60], Iter [628/633], LR: 0.005000, Loss: 3.5557, top1: 17.1875\n",
      "Epoch [12/60], Iter [629/633], LR: 0.005000, Loss: 3.5305, top1: 25.0000\n",
      "Epoch [12/60], Iter [630/633], LR: 0.005000, Loss: 3.5997, top1: 9.3750\n",
      "Epoch [12/60], Iter [631/633], LR: 0.005000, Loss: 3.6308, top1: 14.0625\n",
      "Epoch [12/60], Iter [632/633], LR: 0.005000, Loss: 3.5619, top1: 14.0625\n",
      "Epoch [12/60], Iter [633/633], LR: 0.005000, Loss: 3.5572, top1: 18.7500\n",
      "Epoch [12/60], Iter [634/633], LR: 0.005000, Loss: 3.5648, top1: 17.7419\n",
      "Epoch [12/60], Val_Loss: 3.5723, Val_top1: 14.5582, best_top1: 14.6567\n",
      "epoch time: 4.359999414285024 min\n",
      "Epoch [13/60], Iter [1/633], LR: 0.005000, Loss: 3.5681, top1: 14.0625\n",
      "Epoch [13/60], Iter [2/633], LR: 0.005000, Loss: 3.5898, top1: 10.9375\n",
      "Epoch [13/60], Iter [3/633], LR: 0.005000, Loss: 3.5624, top1: 18.7500\n",
      "Epoch [13/60], Iter [4/633], LR: 0.005000, Loss: 3.5955, top1: 9.3750\n",
      "Epoch [13/60], Iter [5/633], LR: 0.005000, Loss: 3.5673, top1: 14.0625\n",
      "Epoch [13/60], Iter [6/633], LR: 0.005000, Loss: 3.6098, top1: 10.9375\n",
      "Epoch [13/60], Iter [7/633], LR: 0.005000, Loss: 3.5635, top1: 14.0625\n",
      "Epoch [13/60], Iter [8/633], LR: 0.005000, Loss: 3.5721, top1: 14.0625\n",
      "Epoch [13/60], Iter [9/633], LR: 0.005000, Loss: 3.6077, top1: 12.5000\n",
      "Epoch [13/60], Iter [10/633], LR: 0.005000, Loss: 3.6128, top1: 10.9375\n",
      "Epoch [13/60], Iter [11/633], LR: 0.005000, Loss: 3.5663, top1: 10.9375\n",
      "Epoch [13/60], Iter [12/633], LR: 0.005000, Loss: 3.5997, top1: 10.9375\n",
      "Epoch [13/60], Iter [13/633], LR: 0.005000, Loss: 3.6201, top1: 12.5000\n",
      "Epoch [13/60], Iter [14/633], LR: 0.005000, Loss: 3.5821, top1: 14.0625\n",
      "Epoch [13/60], Iter [15/633], LR: 0.005000, Loss: 3.5705, top1: 18.7500\n",
      "Epoch [13/60], Iter [16/633], LR: 0.005000, Loss: 3.6173, top1: 6.2500\n",
      "Epoch [13/60], Iter [17/633], LR: 0.005000, Loss: 3.6233, top1: 6.2500\n",
      "Epoch [13/60], Iter [18/633], LR: 0.005000, Loss: 3.5322, top1: 20.3125\n",
      "Epoch [13/60], Iter [19/633], LR: 0.005000, Loss: 3.5536, top1: 14.0625\n",
      "Epoch [13/60], Iter [20/633], LR: 0.005000, Loss: 3.5823, top1: 14.0625\n",
      "Epoch [13/60], Iter [21/633], LR: 0.005000, Loss: 3.5428, top1: 20.3125\n",
      "Epoch [13/60], Iter [22/633], LR: 0.005000, Loss: 3.5406, top1: 15.6250\n",
      "Epoch [13/60], Iter [23/633], LR: 0.005000, Loss: 3.5357, top1: 15.6250\n",
      "Epoch [13/60], Iter [24/633], LR: 0.005000, Loss: 3.5348, top1: 23.4375\n",
      "Epoch [13/60], Iter [25/633], LR: 0.005000, Loss: 3.5280, top1: 20.3125\n",
      "Epoch [13/60], Iter [26/633], LR: 0.005000, Loss: 3.5929, top1: 10.9375\n",
      "Epoch [13/60], Iter [27/633], LR: 0.005000, Loss: 3.4966, top1: 29.6875\n",
      "Epoch [13/60], Iter [28/633], LR: 0.005000, Loss: 3.5840, top1: 9.3750\n",
      "Epoch [13/60], Iter [29/633], LR: 0.005000, Loss: 3.5761, top1: 12.5000\n",
      "Epoch [13/60], Iter [30/633], LR: 0.005000, Loss: 3.5488, top1: 15.6250\n",
      "Epoch [13/60], Iter [31/633], LR: 0.005000, Loss: 3.5789, top1: 18.7500\n",
      "Epoch [13/60], Iter [32/633], LR: 0.005000, Loss: 3.6374, top1: 7.8125\n",
      "Epoch [13/60], Iter [33/633], LR: 0.005000, Loss: 3.5576, top1: 14.0625\n",
      "Epoch [13/60], Iter [34/633], LR: 0.005000, Loss: 3.5951, top1: 9.3750\n",
      "Epoch [13/60], Iter [35/633], LR: 0.005000, Loss: 3.5675, top1: 14.0625\n",
      "Epoch [13/60], Iter [36/633], LR: 0.005000, Loss: 3.5591, top1: 14.0625\n",
      "Epoch [13/60], Iter [37/633], LR: 0.005000, Loss: 3.6040, top1: 10.9375\n",
      "Epoch [13/60], Iter [38/633], LR: 0.005000, Loss: 3.5584, top1: 14.0625\n",
      "Epoch [13/60], Iter [39/633], LR: 0.005000, Loss: 3.5395, top1: 18.7500\n",
      "Epoch [13/60], Iter [40/633], LR: 0.005000, Loss: 3.5851, top1: 14.0625\n",
      "Epoch [13/60], Iter [41/633], LR: 0.005000, Loss: 3.5492, top1: 17.1875\n",
      "Epoch [13/60], Iter [42/633], LR: 0.005000, Loss: 3.6142, top1: 10.9375\n",
      "Epoch [13/60], Iter [43/633], LR: 0.005000, Loss: 3.5639, top1: 15.6250\n",
      "Epoch [13/60], Iter [44/633], LR: 0.005000, Loss: 3.5959, top1: 12.5000\n",
      "Epoch [13/60], Iter [45/633], LR: 0.005000, Loss: 3.5972, top1: 12.5000\n",
      "Epoch [13/60], Iter [46/633], LR: 0.005000, Loss: 3.5743, top1: 10.9375\n",
      "Epoch [13/60], Iter [47/633], LR: 0.005000, Loss: 3.5880, top1: 12.5000\n",
      "Epoch [13/60], Iter [48/633], LR: 0.005000, Loss: 3.5175, top1: 18.7500\n",
      "Epoch [13/60], Iter [49/633], LR: 0.005000, Loss: 3.5791, top1: 10.9375\n",
      "Epoch [13/60], Iter [50/633], LR: 0.005000, Loss: 3.6030, top1: 14.0625\n",
      "Epoch [13/60], Iter [51/633], LR: 0.005000, Loss: 3.5677, top1: 12.5000\n",
      "Epoch [13/60], Iter [52/633], LR: 0.005000, Loss: 3.6137, top1: 12.5000\n",
      "Epoch [13/60], Iter [53/633], LR: 0.005000, Loss: 3.5360, top1: 20.3125\n",
      "Epoch [13/60], Iter [54/633], LR: 0.005000, Loss: 3.5162, top1: 25.0000\n",
      "Epoch [13/60], Iter [55/633], LR: 0.005000, Loss: 3.6134, top1: 7.8125\n",
      "Epoch [13/60], Iter [56/633], LR: 0.005000, Loss: 3.6319, top1: 6.2500\n",
      "Epoch [13/60], Iter [57/633], LR: 0.005000, Loss: 3.5751, top1: 15.6250\n",
      "Epoch [13/60], Iter [58/633], LR: 0.005000, Loss: 3.5919, top1: 15.6250\n",
      "Epoch [13/60], Iter [59/633], LR: 0.005000, Loss: 3.5187, top1: 18.7500\n",
      "Epoch [13/60], Iter [60/633], LR: 0.005000, Loss: 3.5544, top1: 17.1875\n",
      "Epoch [13/60], Iter [61/633], LR: 0.005000, Loss: 3.5732, top1: 17.1875\n",
      "Epoch [13/60], Iter [62/633], LR: 0.005000, Loss: 3.5149, top1: 23.4375\n",
      "Epoch [13/60], Iter [63/633], LR: 0.005000, Loss: 3.6113, top1: 9.3750\n",
      "Epoch [13/60], Iter [64/633], LR: 0.005000, Loss: 3.5931, top1: 9.3750\n",
      "Epoch [13/60], Iter [65/633], LR: 0.005000, Loss: 3.5593, top1: 15.6250\n",
      "Epoch [13/60], Iter [66/633], LR: 0.005000, Loss: 3.5835, top1: 14.0625\n",
      "Epoch [13/60], Iter [67/633], LR: 0.005000, Loss: 3.5869, top1: 9.3750\n",
      "Epoch [13/60], Iter [68/633], LR: 0.005000, Loss: 3.5854, top1: 15.6250\n",
      "Epoch [13/60], Iter [69/633], LR: 0.005000, Loss: 3.5701, top1: 15.6250\n",
      "Epoch [13/60], Iter [70/633], LR: 0.005000, Loss: 3.6102, top1: 9.3750\n",
      "Epoch [13/60], Iter [71/633], LR: 0.005000, Loss: 3.6027, top1: 9.3750\n",
      "Epoch [13/60], Iter [72/633], LR: 0.005000, Loss: 3.5716, top1: 9.3750\n",
      "Epoch [13/60], Iter [73/633], LR: 0.005000, Loss: 3.5730, top1: 12.5000\n",
      "Epoch [13/60], Iter [74/633], LR: 0.005000, Loss: 3.5197, top1: 25.0000\n",
      "Epoch [13/60], Iter [75/633], LR: 0.005000, Loss: 3.5874, top1: 14.0625\n",
      "Epoch [13/60], Iter [76/633], LR: 0.005000, Loss: 3.6181, top1: 9.3750\n",
      "Epoch [13/60], Iter [77/633], LR: 0.005000, Loss: 3.5959, top1: 15.6250\n",
      "Epoch [13/60], Iter [78/633], LR: 0.005000, Loss: 3.5669, top1: 17.1875\n",
      "Epoch [13/60], Iter [79/633], LR: 0.005000, Loss: 3.5648, top1: 12.5000\n",
      "Epoch [13/60], Iter [80/633], LR: 0.005000, Loss: 3.5390, top1: 18.7500\n",
      "Epoch [13/60], Iter [81/633], LR: 0.005000, Loss: 3.5726, top1: 15.6250\n",
      "Epoch [13/60], Iter [82/633], LR: 0.005000, Loss: 3.5574, top1: 17.1875\n",
      "Epoch [13/60], Iter [83/633], LR: 0.005000, Loss: 3.5378, top1: 21.8750\n",
      "Epoch [13/60], Iter [84/633], LR: 0.005000, Loss: 3.5758, top1: 18.7500\n",
      "Epoch [13/60], Iter [85/633], LR: 0.005000, Loss: 3.5999, top1: 15.6250\n",
      "Epoch [13/60], Iter [86/633], LR: 0.005000, Loss: 3.5723, top1: 15.6250\n",
      "Epoch [13/60], Iter [87/633], LR: 0.005000, Loss: 3.5577, top1: 14.0625\n",
      "Epoch [13/60], Iter [88/633], LR: 0.005000, Loss: 3.5811, top1: 10.9375\n",
      "Epoch [13/60], Iter [89/633], LR: 0.005000, Loss: 3.5791, top1: 15.6250\n",
      "Epoch [13/60], Iter [90/633], LR: 0.005000, Loss: 3.6054, top1: 7.8125\n",
      "Epoch [13/60], Iter [91/633], LR: 0.005000, Loss: 3.5554, top1: 14.0625\n",
      "Epoch [13/60], Iter [92/633], LR: 0.005000, Loss: 3.5959, top1: 15.6250\n",
      "Epoch [13/60], Iter [93/633], LR: 0.005000, Loss: 3.5507, top1: 18.7500\n",
      "Epoch [13/60], Iter [94/633], LR: 0.005000, Loss: 3.5252, top1: 20.3125\n",
      "Epoch [13/60], Iter [95/633], LR: 0.005000, Loss: 3.5509, top1: 12.5000\n",
      "Epoch [13/60], Iter [96/633], LR: 0.005000, Loss: 3.5393, top1: 17.1875\n",
      "Epoch [13/60], Iter [97/633], LR: 0.005000, Loss: 3.5995, top1: 14.0625\n",
      "Epoch [13/60], Iter [98/633], LR: 0.005000, Loss: 3.6579, top1: 7.8125\n",
      "Epoch [13/60], Iter [99/633], LR: 0.005000, Loss: 3.5121, top1: 26.5625\n",
      "Epoch [13/60], Iter [100/633], LR: 0.005000, Loss: 3.5815, top1: 15.6250\n",
      "Epoch [13/60], Iter [101/633], LR: 0.005000, Loss: 3.5617, top1: 10.9375\n",
      "Epoch [13/60], Iter [102/633], LR: 0.005000, Loss: 3.5067, top1: 15.6250\n",
      "Epoch [13/60], Iter [103/633], LR: 0.005000, Loss: 3.5407, top1: 15.6250\n",
      "Epoch [13/60], Iter [104/633], LR: 0.005000, Loss: 3.5678, top1: 7.8125\n",
      "Epoch [13/60], Iter [105/633], LR: 0.005000, Loss: 3.5464, top1: 17.1875\n",
      "Epoch [13/60], Iter [106/633], LR: 0.005000, Loss: 3.6015, top1: 6.2500\n",
      "Epoch [13/60], Iter [107/633], LR: 0.005000, Loss: 3.5703, top1: 10.9375\n",
      "Epoch [13/60], Iter [108/633], LR: 0.005000, Loss: 3.6179, top1: 9.3750\n",
      "Epoch [13/60], Iter [109/633], LR: 0.005000, Loss: 3.5524, top1: 15.6250\n",
      "Epoch [13/60], Iter [110/633], LR: 0.005000, Loss: 3.5789, top1: 7.8125\n",
      "Epoch [13/60], Iter [111/633], LR: 0.005000, Loss: 3.5514, top1: 14.0625\n",
      "Epoch [13/60], Iter [112/633], LR: 0.005000, Loss: 3.5897, top1: 9.3750\n",
      "Epoch [13/60], Iter [113/633], LR: 0.005000, Loss: 3.5874, top1: 10.9375\n",
      "Epoch [13/60], Iter [114/633], LR: 0.005000, Loss: 3.5305, top1: 23.4375\n",
      "Epoch [13/60], Iter [115/633], LR: 0.005000, Loss: 3.5669, top1: 12.5000\n",
      "Epoch [13/60], Iter [116/633], LR: 0.005000, Loss: 3.6021, top1: 12.5000\n",
      "Epoch [13/60], Iter [117/633], LR: 0.005000, Loss: 3.6134, top1: 7.8125\n",
      "Epoch [13/60], Iter [118/633], LR: 0.005000, Loss: 3.5944, top1: 10.9375\n",
      "Epoch [13/60], Iter [119/633], LR: 0.005000, Loss: 3.5505, top1: 15.6250\n",
      "Epoch [13/60], Iter [120/633], LR: 0.005000, Loss: 3.6329, top1: 6.2500\n",
      "Epoch [13/60], Iter [121/633], LR: 0.005000, Loss: 3.5561, top1: 12.5000\n",
      "Epoch [13/60], Iter [122/633], LR: 0.005000, Loss: 3.5957, top1: 14.0625\n",
      "Epoch [13/60], Iter [123/633], LR: 0.005000, Loss: 3.5518, top1: 17.1875\n",
      "Epoch [13/60], Iter [124/633], LR: 0.005000, Loss: 3.5695, top1: 12.5000\n",
      "Epoch [13/60], Iter [125/633], LR: 0.005000, Loss: 3.5494, top1: 17.1875\n",
      "Epoch [13/60], Iter [126/633], LR: 0.005000, Loss: 3.6074, top1: 12.5000\n",
      "Epoch [13/60], Iter [127/633], LR: 0.005000, Loss: 3.5836, top1: 10.9375\n",
      "Epoch [13/60], Iter [128/633], LR: 0.005000, Loss: 3.5358, top1: 20.3125\n",
      "Epoch [13/60], Iter [129/633], LR: 0.005000, Loss: 3.5747, top1: 21.8750\n",
      "Epoch [13/60], Iter [130/633], LR: 0.005000, Loss: 3.5725, top1: 17.1875\n",
      "Epoch [13/60], Iter [131/633], LR: 0.005000, Loss: 3.5455, top1: 17.1875\n",
      "Epoch [13/60], Iter [132/633], LR: 0.005000, Loss: 3.5551, top1: 21.8750\n",
      "Epoch [13/60], Iter [133/633], LR: 0.005000, Loss: 3.5302, top1: 12.5000\n",
      "Epoch [13/60], Iter [134/633], LR: 0.005000, Loss: 3.5433, top1: 15.6250\n",
      "Epoch [13/60], Iter [135/633], LR: 0.005000, Loss: 3.5752, top1: 14.0625\n",
      "Epoch [13/60], Iter [136/633], LR: 0.005000, Loss: 3.5148, top1: 25.0000\n",
      "Epoch [13/60], Iter [137/633], LR: 0.005000, Loss: 3.5779, top1: 6.2500\n",
      "Epoch [13/60], Iter [138/633], LR: 0.005000, Loss: 3.5968, top1: 12.5000\n",
      "Epoch [13/60], Iter [139/633], LR: 0.005000, Loss: 3.5974, top1: 14.0625\n",
      "Epoch [13/60], Iter [140/633], LR: 0.005000, Loss: 3.5905, top1: 15.6250\n",
      "Epoch [13/60], Iter [141/633], LR: 0.005000, Loss: 3.5424, top1: 15.6250\n",
      "Epoch [13/60], Iter [142/633], LR: 0.005000, Loss: 3.6001, top1: 17.1875\n",
      "Epoch [13/60], Iter [143/633], LR: 0.005000, Loss: 3.5247, top1: 18.7500\n",
      "Epoch [13/60], Iter [144/633], LR: 0.005000, Loss: 3.5702, top1: 14.0625\n",
      "Epoch [13/60], Iter [145/633], LR: 0.005000, Loss: 3.5272, top1: 20.3125\n",
      "Epoch [13/60], Iter [146/633], LR: 0.005000, Loss: 3.6128, top1: 14.0625\n",
      "Epoch [13/60], Iter [147/633], LR: 0.005000, Loss: 3.5579, top1: 15.6250\n",
      "Epoch [13/60], Iter [148/633], LR: 0.005000, Loss: 3.5708, top1: 20.3125\n",
      "Epoch [13/60], Iter [149/633], LR: 0.005000, Loss: 3.5274, top1: 18.7500\n",
      "Epoch [13/60], Iter [150/633], LR: 0.005000, Loss: 3.6336, top1: 9.3750\n",
      "Epoch [13/60], Iter [151/633], LR: 0.005000, Loss: 3.6157, top1: 10.9375\n",
      "Epoch [13/60], Iter [152/633], LR: 0.005000, Loss: 3.5917, top1: 12.5000\n",
      "Epoch [13/60], Iter [153/633], LR: 0.005000, Loss: 3.5499, top1: 17.1875\n",
      "Epoch [13/60], Iter [154/633], LR: 0.005000, Loss: 3.5451, top1: 21.8750\n",
      "Epoch [13/60], Iter [155/633], LR: 0.005000, Loss: 3.4952, top1: 25.0000\n",
      "Epoch [13/60], Iter [156/633], LR: 0.005000, Loss: 3.5485, top1: 21.8750\n",
      "Epoch [13/60], Iter [157/633], LR: 0.005000, Loss: 3.5121, top1: 20.3125\n",
      "Epoch [13/60], Iter [158/633], LR: 0.005000, Loss: 3.6000, top1: 12.5000\n",
      "Epoch [13/60], Iter [159/633], LR: 0.005000, Loss: 3.5892, top1: 9.3750\n",
      "Epoch [13/60], Iter [160/633], LR: 0.005000, Loss: 3.6365, top1: 10.9375\n",
      "Epoch [13/60], Iter [161/633], LR: 0.005000, Loss: 3.5826, top1: 14.0625\n",
      "Epoch [13/60], Iter [162/633], LR: 0.005000, Loss: 3.5839, top1: 12.5000\n",
      "Epoch [13/60], Iter [163/633], LR: 0.005000, Loss: 3.5713, top1: 14.0625\n",
      "Epoch [13/60], Iter [164/633], LR: 0.005000, Loss: 3.5784, top1: 9.3750\n",
      "Epoch [13/60], Iter [165/633], LR: 0.005000, Loss: 3.6009, top1: 10.9375\n",
      "Epoch [13/60], Iter [166/633], LR: 0.005000, Loss: 3.5892, top1: 18.7500\n",
      "Epoch [13/60], Iter [167/633], LR: 0.005000, Loss: 3.5859, top1: 12.5000\n",
      "Epoch [13/60], Iter [168/633], LR: 0.005000, Loss: 3.6038, top1: 9.3750\n",
      "Epoch [13/60], Iter [169/633], LR: 0.005000, Loss: 3.5967, top1: 14.0625\n",
      "Epoch [13/60], Iter [170/633], LR: 0.005000, Loss: 3.5936, top1: 15.6250\n",
      "Epoch [13/60], Iter [171/633], LR: 0.005000, Loss: 3.5477, top1: 15.6250\n",
      "Epoch [13/60], Iter [172/633], LR: 0.005000, Loss: 3.5412, top1: 18.7500\n",
      "Epoch [13/60], Iter [173/633], LR: 0.005000, Loss: 3.5732, top1: 17.1875\n",
      "Epoch [13/60], Iter [174/633], LR: 0.005000, Loss: 3.5939, top1: 7.8125\n",
      "Epoch [13/60], Iter [175/633], LR: 0.005000, Loss: 3.6033, top1: 9.3750\n",
      "Epoch [13/60], Iter [176/633], LR: 0.005000, Loss: 3.5447, top1: 20.3125\n",
      "Epoch [13/60], Iter [177/633], LR: 0.005000, Loss: 3.5638, top1: 15.6250\n",
      "Epoch [13/60], Iter [178/633], LR: 0.005000, Loss: 3.5759, top1: 17.1875\n",
      "Epoch [13/60], Iter [179/633], LR: 0.005000, Loss: 3.5677, top1: 17.1875\n",
      "Epoch [13/60], Iter [180/633], LR: 0.005000, Loss: 3.5160, top1: 20.3125\n",
      "Epoch [13/60], Iter [181/633], LR: 0.005000, Loss: 3.5337, top1: 23.4375\n",
      "Epoch [13/60], Iter [182/633], LR: 0.005000, Loss: 3.5789, top1: 12.5000\n",
      "Epoch [13/60], Iter [183/633], LR: 0.005000, Loss: 3.6114, top1: 14.0625\n",
      "Epoch [13/60], Iter [184/633], LR: 0.005000, Loss: 3.5813, top1: 14.0625\n",
      "Epoch [13/60], Iter [185/633], LR: 0.005000, Loss: 3.5674, top1: 9.3750\n",
      "Epoch [13/60], Iter [186/633], LR: 0.005000, Loss: 3.6017, top1: 17.1875\n",
      "Epoch [13/60], Iter [187/633], LR: 0.005000, Loss: 3.5372, top1: 21.8750\n",
      "Epoch [13/60], Iter [188/633], LR: 0.005000, Loss: 3.5810, top1: 15.6250\n",
      "Epoch [13/60], Iter [189/633], LR: 0.005000, Loss: 3.5893, top1: 15.6250\n",
      "Epoch [13/60], Iter [190/633], LR: 0.005000, Loss: 3.6315, top1: 9.3750\n",
      "Epoch [13/60], Iter [191/633], LR: 0.005000, Loss: 3.5692, top1: 17.1875\n",
      "Epoch [13/60], Iter [192/633], LR: 0.005000, Loss: 3.5030, top1: 14.0625\n",
      "Epoch [13/60], Iter [193/633], LR: 0.005000, Loss: 3.6124, top1: 10.9375\n",
      "Epoch [13/60], Iter [194/633], LR: 0.005000, Loss: 3.5866, top1: 14.0625\n",
      "Epoch [13/60], Iter [195/633], LR: 0.005000, Loss: 3.6129, top1: 10.9375\n",
      "Epoch [13/60], Iter [196/633], LR: 0.005000, Loss: 3.5797, top1: 10.9375\n",
      "Epoch [13/60], Iter [197/633], LR: 0.005000, Loss: 3.6148, top1: 9.3750\n",
      "Epoch [13/60], Iter [198/633], LR: 0.005000, Loss: 3.5691, top1: 15.6250\n",
      "Epoch [13/60], Iter [199/633], LR: 0.005000, Loss: 3.5691, top1: 14.0625\n",
      "Epoch [13/60], Iter [200/633], LR: 0.005000, Loss: 3.6155, top1: 9.3750\n",
      "Epoch [13/60], Iter [201/633], LR: 0.005000, Loss: 3.5656, top1: 17.1875\n",
      "Epoch [13/60], Iter [202/633], LR: 0.005000, Loss: 3.6197, top1: 10.9375\n",
      "Epoch [13/60], Iter [203/633], LR: 0.005000, Loss: 3.5456, top1: 17.1875\n",
      "Epoch [13/60], Iter [204/633], LR: 0.005000, Loss: 3.5911, top1: 9.3750\n",
      "Epoch [13/60], Iter [205/633], LR: 0.005000, Loss: 3.5358, top1: 18.7500\n",
      "Epoch [13/60], Iter [206/633], LR: 0.005000, Loss: 3.5650, top1: 15.6250\n",
      "Epoch [13/60], Iter [207/633], LR: 0.005000, Loss: 3.5663, top1: 15.6250\n",
      "Epoch [13/60], Iter [208/633], LR: 0.005000, Loss: 3.5789, top1: 15.6250\n",
      "Epoch [13/60], Iter [209/633], LR: 0.005000, Loss: 3.5940, top1: 7.8125\n",
      "Epoch [13/60], Iter [210/633], LR: 0.005000, Loss: 3.5689, top1: 17.1875\n",
      "Epoch [13/60], Iter [211/633], LR: 0.005000, Loss: 3.5493, top1: 10.9375\n",
      "Epoch [13/60], Iter [212/633], LR: 0.005000, Loss: 3.5804, top1: 10.9375\n",
      "Epoch [13/60], Iter [213/633], LR: 0.005000, Loss: 3.6104, top1: 9.3750\n",
      "Epoch [13/60], Iter [214/633], LR: 0.005000, Loss: 3.5342, top1: 21.8750\n",
      "Epoch [13/60], Iter [215/633], LR: 0.005000, Loss: 3.5680, top1: 12.5000\n",
      "Epoch [13/60], Iter [216/633], LR: 0.005000, Loss: 3.5790, top1: 10.9375\n",
      "Epoch [13/60], Iter [217/633], LR: 0.005000, Loss: 3.5845, top1: 9.3750\n",
      "Epoch [13/60], Iter [218/633], LR: 0.005000, Loss: 3.5265, top1: 20.3125\n",
      "Epoch [13/60], Iter [219/633], LR: 0.005000, Loss: 3.5783, top1: 20.3125\n",
      "Epoch [13/60], Iter [220/633], LR: 0.005000, Loss: 3.6232, top1: 10.9375\n",
      "Epoch [13/60], Iter [221/633], LR: 0.005000, Loss: 3.5865, top1: 14.0625\n",
      "Epoch [13/60], Iter [222/633], LR: 0.005000, Loss: 3.4876, top1: 26.5625\n",
      "Epoch [13/60], Iter [223/633], LR: 0.005000, Loss: 3.6195, top1: 9.3750\n",
      "Epoch [13/60], Iter [224/633], LR: 0.005000, Loss: 3.6113, top1: 10.9375\n",
      "Epoch [13/60], Iter [225/633], LR: 0.005000, Loss: 3.5786, top1: 14.0625\n",
      "Epoch [13/60], Iter [226/633], LR: 0.005000, Loss: 3.6261, top1: 17.1875\n",
      "Epoch [13/60], Iter [227/633], LR: 0.005000, Loss: 3.5871, top1: 15.6250\n",
      "Epoch [13/60], Iter [228/633], LR: 0.005000, Loss: 3.5827, top1: 7.8125\n",
      "Epoch [13/60], Iter [229/633], LR: 0.005000, Loss: 3.5369, top1: 18.7500\n",
      "Epoch [13/60], Iter [230/633], LR: 0.005000, Loss: 3.5676, top1: 15.6250\n",
      "Epoch [13/60], Iter [231/633], LR: 0.005000, Loss: 3.5992, top1: 10.9375\n",
      "Epoch [13/60], Iter [232/633], LR: 0.005000, Loss: 3.5068, top1: 17.1875\n",
      "Epoch [13/60], Iter [233/633], LR: 0.005000, Loss: 3.5835, top1: 15.6250\n",
      "Epoch [13/60], Iter [234/633], LR: 0.005000, Loss: 3.6184, top1: 6.2500\n",
      "Epoch [13/60], Iter [235/633], LR: 0.005000, Loss: 3.5378, top1: 18.7500\n",
      "Epoch [13/60], Iter [236/633], LR: 0.005000, Loss: 3.6013, top1: 9.3750\n",
      "Epoch [13/60], Iter [237/633], LR: 0.005000, Loss: 3.5899, top1: 14.0625\n",
      "Epoch [13/60], Iter [238/633], LR: 0.005000, Loss: 3.5824, top1: 15.6250\n",
      "Epoch [13/60], Iter [239/633], LR: 0.005000, Loss: 3.5808, top1: 14.0625\n",
      "Epoch [13/60], Iter [240/633], LR: 0.005000, Loss: 3.6013, top1: 14.0625\n",
      "Epoch [13/60], Iter [241/633], LR: 0.005000, Loss: 3.5310, top1: 20.3125\n",
      "Epoch [13/60], Iter [242/633], LR: 0.005000, Loss: 3.5173, top1: 21.8750\n",
      "Epoch [13/60], Iter [243/633], LR: 0.005000, Loss: 3.5723, top1: 17.1875\n",
      "Epoch [13/60], Iter [244/633], LR: 0.005000, Loss: 3.5555, top1: 18.7500\n",
      "Epoch [13/60], Iter [245/633], LR: 0.005000, Loss: 3.5662, top1: 18.7500\n",
      "Epoch [13/60], Iter [246/633], LR: 0.005000, Loss: 3.5372, top1: 17.1875\n",
      "Epoch [13/60], Iter [247/633], LR: 0.005000, Loss: 3.5547, top1: 14.0625\n",
      "Epoch [13/60], Iter [248/633], LR: 0.005000, Loss: 3.5736, top1: 12.5000\n",
      "Epoch [13/60], Iter [249/633], LR: 0.005000, Loss: 3.6331, top1: 12.5000\n",
      "Epoch [13/60], Iter [250/633], LR: 0.005000, Loss: 3.5604, top1: 17.1875\n",
      "Epoch [13/60], Iter [251/633], LR: 0.005000, Loss: 3.5607, top1: 12.5000\n",
      "Epoch [13/60], Iter [252/633], LR: 0.005000, Loss: 3.5781, top1: 15.6250\n",
      "Epoch [13/60], Iter [253/633], LR: 0.005000, Loss: 3.6046, top1: 12.5000\n",
      "Epoch [13/60], Iter [254/633], LR: 0.005000, Loss: 3.5595, top1: 21.8750\n",
      "Epoch [13/60], Iter [255/633], LR: 0.005000, Loss: 3.5984, top1: 9.3750\n",
      "Epoch [13/60], Iter [256/633], LR: 0.005000, Loss: 3.5896, top1: 12.5000\n",
      "Epoch [13/60], Iter [257/633], LR: 0.005000, Loss: 3.5744, top1: 15.6250\n",
      "Epoch [13/60], Iter [258/633], LR: 0.005000, Loss: 3.6040, top1: 10.9375\n",
      "Epoch [13/60], Iter [259/633], LR: 0.005000, Loss: 3.5599, top1: 17.1875\n",
      "Epoch [13/60], Iter [260/633], LR: 0.005000, Loss: 3.5585, top1: 18.7500\n",
      "Epoch [13/60], Iter [261/633], LR: 0.005000, Loss: 3.5544, top1: 20.3125\n",
      "Epoch [13/60], Iter [262/633], LR: 0.005000, Loss: 3.6032, top1: 12.5000\n",
      "Epoch [13/60], Iter [263/633], LR: 0.005000, Loss: 3.5456, top1: 15.6250\n",
      "Epoch [13/60], Iter [264/633], LR: 0.005000, Loss: 3.5161, top1: 23.4375\n",
      "Epoch [13/60], Iter [265/633], LR: 0.005000, Loss: 3.5392, top1: 17.1875\n",
      "Epoch [13/60], Iter [266/633], LR: 0.005000, Loss: 3.5818, top1: 17.1875\n",
      "Epoch [13/60], Iter [267/633], LR: 0.005000, Loss: 3.6176, top1: 6.2500\n",
      "Epoch [13/60], Iter [268/633], LR: 0.005000, Loss: 3.5636, top1: 15.6250\n",
      "Epoch [13/60], Iter [269/633], LR: 0.005000, Loss: 3.6042, top1: 12.5000\n",
      "Epoch [13/60], Iter [270/633], LR: 0.005000, Loss: 3.5842, top1: 10.9375\n",
      "Epoch [13/60], Iter [271/633], LR: 0.005000, Loss: 3.6192, top1: 9.3750\n",
      "Epoch [13/60], Iter [272/633], LR: 0.005000, Loss: 3.5813, top1: 12.5000\n",
      "Epoch [13/60], Iter [273/633], LR: 0.005000, Loss: 3.5913, top1: 10.9375\n",
      "Epoch [13/60], Iter [274/633], LR: 0.005000, Loss: 3.6139, top1: 12.5000\n",
      "Epoch [13/60], Iter [275/633], LR: 0.005000, Loss: 3.5522, top1: 17.1875\n",
      "Epoch [13/60], Iter [276/633], LR: 0.005000, Loss: 3.5644, top1: 14.0625\n",
      "Epoch [13/60], Iter [277/633], LR: 0.005000, Loss: 3.5196, top1: 20.3125\n",
      "Epoch [13/60], Iter [278/633], LR: 0.005000, Loss: 3.5462, top1: 20.3125\n",
      "Epoch [13/60], Iter [279/633], LR: 0.005000, Loss: 3.5900, top1: 10.9375\n",
      "Epoch [13/60], Iter [280/633], LR: 0.005000, Loss: 3.5392, top1: 18.7500\n",
      "Epoch [13/60], Iter [281/633], LR: 0.005000, Loss: 3.5879, top1: 17.1875\n",
      "Epoch [13/60], Iter [282/633], LR: 0.005000, Loss: 3.5385, top1: 15.6250\n",
      "Epoch [13/60], Iter [283/633], LR: 0.005000, Loss: 3.6261, top1: 3.1250\n",
      "Epoch [13/60], Iter [284/633], LR: 0.005000, Loss: 3.5872, top1: 17.1875\n",
      "Epoch [13/60], Iter [285/633], LR: 0.005000, Loss: 3.5471, top1: 17.1875\n",
      "Epoch [13/60], Iter [286/633], LR: 0.005000, Loss: 3.5187, top1: 23.4375\n",
      "Epoch [13/60], Iter [287/633], LR: 0.005000, Loss: 3.6173, top1: 12.5000\n",
      "Epoch [13/60], Iter [288/633], LR: 0.005000, Loss: 3.5882, top1: 9.3750\n",
      "Epoch [13/60], Iter [289/633], LR: 0.005000, Loss: 3.5700, top1: 14.0625\n",
      "Epoch [13/60], Iter [290/633], LR: 0.005000, Loss: 3.5328, top1: 18.7500\n",
      "Epoch [13/60], Iter [291/633], LR: 0.005000, Loss: 3.6106, top1: 15.6250\n",
      "Epoch [13/60], Iter [292/633], LR: 0.005000, Loss: 3.5889, top1: 14.0625\n",
      "Epoch [13/60], Iter [293/633], LR: 0.005000, Loss: 3.5390, top1: 20.3125\n",
      "Epoch [13/60], Iter [294/633], LR: 0.005000, Loss: 3.5689, top1: 10.9375\n",
      "Epoch [13/60], Iter [295/633], LR: 0.005000, Loss: 3.6202, top1: 7.8125\n",
      "Epoch [13/60], Iter [296/633], LR: 0.005000, Loss: 3.5916, top1: 12.5000\n",
      "Epoch [13/60], Iter [297/633], LR: 0.005000, Loss: 3.5379, top1: 20.3125\n",
      "Epoch [13/60], Iter [298/633], LR: 0.005000, Loss: 3.6134, top1: 4.6875\n",
      "Epoch [13/60], Iter [299/633], LR: 0.005000, Loss: 3.5278, top1: 20.3125\n",
      "Epoch [13/60], Iter [300/633], LR: 0.005000, Loss: 3.5744, top1: 15.6250\n",
      "Epoch [13/60], Iter [301/633], LR: 0.005000, Loss: 3.5761, top1: 20.3125\n",
      "Epoch [13/60], Iter [302/633], LR: 0.005000, Loss: 3.5869, top1: 12.5000\n",
      "Epoch [13/60], Iter [303/633], LR: 0.005000, Loss: 3.5482, top1: 17.1875\n",
      "Epoch [13/60], Iter [304/633], LR: 0.005000, Loss: 3.5511, top1: 20.3125\n",
      "Epoch [13/60], Iter [305/633], LR: 0.005000, Loss: 3.5626, top1: 14.0625\n",
      "Epoch [13/60], Iter [306/633], LR: 0.005000, Loss: 3.5849, top1: 12.5000\n",
      "Epoch [13/60], Iter [307/633], LR: 0.005000, Loss: 3.5555, top1: 20.3125\n",
      "Epoch [13/60], Iter [308/633], LR: 0.005000, Loss: 3.5444, top1: 23.4375\n",
      "Epoch [13/60], Iter [309/633], LR: 0.005000, Loss: 3.6288, top1: 10.9375\n",
      "Epoch [13/60], Iter [310/633], LR: 0.005000, Loss: 3.5862, top1: 15.6250\n",
      "Epoch [13/60], Iter [311/633], LR: 0.005000, Loss: 3.5262, top1: 28.1250\n",
      "Epoch [13/60], Iter [312/633], LR: 0.005000, Loss: 3.6043, top1: 10.9375\n",
      "Epoch [13/60], Iter [313/633], LR: 0.005000, Loss: 3.5610, top1: 12.5000\n",
      "Epoch [13/60], Iter [314/633], LR: 0.005000, Loss: 3.5805, top1: 17.1875\n",
      "Epoch [13/60], Iter [315/633], LR: 0.005000, Loss: 3.5551, top1: 21.8750\n",
      "Epoch [13/60], Iter [316/633], LR: 0.005000, Loss: 3.5650, top1: 12.5000\n",
      "Epoch [13/60], Iter [317/633], LR: 0.005000, Loss: 3.5591, top1: 15.6250\n",
      "Epoch [13/60], Iter [318/633], LR: 0.005000, Loss: 3.5644, top1: 17.1875\n",
      "Epoch [13/60], Iter [319/633], LR: 0.005000, Loss: 3.5804, top1: 17.1875\n",
      "Epoch [13/60], Iter [320/633], LR: 0.005000, Loss: 3.6170, top1: 9.3750\n",
      "Epoch [13/60], Iter [321/633], LR: 0.005000, Loss: 3.5561, top1: 18.7500\n",
      "Epoch [13/60], Iter [322/633], LR: 0.005000, Loss: 3.5677, top1: 12.5000\n",
      "Epoch [13/60], Iter [323/633], LR: 0.005000, Loss: 3.5649, top1: 12.5000\n",
      "Epoch [13/60], Iter [324/633], LR: 0.005000, Loss: 3.5991, top1: 15.6250\n",
      "Epoch [13/60], Iter [325/633], LR: 0.005000, Loss: 3.5845, top1: 15.6250\n",
      "Epoch [13/60], Iter [326/633], LR: 0.005000, Loss: 3.5296, top1: 25.0000\n",
      "Epoch [13/60], Iter [327/633], LR: 0.005000, Loss: 3.5366, top1: 18.7500\n",
      "Epoch [13/60], Iter [328/633], LR: 0.005000, Loss: 3.5439, top1: 23.4375\n",
      "Epoch [13/60], Iter [329/633], LR: 0.005000, Loss: 3.5146, top1: 20.3125\n",
      "Epoch [13/60], Iter [330/633], LR: 0.005000, Loss: 3.6102, top1: 14.0625\n",
      "Epoch [13/60], Iter [331/633], LR: 0.005000, Loss: 3.4970, top1: 28.1250\n",
      "Epoch [13/60], Iter [332/633], LR: 0.005000, Loss: 3.5632, top1: 14.0625\n",
      "Epoch [13/60], Iter [333/633], LR: 0.005000, Loss: 3.5990, top1: 17.1875\n",
      "Epoch [13/60], Iter [334/633], LR: 0.005000, Loss: 3.5585, top1: 15.6250\n",
      "Epoch [13/60], Iter [335/633], LR: 0.005000, Loss: 3.6076, top1: 10.9375\n",
      "Epoch [13/60], Iter [336/633], LR: 0.005000, Loss: 3.5683, top1: 10.9375\n",
      "Epoch [13/60], Iter [337/633], LR: 0.005000, Loss: 3.5292, top1: 14.0625\n",
      "Epoch [13/60], Iter [338/633], LR: 0.005000, Loss: 3.5694, top1: 14.0625\n",
      "Epoch [13/60], Iter [339/633], LR: 0.005000, Loss: 3.6114, top1: 9.3750\n",
      "Epoch [13/60], Iter [340/633], LR: 0.005000, Loss: 3.6193, top1: 7.8125\n",
      "Epoch [13/60], Iter [341/633], LR: 0.005000, Loss: 3.6272, top1: 7.8125\n",
      "Epoch [13/60], Iter [342/633], LR: 0.005000, Loss: 3.5485, top1: 15.6250\n",
      "Epoch [13/60], Iter [343/633], LR: 0.005000, Loss: 3.5972, top1: 12.5000\n",
      "Epoch [13/60], Iter [344/633], LR: 0.005000, Loss: 3.5873, top1: 17.1875\n",
      "Epoch [13/60], Iter [345/633], LR: 0.005000, Loss: 3.5529, top1: 18.7500\n",
      "Epoch [13/60], Iter [346/633], LR: 0.005000, Loss: 3.5742, top1: 15.6250\n",
      "Epoch [13/60], Iter [347/633], LR: 0.005000, Loss: 3.5540, top1: 14.0625\n",
      "Epoch [13/60], Iter [348/633], LR: 0.005000, Loss: 3.6072, top1: 14.0625\n",
      "Epoch [13/60], Iter [349/633], LR: 0.005000, Loss: 3.6207, top1: 9.3750\n",
      "Epoch [13/60], Iter [350/633], LR: 0.005000, Loss: 3.5915, top1: 14.0625\n",
      "Epoch [13/60], Iter [351/633], LR: 0.005000, Loss: 3.5328, top1: 25.0000\n",
      "Epoch [13/60], Iter [352/633], LR: 0.005000, Loss: 3.5992, top1: 9.3750\n",
      "Epoch [13/60], Iter [353/633], LR: 0.005000, Loss: 3.6062, top1: 10.9375\n",
      "Epoch [13/60], Iter [354/633], LR: 0.005000, Loss: 3.6060, top1: 12.5000\n",
      "Epoch [13/60], Iter [355/633], LR: 0.005000, Loss: 3.5773, top1: 15.6250\n",
      "Epoch [13/60], Iter [356/633], LR: 0.005000, Loss: 3.5794, top1: 14.0625\n",
      "Epoch [13/60], Iter [357/633], LR: 0.005000, Loss: 3.5979, top1: 14.0625\n",
      "Epoch [13/60], Iter [358/633], LR: 0.005000, Loss: 3.5538, top1: 18.7500\n",
      "Epoch [13/60], Iter [359/633], LR: 0.005000, Loss: 3.5302, top1: 12.5000\n",
      "Epoch [13/60], Iter [360/633], LR: 0.005000, Loss: 3.5291, top1: 18.7500\n",
      "Epoch [13/60], Iter [361/633], LR: 0.005000, Loss: 3.5274, top1: 14.0625\n",
      "Epoch [13/60], Iter [362/633], LR: 0.005000, Loss: 3.6284, top1: 12.5000\n",
      "Epoch [13/60], Iter [363/633], LR: 0.005000, Loss: 3.5500, top1: 18.7500\n",
      "Epoch [13/60], Iter [364/633], LR: 0.005000, Loss: 3.5496, top1: 18.7500\n",
      "Epoch [13/60], Iter [365/633], LR: 0.005000, Loss: 3.5857, top1: 17.1875\n",
      "Epoch [13/60], Iter [366/633], LR: 0.005000, Loss: 3.5311, top1: 18.7500\n",
      "Epoch [13/60], Iter [367/633], LR: 0.005000, Loss: 3.4960, top1: 20.3125\n",
      "Epoch [13/60], Iter [368/633], LR: 0.005000, Loss: 3.5551, top1: 14.0625\n",
      "Epoch [13/60], Iter [369/633], LR: 0.005000, Loss: 3.5974, top1: 17.1875\n",
      "Epoch [13/60], Iter [370/633], LR: 0.005000, Loss: 3.5950, top1: 12.5000\n",
      "Epoch [13/60], Iter [371/633], LR: 0.005000, Loss: 3.5847, top1: 9.3750\n",
      "Epoch [13/60], Iter [372/633], LR: 0.005000, Loss: 3.4900, top1: 23.4375\n",
      "Epoch [13/60], Iter [373/633], LR: 0.005000, Loss: 3.5449, top1: 10.9375\n",
      "Epoch [13/60], Iter [374/633], LR: 0.005000, Loss: 3.5918, top1: 9.3750\n",
      "Epoch [13/60], Iter [375/633], LR: 0.005000, Loss: 3.5836, top1: 14.0625\n",
      "Epoch [13/60], Iter [376/633], LR: 0.005000, Loss: 3.5093, top1: 23.4375\n",
      "Epoch [13/60], Iter [377/633], LR: 0.005000, Loss: 3.5598, top1: 10.9375\n",
      "Epoch [13/60], Iter [378/633], LR: 0.005000, Loss: 3.5098, top1: 20.3125\n",
      "Epoch [13/60], Iter [379/633], LR: 0.005000, Loss: 3.5864, top1: 21.8750\n",
      "Epoch [13/60], Iter [380/633], LR: 0.005000, Loss: 3.5443, top1: 17.1875\n",
      "Epoch [13/60], Iter [381/633], LR: 0.005000, Loss: 3.5765, top1: 14.0625\n",
      "Epoch [13/60], Iter [382/633], LR: 0.005000, Loss: 3.5543, top1: 14.0625\n",
      "Epoch [13/60], Iter [383/633], LR: 0.005000, Loss: 3.5855, top1: 10.9375\n",
      "Epoch [13/60], Iter [384/633], LR: 0.005000, Loss: 3.5585, top1: 20.3125\n",
      "Epoch [13/60], Iter [385/633], LR: 0.005000, Loss: 3.5629, top1: 12.5000\n",
      "Epoch [13/60], Iter [386/633], LR: 0.005000, Loss: 3.5501, top1: 12.5000\n",
      "Epoch [13/60], Iter [387/633], LR: 0.005000, Loss: 3.5992, top1: 9.3750\n",
      "Epoch [13/60], Iter [388/633], LR: 0.005000, Loss: 3.5965, top1: 14.0625\n",
      "Epoch [13/60], Iter [389/633], LR: 0.005000, Loss: 3.5512, top1: 18.7500\n",
      "Epoch [13/60], Iter [390/633], LR: 0.005000, Loss: 3.4965, top1: 20.3125\n",
      "Epoch [13/60], Iter [391/633], LR: 0.005000, Loss: 3.6105, top1: 10.9375\n",
      "Epoch [13/60], Iter [392/633], LR: 0.005000, Loss: 3.6054, top1: 7.8125\n",
      "Epoch [13/60], Iter [393/633], LR: 0.005000, Loss: 3.6072, top1: 10.9375\n",
      "Epoch [13/60], Iter [394/633], LR: 0.005000, Loss: 3.5034, top1: 25.0000\n",
      "Epoch [13/60], Iter [395/633], LR: 0.005000, Loss: 3.6014, top1: 17.1875\n",
      "Epoch [13/60], Iter [396/633], LR: 0.005000, Loss: 3.6010, top1: 7.8125\n",
      "Epoch [13/60], Iter [397/633], LR: 0.005000, Loss: 3.5916, top1: 10.9375\n",
      "Epoch [13/60], Iter [398/633], LR: 0.005000, Loss: 3.5738, top1: 15.6250\n",
      "Epoch [13/60], Iter [399/633], LR: 0.005000, Loss: 3.5903, top1: 15.6250\n",
      "Epoch [13/60], Iter [400/633], LR: 0.005000, Loss: 3.6075, top1: 14.0625\n",
      "Epoch [13/60], Iter [401/633], LR: 0.005000, Loss: 3.5423, top1: 20.3125\n",
      "Epoch [13/60], Iter [402/633], LR: 0.005000, Loss: 3.5556, top1: 15.6250\n",
      "Epoch [13/60], Iter [403/633], LR: 0.005000, Loss: 3.5594, top1: 10.9375\n",
      "Epoch [13/60], Iter [404/633], LR: 0.005000, Loss: 3.5518, top1: 15.6250\n",
      "Epoch [13/60], Iter [405/633], LR: 0.005000, Loss: 3.5827, top1: 14.0625\n",
      "Epoch [13/60], Iter [406/633], LR: 0.005000, Loss: 3.5921, top1: 15.6250\n",
      "Epoch [13/60], Iter [407/633], LR: 0.005000, Loss: 3.5557, top1: 17.1875\n",
      "Epoch [13/60], Iter [408/633], LR: 0.005000, Loss: 3.6225, top1: 4.6875\n",
      "Epoch [13/60], Iter [409/633], LR: 0.005000, Loss: 3.5823, top1: 12.5000\n",
      "Epoch [13/60], Iter [410/633], LR: 0.005000, Loss: 3.5519, top1: 18.7500\n",
      "Epoch [13/60], Iter [411/633], LR: 0.005000, Loss: 3.5571, top1: 15.6250\n",
      "Epoch [13/60], Iter [412/633], LR: 0.005000, Loss: 3.6242, top1: 4.6875\n",
      "Epoch [13/60], Iter [413/633], LR: 0.005000, Loss: 3.6154, top1: 12.5000\n",
      "Epoch [13/60], Iter [414/633], LR: 0.005000, Loss: 3.5756, top1: 14.0625\n",
      "Epoch [13/60], Iter [415/633], LR: 0.005000, Loss: 3.5171, top1: 23.4375\n",
      "Epoch [13/60], Iter [416/633], LR: 0.005000, Loss: 3.5797, top1: 15.6250\n",
      "Epoch [13/60], Iter [417/633], LR: 0.005000, Loss: 3.6170, top1: 12.5000\n",
      "Epoch [13/60], Iter [418/633], LR: 0.005000, Loss: 3.5673, top1: 10.9375\n",
      "Epoch [13/60], Iter [419/633], LR: 0.005000, Loss: 3.5442, top1: 17.1875\n",
      "Epoch [13/60], Iter [420/633], LR: 0.005000, Loss: 3.5788, top1: 15.6250\n",
      "Epoch [13/60], Iter [421/633], LR: 0.005000, Loss: 3.5811, top1: 14.0625\n",
      "Epoch [13/60], Iter [422/633], LR: 0.005000, Loss: 3.5490, top1: 20.3125\n",
      "Epoch [13/60], Iter [423/633], LR: 0.005000, Loss: 3.5097, top1: 25.0000\n",
      "Epoch [13/60], Iter [424/633], LR: 0.005000, Loss: 3.6158, top1: 9.3750\n",
      "Epoch [13/60], Iter [425/633], LR: 0.005000, Loss: 3.5345, top1: 15.6250\n",
      "Epoch [13/60], Iter [426/633], LR: 0.005000, Loss: 3.5971, top1: 10.9375\n",
      "Epoch [13/60], Iter [427/633], LR: 0.005000, Loss: 3.5794, top1: 20.3125\n",
      "Epoch [13/60], Iter [428/633], LR: 0.005000, Loss: 3.5853, top1: 18.7500\n",
      "Epoch [13/60], Iter [429/633], LR: 0.005000, Loss: 3.5342, top1: 20.3125\n",
      "Epoch [13/60], Iter [430/633], LR: 0.005000, Loss: 3.5959, top1: 14.0625\n",
      "Epoch [13/60], Iter [431/633], LR: 0.005000, Loss: 3.5841, top1: 12.5000\n",
      "Epoch [13/60], Iter [432/633], LR: 0.005000, Loss: 3.5967, top1: 10.9375\n",
      "Epoch [13/60], Iter [433/633], LR: 0.005000, Loss: 3.5602, top1: 15.6250\n",
      "Epoch [13/60], Iter [434/633], LR: 0.005000, Loss: 3.5723, top1: 14.0625\n",
      "Epoch [13/60], Iter [435/633], LR: 0.005000, Loss: 3.5648, top1: 14.0625\n",
      "Epoch [13/60], Iter [436/633], LR: 0.005000, Loss: 3.5703, top1: 17.1875\n",
      "Epoch [13/60], Iter [437/633], LR: 0.005000, Loss: 3.5832, top1: 10.9375\n",
      "Epoch [13/60], Iter [438/633], LR: 0.005000, Loss: 3.5706, top1: 14.0625\n",
      "Epoch [13/60], Iter [439/633], LR: 0.005000, Loss: 3.5874, top1: 15.6250\n",
      "Epoch [13/60], Iter [440/633], LR: 0.005000, Loss: 3.5780, top1: 14.0625\n",
      "Epoch [13/60], Iter [441/633], LR: 0.005000, Loss: 3.5649, top1: 17.1875\n",
      "Epoch [13/60], Iter [442/633], LR: 0.005000, Loss: 3.5731, top1: 9.3750\n",
      "Epoch [13/60], Iter [443/633], LR: 0.005000, Loss: 3.6271, top1: 9.3750\n",
      "Epoch [13/60], Iter [444/633], LR: 0.005000, Loss: 3.5638, top1: 17.1875\n",
      "Epoch [13/60], Iter [445/633], LR: 0.005000, Loss: 3.5622, top1: 18.7500\n",
      "Epoch [13/60], Iter [446/633], LR: 0.005000, Loss: 3.5802, top1: 15.6250\n",
      "Epoch [13/60], Iter [447/633], LR: 0.005000, Loss: 3.5750, top1: 15.6250\n",
      "Epoch [13/60], Iter [448/633], LR: 0.005000, Loss: 3.5777, top1: 17.1875\n",
      "Epoch [13/60], Iter [449/633], LR: 0.005000, Loss: 3.6014, top1: 6.2500\n",
      "Epoch [13/60], Iter [450/633], LR: 0.005000, Loss: 3.5997, top1: 10.9375\n",
      "Epoch [13/60], Iter [451/633], LR: 0.005000, Loss: 3.5731, top1: 18.7500\n",
      "Epoch [13/60], Iter [452/633], LR: 0.005000, Loss: 3.6121, top1: 10.9375\n",
      "Epoch [13/60], Iter [453/633], LR: 0.005000, Loss: 3.6071, top1: 12.5000\n",
      "Epoch [13/60], Iter [454/633], LR: 0.005000, Loss: 3.5556, top1: 18.7500\n",
      "Epoch [13/60], Iter [455/633], LR: 0.005000, Loss: 3.5590, top1: 12.5000\n",
      "Epoch [13/60], Iter [456/633], LR: 0.005000, Loss: 3.5834, top1: 10.9375\n",
      "Epoch [13/60], Iter [457/633], LR: 0.005000, Loss: 3.5974, top1: 14.0625\n",
      "Epoch [13/60], Iter [458/633], LR: 0.005000, Loss: 3.5662, top1: 14.0625\n",
      "Epoch [13/60], Iter [459/633], LR: 0.005000, Loss: 3.5889, top1: 9.3750\n",
      "Epoch [13/60], Iter [460/633], LR: 0.005000, Loss: 3.5619, top1: 12.5000\n",
      "Epoch [13/60], Iter [461/633], LR: 0.005000, Loss: 3.5776, top1: 15.6250\n",
      "Epoch [13/60], Iter [462/633], LR: 0.005000, Loss: 3.5411, top1: 20.3125\n",
      "Epoch [13/60], Iter [463/633], LR: 0.005000, Loss: 3.5563, top1: 14.0625\n",
      "Epoch [13/60], Iter [464/633], LR: 0.005000, Loss: 3.5482, top1: 15.6250\n",
      "Epoch [13/60], Iter [465/633], LR: 0.005000, Loss: 3.5402, top1: 12.5000\n",
      "Epoch [13/60], Iter [466/633], LR: 0.005000, Loss: 3.5720, top1: 14.0625\n",
      "Epoch [13/60], Iter [467/633], LR: 0.005000, Loss: 3.5774, top1: 14.0625\n",
      "Epoch [13/60], Iter [468/633], LR: 0.005000, Loss: 3.5525, top1: 17.1875\n",
      "Epoch [13/60], Iter [469/633], LR: 0.005000, Loss: 3.5993, top1: 17.1875\n",
      "Epoch [13/60], Iter [470/633], LR: 0.005000, Loss: 3.6164, top1: 10.9375\n",
      "Epoch [13/60], Iter [471/633], LR: 0.005000, Loss: 3.5973, top1: 10.9375\n",
      "Epoch [13/60], Iter [472/633], LR: 0.005000, Loss: 3.5683, top1: 15.6250\n",
      "Epoch [13/60], Iter [473/633], LR: 0.005000, Loss: 3.5822, top1: 12.5000\n",
      "Epoch [13/60], Iter [474/633], LR: 0.005000, Loss: 3.5607, top1: 14.0625\n",
      "Epoch [13/60], Iter [475/633], LR: 0.005000, Loss: 3.5740, top1: 9.3750\n",
      "Epoch [13/60], Iter [476/633], LR: 0.005000, Loss: 3.5541, top1: 18.7500\n",
      "Epoch [13/60], Iter [477/633], LR: 0.005000, Loss: 3.5881, top1: 15.6250\n",
      "Epoch [13/60], Iter [478/633], LR: 0.005000, Loss: 3.5921, top1: 9.3750\n",
      "Epoch [13/60], Iter [479/633], LR: 0.005000, Loss: 3.5841, top1: 17.1875\n",
      "Epoch [13/60], Iter [480/633], LR: 0.005000, Loss: 3.6212, top1: 6.2500\n",
      "Epoch [13/60], Iter [481/633], LR: 0.005000, Loss: 3.5553, top1: 20.3125\n",
      "Epoch [13/60], Iter [482/633], LR: 0.005000, Loss: 3.6163, top1: 10.9375\n",
      "Epoch [13/60], Iter [483/633], LR: 0.005000, Loss: 3.6295, top1: 4.6875\n",
      "Epoch [13/60], Iter [484/633], LR: 0.005000, Loss: 3.5780, top1: 12.5000\n",
      "Epoch [13/60], Iter [485/633], LR: 0.005000, Loss: 3.5831, top1: 12.5000\n",
      "Epoch [13/60], Iter [486/633], LR: 0.005000, Loss: 3.5348, top1: 18.7500\n",
      "Epoch [13/60], Iter [487/633], LR: 0.005000, Loss: 3.5856, top1: 12.5000\n",
      "Epoch [13/60], Iter [488/633], LR: 0.005000, Loss: 3.5977, top1: 9.3750\n",
      "Epoch [13/60], Iter [489/633], LR: 0.005000, Loss: 3.5847, top1: 9.3750\n",
      "Epoch [13/60], Iter [490/633], LR: 0.005000, Loss: 3.6156, top1: 9.3750\n",
      "Epoch [13/60], Iter [491/633], LR: 0.005000, Loss: 3.6092, top1: 10.9375\n",
      "Epoch [13/60], Iter [492/633], LR: 0.005000, Loss: 3.5289, top1: 26.5625\n",
      "Epoch [13/60], Iter [493/633], LR: 0.005000, Loss: 3.5897, top1: 10.9375\n",
      "Epoch [13/60], Iter [494/633], LR: 0.005000, Loss: 3.5316, top1: 21.8750\n",
      "Epoch [13/60], Iter [495/633], LR: 0.005000, Loss: 3.5672, top1: 18.7500\n",
      "Epoch [13/60], Iter [496/633], LR: 0.005000, Loss: 3.5638, top1: 17.1875\n",
      "Epoch [13/60], Iter [497/633], LR: 0.005000, Loss: 3.5738, top1: 10.9375\n",
      "Epoch [13/60], Iter [498/633], LR: 0.005000, Loss: 3.5855, top1: 14.0625\n",
      "Epoch [13/60], Iter [499/633], LR: 0.005000, Loss: 3.5894, top1: 15.6250\n",
      "Epoch [13/60], Iter [500/633], LR: 0.005000, Loss: 3.6031, top1: 7.8125\n",
      "Epoch [13/60], Iter [501/633], LR: 0.005000, Loss: 3.6185, top1: 9.3750\n",
      "Epoch [13/60], Iter [502/633], LR: 0.005000, Loss: 3.6160, top1: 9.3750\n",
      "Epoch [13/60], Iter [503/633], LR: 0.005000, Loss: 3.5679, top1: 14.0625\n",
      "Epoch [13/60], Iter [504/633], LR: 0.005000, Loss: 3.5543, top1: 12.5000\n",
      "Epoch [13/60], Iter [505/633], LR: 0.005000, Loss: 3.5276, top1: 25.0000\n",
      "Epoch [13/60], Iter [506/633], LR: 0.005000, Loss: 3.6147, top1: 14.0625\n",
      "Epoch [13/60], Iter [507/633], LR: 0.005000, Loss: 3.5127, top1: 23.4375\n",
      "Epoch [13/60], Iter [508/633], LR: 0.005000, Loss: 3.5788, top1: 7.8125\n",
      "Epoch [13/60], Iter [509/633], LR: 0.005000, Loss: 3.6261, top1: 9.3750\n",
      "Epoch [13/60], Iter [510/633], LR: 0.005000, Loss: 3.5705, top1: 12.5000\n",
      "Epoch [13/60], Iter [511/633], LR: 0.005000, Loss: 3.5896, top1: 9.3750\n",
      "Epoch [13/60], Iter [512/633], LR: 0.005000, Loss: 3.5714, top1: 17.1875\n",
      "Epoch [13/60], Iter [513/633], LR: 0.005000, Loss: 3.5925, top1: 10.9375\n",
      "Epoch [13/60], Iter [514/633], LR: 0.005000, Loss: 3.5798, top1: 9.3750\n",
      "Epoch [13/60], Iter [515/633], LR: 0.005000, Loss: 3.5737, top1: 14.0625\n",
      "Epoch [13/60], Iter [516/633], LR: 0.005000, Loss: 3.6111, top1: 6.2500\n",
      "Epoch [13/60], Iter [517/633], LR: 0.005000, Loss: 3.5517, top1: 18.7500\n",
      "Epoch [13/60], Iter [518/633], LR: 0.005000, Loss: 3.5984, top1: 18.7500\n",
      "Epoch [13/60], Iter [519/633], LR: 0.005000, Loss: 3.5249, top1: 20.3125\n",
      "Epoch [13/60], Iter [520/633], LR: 0.005000, Loss: 3.6066, top1: 18.7500\n",
      "Epoch [13/60], Iter [521/633], LR: 0.005000, Loss: 3.5674, top1: 12.5000\n",
      "Epoch [13/60], Iter [522/633], LR: 0.005000, Loss: 3.5778, top1: 15.6250\n",
      "Epoch [13/60], Iter [523/633], LR: 0.005000, Loss: 3.6024, top1: 9.3750\n",
      "Epoch [13/60], Iter [524/633], LR: 0.005000, Loss: 3.5969, top1: 9.3750\n",
      "Epoch [13/60], Iter [525/633], LR: 0.005000, Loss: 3.5804, top1: 14.0625\n",
      "Epoch [13/60], Iter [526/633], LR: 0.005000, Loss: 3.5101, top1: 17.1875\n",
      "Epoch [13/60], Iter [527/633], LR: 0.005000, Loss: 3.5394, top1: 20.3125\n",
      "Epoch [13/60], Iter [528/633], LR: 0.005000, Loss: 3.5605, top1: 20.3125\n",
      "Epoch [13/60], Iter [529/633], LR: 0.005000, Loss: 3.6227, top1: 10.9375\n",
      "Epoch [13/60], Iter [530/633], LR: 0.005000, Loss: 3.6135, top1: 10.9375\n",
      "Epoch [13/60], Iter [531/633], LR: 0.005000, Loss: 3.5645, top1: 21.8750\n",
      "Epoch [13/60], Iter [532/633], LR: 0.005000, Loss: 3.5373, top1: 15.6250\n",
      "Epoch [13/60], Iter [533/633], LR: 0.005000, Loss: 3.5878, top1: 15.6250\n",
      "Epoch [13/60], Iter [534/633], LR: 0.005000, Loss: 3.6083, top1: 14.0625\n",
      "Epoch [13/60], Iter [535/633], LR: 0.005000, Loss: 3.6082, top1: 10.9375\n",
      "Epoch [13/60], Iter [536/633], LR: 0.005000, Loss: 3.5133, top1: 21.8750\n",
      "Epoch [13/60], Iter [537/633], LR: 0.005000, Loss: 3.5977, top1: 14.0625\n",
      "Epoch [13/60], Iter [538/633], LR: 0.005000, Loss: 3.5982, top1: 17.1875\n",
      "Epoch [13/60], Iter [539/633], LR: 0.005000, Loss: 3.6114, top1: 9.3750\n",
      "Epoch [13/60], Iter [540/633], LR: 0.005000, Loss: 3.5901, top1: 6.2500\n",
      "Epoch [13/60], Iter [541/633], LR: 0.005000, Loss: 3.6171, top1: 9.3750\n",
      "Epoch [13/60], Iter [542/633], LR: 0.005000, Loss: 3.5899, top1: 9.3750\n",
      "Epoch [13/60], Iter [543/633], LR: 0.005000, Loss: 3.5462, top1: 20.3125\n",
      "Epoch [13/60], Iter [544/633], LR: 0.005000, Loss: 3.5703, top1: 20.3125\n",
      "Epoch [13/60], Iter [545/633], LR: 0.005000, Loss: 3.5465, top1: 20.3125\n",
      "Epoch [13/60], Iter [546/633], LR: 0.005000, Loss: 3.5831, top1: 15.6250\n",
      "Epoch [13/60], Iter [547/633], LR: 0.005000, Loss: 3.5627, top1: 17.1875\n",
      "Epoch [13/60], Iter [548/633], LR: 0.005000, Loss: 3.5820, top1: 17.1875\n",
      "Epoch [13/60], Iter [549/633], LR: 0.005000, Loss: 3.5814, top1: 14.0625\n",
      "Epoch [13/60], Iter [550/633], LR: 0.005000, Loss: 3.6197, top1: 4.6875\n",
      "Epoch [13/60], Iter [551/633], LR: 0.005000, Loss: 3.5271, top1: 17.1875\n",
      "Epoch [13/60], Iter [552/633], LR: 0.005000, Loss: 3.5605, top1: 14.0625\n",
      "Epoch [13/60], Iter [553/633], LR: 0.005000, Loss: 3.5494, top1: 17.1875\n",
      "Epoch [13/60], Iter [554/633], LR: 0.005000, Loss: 3.5835, top1: 14.0625\n",
      "Epoch [13/60], Iter [555/633], LR: 0.005000, Loss: 3.5803, top1: 10.9375\n",
      "Epoch [13/60], Iter [556/633], LR: 0.005000, Loss: 3.6031, top1: 14.0625\n",
      "Epoch [13/60], Iter [557/633], LR: 0.005000, Loss: 3.6284, top1: 6.2500\n",
      "Epoch [13/60], Iter [558/633], LR: 0.005000, Loss: 3.5848, top1: 10.9375\n",
      "Epoch [13/60], Iter [559/633], LR: 0.005000, Loss: 3.5921, top1: 18.7500\n",
      "Epoch [13/60], Iter [560/633], LR: 0.005000, Loss: 3.5589, top1: 12.5000\n",
      "Epoch [13/60], Iter [561/633], LR: 0.005000, Loss: 3.6109, top1: 9.3750\n",
      "Epoch [13/60], Iter [562/633], LR: 0.005000, Loss: 3.6192, top1: 9.3750\n",
      "Epoch [13/60], Iter [563/633], LR: 0.005000, Loss: 3.5529, top1: 21.8750\n",
      "Epoch [13/60], Iter [564/633], LR: 0.005000, Loss: 3.6286, top1: 4.6875\n",
      "Epoch [13/60], Iter [565/633], LR: 0.005000, Loss: 3.5431, top1: 23.4375\n",
      "Epoch [13/60], Iter [566/633], LR: 0.005000, Loss: 3.5743, top1: 15.6250\n",
      "Epoch [13/60], Iter [567/633], LR: 0.005000, Loss: 3.5839, top1: 15.6250\n",
      "Epoch [13/60], Iter [568/633], LR: 0.005000, Loss: 3.5580, top1: 15.6250\n",
      "Epoch [13/60], Iter [569/633], LR: 0.005000, Loss: 3.5661, top1: 20.3125\n",
      "Epoch [13/60], Iter [570/633], LR: 0.005000, Loss: 3.5749, top1: 15.6250\n",
      "Epoch [13/60], Iter [571/633], LR: 0.005000, Loss: 3.5957, top1: 12.5000\n",
      "Epoch [13/60], Iter [572/633], LR: 0.005000, Loss: 3.5899, top1: 18.7500\n",
      "Epoch [13/60], Iter [573/633], LR: 0.005000, Loss: 3.5779, top1: 10.9375\n",
      "Epoch [13/60], Iter [574/633], LR: 0.005000, Loss: 3.5314, top1: 23.4375\n",
      "Epoch [13/60], Iter [575/633], LR: 0.005000, Loss: 3.5299, top1: 20.3125\n",
      "Epoch [13/60], Iter [576/633], LR: 0.005000, Loss: 3.5442, top1: 20.3125\n",
      "Epoch [13/60], Iter [577/633], LR: 0.005000, Loss: 3.5197, top1: 20.3125\n",
      "Epoch [13/60], Iter [578/633], LR: 0.005000, Loss: 3.5739, top1: 14.0625\n",
      "Epoch [13/60], Iter [579/633], LR: 0.005000, Loss: 3.5364, top1: 20.3125\n",
      "Epoch [13/60], Iter [580/633], LR: 0.005000, Loss: 3.5908, top1: 10.9375\n",
      "Epoch [13/60], Iter [581/633], LR: 0.005000, Loss: 3.6155, top1: 6.2500\n",
      "Epoch [13/60], Iter [582/633], LR: 0.005000, Loss: 3.6157, top1: 14.0625\n",
      "Epoch [13/60], Iter [583/633], LR: 0.005000, Loss: 3.5394, top1: 21.8750\n",
      "Epoch [13/60], Iter [584/633], LR: 0.005000, Loss: 3.5913, top1: 12.5000\n",
      "Epoch [13/60], Iter [585/633], LR: 0.005000, Loss: 3.5401, top1: 15.6250\n",
      "Epoch [13/60], Iter [586/633], LR: 0.005000, Loss: 3.5226, top1: 23.4375\n",
      "Epoch [13/60], Iter [587/633], LR: 0.005000, Loss: 3.5282, top1: 18.7500\n",
      "Epoch [13/60], Iter [588/633], LR: 0.005000, Loss: 3.5518, top1: 17.1875\n",
      "Epoch [13/60], Iter [589/633], LR: 0.005000, Loss: 3.5512, top1: 14.0625\n",
      "Epoch [13/60], Iter [590/633], LR: 0.005000, Loss: 3.6253, top1: 7.8125\n",
      "Epoch [13/60], Iter [591/633], LR: 0.005000, Loss: 3.5377, top1: 23.4375\n",
      "Epoch [13/60], Iter [592/633], LR: 0.005000, Loss: 3.5903, top1: 12.5000\n",
      "Epoch [13/60], Iter [593/633], LR: 0.005000, Loss: 3.5737, top1: 17.1875\n",
      "Epoch [13/60], Iter [594/633], LR: 0.005000, Loss: 3.5962, top1: 15.6250\n",
      "Epoch [13/60], Iter [595/633], LR: 0.005000, Loss: 3.5941, top1: 10.9375\n",
      "Epoch [13/60], Iter [596/633], LR: 0.005000, Loss: 3.5714, top1: 14.0625\n",
      "Epoch [13/60], Iter [597/633], LR: 0.005000, Loss: 3.5947, top1: 18.7500\n",
      "Epoch [13/60], Iter [598/633], LR: 0.005000, Loss: 3.5473, top1: 15.6250\n",
      "Epoch [13/60], Iter [599/633], LR: 0.005000, Loss: 3.5800, top1: 12.5000\n",
      "Epoch [13/60], Iter [600/633], LR: 0.005000, Loss: 3.5658, top1: 20.3125\n",
      "Epoch [13/60], Iter [601/633], LR: 0.005000, Loss: 3.5820, top1: 17.1875\n",
      "Epoch [13/60], Iter [602/633], LR: 0.005000, Loss: 3.5464, top1: 18.7500\n",
      "Epoch [13/60], Iter [603/633], LR: 0.005000, Loss: 3.5450, top1: 12.5000\n",
      "Epoch [13/60], Iter [604/633], LR: 0.005000, Loss: 3.5981, top1: 15.6250\n",
      "Epoch [13/60], Iter [605/633], LR: 0.005000, Loss: 3.5716, top1: 7.8125\n",
      "Epoch [13/60], Iter [606/633], LR: 0.005000, Loss: 3.5157, top1: 23.4375\n",
      "Epoch [13/60], Iter [607/633], LR: 0.005000, Loss: 3.5420, top1: 15.6250\n",
      "Epoch [13/60], Iter [608/633], LR: 0.005000, Loss: 3.5886, top1: 14.0625\n",
      "Epoch [13/60], Iter [609/633], LR: 0.005000, Loss: 3.5841, top1: 15.6250\n",
      "Epoch [13/60], Iter [610/633], LR: 0.005000, Loss: 3.5431, top1: 12.5000\n",
      "Epoch [13/60], Iter [611/633], LR: 0.005000, Loss: 3.5423, top1: 15.6250\n",
      "Epoch [13/60], Iter [612/633], LR: 0.005000, Loss: 3.5904, top1: 12.5000\n",
      "Epoch [13/60], Iter [613/633], LR: 0.005000, Loss: 3.6001, top1: 9.3750\n",
      "Epoch [13/60], Iter [614/633], LR: 0.005000, Loss: 3.5866, top1: 9.3750\n",
      "Epoch [13/60], Iter [615/633], LR: 0.005000, Loss: 3.6014, top1: 12.5000\n",
      "Epoch [13/60], Iter [616/633], LR: 0.005000, Loss: 3.5987, top1: 7.8125\n",
      "Epoch [13/60], Iter [617/633], LR: 0.005000, Loss: 3.5434, top1: 15.6250\n",
      "Epoch [13/60], Iter [618/633], LR: 0.005000, Loss: 3.5856, top1: 18.7500\n",
      "Epoch [13/60], Iter [619/633], LR: 0.005000, Loss: 3.5471, top1: 17.1875\n",
      "Epoch [13/60], Iter [620/633], LR: 0.005000, Loss: 3.5822, top1: 15.6250\n",
      "Epoch [13/60], Iter [621/633], LR: 0.005000, Loss: 3.6208, top1: 14.0625\n",
      "Epoch [13/60], Iter [622/633], LR: 0.005000, Loss: 3.5833, top1: 9.3750\n",
      "Epoch [13/60], Iter [623/633], LR: 0.005000, Loss: 3.5467, top1: 10.9375\n",
      "Epoch [13/60], Iter [624/633], LR: 0.005000, Loss: 3.5872, top1: 3.1250\n",
      "Epoch [13/60], Iter [625/633], LR: 0.005000, Loss: 3.5621, top1: 12.5000\n",
      "Epoch [13/60], Iter [626/633], LR: 0.005000, Loss: 3.5610, top1: 14.0625\n",
      "Epoch [13/60], Iter [627/633], LR: 0.005000, Loss: 3.5621, top1: 14.0625\n",
      "Epoch [13/60], Iter [628/633], LR: 0.005000, Loss: 3.5478, top1: 14.0625\n",
      "Epoch [13/60], Iter [629/633], LR: 0.005000, Loss: 3.6083, top1: 9.3750\n",
      "Epoch [13/60], Iter [630/633], LR: 0.005000, Loss: 3.5511, top1: 18.7500\n",
      "Epoch [13/60], Iter [631/633], LR: 0.005000, Loss: 3.5792, top1: 18.7500\n",
      "Epoch [13/60], Iter [632/633], LR: 0.005000, Loss: 3.6516, top1: 7.8125\n",
      "Epoch [13/60], Iter [633/633], LR: 0.005000, Loss: 3.5703, top1: 14.0625\n",
      "Epoch [13/60], Iter [634/633], LR: 0.005000, Loss: 3.5584, top1: 14.5161\n",
      "Epoch [13/60], Val_Loss: 3.5755, Val_top1: 13.8644, best_top1: 14.6567\n",
      "epoch time: 4.488940095901489 min\n",
      "Epoch [14/60], Iter [1/633], LR: 0.005000, Loss: 3.5911, top1: 9.3750\n",
      "Epoch [14/60], Iter [2/633], LR: 0.005000, Loss: 3.5968, top1: 10.9375\n",
      "Epoch [14/60], Iter [3/633], LR: 0.005000, Loss: 3.6139, top1: 12.5000\n",
      "Epoch [14/60], Iter [4/633], LR: 0.005000, Loss: 3.5107, top1: 17.1875\n",
      "Epoch [14/60], Iter [5/633], LR: 0.005000, Loss: 3.5957, top1: 10.9375\n",
      "Epoch [14/60], Iter [6/633], LR: 0.005000, Loss: 3.5745, top1: 14.0625\n",
      "Epoch [14/60], Iter [7/633], LR: 0.005000, Loss: 3.5967, top1: 12.5000\n",
      "Epoch [14/60], Iter [8/633], LR: 0.005000, Loss: 3.5239, top1: 17.1875\n",
      "Epoch [14/60], Iter [9/633], LR: 0.005000, Loss: 3.5141, top1: 17.1875\n",
      "Epoch [14/60], Iter [10/633], LR: 0.005000, Loss: 3.5601, top1: 20.3125\n",
      "Epoch [14/60], Iter [11/633], LR: 0.005000, Loss: 3.5728, top1: 17.1875\n",
      "Epoch [14/60], Iter [12/633], LR: 0.005000, Loss: 3.5458, top1: 15.6250\n",
      "Epoch [14/60], Iter [13/633], LR: 0.005000, Loss: 3.5948, top1: 12.5000\n",
      "Epoch [14/60], Iter [14/633], LR: 0.005000, Loss: 3.5882, top1: 17.1875\n",
      "Epoch [14/60], Iter [15/633], LR: 0.005000, Loss: 3.5733, top1: 15.6250\n",
      "Epoch [14/60], Iter [16/633], LR: 0.005000, Loss: 3.5953, top1: 9.3750\n",
      "Epoch [14/60], Iter [17/633], LR: 0.005000, Loss: 3.6205, top1: 7.8125\n",
      "Epoch [14/60], Iter [18/633], LR: 0.005000, Loss: 3.5359, top1: 21.8750\n",
      "Epoch [14/60], Iter [19/633], LR: 0.005000, Loss: 3.5890, top1: 10.9375\n",
      "Epoch [14/60], Iter [20/633], LR: 0.005000, Loss: 3.5724, top1: 12.5000\n",
      "Epoch [14/60], Iter [21/633], LR: 0.005000, Loss: 3.5742, top1: 12.5000\n",
      "Epoch [14/60], Iter [22/633], LR: 0.005000, Loss: 3.5305, top1: 21.8750\n",
      "Epoch [14/60], Iter [23/633], LR: 0.005000, Loss: 3.5620, top1: 15.6250\n",
      "Epoch [14/60], Iter [24/633], LR: 0.005000, Loss: 3.5607, top1: 18.7500\n",
      "Epoch [14/60], Iter [25/633], LR: 0.005000, Loss: 3.5955, top1: 9.3750\n",
      "Epoch [14/60], Iter [26/633], LR: 0.005000, Loss: 3.5720, top1: 15.6250\n",
      "Epoch [14/60], Iter [27/633], LR: 0.005000, Loss: 3.5934, top1: 14.0625\n",
      "Epoch [14/60], Iter [28/633], LR: 0.005000, Loss: 3.5410, top1: 20.3125\n",
      "Epoch [14/60], Iter [29/633], LR: 0.005000, Loss: 3.6178, top1: 6.2500\n",
      "Epoch [14/60], Iter [30/633], LR: 0.005000, Loss: 3.5705, top1: 23.4375\n",
      "Epoch [14/60], Iter [31/633], LR: 0.005000, Loss: 3.5517, top1: 15.6250\n",
      "Epoch [14/60], Iter [32/633], LR: 0.005000, Loss: 3.5504, top1: 17.1875\n",
      "Epoch [14/60], Iter [33/633], LR: 0.005000, Loss: 3.5696, top1: 10.9375\n",
      "Epoch [14/60], Iter [34/633], LR: 0.005000, Loss: 3.5533, top1: 15.6250\n",
      "Epoch [14/60], Iter [35/633], LR: 0.005000, Loss: 3.5598, top1: 15.6250\n",
      "Epoch [14/60], Iter [36/633], LR: 0.005000, Loss: 3.5959, top1: 12.5000\n",
      "Epoch [14/60], Iter [37/633], LR: 0.005000, Loss: 3.5814, top1: 15.6250\n",
      "Epoch [14/60], Iter [38/633], LR: 0.005000, Loss: 3.5582, top1: 17.1875\n",
      "Epoch [14/60], Iter [39/633], LR: 0.005000, Loss: 3.6246, top1: 10.9375\n",
      "Epoch [14/60], Iter [40/633], LR: 0.005000, Loss: 3.5680, top1: 14.0625\n",
      "Epoch [14/60], Iter [41/633], LR: 0.005000, Loss: 3.6306, top1: 9.3750\n",
      "Epoch [14/60], Iter [42/633], LR: 0.005000, Loss: 3.5976, top1: 14.0625\n",
      "Epoch [14/60], Iter [43/633], LR: 0.005000, Loss: 3.5581, top1: 20.3125\n",
      "Epoch [14/60], Iter [44/633], LR: 0.005000, Loss: 3.5307, top1: 17.1875\n",
      "Epoch [14/60], Iter [45/633], LR: 0.005000, Loss: 3.5555, top1: 20.3125\n",
      "Epoch [14/60], Iter [46/633], LR: 0.005000, Loss: 3.6653, top1: 9.3750\n",
      "Epoch [14/60], Iter [47/633], LR: 0.005000, Loss: 3.5048, top1: 18.7500\n",
      "Epoch [14/60], Iter [48/633], LR: 0.005000, Loss: 3.5430, top1: 10.9375\n",
      "Epoch [14/60], Iter [49/633], LR: 0.005000, Loss: 3.5602, top1: 20.3125\n",
      "Epoch [14/60], Iter [50/633], LR: 0.005000, Loss: 3.5572, top1: 17.1875\n",
      "Epoch [14/60], Iter [51/633], LR: 0.005000, Loss: 3.5871, top1: 15.6250\n",
      "Epoch [14/60], Iter [52/633], LR: 0.005000, Loss: 3.5601, top1: 14.0625\n",
      "Epoch [14/60], Iter [53/633], LR: 0.005000, Loss: 3.5692, top1: 14.0625\n",
      "Epoch [14/60], Iter [54/633], LR: 0.005000, Loss: 3.5769, top1: 20.3125\n",
      "Epoch [14/60], Iter [55/633], LR: 0.005000, Loss: 3.5701, top1: 10.9375\n",
      "Epoch [14/60], Iter [56/633], LR: 0.005000, Loss: 3.5971, top1: 14.0625\n",
      "Epoch [14/60], Iter [57/633], LR: 0.005000, Loss: 3.5660, top1: 18.7500\n",
      "Epoch [14/60], Iter [58/633], LR: 0.005000, Loss: 3.5715, top1: 17.1875\n",
      "Epoch [14/60], Iter [59/633], LR: 0.005000, Loss: 3.6000, top1: 12.5000\n",
      "Epoch [14/60], Iter [60/633], LR: 0.005000, Loss: 3.5691, top1: 17.1875\n",
      "Epoch [14/60], Iter [61/633], LR: 0.005000, Loss: 3.5792, top1: 7.8125\n",
      "Epoch [14/60], Iter [62/633], LR: 0.005000, Loss: 3.5990, top1: 15.6250\n",
      "Epoch [14/60], Iter [63/633], LR: 0.005000, Loss: 3.6147, top1: 9.3750\n",
      "Epoch [14/60], Iter [64/633], LR: 0.005000, Loss: 3.5784, top1: 10.9375\n",
      "Epoch [14/60], Iter [65/633], LR: 0.005000, Loss: 3.5884, top1: 9.3750\n",
      "Epoch [14/60], Iter [66/633], LR: 0.005000, Loss: 3.4771, top1: 25.0000\n",
      "Epoch [14/60], Iter [67/633], LR: 0.005000, Loss: 3.5535, top1: 18.7500\n",
      "Epoch [14/60], Iter [68/633], LR: 0.005000, Loss: 3.5845, top1: 15.6250\n",
      "Epoch [14/60], Iter [69/633], LR: 0.005000, Loss: 3.6373, top1: 6.2500\n",
      "Epoch [14/60], Iter [70/633], LR: 0.005000, Loss: 3.5795, top1: 12.5000\n",
      "Epoch [14/60], Iter [71/633], LR: 0.005000, Loss: 3.5243, top1: 17.1875\n",
      "Epoch [14/60], Iter [72/633], LR: 0.005000, Loss: 3.5989, top1: 10.9375\n",
      "Epoch [14/60], Iter [73/633], LR: 0.005000, Loss: 3.5522, top1: 20.3125\n",
      "Epoch [14/60], Iter [74/633], LR: 0.005000, Loss: 3.5422, top1: 20.3125\n",
      "Epoch [14/60], Iter [75/633], LR: 0.005000, Loss: 3.5855, top1: 14.0625\n",
      "Epoch [14/60], Iter [76/633], LR: 0.005000, Loss: 3.5591, top1: 17.1875\n",
      "Epoch [14/60], Iter [77/633], LR: 0.005000, Loss: 3.5961, top1: 14.0625\n",
      "Epoch [14/60], Iter [78/633], LR: 0.005000, Loss: 3.5352, top1: 15.6250\n",
      "Epoch [14/60], Iter [79/633], LR: 0.005000, Loss: 3.5517, top1: 18.7500\n",
      "Epoch [14/60], Iter [80/633], LR: 0.005000, Loss: 3.5248, top1: 18.7500\n",
      "Epoch [14/60], Iter [81/633], LR: 0.005000, Loss: 3.5290, top1: 15.6250\n",
      "Epoch [14/60], Iter [82/633], LR: 0.005000, Loss: 3.5622, top1: 14.0625\n",
      "Epoch [14/60], Iter [83/633], LR: 0.005000, Loss: 3.6125, top1: 6.2500\n",
      "Epoch [14/60], Iter [84/633], LR: 0.005000, Loss: 3.6020, top1: 14.0625\n",
      "Epoch [14/60], Iter [85/633], LR: 0.005000, Loss: 3.5934, top1: 14.0625\n",
      "Epoch [14/60], Iter [86/633], LR: 0.005000, Loss: 3.5602, top1: 18.7500\n",
      "Epoch [14/60], Iter [87/633], LR: 0.005000, Loss: 3.5344, top1: 23.4375\n",
      "Epoch [14/60], Iter [88/633], LR: 0.005000, Loss: 3.5832, top1: 15.6250\n",
      "Epoch [14/60], Iter [89/633], LR: 0.005000, Loss: 3.5872, top1: 9.3750\n",
      "Epoch [14/60], Iter [90/633], LR: 0.005000, Loss: 3.5678, top1: 14.0625\n",
      "Epoch [14/60], Iter [91/633], LR: 0.005000, Loss: 3.5546, top1: 14.0625\n",
      "Epoch [14/60], Iter [92/633], LR: 0.005000, Loss: 3.5538, top1: 15.6250\n",
      "Epoch [14/60], Iter [93/633], LR: 0.005000, Loss: 3.5558, top1: 18.7500\n",
      "Epoch [14/60], Iter [94/633], LR: 0.005000, Loss: 3.5678, top1: 14.0625\n",
      "Epoch [14/60], Iter [95/633], LR: 0.005000, Loss: 3.5056, top1: 18.7500\n",
      "Epoch [14/60], Iter [96/633], LR: 0.005000, Loss: 3.5522, top1: 15.6250\n",
      "Epoch [14/60], Iter [97/633], LR: 0.005000, Loss: 3.6098, top1: 15.6250\n",
      "Epoch [14/60], Iter [98/633], LR: 0.005000, Loss: 3.5552, top1: 18.7500\n",
      "Epoch [14/60], Iter [99/633], LR: 0.005000, Loss: 3.5823, top1: 14.0625\n",
      "Epoch [14/60], Iter [100/633], LR: 0.005000, Loss: 3.5451, top1: 21.8750\n",
      "Epoch [14/60], Iter [101/633], LR: 0.005000, Loss: 3.5372, top1: 20.3125\n",
      "Epoch [14/60], Iter [102/633], LR: 0.005000, Loss: 3.5477, top1: 20.3125\n",
      "Epoch [14/60], Iter [103/633], LR: 0.005000, Loss: 3.5565, top1: 10.9375\n",
      "Epoch [14/60], Iter [104/633], LR: 0.005000, Loss: 3.5789, top1: 10.9375\n",
      "Epoch [14/60], Iter [105/633], LR: 0.005000, Loss: 3.5891, top1: 12.5000\n",
      "Epoch [14/60], Iter [106/633], LR: 0.005000, Loss: 3.5414, top1: 18.7500\n",
      "Epoch [14/60], Iter [107/633], LR: 0.005000, Loss: 3.5541, top1: 17.1875\n",
      "Epoch [14/60], Iter [108/633], LR: 0.005000, Loss: 3.5778, top1: 14.0625\n",
      "Epoch [14/60], Iter [109/633], LR: 0.005000, Loss: 3.5536, top1: 14.0625\n",
      "Epoch [14/60], Iter [110/633], LR: 0.005000, Loss: 3.5841, top1: 15.6250\n",
      "Epoch [14/60], Iter [111/633], LR: 0.005000, Loss: 3.5906, top1: 14.0625\n",
      "Epoch [14/60], Iter [112/633], LR: 0.005000, Loss: 3.6398, top1: 10.9375\n",
      "Epoch [14/60], Iter [113/633], LR: 0.005000, Loss: 3.5703, top1: 10.9375\n",
      "Epoch [14/60], Iter [114/633], LR: 0.005000, Loss: 3.5219, top1: 25.0000\n",
      "Epoch [14/60], Iter [115/633], LR: 0.005000, Loss: 3.5074, top1: 31.2500\n",
      "Epoch [14/60], Iter [116/633], LR: 0.005000, Loss: 3.5595, top1: 17.1875\n",
      "Epoch [14/60], Iter [117/633], LR: 0.005000, Loss: 3.5521, top1: 17.1875\n",
      "Epoch [14/60], Iter [118/633], LR: 0.005000, Loss: 3.5648, top1: 20.3125\n",
      "Epoch [14/60], Iter [119/633], LR: 0.005000, Loss: 3.5561, top1: 12.5000\n",
      "Epoch [14/60], Iter [120/633], LR: 0.005000, Loss: 3.5217, top1: 23.4375\n",
      "Epoch [14/60], Iter [121/633], LR: 0.005000, Loss: 3.5353, top1: 20.3125\n",
      "Epoch [14/60], Iter [122/633], LR: 0.005000, Loss: 3.5109, top1: 26.5625\n",
      "Epoch [14/60], Iter [123/633], LR: 0.005000, Loss: 3.5676, top1: 17.1875\n",
      "Epoch [14/60], Iter [124/633], LR: 0.005000, Loss: 3.5608, top1: 12.5000\n",
      "Epoch [14/60], Iter [125/633], LR: 0.005000, Loss: 3.6113, top1: 7.8125\n",
      "Epoch [14/60], Iter [126/633], LR: 0.005000, Loss: 3.6032, top1: 15.6250\n",
      "Epoch [14/60], Iter [127/633], LR: 0.005000, Loss: 3.5818, top1: 7.8125\n",
      "Epoch [14/60], Iter [128/633], LR: 0.005000, Loss: 3.5950, top1: 12.5000\n",
      "Epoch [14/60], Iter [129/633], LR: 0.005000, Loss: 3.5824, top1: 17.1875\n",
      "Epoch [14/60], Iter [130/633], LR: 0.005000, Loss: 3.6120, top1: 7.8125\n",
      "Epoch [14/60], Iter [131/633], LR: 0.005000, Loss: 3.5779, top1: 9.3750\n",
      "Epoch [14/60], Iter [132/633], LR: 0.005000, Loss: 3.6121, top1: 10.9375\n",
      "Epoch [14/60], Iter [133/633], LR: 0.005000, Loss: 3.5922, top1: 10.9375\n",
      "Epoch [14/60], Iter [134/633], LR: 0.005000, Loss: 3.5621, top1: 14.0625\n",
      "Epoch [14/60], Iter [135/633], LR: 0.005000, Loss: 3.5682, top1: 14.0625\n",
      "Epoch [14/60], Iter [136/633], LR: 0.005000, Loss: 3.5608, top1: 17.1875\n",
      "Epoch [14/60], Iter [137/633], LR: 0.005000, Loss: 3.5737, top1: 14.0625\n",
      "Epoch [14/60], Iter [138/633], LR: 0.005000, Loss: 3.5659, top1: 17.1875\n",
      "Epoch [14/60], Iter [139/633], LR: 0.005000, Loss: 3.5589, top1: 9.3750\n",
      "Epoch [14/60], Iter [140/633], LR: 0.005000, Loss: 3.5324, top1: 21.8750\n",
      "Epoch [14/60], Iter [141/633], LR: 0.005000, Loss: 3.5711, top1: 15.6250\n",
      "Epoch [14/60], Iter [142/633], LR: 0.005000, Loss: 3.5957, top1: 12.5000\n",
      "Epoch [14/60], Iter [143/633], LR: 0.005000, Loss: 3.5593, top1: 15.6250\n",
      "Epoch [14/60], Iter [144/633], LR: 0.005000, Loss: 3.5357, top1: 18.7500\n",
      "Epoch [14/60], Iter [145/633], LR: 0.005000, Loss: 3.5648, top1: 10.9375\n",
      "Epoch [14/60], Iter [146/633], LR: 0.005000, Loss: 3.6007, top1: 12.5000\n",
      "Epoch [14/60], Iter [147/633], LR: 0.005000, Loss: 3.5676, top1: 21.8750\n",
      "Epoch [14/60], Iter [148/633], LR: 0.005000, Loss: 3.5828, top1: 10.9375\n",
      "Epoch [14/60], Iter [149/633], LR: 0.005000, Loss: 3.5759, top1: 12.5000\n",
      "Epoch [14/60], Iter [150/633], LR: 0.005000, Loss: 3.5491, top1: 14.0625\n",
      "Epoch [14/60], Iter [151/633], LR: 0.005000, Loss: 3.5752, top1: 9.3750\n",
      "Epoch [14/60], Iter [152/633], LR: 0.005000, Loss: 3.5414, top1: 20.3125\n",
      "Epoch [14/60], Iter [153/633], LR: 0.005000, Loss: 3.5661, top1: 20.3125\n",
      "Epoch [14/60], Iter [154/633], LR: 0.005000, Loss: 3.6111, top1: 9.3750\n",
      "Epoch [14/60], Iter [155/633], LR: 0.005000, Loss: 3.5519, top1: 15.6250\n",
      "Epoch [14/60], Iter [156/633], LR: 0.005000, Loss: 3.6325, top1: 10.9375\n",
      "Epoch [14/60], Iter [157/633], LR: 0.005000, Loss: 3.5973, top1: 10.9375\n",
      "Epoch [14/60], Iter [158/633], LR: 0.005000, Loss: 3.5955, top1: 10.9375\n",
      "Epoch [14/60], Iter [159/633], LR: 0.005000, Loss: 3.5790, top1: 14.0625\n",
      "Epoch [14/60], Iter [160/633], LR: 0.005000, Loss: 3.5909, top1: 14.0625\n",
      "Epoch [14/60], Iter [161/633], LR: 0.005000, Loss: 3.5984, top1: 17.1875\n",
      "Epoch [14/60], Iter [162/633], LR: 0.005000, Loss: 3.5598, top1: 21.8750\n",
      "Epoch [14/60], Iter [163/633], LR: 0.005000, Loss: 3.5609, top1: 15.6250\n",
      "Epoch [14/60], Iter [164/633], LR: 0.005000, Loss: 3.5161, top1: 18.7500\n",
      "Epoch [14/60], Iter [165/633], LR: 0.005000, Loss: 3.6008, top1: 12.5000\n",
      "Epoch [14/60], Iter [166/633], LR: 0.005000, Loss: 3.5857, top1: 15.6250\n",
      "Epoch [14/60], Iter [167/633], LR: 0.005000, Loss: 3.5743, top1: 12.5000\n",
      "Epoch [14/60], Iter [168/633], LR: 0.005000, Loss: 3.5450, top1: 18.7500\n",
      "Epoch [14/60], Iter [169/633], LR: 0.005000, Loss: 3.6078, top1: 14.0625\n",
      "Epoch [14/60], Iter [170/633], LR: 0.005000, Loss: 3.5966, top1: 7.8125\n",
      "Epoch [14/60], Iter [171/633], LR: 0.005000, Loss: 3.5359, top1: 17.1875\n",
      "Epoch [14/60], Iter [172/633], LR: 0.005000, Loss: 3.5840, top1: 9.3750\n",
      "Epoch [14/60], Iter [173/633], LR: 0.005000, Loss: 3.5171, top1: 18.7500\n",
      "Epoch [14/60], Iter [174/633], LR: 0.005000, Loss: 3.5689, top1: 15.6250\n",
      "Epoch [14/60], Iter [175/633], LR: 0.005000, Loss: 3.5647, top1: 15.6250\n",
      "Epoch [14/60], Iter [176/633], LR: 0.005000, Loss: 3.5859, top1: 9.3750\n",
      "Epoch [14/60], Iter [177/633], LR: 0.005000, Loss: 3.5527, top1: 15.6250\n",
      "Epoch [14/60], Iter [178/633], LR: 0.005000, Loss: 3.5622, top1: 17.1875\n",
      "Epoch [14/60], Iter [179/633], LR: 0.005000, Loss: 3.5883, top1: 15.6250\n",
      "Epoch [14/60], Iter [180/633], LR: 0.005000, Loss: 3.5538, top1: 18.7500\n",
      "Epoch [14/60], Iter [181/633], LR: 0.005000, Loss: 3.5800, top1: 17.1875\n",
      "Epoch [14/60], Iter [182/633], LR: 0.005000, Loss: 3.5602, top1: 17.1875\n",
      "Epoch [14/60], Iter [183/633], LR: 0.005000, Loss: 3.5797, top1: 14.0625\n",
      "Epoch [14/60], Iter [184/633], LR: 0.005000, Loss: 3.5588, top1: 23.4375\n",
      "Epoch [14/60], Iter [185/633], LR: 0.005000, Loss: 3.5235, top1: 25.0000\n",
      "Epoch [14/60], Iter [186/633], LR: 0.005000, Loss: 3.6125, top1: 12.5000\n",
      "Epoch [14/60], Iter [187/633], LR: 0.005000, Loss: 3.5916, top1: 7.8125\n",
      "Epoch [14/60], Iter [188/633], LR: 0.005000, Loss: 3.6409, top1: 9.3750\n",
      "Epoch [14/60], Iter [189/633], LR: 0.005000, Loss: 3.5537, top1: 12.5000\n",
      "Epoch [14/60], Iter [190/633], LR: 0.005000, Loss: 3.6048, top1: 9.3750\n",
      "Epoch [14/60], Iter [191/633], LR: 0.005000, Loss: 3.6445, top1: 6.2500\n",
      "Epoch [14/60], Iter [192/633], LR: 0.005000, Loss: 3.5754, top1: 18.7500\n",
      "Epoch [14/60], Iter [193/633], LR: 0.005000, Loss: 3.5839, top1: 9.3750\n",
      "Epoch [14/60], Iter [194/633], LR: 0.005000, Loss: 3.5792, top1: 14.0625\n",
      "Epoch [14/60], Iter [195/633], LR: 0.005000, Loss: 3.4762, top1: 26.5625\n",
      "Epoch [14/60], Iter [196/633], LR: 0.005000, Loss: 3.5930, top1: 10.9375\n",
      "Epoch [14/60], Iter [197/633], LR: 0.005000, Loss: 3.5423, top1: 18.7500\n",
      "Epoch [14/60], Iter [198/633], LR: 0.005000, Loss: 3.5602, top1: 18.7500\n",
      "Epoch [14/60], Iter [199/633], LR: 0.005000, Loss: 3.5378, top1: 17.1875\n",
      "Epoch [14/60], Iter [200/633], LR: 0.005000, Loss: 3.5593, top1: 18.7500\n",
      "Epoch [14/60], Iter [201/633], LR: 0.005000, Loss: 3.5849, top1: 10.9375\n",
      "Epoch [14/60], Iter [202/633], LR: 0.005000, Loss: 3.5409, top1: 25.0000\n",
      "Epoch [14/60], Iter [203/633], LR: 0.005000, Loss: 3.5310, top1: 20.3125\n",
      "Epoch [14/60], Iter [204/633], LR: 0.005000, Loss: 3.5675, top1: 15.6250\n",
      "Epoch [14/60], Iter [205/633], LR: 0.005000, Loss: 3.5771, top1: 7.8125\n",
      "Epoch [14/60], Iter [206/633], LR: 0.005000, Loss: 3.5669, top1: 17.1875\n",
      "Epoch [14/60], Iter [207/633], LR: 0.005000, Loss: 3.5571, top1: 17.1875\n",
      "Epoch [14/60], Iter [208/633], LR: 0.005000, Loss: 3.6008, top1: 10.9375\n",
      "Epoch [14/60], Iter [209/633], LR: 0.005000, Loss: 3.6132, top1: 9.3750\n",
      "Epoch [14/60], Iter [210/633], LR: 0.005000, Loss: 3.5553, top1: 17.1875\n",
      "Epoch [14/60], Iter [211/633], LR: 0.005000, Loss: 3.5816, top1: 15.6250\n",
      "Epoch [14/60], Iter [212/633], LR: 0.005000, Loss: 3.5677, top1: 17.1875\n",
      "Epoch [14/60], Iter [213/633], LR: 0.005000, Loss: 3.6343, top1: 6.2500\n",
      "Epoch [14/60], Iter [214/633], LR: 0.005000, Loss: 3.5400, top1: 21.8750\n",
      "Epoch [14/60], Iter [215/633], LR: 0.005000, Loss: 3.5994, top1: 9.3750\n",
      "Epoch [14/60], Iter [216/633], LR: 0.005000, Loss: 3.5401, top1: 23.4375\n",
      "Epoch [14/60], Iter [217/633], LR: 0.005000, Loss: 3.6401, top1: 3.1250\n",
      "Epoch [14/60], Iter [218/633], LR: 0.005000, Loss: 3.5791, top1: 15.6250\n",
      "Epoch [14/60], Iter [219/633], LR: 0.005000, Loss: 3.5672, top1: 12.5000\n",
      "Epoch [14/60], Iter [220/633], LR: 0.005000, Loss: 3.5810, top1: 12.5000\n",
      "Epoch [14/60], Iter [221/633], LR: 0.005000, Loss: 3.5843, top1: 10.9375\n",
      "Epoch [14/60], Iter [222/633], LR: 0.005000, Loss: 3.5663, top1: 15.6250\n",
      "Epoch [14/60], Iter [223/633], LR: 0.005000, Loss: 3.5494, top1: 12.5000\n",
      "Epoch [14/60], Iter [224/633], LR: 0.005000, Loss: 3.5363, top1: 14.0625\n",
      "Epoch [14/60], Iter [225/633], LR: 0.005000, Loss: 3.5486, top1: 17.1875\n",
      "Epoch [14/60], Iter [226/633], LR: 0.005000, Loss: 3.5573, top1: 17.1875\n",
      "Epoch [14/60], Iter [227/633], LR: 0.005000, Loss: 3.5829, top1: 12.5000\n",
      "Epoch [14/60], Iter [228/633], LR: 0.005000, Loss: 3.5641, top1: 15.6250\n",
      "Epoch [14/60], Iter [229/633], LR: 0.005000, Loss: 3.5053, top1: 20.3125\n",
      "Epoch [14/60], Iter [230/633], LR: 0.005000, Loss: 3.5740, top1: 17.1875\n",
      "Epoch [14/60], Iter [231/633], LR: 0.005000, Loss: 3.5968, top1: 12.5000\n",
      "Epoch [14/60], Iter [232/633], LR: 0.005000, Loss: 3.5420, top1: 12.5000\n",
      "Epoch [14/60], Iter [233/633], LR: 0.005000, Loss: 3.5757, top1: 12.5000\n",
      "Epoch [14/60], Iter [234/633], LR: 0.005000, Loss: 3.6103, top1: 12.5000\n",
      "Epoch [14/60], Iter [235/633], LR: 0.005000, Loss: 3.5345, top1: 17.1875\n",
      "Epoch [14/60], Iter [236/633], LR: 0.005000, Loss: 3.4977, top1: 20.3125\n",
      "Epoch [14/60], Iter [237/633], LR: 0.005000, Loss: 3.5979, top1: 10.9375\n",
      "Epoch [14/60], Iter [238/633], LR: 0.005000, Loss: 3.5385, top1: 20.3125\n",
      "Epoch [14/60], Iter [239/633], LR: 0.005000, Loss: 3.6102, top1: 14.0625\n",
      "Epoch [14/60], Iter [240/633], LR: 0.005000, Loss: 3.5405, top1: 18.7500\n",
      "Epoch [14/60], Iter [241/633], LR: 0.005000, Loss: 3.5806, top1: 15.6250\n",
      "Epoch [14/60], Iter [242/633], LR: 0.005000, Loss: 3.6016, top1: 7.8125\n",
      "Epoch [14/60], Iter [243/633], LR: 0.005000, Loss: 3.5746, top1: 14.0625\n",
      "Epoch [14/60], Iter [244/633], LR: 0.005000, Loss: 3.5229, top1: 15.6250\n",
      "Epoch [14/60], Iter [245/633], LR: 0.005000, Loss: 3.5716, top1: 15.6250\n",
      "Epoch [14/60], Iter [246/633], LR: 0.005000, Loss: 3.5610, top1: 17.1875\n",
      "Epoch [14/60], Iter [247/633], LR: 0.005000, Loss: 3.5849, top1: 9.3750\n",
      "Epoch [14/60], Iter [248/633], LR: 0.005000, Loss: 3.6068, top1: 10.9375\n",
      "Epoch [14/60], Iter [249/633], LR: 0.005000, Loss: 3.5402, top1: 17.1875\n",
      "Epoch [14/60], Iter [250/633], LR: 0.005000, Loss: 3.5311, top1: 17.1875\n",
      "Epoch [14/60], Iter [251/633], LR: 0.005000, Loss: 3.5799, top1: 9.3750\n",
      "Epoch [14/60], Iter [252/633], LR: 0.005000, Loss: 3.5247, top1: 14.0625\n",
      "Epoch [14/60], Iter [253/633], LR: 0.005000, Loss: 3.5788, top1: 9.3750\n",
      "Epoch [14/60], Iter [254/633], LR: 0.005000, Loss: 3.5944, top1: 12.5000\n",
      "Epoch [14/60], Iter [255/633], LR: 0.005000, Loss: 3.5538, top1: 17.1875\n",
      "Epoch [14/60], Iter [256/633], LR: 0.005000, Loss: 3.5752, top1: 10.9375\n",
      "Epoch [14/60], Iter [257/633], LR: 0.005000, Loss: 3.6085, top1: 7.8125\n",
      "Epoch [14/60], Iter [258/633], LR: 0.005000, Loss: 3.5778, top1: 17.1875\n",
      "Epoch [14/60], Iter [259/633], LR: 0.005000, Loss: 3.5615, top1: 23.4375\n",
      "Epoch [14/60], Iter [260/633], LR: 0.005000, Loss: 3.6059, top1: 14.0625\n",
      "Epoch [14/60], Iter [261/633], LR: 0.005000, Loss: 3.5432, top1: 21.8750\n",
      "Epoch [14/60], Iter [262/633], LR: 0.005000, Loss: 3.6015, top1: 15.6250\n",
      "Epoch [14/60], Iter [263/633], LR: 0.005000, Loss: 3.5845, top1: 14.0625\n",
      "Epoch [14/60], Iter [264/633], LR: 0.005000, Loss: 3.5987, top1: 12.5000\n",
      "Epoch [14/60], Iter [265/633], LR: 0.005000, Loss: 3.5815, top1: 10.9375\n",
      "Epoch [14/60], Iter [266/633], LR: 0.005000, Loss: 3.5577, top1: 12.5000\n",
      "Epoch [14/60], Iter [267/633], LR: 0.005000, Loss: 3.6024, top1: 10.9375\n",
      "Epoch [14/60], Iter [268/633], LR: 0.005000, Loss: 3.5335, top1: 17.1875\n",
      "Epoch [14/60], Iter [269/633], LR: 0.005000, Loss: 3.5815, top1: 17.1875\n",
      "Epoch [14/60], Iter [270/633], LR: 0.005000, Loss: 3.5939, top1: 10.9375\n",
      "Epoch [14/60], Iter [271/633], LR: 0.005000, Loss: 3.5570, top1: 18.7500\n",
      "Epoch [14/60], Iter [272/633], LR: 0.005000, Loss: 3.5843, top1: 12.5000\n",
      "Epoch [14/60], Iter [273/633], LR: 0.005000, Loss: 3.6019, top1: 21.8750\n",
      "Epoch [14/60], Iter [274/633], LR: 0.005000, Loss: 3.5700, top1: 18.7500\n",
      "Epoch [14/60], Iter [275/633], LR: 0.005000, Loss: 3.5689, top1: 20.3125\n",
      "Epoch [14/60], Iter [276/633], LR: 0.005000, Loss: 3.5749, top1: 15.6250\n",
      "Epoch [14/60], Iter [277/633], LR: 0.005000, Loss: 3.5777, top1: 17.1875\n",
      "Epoch [14/60], Iter [278/633], LR: 0.005000, Loss: 3.4697, top1: 29.6875\n",
      "Epoch [14/60], Iter [279/633], LR: 0.005000, Loss: 3.6090, top1: 10.9375\n",
      "Epoch [14/60], Iter [280/633], LR: 0.005000, Loss: 3.6133, top1: 9.3750\n",
      "Epoch [14/60], Iter [281/633], LR: 0.005000, Loss: 3.6325, top1: 9.3750\n",
      "Epoch [14/60], Iter [282/633], LR: 0.005000, Loss: 3.5815, top1: 14.0625\n",
      "Epoch [14/60], Iter [283/633], LR: 0.005000, Loss: 3.5562, top1: 17.1875\n",
      "Epoch [14/60], Iter [284/633], LR: 0.005000, Loss: 3.5890, top1: 18.7500\n",
      "Epoch [14/60], Iter [285/633], LR: 0.005000, Loss: 3.6274, top1: 7.8125\n",
      "Epoch [14/60], Iter [286/633], LR: 0.005000, Loss: 3.5979, top1: 14.0625\n",
      "Epoch [14/60], Iter [287/633], LR: 0.005000, Loss: 3.6127, top1: 7.8125\n",
      "Epoch [14/60], Iter [288/633], LR: 0.005000, Loss: 3.5912, top1: 12.5000\n",
      "Epoch [14/60], Iter [289/633], LR: 0.005000, Loss: 3.6153, top1: 10.9375\n",
      "Epoch [14/60], Iter [290/633], LR: 0.005000, Loss: 3.5976, top1: 10.9375\n",
      "Epoch [14/60], Iter [291/633], LR: 0.005000, Loss: 3.6059, top1: 9.3750\n",
      "Epoch [14/60], Iter [292/633], LR: 0.005000, Loss: 3.6063, top1: 10.9375\n",
      "Epoch [14/60], Iter [293/633], LR: 0.005000, Loss: 3.5235, top1: 20.3125\n",
      "Epoch [14/60], Iter [294/633], LR: 0.005000, Loss: 3.5584, top1: 20.3125\n",
      "Epoch [14/60], Iter [295/633], LR: 0.005000, Loss: 3.5683, top1: 18.7500\n",
      "Epoch [14/60], Iter [296/633], LR: 0.005000, Loss: 3.5407, top1: 20.3125\n",
      "Epoch [14/60], Iter [297/633], LR: 0.005000, Loss: 3.5423, top1: 17.1875\n",
      "Epoch [14/60], Iter [298/633], LR: 0.005000, Loss: 3.5355, top1: 20.3125\n",
      "Epoch [14/60], Iter [299/633], LR: 0.005000, Loss: 3.5378, top1: 20.3125\n",
      "Epoch [14/60], Iter [300/633], LR: 0.005000, Loss: 3.5870, top1: 15.6250\n",
      "Epoch [14/60], Iter [301/633], LR: 0.005000, Loss: 3.5952, top1: 9.3750\n",
      "Epoch [14/60], Iter [302/633], LR: 0.005000, Loss: 3.6274, top1: 7.8125\n",
      "Epoch [14/60], Iter [303/633], LR: 0.005000, Loss: 3.6226, top1: 9.3750\n",
      "Epoch [14/60], Iter [304/633], LR: 0.005000, Loss: 3.5969, top1: 12.5000\n",
      "Epoch [14/60], Iter [305/633], LR: 0.005000, Loss: 3.5943, top1: 15.6250\n",
      "Epoch [14/60], Iter [306/633], LR: 0.005000, Loss: 3.5854, top1: 14.0625\n",
      "Epoch [14/60], Iter [307/633], LR: 0.005000, Loss: 3.6076, top1: 17.1875\n",
      "Epoch [14/60], Iter [308/633], LR: 0.005000, Loss: 3.5717, top1: 18.7500\n",
      "Epoch [14/60], Iter [309/633], LR: 0.005000, Loss: 3.5786, top1: 7.8125\n",
      "Epoch [14/60], Iter [310/633], LR: 0.005000, Loss: 3.5216, top1: 23.4375\n",
      "Epoch [14/60], Iter [311/633], LR: 0.005000, Loss: 3.5664, top1: 12.5000\n",
      "Epoch [14/60], Iter [312/633], LR: 0.005000, Loss: 3.5521, top1: 21.8750\n",
      "Epoch [14/60], Iter [313/633], LR: 0.005000, Loss: 3.5812, top1: 12.5000\n",
      "Epoch [14/60], Iter [314/633], LR: 0.005000, Loss: 3.5697, top1: 14.0625\n",
      "Epoch [14/60], Iter [315/633], LR: 0.005000, Loss: 3.5727, top1: 15.6250\n",
      "Epoch [14/60], Iter [316/633], LR: 0.005000, Loss: 3.5971, top1: 12.5000\n",
      "Epoch [14/60], Iter [317/633], LR: 0.005000, Loss: 3.6023, top1: 7.8125\n",
      "Epoch [14/60], Iter [318/633], LR: 0.005000, Loss: 3.5864, top1: 12.5000\n",
      "Epoch [14/60], Iter [319/633], LR: 0.005000, Loss: 3.5550, top1: 21.8750\n",
      "Epoch [14/60], Iter [320/633], LR: 0.005000, Loss: 3.5893, top1: 17.1875\n",
      "Epoch [14/60], Iter [321/633], LR: 0.005000, Loss: 3.5511, top1: 15.6250\n",
      "Epoch [14/60], Iter [322/633], LR: 0.005000, Loss: 3.5531, top1: 18.7500\n",
      "Epoch [14/60], Iter [323/633], LR: 0.005000, Loss: 3.5209, top1: 25.0000\n",
      "Epoch [14/60], Iter [324/633], LR: 0.005000, Loss: 3.6125, top1: 12.5000\n",
      "Epoch [14/60], Iter [325/633], LR: 0.005000, Loss: 3.5527, top1: 20.3125\n",
      "Epoch [14/60], Iter [326/633], LR: 0.005000, Loss: 3.5936, top1: 14.0625\n",
      "Epoch [14/60], Iter [327/633], LR: 0.005000, Loss: 3.5498, top1: 18.7500\n",
      "Epoch [14/60], Iter [328/633], LR: 0.005000, Loss: 3.5983, top1: 14.0625\n",
      "Epoch [14/60], Iter [329/633], LR: 0.005000, Loss: 3.5752, top1: 15.6250\n",
      "Epoch [14/60], Iter [330/633], LR: 0.005000, Loss: 3.5713, top1: 17.1875\n",
      "Epoch [14/60], Iter [331/633], LR: 0.005000, Loss: 3.5376, top1: 17.1875\n",
      "Epoch [14/60], Iter [332/633], LR: 0.005000, Loss: 3.5692, top1: 20.3125\n",
      "Epoch [14/60], Iter [333/633], LR: 0.005000, Loss: 3.5858, top1: 20.3125\n",
      "Epoch [14/60], Iter [334/633], LR: 0.005000, Loss: 3.5643, top1: 20.3125\n",
      "Epoch [14/60], Iter [335/633], LR: 0.005000, Loss: 3.5000, top1: 26.5625\n",
      "Epoch [14/60], Iter [336/633], LR: 0.005000, Loss: 3.5701, top1: 20.3125\n",
      "Epoch [14/60], Iter [337/633], LR: 0.005000, Loss: 3.5775, top1: 14.0625\n",
      "Epoch [14/60], Iter [338/633], LR: 0.005000, Loss: 3.5492, top1: 15.6250\n",
      "Epoch [14/60], Iter [339/633], LR: 0.005000, Loss: 3.5971, top1: 9.3750\n",
      "Epoch [14/60], Iter [340/633], LR: 0.005000, Loss: 3.5719, top1: 14.0625\n",
      "Epoch [14/60], Iter [341/633], LR: 0.005000, Loss: 3.5031, top1: 18.7500\n",
      "Epoch [14/60], Iter [342/633], LR: 0.005000, Loss: 3.5691, top1: 14.0625\n",
      "Epoch [14/60], Iter [343/633], LR: 0.005000, Loss: 3.5488, top1: 20.3125\n",
      "Epoch [14/60], Iter [344/633], LR: 0.005000, Loss: 3.5856, top1: 18.7500\n",
      "Epoch [14/60], Iter [345/633], LR: 0.005000, Loss: 3.5623, top1: 14.0625\n",
      "Epoch [14/60], Iter [346/633], LR: 0.005000, Loss: 3.5609, top1: 20.3125\n",
      "Epoch [14/60], Iter [347/633], LR: 0.005000, Loss: 3.5519, top1: 15.6250\n",
      "Epoch [14/60], Iter [348/633], LR: 0.005000, Loss: 3.5106, top1: 17.1875\n",
      "Epoch [14/60], Iter [349/633], LR: 0.005000, Loss: 3.5807, top1: 18.7500\n",
      "Epoch [14/60], Iter [350/633], LR: 0.005000, Loss: 3.5923, top1: 15.6250\n",
      "Epoch [14/60], Iter [351/633], LR: 0.005000, Loss: 3.5727, top1: 12.5000\n",
      "Epoch [14/60], Iter [352/633], LR: 0.005000, Loss: 3.5840, top1: 14.0625\n",
      "Epoch [14/60], Iter [353/633], LR: 0.005000, Loss: 3.5591, top1: 14.0625\n",
      "Epoch [14/60], Iter [354/633], LR: 0.005000, Loss: 3.5576, top1: 18.7500\n",
      "Epoch [14/60], Iter [355/633], LR: 0.005000, Loss: 3.5657, top1: 15.6250\n",
      "Epoch [14/60], Iter [356/633], LR: 0.005000, Loss: 3.5329, top1: 17.1875\n",
      "Epoch [14/60], Iter [357/633], LR: 0.005000, Loss: 3.5678, top1: 17.1875\n",
      "Epoch [14/60], Iter [358/633], LR: 0.005000, Loss: 3.5468, top1: 18.7500\n",
      "Epoch [14/60], Iter [359/633], LR: 0.005000, Loss: 3.5447, top1: 25.0000\n",
      "Epoch [14/60], Iter [360/633], LR: 0.005000, Loss: 3.6059, top1: 10.9375\n",
      "Epoch [14/60], Iter [361/633], LR: 0.005000, Loss: 3.5342, top1: 14.0625\n",
      "Epoch [14/60], Iter [362/633], LR: 0.005000, Loss: 3.5744, top1: 14.0625\n",
      "Epoch [14/60], Iter [363/633], LR: 0.005000, Loss: 3.5553, top1: 14.0625\n",
      "Epoch [14/60], Iter [364/633], LR: 0.005000, Loss: 3.5901, top1: 9.3750\n",
      "Epoch [14/60], Iter [365/633], LR: 0.005000, Loss: 3.5713, top1: 12.5000\n",
      "Epoch [14/60], Iter [366/633], LR: 0.005000, Loss: 3.6449, top1: 6.2500\n",
      "Epoch [14/60], Iter [367/633], LR: 0.005000, Loss: 3.5650, top1: 15.6250\n",
      "Epoch [14/60], Iter [368/633], LR: 0.005000, Loss: 3.6140, top1: 10.9375\n",
      "Epoch [14/60], Iter [369/633], LR: 0.005000, Loss: 3.4907, top1: 28.1250\n",
      "Epoch [14/60], Iter [370/633], LR: 0.005000, Loss: 3.5948, top1: 9.3750\n",
      "Epoch [14/60], Iter [371/633], LR: 0.005000, Loss: 3.6169, top1: 9.3750\n",
      "Epoch [14/60], Iter [372/633], LR: 0.005000, Loss: 3.5757, top1: 15.6250\n",
      "Epoch [14/60], Iter [373/633], LR: 0.005000, Loss: 3.5511, top1: 20.3125\n",
      "Epoch [14/60], Iter [374/633], LR: 0.005000, Loss: 3.5477, top1: 17.1875\n",
      "Epoch [14/60], Iter [375/633], LR: 0.005000, Loss: 3.5862, top1: 12.5000\n",
      "Epoch [14/60], Iter [376/633], LR: 0.005000, Loss: 3.5905, top1: 10.9375\n",
      "Epoch [14/60], Iter [377/633], LR: 0.005000, Loss: 3.6228, top1: 10.9375\n",
      "Epoch [14/60], Iter [378/633], LR: 0.005000, Loss: 3.5402, top1: 14.0625\n",
      "Epoch [14/60], Iter [379/633], LR: 0.005000, Loss: 3.5711, top1: 15.6250\n",
      "Epoch [14/60], Iter [380/633], LR: 0.005000, Loss: 3.5769, top1: 14.0625\n",
      "Epoch [14/60], Iter [381/633], LR: 0.005000, Loss: 3.5660, top1: 14.0625\n",
      "Epoch [14/60], Iter [382/633], LR: 0.005000, Loss: 3.6138, top1: 9.3750\n",
      "Epoch [14/60], Iter [383/633], LR: 0.005000, Loss: 3.5999, top1: 10.9375\n",
      "Epoch [14/60], Iter [384/633], LR: 0.005000, Loss: 3.5689, top1: 15.6250\n",
      "Epoch [14/60], Iter [385/633], LR: 0.005000, Loss: 3.5119, top1: 21.8750\n",
      "Epoch [14/60], Iter [386/633], LR: 0.005000, Loss: 3.5589, top1: 18.7500\n",
      "Epoch [14/60], Iter [387/633], LR: 0.005000, Loss: 3.5837, top1: 15.6250\n",
      "Epoch [14/60], Iter [388/633], LR: 0.005000, Loss: 3.6438, top1: 9.3750\n",
      "Epoch [14/60], Iter [389/633], LR: 0.005000, Loss: 3.5293, top1: 15.6250\n",
      "Epoch [14/60], Iter [390/633], LR: 0.005000, Loss: 3.5501, top1: 12.5000\n",
      "Epoch [14/60], Iter [391/633], LR: 0.005000, Loss: 3.5887, top1: 15.6250\n",
      "Epoch [14/60], Iter [392/633], LR: 0.005000, Loss: 3.5934, top1: 12.5000\n",
      "Epoch [14/60], Iter [393/633], LR: 0.005000, Loss: 3.5411, top1: 17.1875\n",
      "Epoch [14/60], Iter [394/633], LR: 0.005000, Loss: 3.5785, top1: 9.3750\n",
      "Epoch [14/60], Iter [395/633], LR: 0.005000, Loss: 3.5553, top1: 17.1875\n",
      "Epoch [14/60], Iter [396/633], LR: 0.005000, Loss: 3.5550, top1: 14.0625\n",
      "Epoch [14/60], Iter [397/633], LR: 0.005000, Loss: 3.5781, top1: 12.5000\n",
      "Epoch [14/60], Iter [398/633], LR: 0.005000, Loss: 3.5840, top1: 10.9375\n",
      "Epoch [14/60], Iter [399/633], LR: 0.005000, Loss: 3.5653, top1: 18.7500\n",
      "Epoch [14/60], Iter [400/633], LR: 0.005000, Loss: 3.5738, top1: 15.6250\n",
      "Epoch [14/60], Iter [401/633], LR: 0.005000, Loss: 3.5262, top1: 21.8750\n",
      "Epoch [14/60], Iter [402/633], LR: 0.005000, Loss: 3.6016, top1: 12.5000\n",
      "Epoch [14/60], Iter [403/633], LR: 0.005000, Loss: 3.5779, top1: 15.6250\n",
      "Epoch [14/60], Iter [404/633], LR: 0.005000, Loss: 3.4963, top1: 29.6875\n",
      "Epoch [14/60], Iter [405/633], LR: 0.005000, Loss: 3.6019, top1: 10.9375\n",
      "Epoch [14/60], Iter [406/633], LR: 0.005000, Loss: 3.5818, top1: 14.0625\n",
      "Epoch [14/60], Iter [407/633], LR: 0.005000, Loss: 3.5820, top1: 14.0625\n",
      "Epoch [14/60], Iter [408/633], LR: 0.005000, Loss: 3.5504, top1: 15.6250\n",
      "Epoch [14/60], Iter [409/633], LR: 0.005000, Loss: 3.5291, top1: 17.1875\n",
      "Epoch [14/60], Iter [410/633], LR: 0.005000, Loss: 3.5779, top1: 14.0625\n",
      "Epoch [14/60], Iter [411/633], LR: 0.005000, Loss: 3.5928, top1: 15.6250\n",
      "Epoch [14/60], Iter [412/633], LR: 0.005000, Loss: 3.5513, top1: 20.3125\n",
      "Epoch [14/60], Iter [413/633], LR: 0.005000, Loss: 3.6079, top1: 9.3750\n",
      "Epoch [14/60], Iter [414/633], LR: 0.005000, Loss: 3.5670, top1: 14.0625\n",
      "Epoch [14/60], Iter [415/633], LR: 0.005000, Loss: 3.6104, top1: 12.5000\n",
      "Epoch [14/60], Iter [416/633], LR: 0.005000, Loss: 3.6092, top1: 14.0625\n",
      "Epoch [14/60], Iter [417/633], LR: 0.005000, Loss: 3.5435, top1: 18.7500\n",
      "Epoch [14/60], Iter [418/633], LR: 0.005000, Loss: 3.5586, top1: 14.0625\n",
      "Epoch [14/60], Iter [419/633], LR: 0.005000, Loss: 3.5882, top1: 14.0625\n",
      "Epoch [14/60], Iter [420/633], LR: 0.005000, Loss: 3.5456, top1: 17.1875\n",
      "Epoch [14/60], Iter [421/633], LR: 0.005000, Loss: 3.5882, top1: 12.5000\n",
      "Epoch [14/60], Iter [422/633], LR: 0.005000, Loss: 3.6032, top1: 10.9375\n",
      "Epoch [14/60], Iter [423/633], LR: 0.005000, Loss: 3.5310, top1: 25.0000\n",
      "Epoch [14/60], Iter [424/633], LR: 0.005000, Loss: 3.5650, top1: 15.6250\n",
      "Epoch [14/60], Iter [425/633], LR: 0.005000, Loss: 3.5679, top1: 17.1875\n",
      "Epoch [14/60], Iter [426/633], LR: 0.005000, Loss: 3.5204, top1: 23.4375\n",
      "Epoch [14/60], Iter [427/633], LR: 0.005000, Loss: 3.5499, top1: 18.7500\n",
      "Epoch [14/60], Iter [428/633], LR: 0.005000, Loss: 3.5744, top1: 18.7500\n",
      "Epoch [14/60], Iter [429/633], LR: 0.005000, Loss: 3.5744, top1: 15.6250\n",
      "Epoch [14/60], Iter [430/633], LR: 0.005000, Loss: 3.5574, top1: 12.5000\n",
      "Epoch [14/60], Iter [431/633], LR: 0.005000, Loss: 3.5996, top1: 14.0625\n",
      "Epoch [14/60], Iter [432/633], LR: 0.005000, Loss: 3.5513, top1: 18.7500\n",
      "Epoch [14/60], Iter [433/633], LR: 0.005000, Loss: 3.5833, top1: 9.3750\n",
      "Epoch [14/60], Iter [434/633], LR: 0.005000, Loss: 3.5910, top1: 9.3750\n",
      "Epoch [14/60], Iter [435/633], LR: 0.005000, Loss: 3.5465, top1: 14.0625\n",
      "Epoch [14/60], Iter [436/633], LR: 0.005000, Loss: 3.5862, top1: 14.0625\n",
      "Epoch [14/60], Iter [437/633], LR: 0.005000, Loss: 3.6356, top1: 7.8125\n",
      "Epoch [14/60], Iter [438/633], LR: 0.005000, Loss: 3.5481, top1: 17.1875\n",
      "Epoch [14/60], Iter [439/633], LR: 0.005000, Loss: 3.5377, top1: 18.7500\n",
      "Epoch [14/60], Iter [440/633], LR: 0.005000, Loss: 3.5328, top1: 18.7500\n",
      "Epoch [14/60], Iter [441/633], LR: 0.005000, Loss: 3.5758, top1: 12.5000\n",
      "Epoch [14/60], Iter [442/633], LR: 0.005000, Loss: 3.5287, top1: 18.7500\n",
      "Epoch [14/60], Iter [443/633], LR: 0.005000, Loss: 3.5572, top1: 14.0625\n",
      "Epoch [14/60], Iter [444/633], LR: 0.005000, Loss: 3.5141, top1: 25.0000\n",
      "Epoch [14/60], Iter [445/633], LR: 0.005000, Loss: 3.5168, top1: 20.3125\n",
      "Epoch [14/60], Iter [446/633], LR: 0.005000, Loss: 3.6115, top1: 15.6250\n",
      "Epoch [14/60], Iter [447/633], LR: 0.005000, Loss: 3.6073, top1: 10.9375\n",
      "Epoch [14/60], Iter [448/633], LR: 0.005000, Loss: 3.5454, top1: 12.5000\n",
      "Epoch [14/60], Iter [449/633], LR: 0.005000, Loss: 3.5895, top1: 15.6250\n",
      "Epoch [14/60], Iter [450/633], LR: 0.005000, Loss: 3.5616, top1: 17.1875\n",
      "Epoch [14/60], Iter [451/633], LR: 0.005000, Loss: 3.5528, top1: 15.6250\n",
      "Epoch [14/60], Iter [452/633], LR: 0.005000, Loss: 3.5891, top1: 10.9375\n",
      "Epoch [14/60], Iter [453/633], LR: 0.005000, Loss: 3.5499, top1: 18.7500\n",
      "Epoch [14/60], Iter [454/633], LR: 0.005000, Loss: 3.5643, top1: 20.3125\n",
      "Epoch [14/60], Iter [455/633], LR: 0.005000, Loss: 3.5337, top1: 21.8750\n",
      "Epoch [14/60], Iter [456/633], LR: 0.005000, Loss: 3.6057, top1: 15.6250\n",
      "Epoch [14/60], Iter [457/633], LR: 0.005000, Loss: 3.6116, top1: 9.3750\n",
      "Epoch [14/60], Iter [458/633], LR: 0.005000, Loss: 3.6240, top1: 9.3750\n",
      "Epoch [14/60], Iter [459/633], LR: 0.005000, Loss: 3.5248, top1: 20.3125\n",
      "Epoch [14/60], Iter [460/633], LR: 0.005000, Loss: 3.5644, top1: 14.0625\n",
      "Epoch [14/60], Iter [461/633], LR: 0.005000, Loss: 3.6156, top1: 4.6875\n",
      "Epoch [14/60], Iter [462/633], LR: 0.005000, Loss: 3.5244, top1: 18.7500\n",
      "Epoch [14/60], Iter [463/633], LR: 0.005000, Loss: 3.6275, top1: 4.6875\n",
      "Epoch [14/60], Iter [464/633], LR: 0.005000, Loss: 3.5183, top1: 20.3125\n",
      "Epoch [14/60], Iter [465/633], LR: 0.005000, Loss: 3.6149, top1: 12.5000\n",
      "Epoch [14/60], Iter [466/633], LR: 0.005000, Loss: 3.5719, top1: 17.1875\n",
      "Epoch [14/60], Iter [467/633], LR: 0.005000, Loss: 3.5384, top1: 17.1875\n",
      "Epoch [14/60], Iter [468/633], LR: 0.005000, Loss: 3.6061, top1: 17.1875\n",
      "Epoch [14/60], Iter [469/633], LR: 0.005000, Loss: 3.6239, top1: 9.3750\n",
      "Epoch [14/60], Iter [470/633], LR: 0.005000, Loss: 3.5456, top1: 17.1875\n",
      "Epoch [14/60], Iter [471/633], LR: 0.005000, Loss: 3.5605, top1: 15.6250\n",
      "Epoch [14/60], Iter [472/633], LR: 0.005000, Loss: 3.5921, top1: 9.3750\n",
      "Epoch [14/60], Iter [473/633], LR: 0.005000, Loss: 3.5564, top1: 15.6250\n",
      "Epoch [14/60], Iter [474/633], LR: 0.005000, Loss: 3.5336, top1: 15.6250\n",
      "Epoch [14/60], Iter [475/633], LR: 0.005000, Loss: 3.6115, top1: 10.9375\n",
      "Epoch [14/60], Iter [476/633], LR: 0.005000, Loss: 3.5419, top1: 21.8750\n",
      "Epoch [14/60], Iter [477/633], LR: 0.005000, Loss: 3.5988, top1: 9.3750\n",
      "Epoch [14/60], Iter [478/633], LR: 0.005000, Loss: 3.5594, top1: 18.7500\n",
      "Epoch [14/60], Iter [479/633], LR: 0.005000, Loss: 3.6185, top1: 6.2500\n",
      "Epoch [14/60], Iter [480/633], LR: 0.005000, Loss: 3.5411, top1: 18.7500\n",
      "Epoch [14/60], Iter [481/633], LR: 0.005000, Loss: 3.5322, top1: 17.1875\n",
      "Epoch [14/60], Iter [482/633], LR: 0.005000, Loss: 3.5784, top1: 17.1875\n",
      "Epoch [14/60], Iter [483/633], LR: 0.005000, Loss: 3.5737, top1: 14.0625\n",
      "Epoch [14/60], Iter [484/633], LR: 0.005000, Loss: 3.5871, top1: 12.5000\n",
      "Epoch [14/60], Iter [485/633], LR: 0.005000, Loss: 3.6036, top1: 9.3750\n",
      "Epoch [14/60], Iter [486/633], LR: 0.005000, Loss: 3.5534, top1: 15.6250\n",
      "Epoch [14/60], Iter [487/633], LR: 0.005000, Loss: 3.6102, top1: 10.9375\n",
      "Epoch [14/60], Iter [488/633], LR: 0.005000, Loss: 3.4763, top1: 25.0000\n",
      "Epoch [14/60], Iter [489/633], LR: 0.005000, Loss: 3.5430, top1: 21.8750\n",
      "Epoch [14/60], Iter [490/633], LR: 0.005000, Loss: 3.5304, top1: 23.4375\n",
      "Epoch [14/60], Iter [491/633], LR: 0.005000, Loss: 3.5873, top1: 9.3750\n",
      "Epoch [14/60], Iter [492/633], LR: 0.005000, Loss: 3.5677, top1: 10.9375\n",
      "Epoch [14/60], Iter [493/633], LR: 0.005000, Loss: 3.6258, top1: 10.9375\n",
      "Epoch [14/60], Iter [494/633], LR: 0.005000, Loss: 3.5874, top1: 12.5000\n",
      "Epoch [14/60], Iter [495/633], LR: 0.005000, Loss: 3.5615, top1: 14.0625\n",
      "Epoch [14/60], Iter [496/633], LR: 0.005000, Loss: 3.5462, top1: 20.3125\n",
      "Epoch [14/60], Iter [497/633], LR: 0.005000, Loss: 3.6040, top1: 9.3750\n",
      "Epoch [14/60], Iter [498/633], LR: 0.005000, Loss: 3.5413, top1: 20.3125\n",
      "Epoch [14/60], Iter [499/633], LR: 0.005000, Loss: 3.5590, top1: 14.0625\n",
      "Epoch [14/60], Iter [500/633], LR: 0.005000, Loss: 3.5818, top1: 14.0625\n",
      "Epoch [14/60], Iter [501/633], LR: 0.005000, Loss: 3.5773, top1: 15.6250\n",
      "Epoch [14/60], Iter [502/633], LR: 0.005000, Loss: 3.5493, top1: 15.6250\n",
      "Epoch [14/60], Iter [503/633], LR: 0.005000, Loss: 3.6031, top1: 12.5000\n",
      "Epoch [14/60], Iter [504/633], LR: 0.005000, Loss: 3.6387, top1: 6.2500\n",
      "Epoch [14/60], Iter [505/633], LR: 0.005000, Loss: 3.5618, top1: 15.6250\n",
      "Epoch [14/60], Iter [506/633], LR: 0.005000, Loss: 3.6015, top1: 9.3750\n",
      "Epoch [14/60], Iter [507/633], LR: 0.005000, Loss: 3.6163, top1: 14.0625\n",
      "Epoch [14/60], Iter [508/633], LR: 0.005000, Loss: 3.5832, top1: 14.0625\n",
      "Epoch [14/60], Iter [509/633], LR: 0.005000, Loss: 3.5644, top1: 14.0625\n",
      "Epoch [14/60], Iter [510/633], LR: 0.005000, Loss: 3.5843, top1: 14.0625\n",
      "Epoch [14/60], Iter [511/633], LR: 0.005000, Loss: 3.5985, top1: 15.6250\n",
      "Epoch [14/60], Iter [512/633], LR: 0.005000, Loss: 3.5701, top1: 9.3750\n",
      "Epoch [14/60], Iter [513/633], LR: 0.005000, Loss: 3.5429, top1: 14.0625\n",
      "Epoch [14/60], Iter [514/633], LR: 0.005000, Loss: 3.5359, top1: 18.7500\n",
      "Epoch [14/60], Iter [515/633], LR: 0.005000, Loss: 3.5769, top1: 15.6250\n",
      "Epoch [14/60], Iter [516/633], LR: 0.005000, Loss: 3.6149, top1: 9.3750\n",
      "Epoch [14/60], Iter [517/633], LR: 0.005000, Loss: 3.5169, top1: 23.4375\n",
      "Epoch [14/60], Iter [518/633], LR: 0.005000, Loss: 3.5566, top1: 21.8750\n",
      "Epoch [14/60], Iter [519/633], LR: 0.005000, Loss: 3.5593, top1: 15.6250\n",
      "Epoch [14/60], Iter [520/633], LR: 0.005000, Loss: 3.5976, top1: 12.5000\n",
      "Epoch [14/60], Iter [521/633], LR: 0.005000, Loss: 3.5908, top1: 10.9375\n",
      "Epoch [14/60], Iter [522/633], LR: 0.005000, Loss: 3.5661, top1: 17.1875\n",
      "Epoch [14/60], Iter [523/633], LR: 0.005000, Loss: 3.5764, top1: 18.7500\n",
      "Epoch [14/60], Iter [524/633], LR: 0.005000, Loss: 3.5409, top1: 17.1875\n",
      "Epoch [14/60], Iter [525/633], LR: 0.005000, Loss: 3.5726, top1: 12.5000\n",
      "Epoch [14/60], Iter [526/633], LR: 0.005000, Loss: 3.5857, top1: 15.6250\n",
      "Epoch [14/60], Iter [527/633], LR: 0.005000, Loss: 3.5516, top1: 18.7500\n",
      "Epoch [14/60], Iter [528/633], LR: 0.005000, Loss: 3.5886, top1: 9.3750\n",
      "Epoch [14/60], Iter [529/633], LR: 0.005000, Loss: 3.5337, top1: 18.7500\n",
      "Epoch [14/60], Iter [530/633], LR: 0.005000, Loss: 3.5910, top1: 9.3750\n",
      "Epoch [14/60], Iter [531/633], LR: 0.005000, Loss: 3.5201, top1: 18.7500\n",
      "Epoch [14/60], Iter [532/633], LR: 0.005000, Loss: 3.5550, top1: 17.1875\n",
      "Epoch [14/60], Iter [533/633], LR: 0.005000, Loss: 3.6218, top1: 6.2500\n",
      "Epoch [14/60], Iter [534/633], LR: 0.005000, Loss: 3.5628, top1: 18.7500\n",
      "Epoch [14/60], Iter [535/633], LR: 0.005000, Loss: 3.5569, top1: 15.6250\n",
      "Epoch [14/60], Iter [536/633], LR: 0.005000, Loss: 3.6490, top1: 12.5000\n",
      "Epoch [14/60], Iter [537/633], LR: 0.005000, Loss: 3.5765, top1: 15.6250\n",
      "Epoch [14/60], Iter [538/633], LR: 0.005000, Loss: 3.6164, top1: 7.8125\n",
      "Epoch [14/60], Iter [539/633], LR: 0.005000, Loss: 3.4978, top1: 26.5625\n",
      "Epoch [14/60], Iter [540/633], LR: 0.005000, Loss: 3.5990, top1: 9.3750\n",
      "Epoch [14/60], Iter [541/633], LR: 0.005000, Loss: 3.5676, top1: 14.0625\n",
      "Epoch [14/60], Iter [542/633], LR: 0.005000, Loss: 3.5664, top1: 14.0625\n",
      "Epoch [14/60], Iter [543/633], LR: 0.005000, Loss: 3.6070, top1: 12.5000\n",
      "Epoch [14/60], Iter [544/633], LR: 0.005000, Loss: 3.5816, top1: 14.0625\n",
      "Epoch [14/60], Iter [545/633], LR: 0.005000, Loss: 3.5513, top1: 20.3125\n",
      "Epoch [14/60], Iter [546/633], LR: 0.005000, Loss: 3.5201, top1: 20.3125\n",
      "Epoch [14/60], Iter [547/633], LR: 0.005000, Loss: 3.5407, top1: 15.6250\n",
      "Epoch [14/60], Iter [548/633], LR: 0.005000, Loss: 3.5870, top1: 12.5000\n",
      "Epoch [14/60], Iter [549/633], LR: 0.005000, Loss: 3.6132, top1: 7.8125\n",
      "Epoch [14/60], Iter [550/633], LR: 0.005000, Loss: 3.5689, top1: 14.0625\n",
      "Epoch [14/60], Iter [551/633], LR: 0.005000, Loss: 3.6048, top1: 14.0625\n",
      "Epoch [14/60], Iter [552/633], LR: 0.005000, Loss: 3.5777, top1: 20.3125\n",
      "Epoch [14/60], Iter [553/633], LR: 0.005000, Loss: 3.6162, top1: 4.6875\n",
      "Epoch [14/60], Iter [554/633], LR: 0.005000, Loss: 3.5357, top1: 20.3125\n",
      "Epoch [14/60], Iter [555/633], LR: 0.005000, Loss: 3.5541, top1: 15.6250\n",
      "Epoch [14/60], Iter [556/633], LR: 0.005000, Loss: 3.5691, top1: 15.6250\n",
      "Epoch [14/60], Iter [557/633], LR: 0.005000, Loss: 3.6116, top1: 6.2500\n",
      "Epoch [14/60], Iter [558/633], LR: 0.005000, Loss: 3.5726, top1: 12.5000\n",
      "Epoch [14/60], Iter [559/633], LR: 0.005000, Loss: 3.5647, top1: 15.6250\n",
      "Epoch [14/60], Iter [560/633], LR: 0.005000, Loss: 3.6208, top1: 12.5000\n",
      "Epoch [14/60], Iter [561/633], LR: 0.005000, Loss: 3.5731, top1: 10.9375\n",
      "Epoch [14/60], Iter [562/633], LR: 0.005000, Loss: 3.6047, top1: 7.8125\n",
      "Epoch [14/60], Iter [563/633], LR: 0.005000, Loss: 3.6159, top1: 10.9375\n",
      "Epoch [14/60], Iter [564/633], LR: 0.005000, Loss: 3.5993, top1: 12.5000\n",
      "Epoch [14/60], Iter [565/633], LR: 0.005000, Loss: 3.5959, top1: 12.5000\n",
      "Epoch [14/60], Iter [566/633], LR: 0.005000, Loss: 3.5857, top1: 17.1875\n",
      "Epoch [14/60], Iter [567/633], LR: 0.005000, Loss: 3.5846, top1: 15.6250\n",
      "Epoch [14/60], Iter [568/633], LR: 0.005000, Loss: 3.5615, top1: 17.1875\n",
      "Epoch [14/60], Iter [569/633], LR: 0.005000, Loss: 3.6127, top1: 6.2500\n",
      "Epoch [14/60], Iter [570/633], LR: 0.005000, Loss: 3.5875, top1: 14.0625\n",
      "Epoch [14/60], Iter [571/633], LR: 0.005000, Loss: 3.5781, top1: 18.7500\n",
      "Epoch [14/60], Iter [572/633], LR: 0.005000, Loss: 3.5850, top1: 12.5000\n",
      "Epoch [14/60], Iter [573/633], LR: 0.005000, Loss: 3.5374, top1: 15.6250\n",
      "Epoch [14/60], Iter [574/633], LR: 0.005000, Loss: 3.6026, top1: 10.9375\n",
      "Epoch [14/60], Iter [575/633], LR: 0.005000, Loss: 3.5736, top1: 14.0625\n",
      "Epoch [14/60], Iter [576/633], LR: 0.005000, Loss: 3.5579, top1: 12.5000\n",
      "Epoch [14/60], Iter [577/633], LR: 0.005000, Loss: 3.6149, top1: 12.5000\n",
      "Epoch [14/60], Iter [578/633], LR: 0.005000, Loss: 3.5493, top1: 9.3750\n",
      "Epoch [14/60], Iter [579/633], LR: 0.005000, Loss: 3.5510, top1: 15.6250\n",
      "Epoch [14/60], Iter [580/633], LR: 0.005000, Loss: 3.5453, top1: 20.3125\n",
      "Epoch [14/60], Iter [581/633], LR: 0.005000, Loss: 3.5885, top1: 9.3750\n",
      "Epoch [14/60], Iter [582/633], LR: 0.005000, Loss: 3.5871, top1: 7.8125\n",
      "Epoch [14/60], Iter [583/633], LR: 0.005000, Loss: 3.5950, top1: 12.5000\n",
      "Epoch [14/60], Iter [584/633], LR: 0.005000, Loss: 3.5455, top1: 20.3125\n",
      "Epoch [14/60], Iter [585/633], LR: 0.005000, Loss: 3.5896, top1: 9.3750\n",
      "Epoch [14/60], Iter [586/633], LR: 0.005000, Loss: 3.5241, top1: 18.7500\n",
      "Epoch [14/60], Iter [587/633], LR: 0.005000, Loss: 3.5208, top1: 23.4375\n",
      "Epoch [14/60], Iter [588/633], LR: 0.005000, Loss: 3.5407, top1: 23.4375\n",
      "Epoch [14/60], Iter [589/633], LR: 0.005000, Loss: 3.5735, top1: 17.1875\n",
      "Epoch [14/60], Iter [590/633], LR: 0.005000, Loss: 3.5538, top1: 18.7500\n",
      "Epoch [14/60], Iter [591/633], LR: 0.005000, Loss: 3.5337, top1: 17.1875\n",
      "Epoch [14/60], Iter [592/633], LR: 0.005000, Loss: 3.5784, top1: 15.6250\n",
      "Epoch [14/60], Iter [593/633], LR: 0.005000, Loss: 3.5537, top1: 18.7500\n",
      "Epoch [14/60], Iter [594/633], LR: 0.005000, Loss: 3.5295, top1: 21.8750\n",
      "Epoch [14/60], Iter [595/633], LR: 0.005000, Loss: 3.5457, top1: 17.1875\n",
      "Epoch [14/60], Iter [596/633], LR: 0.005000, Loss: 3.5959, top1: 10.9375\n",
      "Epoch [14/60], Iter [597/633], LR: 0.005000, Loss: 3.6123, top1: 7.8125\n",
      "Epoch [14/60], Iter [598/633], LR: 0.005000, Loss: 3.5648, top1: 12.5000\n",
      "Epoch [14/60], Iter [599/633], LR: 0.005000, Loss: 3.5350, top1: 15.6250\n",
      "Epoch [14/60], Iter [600/633], LR: 0.005000, Loss: 3.5622, top1: 17.1875\n",
      "Epoch [14/60], Iter [601/633], LR: 0.005000, Loss: 3.4799, top1: 26.5625\n",
      "Epoch [14/60], Iter [602/633], LR: 0.005000, Loss: 3.5756, top1: 14.0625\n",
      "Epoch [14/60], Iter [603/633], LR: 0.005000, Loss: 3.5549, top1: 17.1875\n",
      "Epoch [14/60], Iter [604/633], LR: 0.005000, Loss: 3.5637, top1: 18.7500\n",
      "Epoch [14/60], Iter [605/633], LR: 0.005000, Loss: 3.5369, top1: 17.1875\n",
      "Epoch [14/60], Iter [606/633], LR: 0.005000, Loss: 3.5582, top1: 14.0625\n",
      "Epoch [14/60], Iter [607/633], LR: 0.005000, Loss: 3.5615, top1: 10.9375\n",
      "Epoch [14/60], Iter [608/633], LR: 0.005000, Loss: 3.6102, top1: 18.7500\n",
      "Epoch [14/60], Iter [609/633], LR: 0.005000, Loss: 3.5897, top1: 17.1875\n",
      "Epoch [14/60], Iter [610/633], LR: 0.005000, Loss: 3.5370, top1: 20.3125\n",
      "Epoch [14/60], Iter [611/633], LR: 0.005000, Loss: 3.5779, top1: 14.0625\n",
      "Epoch [14/60], Iter [612/633], LR: 0.005000, Loss: 3.5386, top1: 17.1875\n",
      "Epoch [14/60], Iter [613/633], LR: 0.005000, Loss: 3.5980, top1: 12.5000\n",
      "Epoch [14/60], Iter [614/633], LR: 0.005000, Loss: 3.5145, top1: 21.8750\n",
      "Epoch [14/60], Iter [615/633], LR: 0.005000, Loss: 3.5559, top1: 18.7500\n",
      "Epoch [14/60], Iter [616/633], LR: 0.005000, Loss: 3.5635, top1: 15.6250\n",
      "Epoch [14/60], Iter [617/633], LR: 0.005000, Loss: 3.5903, top1: 15.6250\n",
      "Epoch [14/60], Iter [618/633], LR: 0.005000, Loss: 3.5817, top1: 12.5000\n",
      "Epoch [14/60], Iter [619/633], LR: 0.005000, Loss: 3.6178, top1: 10.9375\n",
      "Epoch [14/60], Iter [620/633], LR: 0.005000, Loss: 3.5689, top1: 9.3750\n",
      "Epoch [14/60], Iter [621/633], LR: 0.005000, Loss: 3.5026, top1: 26.5625\n",
      "Epoch [14/60], Iter [622/633], LR: 0.005000, Loss: 3.6208, top1: 6.2500\n",
      "Epoch [14/60], Iter [623/633], LR: 0.005000, Loss: 3.5928, top1: 14.0625\n",
      "Epoch [14/60], Iter [624/633], LR: 0.005000, Loss: 3.5925, top1: 15.6250\n",
      "Epoch [14/60], Iter [625/633], LR: 0.005000, Loss: 3.5605, top1: 12.5000\n",
      "Epoch [14/60], Iter [626/633], LR: 0.005000, Loss: 3.5663, top1: 12.5000\n",
      "Epoch [14/60], Iter [627/633], LR: 0.005000, Loss: 3.5637, top1: 14.0625\n",
      "Epoch [14/60], Iter [628/633], LR: 0.005000, Loss: 3.5756, top1: 14.0625\n",
      "Epoch [14/60], Iter [629/633], LR: 0.005000, Loss: 3.5573, top1: 21.8750\n",
      "Epoch [14/60], Iter [630/633], LR: 0.005000, Loss: 3.6073, top1: 10.9375\n",
      "Epoch [14/60], Iter [631/633], LR: 0.005000, Loss: 3.6176, top1: 14.0625\n",
      "Epoch [14/60], Iter [632/633], LR: 0.005000, Loss: 3.5924, top1: 12.5000\n",
      "Epoch [14/60], Iter [633/633], LR: 0.005000, Loss: 3.5743, top1: 17.1875\n",
      "Epoch [14/60], Iter [634/633], LR: 0.005000, Loss: 3.5868, top1: 8.0645\n",
      "Epoch [14/60], Val_Loss: 3.5598, Val_top1: 15.2509, best_top1: 14.6567\n",
      "epoch time: 4.376109858353932 min\n",
      "Taking top1 snapshot...\n",
      "Epoch [15/60], Iter [1/633], LR: 0.005000, Loss: 3.5844, top1: 12.5000\n",
      "Epoch [15/60], Iter [2/633], LR: 0.005000, Loss: 3.5451, top1: 23.4375\n",
      "Epoch [15/60], Iter [3/633], LR: 0.005000, Loss: 3.6092, top1: 7.8125\n",
      "Epoch [15/60], Iter [4/633], LR: 0.005000, Loss: 3.5342, top1: 21.8750\n",
      "Epoch [15/60], Iter [5/633], LR: 0.005000, Loss: 3.5842, top1: 10.9375\n",
      "Epoch [15/60], Iter [6/633], LR: 0.005000, Loss: 3.5730, top1: 14.0625\n",
      "Epoch [15/60], Iter [7/633], LR: 0.005000, Loss: 3.5902, top1: 14.0625\n",
      "Epoch [15/60], Iter [8/633], LR: 0.005000, Loss: 3.5983, top1: 10.9375\n",
      "Epoch [15/60], Iter [9/633], LR: 0.005000, Loss: 3.5948, top1: 14.0625\n",
      "Epoch [15/60], Iter [10/633], LR: 0.005000, Loss: 3.5659, top1: 14.0625\n",
      "Epoch [15/60], Iter [11/633], LR: 0.005000, Loss: 3.5674, top1: 14.0625\n",
      "Epoch [15/60], Iter [12/633], LR: 0.005000, Loss: 3.5370, top1: 20.3125\n",
      "Epoch [15/60], Iter [13/633], LR: 0.005000, Loss: 3.5193, top1: 25.0000\n",
      "Epoch [15/60], Iter [14/633], LR: 0.005000, Loss: 3.6194, top1: 10.9375\n",
      "Epoch [15/60], Iter [15/633], LR: 0.005000, Loss: 3.5975, top1: 14.0625\n",
      "Epoch [15/60], Iter [16/633], LR: 0.005000, Loss: 3.5362, top1: 18.7500\n",
      "Epoch [15/60], Iter [17/633], LR: 0.005000, Loss: 3.4921, top1: 26.5625\n",
      "Epoch [15/60], Iter [18/633], LR: 0.005000, Loss: 3.5870, top1: 15.6250\n",
      "Epoch [15/60], Iter [19/633], LR: 0.005000, Loss: 3.5928, top1: 7.8125\n",
      "Epoch [15/60], Iter [20/633], LR: 0.005000, Loss: 3.5488, top1: 18.7500\n",
      "Epoch [15/60], Iter [21/633], LR: 0.005000, Loss: 3.5683, top1: 18.7500\n",
      "Epoch [15/60], Iter [22/633], LR: 0.005000, Loss: 3.5962, top1: 10.9375\n",
      "Epoch [15/60], Iter [23/633], LR: 0.005000, Loss: 3.5177, top1: 17.1875\n",
      "Epoch [15/60], Iter [24/633], LR: 0.005000, Loss: 3.5979, top1: 14.0625\n",
      "Epoch [15/60], Iter [25/633], LR: 0.005000, Loss: 3.5917, top1: 17.1875\n",
      "Epoch [15/60], Iter [26/633], LR: 0.005000, Loss: 3.5802, top1: 12.5000\n",
      "Epoch [15/60], Iter [27/633], LR: 0.005000, Loss: 3.5754, top1: 14.0625\n",
      "Epoch [15/60], Iter [28/633], LR: 0.005000, Loss: 3.5519, top1: 17.1875\n",
      "Epoch [15/60], Iter [29/633], LR: 0.005000, Loss: 3.6156, top1: 6.2500\n",
      "Epoch [15/60], Iter [30/633], LR: 0.005000, Loss: 3.6052, top1: 10.9375\n",
      "Epoch [15/60], Iter [31/633], LR: 0.005000, Loss: 3.5959, top1: 12.5000\n",
      "Epoch [15/60], Iter [32/633], LR: 0.005000, Loss: 3.5694, top1: 17.1875\n",
      "Epoch [15/60], Iter [33/633], LR: 0.005000, Loss: 3.6004, top1: 9.3750\n",
      "Epoch [15/60], Iter [34/633], LR: 0.005000, Loss: 3.6141, top1: 10.9375\n",
      "Epoch [15/60], Iter [35/633], LR: 0.005000, Loss: 3.5955, top1: 15.6250\n",
      "Epoch [15/60], Iter [36/633], LR: 0.005000, Loss: 3.6061, top1: 12.5000\n",
      "Epoch [15/60], Iter [37/633], LR: 0.005000, Loss: 3.5695, top1: 12.5000\n",
      "Epoch [15/60], Iter [38/633], LR: 0.005000, Loss: 3.5699, top1: 17.1875\n",
      "Epoch [15/60], Iter [39/633], LR: 0.005000, Loss: 3.5750, top1: 10.9375\n",
      "Epoch [15/60], Iter [40/633], LR: 0.005000, Loss: 3.5875, top1: 10.9375\n",
      "Epoch [15/60], Iter [41/633], LR: 0.005000, Loss: 3.5773, top1: 18.7500\n",
      "Epoch [15/60], Iter [42/633], LR: 0.005000, Loss: 3.5773, top1: 14.0625\n",
      "Epoch [15/60], Iter [43/633], LR: 0.005000, Loss: 3.5680, top1: 18.7500\n",
      "Epoch [15/60], Iter [44/633], LR: 0.005000, Loss: 3.5995, top1: 15.6250\n",
      "Epoch [15/60], Iter [45/633], LR: 0.005000, Loss: 3.5933, top1: 10.9375\n",
      "Epoch [15/60], Iter [46/633], LR: 0.005000, Loss: 3.5658, top1: 7.8125\n",
      "Epoch [15/60], Iter [47/633], LR: 0.005000, Loss: 3.5653, top1: 18.7500\n",
      "Epoch [15/60], Iter [48/633], LR: 0.005000, Loss: 3.6156, top1: 12.5000\n",
      "Epoch [15/60], Iter [49/633], LR: 0.005000, Loss: 3.6124, top1: 10.9375\n",
      "Epoch [15/60], Iter [50/633], LR: 0.005000, Loss: 3.6135, top1: 7.8125\n",
      "Epoch [15/60], Iter [51/633], LR: 0.005000, Loss: 3.5660, top1: 14.0625\n",
      "Epoch [15/60], Iter [52/633], LR: 0.005000, Loss: 3.5119, top1: 25.0000\n",
      "Epoch [15/60], Iter [53/633], LR: 0.005000, Loss: 3.6132, top1: 9.3750\n",
      "Epoch [15/60], Iter [54/633], LR: 0.005000, Loss: 3.5138, top1: 25.0000\n",
      "Epoch [15/60], Iter [55/633], LR: 0.005000, Loss: 3.6039, top1: 10.9375\n",
      "Epoch [15/60], Iter [56/633], LR: 0.005000, Loss: 3.5900, top1: 12.5000\n",
      "Epoch [15/60], Iter [57/633], LR: 0.005000, Loss: 3.5496, top1: 17.1875\n",
      "Epoch [15/60], Iter [58/633], LR: 0.005000, Loss: 3.5560, top1: 17.1875\n",
      "Epoch [15/60], Iter [59/633], LR: 0.005000, Loss: 3.6114, top1: 12.5000\n",
      "Epoch [15/60], Iter [60/633], LR: 0.005000, Loss: 3.4932, top1: 23.4375\n",
      "Epoch [15/60], Iter [61/633], LR: 0.005000, Loss: 3.5264, top1: 17.1875\n",
      "Epoch [15/60], Iter [62/633], LR: 0.005000, Loss: 3.6035, top1: 15.6250\n",
      "Epoch [15/60], Iter [63/633], LR: 0.005000, Loss: 3.5209, top1: 23.4375\n",
      "Epoch [15/60], Iter [64/633], LR: 0.005000, Loss: 3.5701, top1: 14.0625\n",
      "Epoch [15/60], Iter [65/633], LR: 0.005000, Loss: 3.6013, top1: 6.2500\n",
      "Epoch [15/60], Iter [66/633], LR: 0.005000, Loss: 3.5222, top1: 18.7500\n",
      "Epoch [15/60], Iter [67/633], LR: 0.005000, Loss: 3.5222, top1: 20.3125\n",
      "Epoch [15/60], Iter [68/633], LR: 0.005000, Loss: 3.6153, top1: 12.5000\n",
      "Epoch [15/60], Iter [69/633], LR: 0.005000, Loss: 3.5981, top1: 14.0625\n",
      "Epoch [15/60], Iter [70/633], LR: 0.005000, Loss: 3.5321, top1: 21.8750\n",
      "Epoch [15/60], Iter [71/633], LR: 0.005000, Loss: 3.5804, top1: 10.9375\n",
      "Epoch [15/60], Iter [72/633], LR: 0.005000, Loss: 3.5932, top1: 15.6250\n",
      "Epoch [15/60], Iter [73/633], LR: 0.005000, Loss: 3.5500, top1: 17.1875\n",
      "Epoch [15/60], Iter [74/633], LR: 0.005000, Loss: 3.5783, top1: 14.0625\n",
      "Epoch [15/60], Iter [75/633], LR: 0.005000, Loss: 3.5267, top1: 17.1875\n",
      "Epoch [15/60], Iter [76/633], LR: 0.005000, Loss: 3.5366, top1: 18.7500\n",
      "Epoch [15/60], Iter [77/633], LR: 0.005000, Loss: 3.5415, top1: 23.4375\n",
      "Epoch [15/60], Iter [78/633], LR: 0.005000, Loss: 3.5554, top1: 23.4375\n",
      "Epoch [15/60], Iter [79/633], LR: 0.005000, Loss: 3.5892, top1: 15.6250\n",
      "Epoch [15/60], Iter [80/633], LR: 0.005000, Loss: 3.5883, top1: 14.0625\n",
      "Epoch [15/60], Iter [81/633], LR: 0.005000, Loss: 3.5747, top1: 17.1875\n",
      "Epoch [15/60], Iter [82/633], LR: 0.005000, Loss: 3.6142, top1: 9.3750\n",
      "Epoch [15/60], Iter [83/633], LR: 0.005000, Loss: 3.5681, top1: 12.5000\n",
      "Epoch [15/60], Iter [84/633], LR: 0.005000, Loss: 3.5822, top1: 14.0625\n",
      "Epoch [15/60], Iter [85/633], LR: 0.005000, Loss: 3.5795, top1: 14.0625\n",
      "Epoch [15/60], Iter [86/633], LR: 0.005000, Loss: 3.5648, top1: 18.7500\n",
      "Epoch [15/60], Iter [87/633], LR: 0.005000, Loss: 3.6016, top1: 12.5000\n",
      "Epoch [15/60], Iter [88/633], LR: 0.005000, Loss: 3.5535, top1: 18.7500\n",
      "Epoch [15/60], Iter [89/633], LR: 0.005000, Loss: 3.6166, top1: 12.5000\n",
      "Epoch [15/60], Iter [90/633], LR: 0.005000, Loss: 3.6009, top1: 9.3750\n",
      "Epoch [15/60], Iter [91/633], LR: 0.005000, Loss: 3.5628, top1: 18.7500\n",
      "Epoch [15/60], Iter [92/633], LR: 0.005000, Loss: 3.5699, top1: 15.6250\n",
      "Epoch [15/60], Iter [93/633], LR: 0.005000, Loss: 3.6035, top1: 9.3750\n",
      "Epoch [15/60], Iter [94/633], LR: 0.005000, Loss: 3.5347, top1: 18.7500\n",
      "Epoch [15/60], Iter [95/633], LR: 0.005000, Loss: 3.5900, top1: 12.5000\n",
      "Epoch [15/60], Iter [96/633], LR: 0.005000, Loss: 3.5793, top1: 14.0625\n",
      "Epoch [15/60], Iter [97/633], LR: 0.005000, Loss: 3.5766, top1: 10.9375\n",
      "Epoch [15/60], Iter [98/633], LR: 0.005000, Loss: 3.5004, top1: 25.0000\n",
      "Epoch [15/60], Iter [99/633], LR: 0.005000, Loss: 3.5834, top1: 9.3750\n",
      "Epoch [15/60], Iter [100/633], LR: 0.005000, Loss: 3.5373, top1: 17.1875\n",
      "Epoch [15/60], Iter [101/633], LR: 0.005000, Loss: 3.5827, top1: 18.7500\n",
      "Epoch [15/60], Iter [102/633], LR: 0.005000, Loss: 3.5673, top1: 14.0625\n",
      "Epoch [15/60], Iter [103/633], LR: 0.005000, Loss: 3.6148, top1: 12.5000\n",
      "Epoch [15/60], Iter [104/633], LR: 0.005000, Loss: 3.5660, top1: 14.0625\n",
      "Epoch [15/60], Iter [105/633], LR: 0.005000, Loss: 3.5859, top1: 12.5000\n",
      "Epoch [15/60], Iter [106/633], LR: 0.005000, Loss: 3.6063, top1: 9.3750\n",
      "Epoch [15/60], Iter [107/633], LR: 0.005000, Loss: 3.5608, top1: 17.1875\n",
      "Epoch [15/60], Iter [108/633], LR: 0.005000, Loss: 3.5137, top1: 20.3125\n",
      "Epoch [15/60], Iter [109/633], LR: 0.005000, Loss: 3.5467, top1: 18.7500\n",
      "Epoch [15/60], Iter [110/633], LR: 0.005000, Loss: 3.5698, top1: 10.9375\n",
      "Epoch [15/60], Iter [111/633], LR: 0.005000, Loss: 3.5723, top1: 18.7500\n",
      "Epoch [15/60], Iter [112/633], LR: 0.005000, Loss: 3.5588, top1: 18.7500\n",
      "Epoch [15/60], Iter [113/633], LR: 0.005000, Loss: 3.5908, top1: 10.9375\n",
      "Epoch [15/60], Iter [114/633], LR: 0.005000, Loss: 3.6010, top1: 12.5000\n",
      "Epoch [15/60], Iter [115/633], LR: 0.005000, Loss: 3.5626, top1: 14.0625\n",
      "Epoch [15/60], Iter [116/633], LR: 0.005000, Loss: 3.5738, top1: 18.7500\n",
      "Epoch [15/60], Iter [117/633], LR: 0.005000, Loss: 3.5908, top1: 12.5000\n",
      "Epoch [15/60], Iter [118/633], LR: 0.005000, Loss: 3.5649, top1: 12.5000\n",
      "Epoch [15/60], Iter [119/633], LR: 0.005000, Loss: 3.5616, top1: 9.3750\n",
      "Epoch [15/60], Iter [120/633], LR: 0.005000, Loss: 3.5513, top1: 18.7500\n",
      "Epoch [15/60], Iter [121/633], LR: 0.005000, Loss: 3.6289, top1: 10.9375\n",
      "Epoch [15/60], Iter [122/633], LR: 0.005000, Loss: 3.5320, top1: 18.7500\n",
      "Epoch [15/60], Iter [123/633], LR: 0.005000, Loss: 3.5668, top1: 15.6250\n",
      "Epoch [15/60], Iter [124/633], LR: 0.005000, Loss: 3.4888, top1: 25.0000\n",
      "Epoch [15/60], Iter [125/633], LR: 0.005000, Loss: 3.5735, top1: 15.6250\n",
      "Epoch [15/60], Iter [126/633], LR: 0.005000, Loss: 3.5796, top1: 21.8750\n",
      "Epoch [15/60], Iter [127/633], LR: 0.005000, Loss: 3.5302, top1: 17.1875\n",
      "Epoch [15/60], Iter [128/633], LR: 0.005000, Loss: 3.5620, top1: 14.0625\n",
      "Epoch [15/60], Iter [129/633], LR: 0.005000, Loss: 3.6079, top1: 7.8125\n",
      "Epoch [15/60], Iter [130/633], LR: 0.005000, Loss: 3.4796, top1: 26.5625\n",
      "Epoch [15/60], Iter [131/633], LR: 0.005000, Loss: 3.5445, top1: 14.0625\n",
      "Epoch [15/60], Iter [132/633], LR: 0.005000, Loss: 3.5886, top1: 14.0625\n",
      "Epoch [15/60], Iter [133/633], LR: 0.005000, Loss: 3.5738, top1: 14.0625\n",
      "Epoch [15/60], Iter [134/633], LR: 0.005000, Loss: 3.5051, top1: 21.8750\n",
      "Epoch [15/60], Iter [135/633], LR: 0.005000, Loss: 3.5387, top1: 21.8750\n",
      "Epoch [15/60], Iter [136/633], LR: 0.005000, Loss: 3.5922, top1: 12.5000\n",
      "Epoch [15/60], Iter [137/633], LR: 0.005000, Loss: 3.5893, top1: 15.6250\n",
      "Epoch [15/60], Iter [138/633], LR: 0.005000, Loss: 3.5736, top1: 15.6250\n",
      "Epoch [15/60], Iter [139/633], LR: 0.005000, Loss: 3.5816, top1: 17.1875\n",
      "Epoch [15/60], Iter [140/633], LR: 0.005000, Loss: 3.5856, top1: 17.1875\n",
      "Epoch [15/60], Iter [141/633], LR: 0.005000, Loss: 3.6008, top1: 12.5000\n",
      "Epoch [15/60], Iter [142/633], LR: 0.005000, Loss: 3.5297, top1: 21.8750\n",
      "Epoch [15/60], Iter [143/633], LR: 0.005000, Loss: 3.5257, top1: 18.7500\n",
      "Epoch [15/60], Iter [144/633], LR: 0.005000, Loss: 3.5883, top1: 12.5000\n",
      "Epoch [15/60], Iter [145/633], LR: 0.005000, Loss: 3.5657, top1: 12.5000\n",
      "Epoch [15/60], Iter [146/633], LR: 0.005000, Loss: 3.5517, top1: 15.6250\n",
      "Epoch [15/60], Iter [147/633], LR: 0.005000, Loss: 3.5734, top1: 15.6250\n",
      "Epoch [15/60], Iter [148/633], LR: 0.005000, Loss: 3.5970, top1: 9.3750\n",
      "Epoch [15/60], Iter [149/633], LR: 0.005000, Loss: 3.5428, top1: 15.6250\n",
      "Epoch [15/60], Iter [150/633], LR: 0.005000, Loss: 3.5695, top1: 9.3750\n",
      "Epoch [15/60], Iter [151/633], LR: 0.005000, Loss: 3.5871, top1: 6.2500\n",
      "Epoch [15/60], Iter [152/633], LR: 0.005000, Loss: 3.5512, top1: 17.1875\n",
      "Epoch [15/60], Iter [153/633], LR: 0.005000, Loss: 3.5739, top1: 14.0625\n",
      "Epoch [15/60], Iter [154/633], LR: 0.005000, Loss: 3.5357, top1: 20.3125\n",
      "Epoch [15/60], Iter [155/633], LR: 0.005000, Loss: 3.5820, top1: 17.1875\n",
      "Epoch [15/60], Iter [156/633], LR: 0.005000, Loss: 3.5259, top1: 17.1875\n",
      "Epoch [15/60], Iter [157/633], LR: 0.005000, Loss: 3.5404, top1: 21.8750\n",
      "Epoch [15/60], Iter [158/633], LR: 0.005000, Loss: 3.5885, top1: 12.5000\n",
      "Epoch [15/60], Iter [159/633], LR: 0.005000, Loss: 3.5872, top1: 7.8125\n",
      "Epoch [15/60], Iter [160/633], LR: 0.005000, Loss: 3.6059, top1: 10.9375\n",
      "Epoch [15/60], Iter [161/633], LR: 0.005000, Loss: 3.5631, top1: 14.0625\n",
      "Epoch [15/60], Iter [162/633], LR: 0.005000, Loss: 3.5639, top1: 14.0625\n",
      "Epoch [15/60], Iter [163/633], LR: 0.005000, Loss: 3.5787, top1: 18.7500\n",
      "Epoch [15/60], Iter [164/633], LR: 0.005000, Loss: 3.5708, top1: 12.5000\n",
      "Epoch [15/60], Iter [165/633], LR: 0.005000, Loss: 3.5772, top1: 9.3750\n",
      "Epoch [15/60], Iter [166/633], LR: 0.005000, Loss: 3.5341, top1: 20.3125\n",
      "Epoch [15/60], Iter [167/633], LR: 0.005000, Loss: 3.6270, top1: 10.9375\n",
      "Epoch [15/60], Iter [168/633], LR: 0.005000, Loss: 3.5677, top1: 12.5000\n",
      "Epoch [15/60], Iter [169/633], LR: 0.005000, Loss: 3.5770, top1: 14.0625\n",
      "Epoch [15/60], Iter [170/633], LR: 0.005000, Loss: 3.5157, top1: 21.8750\n",
      "Epoch [15/60], Iter [171/633], LR: 0.005000, Loss: 3.5501, top1: 12.5000\n",
      "Epoch [15/60], Iter [172/633], LR: 0.005000, Loss: 3.5539, top1: 15.6250\n",
      "Epoch [15/60], Iter [173/633], LR: 0.005000, Loss: 3.5550, top1: 17.1875\n",
      "Epoch [15/60], Iter [174/633], LR: 0.005000, Loss: 3.5904, top1: 12.5000\n",
      "Epoch [15/60], Iter [175/633], LR: 0.005000, Loss: 3.6030, top1: 10.9375\n",
      "Epoch [15/60], Iter [176/633], LR: 0.005000, Loss: 3.5291, top1: 18.7500\n",
      "Epoch [15/60], Iter [177/633], LR: 0.005000, Loss: 3.5888, top1: 15.6250\n",
      "Epoch [15/60], Iter [178/633], LR: 0.005000, Loss: 3.5294, top1: 23.4375\n",
      "Epoch [15/60], Iter [179/633], LR: 0.005000, Loss: 3.5576, top1: 17.1875\n",
      "Epoch [15/60], Iter [180/633], LR: 0.005000, Loss: 3.5570, top1: 10.9375\n",
      "Epoch [15/60], Iter [181/633], LR: 0.005000, Loss: 3.5422, top1: 20.3125\n",
      "Epoch [15/60], Iter [182/633], LR: 0.005000, Loss: 3.5374, top1: 20.3125\n",
      "Epoch [15/60], Iter [183/633], LR: 0.005000, Loss: 3.5604, top1: 17.1875\n",
      "Epoch [15/60], Iter [184/633], LR: 0.005000, Loss: 3.5982, top1: 10.9375\n",
      "Epoch [15/60], Iter [185/633], LR: 0.005000, Loss: 3.5565, top1: 17.1875\n",
      "Epoch [15/60], Iter [186/633], LR: 0.005000, Loss: 3.5534, top1: 18.7500\n",
      "Epoch [15/60], Iter [187/633], LR: 0.005000, Loss: 3.5719, top1: 14.0625\n",
      "Epoch [15/60], Iter [188/633], LR: 0.005000, Loss: 3.5904, top1: 10.9375\n",
      "Epoch [15/60], Iter [189/633], LR: 0.005000, Loss: 3.5500, top1: 18.7500\n",
      "Epoch [15/60], Iter [190/633], LR: 0.005000, Loss: 3.5806, top1: 12.5000\n",
      "Epoch [15/60], Iter [191/633], LR: 0.005000, Loss: 3.5675, top1: 20.3125\n",
      "Epoch [15/60], Iter [192/633], LR: 0.005000, Loss: 3.5840, top1: 14.0625\n",
      "Epoch [15/60], Iter [193/633], LR: 0.005000, Loss: 3.6070, top1: 10.9375\n",
      "Epoch [15/60], Iter [194/633], LR: 0.005000, Loss: 3.5919, top1: 14.0625\n",
      "Epoch [15/60], Iter [195/633], LR: 0.005000, Loss: 3.5836, top1: 17.1875\n",
      "Epoch [15/60], Iter [196/633], LR: 0.005000, Loss: 3.5687, top1: 7.8125\n",
      "Epoch [15/60], Iter [197/633], LR: 0.005000, Loss: 3.6135, top1: 10.9375\n",
      "Epoch [15/60], Iter [198/633], LR: 0.005000, Loss: 3.5530, top1: 17.1875\n",
      "Epoch [15/60], Iter [199/633], LR: 0.005000, Loss: 3.5683, top1: 15.6250\n",
      "Epoch [15/60], Iter [200/633], LR: 0.005000, Loss: 3.6180, top1: 7.8125\n",
      "Epoch [15/60], Iter [201/633], LR: 0.005000, Loss: 3.5824, top1: 12.5000\n",
      "Epoch [15/60], Iter [202/633], LR: 0.005000, Loss: 3.5004, top1: 20.3125\n",
      "Epoch [15/60], Iter [203/633], LR: 0.005000, Loss: 3.5937, top1: 9.3750\n",
      "Epoch [15/60], Iter [204/633], LR: 0.005000, Loss: 3.5479, top1: 15.6250\n",
      "Epoch [15/60], Iter [205/633], LR: 0.005000, Loss: 3.5677, top1: 12.5000\n",
      "Epoch [15/60], Iter [206/633], LR: 0.005000, Loss: 3.5739, top1: 12.5000\n",
      "Epoch [15/60], Iter [207/633], LR: 0.005000, Loss: 3.5529, top1: 17.1875\n",
      "Epoch [15/60], Iter [208/633], LR: 0.005000, Loss: 3.5845, top1: 10.9375\n",
      "Epoch [15/60], Iter [209/633], LR: 0.005000, Loss: 3.5727, top1: 15.6250\n",
      "Epoch [15/60], Iter [210/633], LR: 0.005000, Loss: 3.6075, top1: 9.3750\n",
      "Epoch [15/60], Iter [211/633], LR: 0.005000, Loss: 3.5715, top1: 12.5000\n",
      "Epoch [15/60], Iter [212/633], LR: 0.005000, Loss: 3.6043, top1: 7.8125\n",
      "Epoch [15/60], Iter [213/633], LR: 0.005000, Loss: 3.5894, top1: 14.0625\n",
      "Epoch [15/60], Iter [214/633], LR: 0.005000, Loss: 3.5849, top1: 18.7500\n",
      "Epoch [15/60], Iter [215/633], LR: 0.005000, Loss: 3.5211, top1: 18.7500\n",
      "Epoch [15/60], Iter [216/633], LR: 0.005000, Loss: 3.6338, top1: 4.6875\n",
      "Epoch [15/60], Iter [217/633], LR: 0.005000, Loss: 3.5723, top1: 17.1875\n",
      "Epoch [15/60], Iter [218/633], LR: 0.005000, Loss: 3.5862, top1: 6.2500\n",
      "Epoch [15/60], Iter [219/633], LR: 0.005000, Loss: 3.5375, top1: 21.8750\n",
      "Epoch [15/60], Iter [220/633], LR: 0.005000, Loss: 3.6242, top1: 10.9375\n",
      "Epoch [15/60], Iter [221/633], LR: 0.005000, Loss: 3.5832, top1: 12.5000\n",
      "Epoch [15/60], Iter [222/633], LR: 0.005000, Loss: 3.5357, top1: 20.3125\n",
      "Epoch [15/60], Iter [223/633], LR: 0.005000, Loss: 3.5561, top1: 17.1875\n",
      "Epoch [15/60], Iter [224/633], LR: 0.005000, Loss: 3.5790, top1: 9.3750\n",
      "Epoch [15/60], Iter [225/633], LR: 0.005000, Loss: 3.5906, top1: 9.3750\n",
      "Epoch [15/60], Iter [226/633], LR: 0.005000, Loss: 3.5591, top1: 15.6250\n",
      "Epoch [15/60], Iter [227/633], LR: 0.005000, Loss: 3.5712, top1: 15.6250\n",
      "Epoch [15/60], Iter [228/633], LR: 0.005000, Loss: 3.5386, top1: 15.6250\n",
      "Epoch [15/60], Iter [229/633], LR: 0.005000, Loss: 3.5718, top1: 10.9375\n",
      "Epoch [15/60], Iter [230/633], LR: 0.005000, Loss: 3.5544, top1: 21.8750\n",
      "Epoch [15/60], Iter [231/633], LR: 0.005000, Loss: 3.5719, top1: 14.0625\n",
      "Epoch [15/60], Iter [232/633], LR: 0.005000, Loss: 3.5455, top1: 21.8750\n",
      "Epoch [15/60], Iter [233/633], LR: 0.005000, Loss: 3.5158, top1: 15.6250\n",
      "Epoch [15/60], Iter [234/633], LR: 0.005000, Loss: 3.5519, top1: 26.5625\n",
      "Epoch [15/60], Iter [235/633], LR: 0.005000, Loss: 3.5976, top1: 14.0625\n",
      "Epoch [15/60], Iter [236/633], LR: 0.005000, Loss: 3.5555, top1: 23.4375\n",
      "Epoch [15/60], Iter [237/633], LR: 0.005000, Loss: 3.6085, top1: 10.9375\n",
      "Epoch [15/60], Iter [238/633], LR: 0.005000, Loss: 3.5649, top1: 12.5000\n",
      "Epoch [15/60], Iter [239/633], LR: 0.005000, Loss: 3.6063, top1: 12.5000\n",
      "Epoch [15/60], Iter [240/633], LR: 0.005000, Loss: 3.5350, top1: 15.6250\n",
      "Epoch [15/60], Iter [241/633], LR: 0.005000, Loss: 3.5608, top1: 14.0625\n",
      "Epoch [15/60], Iter [242/633], LR: 0.005000, Loss: 3.5479, top1: 15.6250\n",
      "Epoch [15/60], Iter [243/633], LR: 0.005000, Loss: 3.5724, top1: 12.5000\n",
      "Epoch [15/60], Iter [244/633], LR: 0.005000, Loss: 3.5681, top1: 9.3750\n",
      "Epoch [15/60], Iter [245/633], LR: 0.005000, Loss: 3.5999, top1: 10.9375\n",
      "Epoch [15/60], Iter [246/633], LR: 0.005000, Loss: 3.4999, top1: 23.4375\n",
      "Epoch [15/60], Iter [247/633], LR: 0.005000, Loss: 3.5315, top1: 18.7500\n",
      "Epoch [15/60], Iter [248/633], LR: 0.005000, Loss: 3.5394, top1: 14.0625\n",
      "Epoch [15/60], Iter [249/633], LR: 0.005000, Loss: 3.5693, top1: 17.1875\n",
      "Epoch [15/60], Iter [250/633], LR: 0.005000, Loss: 3.5346, top1: 20.3125\n",
      "Epoch [15/60], Iter [251/633], LR: 0.005000, Loss: 3.5837, top1: 10.9375\n",
      "Epoch [15/60], Iter [252/633], LR: 0.005000, Loss: 3.5472, top1: 10.9375\n",
      "Epoch [15/60], Iter [253/633], LR: 0.005000, Loss: 3.6133, top1: 7.8125\n",
      "Epoch [15/60], Iter [254/633], LR: 0.005000, Loss: 3.5793, top1: 14.0625\n",
      "Epoch [15/60], Iter [255/633], LR: 0.005000, Loss: 3.5828, top1: 18.7500\n",
      "Epoch [15/60], Iter [256/633], LR: 0.005000, Loss: 3.5497, top1: 23.4375\n",
      "Epoch [15/60], Iter [257/633], LR: 0.005000, Loss: 3.6014, top1: 7.8125\n",
      "Epoch [15/60], Iter [258/633], LR: 0.005000, Loss: 3.6031, top1: 10.9375\n",
      "Epoch [15/60], Iter [259/633], LR: 0.005000, Loss: 3.5889, top1: 12.5000\n",
      "Epoch [15/60], Iter [260/633], LR: 0.005000, Loss: 3.5862, top1: 10.9375\n",
      "Epoch [15/60], Iter [261/633], LR: 0.005000, Loss: 3.5493, top1: 18.7500\n",
      "Epoch [15/60], Iter [262/633], LR: 0.005000, Loss: 3.5755, top1: 21.8750\n",
      "Epoch [15/60], Iter [263/633], LR: 0.005000, Loss: 3.5294, top1: 15.6250\n",
      "Epoch [15/60], Iter [264/633], LR: 0.005000, Loss: 3.6282, top1: 10.9375\n",
      "Epoch [15/60], Iter [265/633], LR: 0.005000, Loss: 3.5545, top1: 18.7500\n",
      "Epoch [15/60], Iter [266/633], LR: 0.005000, Loss: 3.6145, top1: 10.9375\n",
      "Epoch [15/60], Iter [267/633], LR: 0.005000, Loss: 3.6018, top1: 12.5000\n",
      "Epoch [15/60], Iter [268/633], LR: 0.005000, Loss: 3.6076, top1: 9.3750\n",
      "Epoch [15/60], Iter [269/633], LR: 0.005000, Loss: 3.5683, top1: 15.6250\n",
      "Epoch [15/60], Iter [270/633], LR: 0.005000, Loss: 3.6051, top1: 12.5000\n",
      "Epoch [15/60], Iter [271/633], LR: 0.005000, Loss: 3.5621, top1: 17.1875\n",
      "Epoch [15/60], Iter [272/633], LR: 0.005000, Loss: 3.5153, top1: 23.4375\n",
      "Epoch [15/60], Iter [273/633], LR: 0.005000, Loss: 3.5261, top1: 20.3125\n",
      "Epoch [15/60], Iter [274/633], LR: 0.005000, Loss: 3.5447, top1: 23.4375\n",
      "Epoch [15/60], Iter [275/633], LR: 0.005000, Loss: 3.4908, top1: 26.5625\n",
      "Epoch [15/60], Iter [276/633], LR: 0.005000, Loss: 3.4894, top1: 25.0000\n",
      "Epoch [15/60], Iter [277/633], LR: 0.005000, Loss: 3.5273, top1: 15.6250\n",
      "Epoch [15/60], Iter [278/633], LR: 0.005000, Loss: 3.6011, top1: 7.8125\n",
      "Epoch [15/60], Iter [279/633], LR: 0.005000, Loss: 3.5482, top1: 18.7500\n",
      "Epoch [15/60], Iter [280/633], LR: 0.005000, Loss: 3.6259, top1: 7.8125\n",
      "Epoch [15/60], Iter [281/633], LR: 0.005000, Loss: 3.5304, top1: 18.7500\n",
      "Epoch [15/60], Iter [282/633], LR: 0.005000, Loss: 3.5673, top1: 21.8750\n",
      "Epoch [15/60], Iter [283/633], LR: 0.005000, Loss: 3.5926, top1: 14.0625\n",
      "Epoch [15/60], Iter [284/633], LR: 0.005000, Loss: 3.5303, top1: 21.8750\n",
      "Epoch [15/60], Iter [285/633], LR: 0.005000, Loss: 3.5645, top1: 18.7500\n",
      "Epoch [15/60], Iter [286/633], LR: 0.005000, Loss: 3.5520, top1: 17.1875\n",
      "Epoch [15/60], Iter [287/633], LR: 0.005000, Loss: 3.5715, top1: 15.6250\n",
      "Epoch [15/60], Iter [288/633], LR: 0.005000, Loss: 3.5270, top1: 28.1250\n",
      "Epoch [15/60], Iter [289/633], LR: 0.005000, Loss: 3.5797, top1: 9.3750\n",
      "Epoch [15/60], Iter [290/633], LR: 0.005000, Loss: 3.6090, top1: 10.9375\n",
      "Epoch [15/60], Iter [291/633], LR: 0.005000, Loss: 3.5630, top1: 15.6250\n",
      "Epoch [15/60], Iter [292/633], LR: 0.005000, Loss: 3.5465, top1: 21.8750\n",
      "Epoch [15/60], Iter [293/633], LR: 0.005000, Loss: 3.5055, top1: 21.8750\n",
      "Epoch [15/60], Iter [294/633], LR: 0.005000, Loss: 3.4987, top1: 26.5625\n",
      "Epoch [15/60], Iter [295/633], LR: 0.005000, Loss: 3.5619, top1: 17.1875\n",
      "Epoch [15/60], Iter [296/633], LR: 0.005000, Loss: 3.6117, top1: 10.9375\n",
      "Epoch [15/60], Iter [297/633], LR: 0.005000, Loss: 3.5335, top1: 18.7500\n",
      "Epoch [15/60], Iter [298/633], LR: 0.005000, Loss: 3.5256, top1: 17.1875\n",
      "Epoch [15/60], Iter [299/633], LR: 0.005000, Loss: 3.5616, top1: 17.1875\n",
      "Epoch [15/60], Iter [300/633], LR: 0.005000, Loss: 3.5691, top1: 14.0625\n",
      "Epoch [15/60], Iter [301/633], LR: 0.005000, Loss: 3.5536, top1: 18.7500\n",
      "Epoch [15/60], Iter [302/633], LR: 0.005000, Loss: 3.5780, top1: 10.9375\n",
      "Epoch [15/60], Iter [303/633], LR: 0.005000, Loss: 3.5543, top1: 14.0625\n",
      "Epoch [15/60], Iter [304/633], LR: 0.005000, Loss: 3.5151, top1: 18.7500\n",
      "Epoch [15/60], Iter [305/633], LR: 0.005000, Loss: 3.5858, top1: 14.0625\n",
      "Epoch [15/60], Iter [306/633], LR: 0.005000, Loss: 3.6408, top1: 6.2500\n",
      "Epoch [15/60], Iter [307/633], LR: 0.005000, Loss: 3.5700, top1: 15.6250\n",
      "Epoch [15/60], Iter [308/633], LR: 0.005000, Loss: 3.5609, top1: 9.3750\n",
      "Epoch [15/60], Iter [309/633], LR: 0.005000, Loss: 3.5853, top1: 12.5000\n",
      "Epoch [15/60], Iter [310/633], LR: 0.005000, Loss: 3.5443, top1: 17.1875\n",
      "Epoch [15/60], Iter [311/633], LR: 0.005000, Loss: 3.5319, top1: 21.8750\n",
      "Epoch [15/60], Iter [312/633], LR: 0.005000, Loss: 3.5277, top1: 20.3125\n",
      "Epoch [15/60], Iter [313/633], LR: 0.005000, Loss: 3.5469, top1: 17.1875\n",
      "Epoch [15/60], Iter [314/633], LR: 0.005000, Loss: 3.5831, top1: 12.5000\n",
      "Epoch [15/60], Iter [315/633], LR: 0.005000, Loss: 3.5589, top1: 12.5000\n",
      "Epoch [15/60], Iter [316/633], LR: 0.005000, Loss: 3.5662, top1: 15.6250\n",
      "Epoch [15/60], Iter [317/633], LR: 0.005000, Loss: 3.5848, top1: 14.0625\n",
      "Epoch [15/60], Iter [318/633], LR: 0.005000, Loss: 3.5026, top1: 23.4375\n",
      "Epoch [15/60], Iter [319/633], LR: 0.005000, Loss: 3.5697, top1: 12.5000\n",
      "Epoch [15/60], Iter [320/633], LR: 0.005000, Loss: 3.5544, top1: 18.7500\n",
      "Epoch [15/60], Iter [321/633], LR: 0.005000, Loss: 3.5748, top1: 12.5000\n",
      "Epoch [15/60], Iter [322/633], LR: 0.005000, Loss: 3.5499, top1: 17.1875\n",
      "Epoch [15/60], Iter [323/633], LR: 0.005000, Loss: 3.5237, top1: 15.6250\n",
      "Epoch [15/60], Iter [324/633], LR: 0.005000, Loss: 3.5839, top1: 17.1875\n",
      "Epoch [15/60], Iter [325/633], LR: 0.005000, Loss: 3.6021, top1: 9.3750\n",
      "Epoch [15/60], Iter [326/633], LR: 0.005000, Loss: 3.6090, top1: 9.3750\n",
      "Epoch [15/60], Iter [327/633], LR: 0.005000, Loss: 3.5687, top1: 17.1875\n",
      "Epoch [15/60], Iter [328/633], LR: 0.005000, Loss: 3.6101, top1: 7.8125\n",
      "Epoch [15/60], Iter [329/633], LR: 0.005000, Loss: 3.5234, top1: 15.6250\n",
      "Epoch [15/60], Iter [330/633], LR: 0.005000, Loss: 3.5642, top1: 18.7500\n",
      "Epoch [15/60], Iter [331/633], LR: 0.005000, Loss: 3.5560, top1: 18.7500\n",
      "Epoch [15/60], Iter [332/633], LR: 0.005000, Loss: 3.5754, top1: 12.5000\n",
      "Epoch [15/60], Iter [333/633], LR: 0.005000, Loss: 3.5892, top1: 6.2500\n",
      "Epoch [15/60], Iter [334/633], LR: 0.005000, Loss: 3.5842, top1: 14.0625\n",
      "Epoch [15/60], Iter [335/633], LR: 0.005000, Loss: 3.5891, top1: 15.6250\n",
      "Epoch [15/60], Iter [336/633], LR: 0.005000, Loss: 3.5886, top1: 15.6250\n",
      "Epoch [15/60], Iter [337/633], LR: 0.005000, Loss: 3.5525, top1: 18.7500\n",
      "Epoch [15/60], Iter [338/633], LR: 0.005000, Loss: 3.5404, top1: 17.1875\n",
      "Epoch [15/60], Iter [339/633], LR: 0.005000, Loss: 3.5565, top1: 20.3125\n",
      "Epoch [15/60], Iter [340/633], LR: 0.005000, Loss: 3.5458, top1: 17.1875\n",
      "Epoch [15/60], Iter [341/633], LR: 0.005000, Loss: 3.5696, top1: 15.6250\n",
      "Epoch [15/60], Iter [342/633], LR: 0.005000, Loss: 3.5314, top1: 20.3125\n",
      "Epoch [15/60], Iter [343/633], LR: 0.005000, Loss: 3.5971, top1: 10.9375\n",
      "Epoch [15/60], Iter [344/633], LR: 0.005000, Loss: 3.5954, top1: 9.3750\n",
      "Epoch [15/60], Iter [345/633], LR: 0.005000, Loss: 3.5817, top1: 10.9375\n",
      "Epoch [15/60], Iter [346/633], LR: 0.005000, Loss: 3.6353, top1: 3.1250\n",
      "Epoch [15/60], Iter [347/633], LR: 0.005000, Loss: 3.5317, top1: 15.6250\n",
      "Epoch [15/60], Iter [348/633], LR: 0.005000, Loss: 3.5432, top1: 15.6250\n",
      "Epoch [15/60], Iter [349/633], LR: 0.005000, Loss: 3.5436, top1: 23.4375\n",
      "Epoch [15/60], Iter [350/633], LR: 0.005000, Loss: 3.5586, top1: 17.1875\n",
      "Epoch [15/60], Iter [351/633], LR: 0.005000, Loss: 3.5614, top1: 17.1875\n",
      "Epoch [15/60], Iter [352/633], LR: 0.005000, Loss: 3.5328, top1: 23.4375\n",
      "Epoch [15/60], Iter [353/633], LR: 0.005000, Loss: 3.6118, top1: 9.3750\n",
      "Epoch [15/60], Iter [354/633], LR: 0.005000, Loss: 3.5315, top1: 18.7500\n",
      "Epoch [15/60], Iter [355/633], LR: 0.005000, Loss: 3.5328, top1: 17.1875\n",
      "Epoch [15/60], Iter [356/633], LR: 0.005000, Loss: 3.5519, top1: 17.1875\n",
      "Epoch [15/60], Iter [357/633], LR: 0.005000, Loss: 3.5605, top1: 18.7500\n",
      "Epoch [15/60], Iter [358/633], LR: 0.005000, Loss: 3.6040, top1: 14.0625\n",
      "Epoch [15/60], Iter [359/633], LR: 0.005000, Loss: 3.5720, top1: 15.6250\n",
      "Epoch [15/60], Iter [360/633], LR: 0.005000, Loss: 3.5764, top1: 12.5000\n",
      "Epoch [15/60], Iter [361/633], LR: 0.005000, Loss: 3.5551, top1: 14.0625\n",
      "Epoch [15/60], Iter [362/633], LR: 0.005000, Loss: 3.5376, top1: 20.3125\n",
      "Epoch [15/60], Iter [363/633], LR: 0.005000, Loss: 3.5379, top1: 20.3125\n",
      "Epoch [15/60], Iter [364/633], LR: 0.005000, Loss: 3.5659, top1: 15.6250\n",
      "Epoch [15/60], Iter [365/633], LR: 0.005000, Loss: 3.5379, top1: 18.7500\n",
      "Epoch [15/60], Iter [366/633], LR: 0.005000, Loss: 3.6113, top1: 12.5000\n",
      "Epoch [15/60], Iter [367/633], LR: 0.005000, Loss: 3.5659, top1: 15.6250\n",
      "Epoch [15/60], Iter [368/633], LR: 0.005000, Loss: 3.5799, top1: 9.3750\n",
      "Epoch [15/60], Iter [369/633], LR: 0.005000, Loss: 3.6041, top1: 10.9375\n",
      "Epoch [15/60], Iter [370/633], LR: 0.005000, Loss: 3.5597, top1: 14.0625\n",
      "Epoch [15/60], Iter [371/633], LR: 0.005000, Loss: 3.5500, top1: 15.6250\n",
      "Epoch [15/60], Iter [372/633], LR: 0.005000, Loss: 3.6173, top1: 7.8125\n",
      "Epoch [15/60], Iter [373/633], LR: 0.005000, Loss: 3.5915, top1: 17.1875\n",
      "Epoch [15/60], Iter [374/633], LR: 0.005000, Loss: 3.4996, top1: 25.0000\n",
      "Epoch [15/60], Iter [375/633], LR: 0.005000, Loss: 3.5116, top1: 26.5625\n",
      "Epoch [15/60], Iter [376/633], LR: 0.005000, Loss: 3.5715, top1: 10.9375\n",
      "Epoch [15/60], Iter [377/633], LR: 0.005000, Loss: 3.6015, top1: 17.1875\n",
      "Epoch [15/60], Iter [378/633], LR: 0.005000, Loss: 3.6132, top1: 9.3750\n",
      "Epoch [15/60], Iter [379/633], LR: 0.005000, Loss: 3.5972, top1: 17.1875\n",
      "Epoch [15/60], Iter [380/633], LR: 0.005000, Loss: 3.5573, top1: 12.5000\n",
      "Epoch [15/60], Iter [381/633], LR: 0.005000, Loss: 3.6014, top1: 10.9375\n",
      "Epoch [15/60], Iter [382/633], LR: 0.005000, Loss: 3.5823, top1: 17.1875\n",
      "Epoch [15/60], Iter [383/633], LR: 0.005000, Loss: 3.5507, top1: 20.3125\n",
      "Epoch [15/60], Iter [384/633], LR: 0.005000, Loss: 3.6315, top1: 7.8125\n",
      "Epoch [15/60], Iter [385/633], LR: 0.005000, Loss: 3.5502, top1: 12.5000\n",
      "Epoch [15/60], Iter [386/633], LR: 0.005000, Loss: 3.5801, top1: 15.6250\n",
      "Epoch [15/60], Iter [387/633], LR: 0.005000, Loss: 3.5636, top1: 12.5000\n",
      "Epoch [15/60], Iter [388/633], LR: 0.005000, Loss: 3.5384, top1: 17.1875\n",
      "Epoch [15/60], Iter [389/633], LR: 0.005000, Loss: 3.5754, top1: 15.6250\n",
      "Epoch [15/60], Iter [390/633], LR: 0.005000, Loss: 3.5799, top1: 15.6250\n",
      "Epoch [15/60], Iter [391/633], LR: 0.005000, Loss: 3.5834, top1: 14.0625\n",
      "Epoch [15/60], Iter [392/633], LR: 0.005000, Loss: 3.5505, top1: 17.1875\n",
      "Epoch [15/60], Iter [393/633], LR: 0.005000, Loss: 3.5845, top1: 9.3750\n",
      "Epoch [15/60], Iter [394/633], LR: 0.005000, Loss: 3.5466, top1: 20.3125\n",
      "Epoch [15/60], Iter [395/633], LR: 0.005000, Loss: 3.5760, top1: 12.5000\n",
      "Epoch [15/60], Iter [396/633], LR: 0.005000, Loss: 3.5833, top1: 17.1875\n",
      "Epoch [15/60], Iter [397/633], LR: 0.005000, Loss: 3.5715, top1: 20.3125\n",
      "Epoch [15/60], Iter [398/633], LR: 0.005000, Loss: 3.5811, top1: 15.6250\n",
      "Epoch [15/60], Iter [399/633], LR: 0.005000, Loss: 3.5641, top1: 18.7500\n",
      "Epoch [15/60], Iter [400/633], LR: 0.005000, Loss: 3.6276, top1: 9.3750\n",
      "Epoch [15/60], Iter [401/633], LR: 0.005000, Loss: 3.5665, top1: 10.9375\n",
      "Epoch [15/60], Iter [402/633], LR: 0.005000, Loss: 3.6080, top1: 7.8125\n",
      "Epoch [15/60], Iter [403/633], LR: 0.005000, Loss: 3.5519, top1: 21.8750\n",
      "Epoch [15/60], Iter [404/633], LR: 0.005000, Loss: 3.5988, top1: 12.5000\n",
      "Epoch [15/60], Iter [405/633], LR: 0.005000, Loss: 3.5768, top1: 14.0625\n",
      "Epoch [15/60], Iter [406/633], LR: 0.005000, Loss: 3.5849, top1: 15.6250\n",
      "Epoch [15/60], Iter [407/633], LR: 0.005000, Loss: 3.5319, top1: 17.1875\n",
      "Epoch [15/60], Iter [408/633], LR: 0.005000, Loss: 3.5124, top1: 20.3125\n",
      "Epoch [15/60], Iter [409/633], LR: 0.005000, Loss: 3.5439, top1: 20.3125\n",
      "Epoch [15/60], Iter [410/633], LR: 0.005000, Loss: 3.5747, top1: 12.5000\n",
      "Epoch [15/60], Iter [411/633], LR: 0.005000, Loss: 3.5561, top1: 20.3125\n",
      "Epoch [15/60], Iter [412/633], LR: 0.005000, Loss: 3.6042, top1: 12.5000\n",
      "Epoch [15/60], Iter [413/633], LR: 0.005000, Loss: 3.5903, top1: 10.9375\n",
      "Epoch [15/60], Iter [414/633], LR: 0.005000, Loss: 3.5289, top1: 20.3125\n",
      "Epoch [15/60], Iter [415/633], LR: 0.005000, Loss: 3.5823, top1: 10.9375\n",
      "Epoch [15/60], Iter [416/633], LR: 0.005000, Loss: 3.5586, top1: 14.0625\n",
      "Epoch [15/60], Iter [417/633], LR: 0.005000, Loss: 3.5978, top1: 14.0625\n",
      "Epoch [15/60], Iter [418/633], LR: 0.005000, Loss: 3.5669, top1: 18.7500\n",
      "Epoch [15/60], Iter [419/633], LR: 0.005000, Loss: 3.6044, top1: 10.9375\n",
      "Epoch [15/60], Iter [420/633], LR: 0.005000, Loss: 3.5267, top1: 20.3125\n",
      "Epoch [15/60], Iter [421/633], LR: 0.005000, Loss: 3.5679, top1: 10.9375\n",
      "Epoch [15/60], Iter [422/633], LR: 0.005000, Loss: 3.5986, top1: 6.2500\n",
      "Epoch [15/60], Iter [423/633], LR: 0.005000, Loss: 3.5838, top1: 12.5000\n",
      "Epoch [15/60], Iter [424/633], LR: 0.005000, Loss: 3.6082, top1: 10.9375\n",
      "Epoch [15/60], Iter [425/633], LR: 0.005000, Loss: 3.5221, top1: 20.3125\n",
      "Epoch [15/60], Iter [426/633], LR: 0.005000, Loss: 3.5429, top1: 18.7500\n",
      "Epoch [15/60], Iter [427/633], LR: 0.005000, Loss: 3.5733, top1: 12.5000\n",
      "Epoch [15/60], Iter [428/633], LR: 0.005000, Loss: 3.5798, top1: 17.1875\n",
      "Epoch [15/60], Iter [429/633], LR: 0.005000, Loss: 3.5217, top1: 21.8750\n",
      "Epoch [15/60], Iter [430/633], LR: 0.005000, Loss: 3.5942, top1: 12.5000\n",
      "Epoch [15/60], Iter [431/633], LR: 0.005000, Loss: 3.6014, top1: 12.5000\n",
      "Epoch [15/60], Iter [432/633], LR: 0.005000, Loss: 3.5858, top1: 12.5000\n",
      "Epoch [15/60], Iter [433/633], LR: 0.005000, Loss: 3.5490, top1: 21.8750\n",
      "Epoch [15/60], Iter [434/633], LR: 0.005000, Loss: 3.6404, top1: 9.3750\n",
      "Epoch [15/60], Iter [435/633], LR: 0.005000, Loss: 3.5821, top1: 12.5000\n",
      "Epoch [15/60], Iter [436/633], LR: 0.005000, Loss: 3.5913, top1: 12.5000\n",
      "Epoch [15/60], Iter [437/633], LR: 0.005000, Loss: 3.6236, top1: 10.9375\n",
      "Epoch [15/60], Iter [438/633], LR: 0.005000, Loss: 3.6042, top1: 15.6250\n",
      "Epoch [15/60], Iter [439/633], LR: 0.005000, Loss: 3.5855, top1: 12.5000\n",
      "Epoch [15/60], Iter [440/633], LR: 0.005000, Loss: 3.5643, top1: 17.1875\n",
      "Epoch [15/60], Iter [441/633], LR: 0.005000, Loss: 3.5613, top1: 15.6250\n",
      "Epoch [15/60], Iter [442/633], LR: 0.005000, Loss: 3.5967, top1: 7.8125\n",
      "Epoch [15/60], Iter [443/633], LR: 0.005000, Loss: 3.5739, top1: 9.3750\n",
      "Epoch [15/60], Iter [444/633], LR: 0.005000, Loss: 3.5460, top1: 21.8750\n",
      "Epoch [15/60], Iter [445/633], LR: 0.005000, Loss: 3.5301, top1: 20.3125\n",
      "Epoch [15/60], Iter [446/633], LR: 0.005000, Loss: 3.5178, top1: 28.1250\n",
      "Epoch [15/60], Iter [447/633], LR: 0.005000, Loss: 3.5504, top1: 15.6250\n",
      "Epoch [15/60], Iter [448/633], LR: 0.005000, Loss: 3.5776, top1: 12.5000\n",
      "Epoch [15/60], Iter [449/633], LR: 0.005000, Loss: 3.5729, top1: 12.5000\n",
      "Epoch [15/60], Iter [450/633], LR: 0.005000, Loss: 3.5256, top1: 23.4375\n",
      "Epoch [15/60], Iter [451/633], LR: 0.005000, Loss: 3.5315, top1: 20.3125\n",
      "Epoch [15/60], Iter [452/633], LR: 0.005000, Loss: 3.5644, top1: 14.0625\n",
      "Epoch [15/60], Iter [453/633], LR: 0.005000, Loss: 3.5731, top1: 14.0625\n",
      "Epoch [15/60], Iter [454/633], LR: 0.005000, Loss: 3.5782, top1: 10.9375\n",
      "Epoch [15/60], Iter [455/633], LR: 0.005000, Loss: 3.4937, top1: 20.3125\n",
      "Epoch [15/60], Iter [456/633], LR: 0.005000, Loss: 3.5934, top1: 15.6250\n",
      "Epoch [15/60], Iter [457/633], LR: 0.005000, Loss: 3.5782, top1: 17.1875\n",
      "Epoch [15/60], Iter [458/633], LR: 0.005000, Loss: 3.5431, top1: 15.6250\n",
      "Epoch [15/60], Iter [459/633], LR: 0.005000, Loss: 3.5246, top1: 21.8750\n",
      "Epoch [15/60], Iter [460/633], LR: 0.005000, Loss: 3.5008, top1: 26.5625\n",
      "Epoch [15/60], Iter [461/633], LR: 0.005000, Loss: 3.5918, top1: 10.9375\n",
      "Epoch [15/60], Iter [462/633], LR: 0.005000, Loss: 3.5676, top1: 14.0625\n",
      "Epoch [15/60], Iter [463/633], LR: 0.005000, Loss: 3.5602, top1: 15.6250\n",
      "Epoch [15/60], Iter [464/633], LR: 0.005000, Loss: 3.5594, top1: 18.7500\n",
      "Epoch [15/60], Iter [465/633], LR: 0.005000, Loss: 3.6071, top1: 6.2500\n",
      "Epoch [15/60], Iter [466/633], LR: 0.005000, Loss: 3.5441, top1: 10.9375\n",
      "Epoch [15/60], Iter [467/633], LR: 0.005000, Loss: 3.5927, top1: 9.3750\n",
      "Epoch [15/60], Iter [468/633], LR: 0.005000, Loss: 3.5877, top1: 14.0625\n",
      "Epoch [15/60], Iter [469/633], LR: 0.005000, Loss: 3.5599, top1: 14.0625\n",
      "Epoch [15/60], Iter [470/633], LR: 0.005000, Loss: 3.5582, top1: 17.1875\n",
      "Epoch [15/60], Iter [471/633], LR: 0.005000, Loss: 3.5409, top1: 21.8750\n",
      "Epoch [15/60], Iter [472/633], LR: 0.005000, Loss: 3.5261, top1: 17.1875\n",
      "Epoch [15/60], Iter [473/633], LR: 0.005000, Loss: 3.5548, top1: 18.7500\n",
      "Epoch [15/60], Iter [474/633], LR: 0.005000, Loss: 3.5935, top1: 7.8125\n",
      "Epoch [15/60], Iter [475/633], LR: 0.005000, Loss: 3.5835, top1: 12.5000\n",
      "Epoch [15/60], Iter [476/633], LR: 0.005000, Loss: 3.6359, top1: 7.8125\n",
      "Epoch [15/60], Iter [477/633], LR: 0.005000, Loss: 3.5459, top1: 20.3125\n",
      "Epoch [15/60], Iter [478/633], LR: 0.005000, Loss: 3.5369, top1: 15.6250\n",
      "Epoch [15/60], Iter [479/633], LR: 0.005000, Loss: 3.6215, top1: 9.3750\n",
      "Epoch [15/60], Iter [480/633], LR: 0.005000, Loss: 3.5726, top1: 12.5000\n",
      "Epoch [15/60], Iter [481/633], LR: 0.005000, Loss: 3.5276, top1: 23.4375\n",
      "Epoch [15/60], Iter [482/633], LR: 0.005000, Loss: 3.5977, top1: 10.9375\n",
      "Epoch [15/60], Iter [483/633], LR: 0.005000, Loss: 3.6351, top1: 10.9375\n",
      "Epoch [15/60], Iter [484/633], LR: 0.005000, Loss: 3.5644, top1: 18.7500\n",
      "Epoch [15/60], Iter [485/633], LR: 0.005000, Loss: 3.5973, top1: 9.3750\n",
      "Epoch [15/60], Iter [486/633], LR: 0.005000, Loss: 3.5676, top1: 17.1875\n",
      "Epoch [15/60], Iter [487/633], LR: 0.005000, Loss: 3.5396, top1: 18.7500\n",
      "Epoch [15/60], Iter [488/633], LR: 0.005000, Loss: 3.5997, top1: 12.5000\n",
      "Epoch [15/60], Iter [489/633], LR: 0.005000, Loss: 3.5837, top1: 14.0625\n",
      "Epoch [15/60], Iter [490/633], LR: 0.005000, Loss: 3.5880, top1: 14.0625\n",
      "Epoch [15/60], Iter [491/633], LR: 0.005000, Loss: 3.6117, top1: 9.3750\n",
      "Epoch [15/60], Iter [492/633], LR: 0.005000, Loss: 3.5800, top1: 15.6250\n",
      "Epoch [15/60], Iter [493/633], LR: 0.005000, Loss: 3.5302, top1: 20.3125\n",
      "Epoch [15/60], Iter [494/633], LR: 0.005000, Loss: 3.5859, top1: 12.5000\n",
      "Epoch [15/60], Iter [495/633], LR: 0.005000, Loss: 3.5537, top1: 15.6250\n",
      "Epoch [15/60], Iter [496/633], LR: 0.005000, Loss: 3.5785, top1: 18.7500\n",
      "Epoch [15/60], Iter [497/633], LR: 0.005000, Loss: 3.5578, top1: 17.1875\n",
      "Epoch [15/60], Iter [498/633], LR: 0.005000, Loss: 3.5993, top1: 14.0625\n",
      "Epoch [15/60], Iter [499/633], LR: 0.005000, Loss: 3.5640, top1: 17.1875\n",
      "Epoch [15/60], Iter [500/633], LR: 0.005000, Loss: 3.5567, top1: 15.6250\n",
      "Epoch [15/60], Iter [501/633], LR: 0.005000, Loss: 3.5683, top1: 14.0625\n",
      "Epoch [15/60], Iter [502/633], LR: 0.005000, Loss: 3.5872, top1: 14.0625\n",
      "Epoch [15/60], Iter [503/633], LR: 0.005000, Loss: 3.5667, top1: 14.0625\n",
      "Epoch [15/60], Iter [504/633], LR: 0.005000, Loss: 3.6149, top1: 10.9375\n",
      "Epoch [15/60], Iter [505/633], LR: 0.005000, Loss: 3.5364, top1: 18.7500\n",
      "Epoch [15/60], Iter [506/633], LR: 0.005000, Loss: 3.5368, top1: 18.7500\n",
      "Epoch [15/60], Iter [507/633], LR: 0.005000, Loss: 3.5654, top1: 14.0625\n",
      "Epoch [15/60], Iter [508/633], LR: 0.005000, Loss: 3.6429, top1: 7.8125\n",
      "Epoch [15/60], Iter [509/633], LR: 0.005000, Loss: 3.5684, top1: 15.6250\n",
      "Epoch [15/60], Iter [510/633], LR: 0.005000, Loss: 3.6290, top1: 6.2500\n",
      "Epoch [15/60], Iter [511/633], LR: 0.005000, Loss: 3.5969, top1: 15.6250\n",
      "Epoch [15/60], Iter [512/633], LR: 0.005000, Loss: 3.5517, top1: 15.6250\n",
      "Epoch [15/60], Iter [513/633], LR: 0.005000, Loss: 3.5685, top1: 12.5000\n",
      "Epoch [15/60], Iter [514/633], LR: 0.005000, Loss: 3.5768, top1: 10.9375\n",
      "Epoch [15/60], Iter [515/633], LR: 0.005000, Loss: 3.5327, top1: 23.4375\n",
      "Epoch [15/60], Iter [516/633], LR: 0.005000, Loss: 3.6025, top1: 17.1875\n",
      "Epoch [15/60], Iter [517/633], LR: 0.005000, Loss: 3.5365, top1: 20.3125\n",
      "Epoch [15/60], Iter [518/633], LR: 0.005000, Loss: 3.5746, top1: 18.7500\n",
      "Epoch [15/60], Iter [519/633], LR: 0.005000, Loss: 3.5681, top1: 21.8750\n",
      "Epoch [15/60], Iter [520/633], LR: 0.005000, Loss: 3.5928, top1: 12.5000\n",
      "Epoch [15/60], Iter [521/633], LR: 0.005000, Loss: 3.6142, top1: 9.3750\n",
      "Epoch [15/60], Iter [522/633], LR: 0.005000, Loss: 3.5768, top1: 9.3750\n",
      "Epoch [15/60], Iter [523/633], LR: 0.005000, Loss: 3.5982, top1: 12.5000\n",
      "Epoch [15/60], Iter [524/633], LR: 0.005000, Loss: 3.5203, top1: 20.3125\n",
      "Epoch [15/60], Iter [525/633], LR: 0.005000, Loss: 3.5789, top1: 12.5000\n",
      "Epoch [15/60], Iter [526/633], LR: 0.005000, Loss: 3.5603, top1: 21.8750\n",
      "Epoch [15/60], Iter [527/633], LR: 0.005000, Loss: 3.5775, top1: 12.5000\n",
      "Epoch [15/60], Iter [528/633], LR: 0.005000, Loss: 3.5869, top1: 6.2500\n",
      "Epoch [15/60], Iter [529/633], LR: 0.005000, Loss: 3.5640, top1: 10.9375\n",
      "Epoch [15/60], Iter [530/633], LR: 0.005000, Loss: 3.5672, top1: 12.5000\n",
      "Epoch [15/60], Iter [531/633], LR: 0.005000, Loss: 3.5310, top1: 21.8750\n",
      "Epoch [15/60], Iter [532/633], LR: 0.005000, Loss: 3.5764, top1: 17.1875\n",
      "Epoch [15/60], Iter [533/633], LR: 0.005000, Loss: 3.6021, top1: 9.3750\n",
      "Epoch [15/60], Iter [534/633], LR: 0.005000, Loss: 3.5573, top1: 18.7500\n",
      "Epoch [15/60], Iter [535/633], LR: 0.005000, Loss: 3.5831, top1: 14.0625\n",
      "Epoch [15/60], Iter [536/633], LR: 0.005000, Loss: 3.5906, top1: 12.5000\n",
      "Epoch [15/60], Iter [537/633], LR: 0.005000, Loss: 3.5384, top1: 20.3125\n",
      "Epoch [15/60], Iter [538/633], LR: 0.005000, Loss: 3.6158, top1: 3.1250\n",
      "Epoch [15/60], Iter [539/633], LR: 0.005000, Loss: 3.5826, top1: 15.6250\n",
      "Epoch [15/60], Iter [540/633], LR: 0.005000, Loss: 3.5592, top1: 18.7500\n",
      "Epoch [15/60], Iter [541/633], LR: 0.005000, Loss: 3.5952, top1: 14.0625\n",
      "Epoch [15/60], Iter [542/633], LR: 0.005000, Loss: 3.5680, top1: 17.1875\n",
      "Epoch [15/60], Iter [543/633], LR: 0.005000, Loss: 3.5979, top1: 18.7500\n",
      "Epoch [15/60], Iter [544/633], LR: 0.005000, Loss: 3.5508, top1: 18.7500\n",
      "Epoch [15/60], Iter [545/633], LR: 0.005000, Loss: 3.5515, top1: 18.7500\n",
      "Epoch [15/60], Iter [546/633], LR: 0.005000, Loss: 3.6019, top1: 7.8125\n",
      "Epoch [15/60], Iter [547/633], LR: 0.005000, Loss: 3.5649, top1: 14.0625\n",
      "Epoch [15/60], Iter [548/633], LR: 0.005000, Loss: 3.5599, top1: 17.1875\n",
      "Epoch [15/60], Iter [549/633], LR: 0.005000, Loss: 3.5762, top1: 9.3750\n",
      "Epoch [15/60], Iter [550/633], LR: 0.005000, Loss: 3.5265, top1: 18.7500\n",
      "Epoch [15/60], Iter [551/633], LR: 0.005000, Loss: 3.6290, top1: 9.3750\n",
      "Epoch [15/60], Iter [552/633], LR: 0.005000, Loss: 3.5457, top1: 23.4375\n",
      "Epoch [15/60], Iter [553/633], LR: 0.005000, Loss: 3.5602, top1: 15.6250\n",
      "Epoch [15/60], Iter [554/633], LR: 0.005000, Loss: 3.5596, top1: 21.8750\n",
      "Epoch [15/60], Iter [555/633], LR: 0.005000, Loss: 3.5870, top1: 14.0625\n",
      "Epoch [15/60], Iter [556/633], LR: 0.005000, Loss: 3.5689, top1: 20.3125\n",
      "Epoch [15/60], Iter [557/633], LR: 0.005000, Loss: 3.5642, top1: 18.7500\n",
      "Epoch [15/60], Iter [558/633], LR: 0.005000, Loss: 3.5351, top1: 18.7500\n",
      "Epoch [15/60], Iter [559/633], LR: 0.005000, Loss: 3.5484, top1: 14.0625\n",
      "Epoch [15/60], Iter [560/633], LR: 0.005000, Loss: 3.5370, top1: 15.6250\n",
      "Epoch [15/60], Iter [561/633], LR: 0.005000, Loss: 3.5339, top1: 21.8750\n",
      "Epoch [15/60], Iter [562/633], LR: 0.005000, Loss: 3.5719, top1: 12.5000\n",
      "Epoch [15/60], Iter [563/633], LR: 0.005000, Loss: 3.5533, top1: 17.1875\n",
      "Epoch [15/60], Iter [564/633], LR: 0.005000, Loss: 3.5540, top1: 18.7500\n",
      "Epoch [15/60], Iter [565/633], LR: 0.005000, Loss: 3.5629, top1: 18.7500\n",
      "Epoch [15/60], Iter [566/633], LR: 0.005000, Loss: 3.5515, top1: 18.7500\n",
      "Epoch [15/60], Iter [567/633], LR: 0.005000, Loss: 3.5708, top1: 14.0625\n",
      "Epoch [15/60], Iter [568/633], LR: 0.005000, Loss: 3.6087, top1: 6.2500\n",
      "Epoch [15/60], Iter [569/633], LR: 0.005000, Loss: 3.6102, top1: 7.8125\n",
      "Epoch [15/60], Iter [570/633], LR: 0.005000, Loss: 3.5472, top1: 18.7500\n",
      "Epoch [15/60], Iter [571/633], LR: 0.005000, Loss: 3.5606, top1: 18.7500\n",
      "Epoch [15/60], Iter [572/633], LR: 0.005000, Loss: 3.5664, top1: 21.8750\n",
      "Epoch [15/60], Iter [573/633], LR: 0.005000, Loss: 3.5799, top1: 12.5000\n",
      "Epoch [15/60], Iter [574/633], LR: 0.005000, Loss: 3.5173, top1: 23.4375\n",
      "Epoch [15/60], Iter [575/633], LR: 0.005000, Loss: 3.5391, top1: 21.8750\n",
      "Epoch [15/60], Iter [576/633], LR: 0.005000, Loss: 3.6054, top1: 7.8125\n",
      "Epoch [15/60], Iter [577/633], LR: 0.005000, Loss: 3.5758, top1: 12.5000\n",
      "Epoch [15/60], Iter [578/633], LR: 0.005000, Loss: 3.5895, top1: 10.9375\n",
      "Epoch [15/60], Iter [579/633], LR: 0.005000, Loss: 3.5886, top1: 15.6250\n",
      "Epoch [15/60], Iter [580/633], LR: 0.005000, Loss: 3.5787, top1: 12.5000\n",
      "Epoch [15/60], Iter [581/633], LR: 0.005000, Loss: 3.5823, top1: 14.0625\n",
      "Epoch [15/60], Iter [582/633], LR: 0.005000, Loss: 3.5719, top1: 17.1875\n",
      "Epoch [15/60], Iter [583/633], LR: 0.005000, Loss: 3.5761, top1: 9.3750\n",
      "Epoch [15/60], Iter [584/633], LR: 0.005000, Loss: 3.6387, top1: 6.2500\n",
      "Epoch [15/60], Iter [585/633], LR: 0.005000, Loss: 3.5781, top1: 18.7500\n",
      "Epoch [15/60], Iter [586/633], LR: 0.005000, Loss: 3.5708, top1: 15.6250\n",
      "Epoch [15/60], Iter [587/633], LR: 0.005000, Loss: 3.5735, top1: 12.5000\n",
      "Epoch [15/60], Iter [588/633], LR: 0.005000, Loss: 3.5646, top1: 14.0625\n",
      "Epoch [15/60], Iter [589/633], LR: 0.005000, Loss: 3.5391, top1: 18.7500\n",
      "Epoch [15/60], Iter [590/633], LR: 0.005000, Loss: 3.5753, top1: 10.9375\n",
      "Epoch [15/60], Iter [591/633], LR: 0.005000, Loss: 3.5568, top1: 18.7500\n",
      "Epoch [15/60], Iter [592/633], LR: 0.005000, Loss: 3.6082, top1: 4.6875\n",
      "Epoch [15/60], Iter [593/633], LR: 0.005000, Loss: 3.5798, top1: 10.9375\n",
      "Epoch [15/60], Iter [594/633], LR: 0.005000, Loss: 3.5784, top1: 18.7500\n",
      "Epoch [15/60], Iter [595/633], LR: 0.005000, Loss: 3.5610, top1: 15.6250\n",
      "Epoch [15/60], Iter [596/633], LR: 0.005000, Loss: 3.5511, top1: 18.7500\n",
      "Epoch [15/60], Iter [597/633], LR: 0.005000, Loss: 3.5867, top1: 10.9375\n",
      "Epoch [15/60], Iter [598/633], LR: 0.005000, Loss: 3.5172, top1: 17.1875\n",
      "Epoch [15/60], Iter [599/633], LR: 0.005000, Loss: 3.6030, top1: 14.0625\n",
      "Epoch [15/60], Iter [600/633], LR: 0.005000, Loss: 3.5828, top1: 12.5000\n",
      "Epoch [15/60], Iter [601/633], LR: 0.005000, Loss: 3.5245, top1: 20.3125\n",
      "Epoch [15/60], Iter [602/633], LR: 0.005000, Loss: 3.5437, top1: 15.6250\n",
      "Epoch [15/60], Iter [603/633], LR: 0.005000, Loss: 3.5309, top1: 18.7500\n",
      "Epoch [15/60], Iter [604/633], LR: 0.005000, Loss: 3.5646, top1: 18.7500\n",
      "Epoch [15/60], Iter [605/633], LR: 0.005000, Loss: 3.5707, top1: 20.3125\n",
      "Epoch [15/60], Iter [606/633], LR: 0.005000, Loss: 3.5732, top1: 20.3125\n",
      "Epoch [15/60], Iter [607/633], LR: 0.005000, Loss: 3.5465, top1: 21.8750\n",
      "Epoch [15/60], Iter [608/633], LR: 0.005000, Loss: 3.5772, top1: 15.6250\n",
      "Epoch [15/60], Iter [609/633], LR: 0.005000, Loss: 3.5258, top1: 20.3125\n",
      "Epoch [15/60], Iter [610/633], LR: 0.005000, Loss: 3.5395, top1: 18.7500\n",
      "Epoch [15/60], Iter [611/633], LR: 0.005000, Loss: 3.5828, top1: 12.5000\n",
      "Epoch [15/60], Iter [612/633], LR: 0.005000, Loss: 3.5984, top1: 7.8125\n",
      "Epoch [15/60], Iter [613/633], LR: 0.005000, Loss: 3.5723, top1: 12.5000\n",
      "Epoch [15/60], Iter [614/633], LR: 0.005000, Loss: 3.5450, top1: 21.8750\n",
      "Epoch [15/60], Iter [615/633], LR: 0.005000, Loss: 3.6044, top1: 14.0625\n",
      "Epoch [15/60], Iter [616/633], LR: 0.005000, Loss: 3.5810, top1: 6.2500\n",
      "Epoch [15/60], Iter [617/633], LR: 0.005000, Loss: 3.5796, top1: 14.0625\n",
      "Epoch [15/60], Iter [618/633], LR: 0.005000, Loss: 3.5690, top1: 14.0625\n",
      "Epoch [15/60], Iter [619/633], LR: 0.005000, Loss: 3.5705, top1: 17.1875\n",
      "Epoch [15/60], Iter [620/633], LR: 0.005000, Loss: 3.5800, top1: 14.0625\n",
      "Epoch [15/60], Iter [621/633], LR: 0.005000, Loss: 3.5538, top1: 15.6250\n",
      "Epoch [15/60], Iter [622/633], LR: 0.005000, Loss: 3.6213, top1: 9.3750\n",
      "Epoch [15/60], Iter [623/633], LR: 0.005000, Loss: 3.6011, top1: 14.0625\n",
      "Epoch [15/60], Iter [624/633], LR: 0.005000, Loss: 3.5744, top1: 12.5000\n",
      "Epoch [15/60], Iter [625/633], LR: 0.005000, Loss: 3.5764, top1: 15.6250\n",
      "Epoch [15/60], Iter [626/633], LR: 0.005000, Loss: 3.5520, top1: 18.7500\n",
      "Epoch [15/60], Iter [627/633], LR: 0.005000, Loss: 3.6222, top1: 7.8125\n",
      "Epoch [15/60], Iter [628/633], LR: 0.005000, Loss: 3.5898, top1: 12.5000\n",
      "Epoch [15/60], Iter [629/633], LR: 0.005000, Loss: 3.5939, top1: 15.6250\n",
      "Epoch [15/60], Iter [630/633], LR: 0.005000, Loss: 3.5967, top1: 10.9375\n",
      "Epoch [15/60], Iter [631/633], LR: 0.005000, Loss: 3.5235, top1: 21.8750\n",
      "Epoch [15/60], Iter [632/633], LR: 0.005000, Loss: 3.5686, top1: 15.6250\n",
      "Epoch [15/60], Iter [633/633], LR: 0.005000, Loss: 3.5764, top1: 14.0625\n",
      "Epoch [15/60], Iter [634/633], LR: 0.005000, Loss: 3.5513, top1: 17.7419\n",
      "Epoch [15/60], Val_Loss: 3.5708, Val_top1: 14.7007, best_top1: 15.2509\n",
      "epoch time: 4.36393031279246 min\n",
      "Epoch [16/60], Iter [1/633], LR: 0.005000, Loss: 3.6013, top1: 7.8125\n",
      "Epoch [16/60], Iter [2/633], LR: 0.005000, Loss: 3.5057, top1: 28.1250\n",
      "Epoch [16/60], Iter [3/633], LR: 0.005000, Loss: 3.6077, top1: 18.7500\n",
      "Epoch [16/60], Iter [4/633], LR: 0.005000, Loss: 3.5563, top1: 15.6250\n",
      "Epoch [16/60], Iter [5/633], LR: 0.005000, Loss: 3.5817, top1: 15.6250\n",
      "Epoch [16/60], Iter [6/633], LR: 0.005000, Loss: 3.5571, top1: 17.1875\n",
      "Epoch [16/60], Iter [7/633], LR: 0.005000, Loss: 3.5781, top1: 12.5000\n",
      "Epoch [16/60], Iter [8/633], LR: 0.005000, Loss: 3.5082, top1: 20.3125\n",
      "Epoch [16/60], Iter [9/633], LR: 0.005000, Loss: 3.6176, top1: 6.2500\n",
      "Epoch [16/60], Iter [10/633], LR: 0.005000, Loss: 3.6080, top1: 15.6250\n",
      "Epoch [16/60], Iter [11/633], LR: 0.005000, Loss: 3.5963, top1: 14.0625\n",
      "Epoch [16/60], Iter [12/633], LR: 0.005000, Loss: 3.5689, top1: 18.7500\n",
      "Epoch [16/60], Iter [13/633], LR: 0.005000, Loss: 3.5526, top1: 14.0625\n",
      "Epoch [16/60], Iter [14/633], LR: 0.005000, Loss: 3.5762, top1: 15.6250\n",
      "Epoch [16/60], Iter [15/633], LR: 0.005000, Loss: 3.5570, top1: 15.6250\n",
      "Epoch [16/60], Iter [16/633], LR: 0.005000, Loss: 3.5422, top1: 15.6250\n",
      "Epoch [16/60], Iter [17/633], LR: 0.005000, Loss: 3.5918, top1: 12.5000\n",
      "Epoch [16/60], Iter [18/633], LR: 0.005000, Loss: 3.5581, top1: 18.7500\n",
      "Epoch [16/60], Iter [19/633], LR: 0.005000, Loss: 3.5788, top1: 9.3750\n",
      "Epoch [16/60], Iter [20/633], LR: 0.005000, Loss: 3.5167, top1: 25.0000\n",
      "Epoch [16/60], Iter [21/633], LR: 0.005000, Loss: 3.6010, top1: 6.2500\n",
      "Epoch [16/60], Iter [22/633], LR: 0.005000, Loss: 3.5584, top1: 18.7500\n",
      "Epoch [16/60], Iter [23/633], LR: 0.005000, Loss: 3.5374, top1: 17.1875\n",
      "Epoch [16/60], Iter [24/633], LR: 0.005000, Loss: 3.5188, top1: 23.4375\n",
      "Epoch [16/60], Iter [25/633], LR: 0.005000, Loss: 3.5845, top1: 7.8125\n",
      "Epoch [16/60], Iter [26/633], LR: 0.005000, Loss: 3.5904, top1: 10.9375\n",
      "Epoch [16/60], Iter [27/633], LR: 0.005000, Loss: 3.6173, top1: 9.3750\n",
      "Epoch [16/60], Iter [28/633], LR: 0.005000, Loss: 3.5881, top1: 10.9375\n",
      "Epoch [16/60], Iter [29/633], LR: 0.005000, Loss: 3.5547, top1: 17.1875\n",
      "Epoch [16/60], Iter [30/633], LR: 0.005000, Loss: 3.5831, top1: 9.3750\n",
      "Epoch [16/60], Iter [31/633], LR: 0.005000, Loss: 3.5866, top1: 14.0625\n",
      "Epoch [16/60], Iter [32/633], LR: 0.005000, Loss: 3.6012, top1: 12.5000\n",
      "Epoch [16/60], Iter [33/633], LR: 0.005000, Loss: 3.5467, top1: 23.4375\n",
      "Epoch [16/60], Iter [34/633], LR: 0.005000, Loss: 3.5682, top1: 18.7500\n",
      "Epoch [16/60], Iter [35/633], LR: 0.005000, Loss: 3.5486, top1: 20.3125\n",
      "Epoch [16/60], Iter [36/633], LR: 0.005000, Loss: 3.5346, top1: 17.1875\n",
      "Epoch [16/60], Iter [37/633], LR: 0.005000, Loss: 3.5527, top1: 10.9375\n",
      "Epoch [16/60], Iter [38/633], LR: 0.005000, Loss: 3.5801, top1: 12.5000\n",
      "Epoch [16/60], Iter [39/633], LR: 0.005000, Loss: 3.5479, top1: 17.1875\n",
      "Epoch [16/60], Iter [40/633], LR: 0.005000, Loss: 3.6199, top1: 10.9375\n",
      "Epoch [16/60], Iter [41/633], LR: 0.005000, Loss: 3.5826, top1: 14.0625\n",
      "Epoch [16/60], Iter [42/633], LR: 0.005000, Loss: 3.5167, top1: 25.0000\n",
      "Epoch [16/60], Iter [43/633], LR: 0.005000, Loss: 3.6117, top1: 10.9375\n",
      "Epoch [16/60], Iter [44/633], LR: 0.005000, Loss: 3.6220, top1: 7.8125\n",
      "Epoch [16/60], Iter [45/633], LR: 0.005000, Loss: 3.5631, top1: 12.5000\n",
      "Epoch [16/60], Iter [46/633], LR: 0.005000, Loss: 3.5503, top1: 17.1875\n",
      "Epoch [16/60], Iter [47/633], LR: 0.005000, Loss: 3.5330, top1: 17.1875\n",
      "Epoch [16/60], Iter [48/633], LR: 0.005000, Loss: 3.5226, top1: 18.7500\n",
      "Epoch [16/60], Iter [49/633], LR: 0.005000, Loss: 3.6290, top1: 6.2500\n",
      "Epoch [16/60], Iter [50/633], LR: 0.005000, Loss: 3.5843, top1: 17.1875\n",
      "Epoch [16/60], Iter [51/633], LR: 0.005000, Loss: 3.5602, top1: 18.7500\n",
      "Epoch [16/60], Iter [52/633], LR: 0.005000, Loss: 3.5255, top1: 20.3125\n",
      "Epoch [16/60], Iter [53/633], LR: 0.005000, Loss: 3.5747, top1: 14.0625\n",
      "Epoch [16/60], Iter [54/633], LR: 0.005000, Loss: 3.5633, top1: 17.1875\n",
      "Epoch [16/60], Iter [55/633], LR: 0.005000, Loss: 3.5876, top1: 9.3750\n",
      "Epoch [16/60], Iter [56/633], LR: 0.005000, Loss: 3.5286, top1: 18.7500\n",
      "Epoch [16/60], Iter [57/633], LR: 0.005000, Loss: 3.5482, top1: 18.7500\n",
      "Epoch [16/60], Iter [58/633], LR: 0.005000, Loss: 3.5463, top1: 15.6250\n",
      "Epoch [16/60], Iter [59/633], LR: 0.005000, Loss: 3.5695, top1: 15.6250\n",
      "Epoch [16/60], Iter [60/633], LR: 0.005000, Loss: 3.5098, top1: 17.1875\n",
      "Epoch [16/60], Iter [61/633], LR: 0.005000, Loss: 3.5619, top1: 17.1875\n",
      "Epoch [16/60], Iter [62/633], LR: 0.005000, Loss: 3.5729, top1: 15.6250\n",
      "Epoch [16/60], Iter [63/633], LR: 0.005000, Loss: 3.5688, top1: 14.0625\n",
      "Epoch [16/60], Iter [64/633], LR: 0.005000, Loss: 3.5706, top1: 17.1875\n",
      "Epoch [16/60], Iter [65/633], LR: 0.005000, Loss: 3.6233, top1: 9.3750\n",
      "Epoch [16/60], Iter [66/633], LR: 0.005000, Loss: 3.5466, top1: 15.6250\n",
      "Epoch [16/60], Iter [67/633], LR: 0.005000, Loss: 3.6054, top1: 9.3750\n",
      "Epoch [16/60], Iter [68/633], LR: 0.005000, Loss: 3.5871, top1: 15.6250\n",
      "Epoch [16/60], Iter [69/633], LR: 0.005000, Loss: 3.5763, top1: 10.9375\n",
      "Epoch [16/60], Iter [70/633], LR: 0.005000, Loss: 3.6024, top1: 9.3750\n",
      "Epoch [16/60], Iter [71/633], LR: 0.005000, Loss: 3.5193, top1: 20.3125\n",
      "Epoch [16/60], Iter [72/633], LR: 0.005000, Loss: 3.5639, top1: 20.3125\n",
      "Epoch [16/60], Iter [73/633], LR: 0.005000, Loss: 3.6123, top1: 4.6875\n",
      "Epoch [16/60], Iter [74/633], LR: 0.005000, Loss: 3.5423, top1: 15.6250\n",
      "Epoch [16/60], Iter [75/633], LR: 0.005000, Loss: 3.4972, top1: 29.6875\n",
      "Epoch [16/60], Iter [76/633], LR: 0.005000, Loss: 3.5457, top1: 18.7500\n",
      "Epoch [16/60], Iter [77/633], LR: 0.005000, Loss: 3.5330, top1: 18.7500\n",
      "Epoch [16/60], Iter [78/633], LR: 0.005000, Loss: 3.5456, top1: 17.1875\n",
      "Epoch [16/60], Iter [79/633], LR: 0.005000, Loss: 3.6091, top1: 14.0625\n",
      "Epoch [16/60], Iter [80/633], LR: 0.005000, Loss: 3.5309, top1: 20.3125\n",
      "Epoch [16/60], Iter [81/633], LR: 0.005000, Loss: 3.5618, top1: 17.1875\n",
      "Epoch [16/60], Iter [82/633], LR: 0.005000, Loss: 3.5942, top1: 12.5000\n",
      "Epoch [16/60], Iter [83/633], LR: 0.005000, Loss: 3.6194, top1: 7.8125\n",
      "Epoch [16/60], Iter [84/633], LR: 0.005000, Loss: 3.6437, top1: 6.2500\n",
      "Epoch [16/60], Iter [85/633], LR: 0.005000, Loss: 3.5643, top1: 17.1875\n",
      "Epoch [16/60], Iter [86/633], LR: 0.005000, Loss: 3.5612, top1: 20.3125\n",
      "Epoch [16/60], Iter [87/633], LR: 0.005000, Loss: 3.5897, top1: 15.6250\n",
      "Epoch [16/60], Iter [88/633], LR: 0.005000, Loss: 3.5549, top1: 12.5000\n",
      "Epoch [16/60], Iter [89/633], LR: 0.005000, Loss: 3.5852, top1: 9.3750\n",
      "Epoch [16/60], Iter [90/633], LR: 0.005000, Loss: 3.5641, top1: 12.5000\n",
      "Epoch [16/60], Iter [91/633], LR: 0.005000, Loss: 3.5672, top1: 15.6250\n",
      "Epoch [16/60], Iter [92/633], LR: 0.005000, Loss: 3.5431, top1: 18.7500\n",
      "Epoch [16/60], Iter [93/633], LR: 0.005000, Loss: 3.5930, top1: 17.1875\n",
      "Epoch [16/60], Iter [94/633], LR: 0.005000, Loss: 3.5961, top1: 12.5000\n",
      "Epoch [16/60], Iter [95/633], LR: 0.005000, Loss: 3.5892, top1: 14.0625\n",
      "Epoch [16/60], Iter [96/633], LR: 0.005000, Loss: 3.5814, top1: 12.5000\n",
      "Epoch [16/60], Iter [97/633], LR: 0.005000, Loss: 3.5307, top1: 15.6250\n",
      "Epoch [16/60], Iter [98/633], LR: 0.005000, Loss: 3.5743, top1: 15.6250\n",
      "Epoch [16/60], Iter [99/633], LR: 0.005000, Loss: 3.5762, top1: 12.5000\n",
      "Epoch [16/60], Iter [100/633], LR: 0.005000, Loss: 3.5795, top1: 14.0625\n",
      "Epoch [16/60], Iter [101/633], LR: 0.005000, Loss: 3.5761, top1: 10.9375\n",
      "Epoch [16/60], Iter [102/633], LR: 0.005000, Loss: 3.5562, top1: 17.1875\n",
      "Epoch [16/60], Iter [103/633], LR: 0.005000, Loss: 3.6012, top1: 12.5000\n",
      "Epoch [16/60], Iter [104/633], LR: 0.005000, Loss: 3.5896, top1: 15.6250\n",
      "Epoch [16/60], Iter [105/633], LR: 0.005000, Loss: 3.5495, top1: 17.1875\n",
      "Epoch [16/60], Iter [106/633], LR: 0.005000, Loss: 3.5710, top1: 15.6250\n",
      "Epoch [16/60], Iter [107/633], LR: 0.005000, Loss: 3.6088, top1: 12.5000\n",
      "Epoch [16/60], Iter [108/633], LR: 0.005000, Loss: 3.5945, top1: 15.6250\n",
      "Epoch [16/60], Iter [109/633], LR: 0.005000, Loss: 3.5390, top1: 26.5625\n",
      "Epoch [16/60], Iter [110/633], LR: 0.005000, Loss: 3.5372, top1: 21.8750\n",
      "Epoch [16/60], Iter [111/633], LR: 0.005000, Loss: 3.5838, top1: 14.0625\n",
      "Epoch [16/60], Iter [112/633], LR: 0.005000, Loss: 3.5424, top1: 20.3125\n",
      "Epoch [16/60], Iter [113/633], LR: 0.005000, Loss: 3.5022, top1: 25.0000\n",
      "Epoch [16/60], Iter [114/633], LR: 0.005000, Loss: 3.5401, top1: 15.6250\n",
      "Epoch [16/60], Iter [115/633], LR: 0.005000, Loss: 3.5486, top1: 21.8750\n",
      "Epoch [16/60], Iter [116/633], LR: 0.005000, Loss: 3.5016, top1: 26.5625\n",
      "Epoch [16/60], Iter [117/633], LR: 0.005000, Loss: 3.5966, top1: 12.5000\n",
      "Epoch [16/60], Iter [118/633], LR: 0.005000, Loss: 3.5772, top1: 10.9375\n",
      "Epoch [16/60], Iter [119/633], LR: 0.005000, Loss: 3.5531, top1: 12.5000\n",
      "Epoch [16/60], Iter [120/633], LR: 0.005000, Loss: 3.5467, top1: 18.7500\n",
      "Epoch [16/60], Iter [121/633], LR: 0.005000, Loss: 3.6014, top1: 15.6250\n",
      "Epoch [16/60], Iter [122/633], LR: 0.005000, Loss: 3.5650, top1: 15.6250\n",
      "Epoch [16/60], Iter [123/633], LR: 0.005000, Loss: 3.6055, top1: 7.8125\n",
      "Epoch [16/60], Iter [124/633], LR: 0.005000, Loss: 3.5937, top1: 9.3750\n",
      "Epoch [16/60], Iter [125/633], LR: 0.005000, Loss: 3.5851, top1: 12.5000\n",
      "Epoch [16/60], Iter [126/633], LR: 0.005000, Loss: 3.6477, top1: 9.3750\n",
      "Epoch [16/60], Iter [127/633], LR: 0.005000, Loss: 3.6020, top1: 12.5000\n",
      "Epoch [16/60], Iter [128/633], LR: 0.005000, Loss: 3.6151, top1: 12.5000\n",
      "Epoch [16/60], Iter [129/633], LR: 0.005000, Loss: 3.5871, top1: 15.6250\n",
      "Epoch [16/60], Iter [130/633], LR: 0.005000, Loss: 3.6012, top1: 10.9375\n",
      "Epoch [16/60], Iter [131/633], LR: 0.005000, Loss: 3.5511, top1: 18.7500\n",
      "Epoch [16/60], Iter [132/633], LR: 0.005000, Loss: 3.5912, top1: 12.5000\n",
      "Epoch [16/60], Iter [133/633], LR: 0.005000, Loss: 3.4822, top1: 21.8750\n",
      "Epoch [16/60], Iter [134/633], LR: 0.005000, Loss: 3.5737, top1: 15.6250\n",
      "Epoch [16/60], Iter [135/633], LR: 0.005000, Loss: 3.5587, top1: 20.3125\n",
      "Epoch [16/60], Iter [136/633], LR: 0.005000, Loss: 3.5333, top1: 18.7500\n",
      "Epoch [16/60], Iter [137/633], LR: 0.005000, Loss: 3.5558, top1: 17.1875\n",
      "Epoch [16/60], Iter [138/633], LR: 0.005000, Loss: 3.5961, top1: 15.6250\n",
      "Epoch [16/60], Iter [139/633], LR: 0.005000, Loss: 3.5761, top1: 14.0625\n",
      "Epoch [16/60], Iter [140/633], LR: 0.005000, Loss: 3.5870, top1: 15.6250\n",
      "Epoch [16/60], Iter [141/633], LR: 0.005000, Loss: 3.6101, top1: 12.5000\n",
      "Epoch [16/60], Iter [142/633], LR: 0.005000, Loss: 3.5697, top1: 17.1875\n",
      "Epoch [16/60], Iter [143/633], LR: 0.005000, Loss: 3.5411, top1: 15.6250\n",
      "Epoch [16/60], Iter [144/633], LR: 0.005000, Loss: 3.5306, top1: 15.6250\n",
      "Epoch [16/60], Iter [145/633], LR: 0.005000, Loss: 3.5253, top1: 15.6250\n",
      "Epoch [16/60], Iter [146/633], LR: 0.005000, Loss: 3.5708, top1: 10.9375\n",
      "Epoch [16/60], Iter [147/633], LR: 0.005000, Loss: 3.5485, top1: 12.5000\n",
      "Epoch [16/60], Iter [148/633], LR: 0.005000, Loss: 3.5489, top1: 12.5000\n",
      "Epoch [16/60], Iter [149/633], LR: 0.005000, Loss: 3.5678, top1: 14.0625\n",
      "Epoch [16/60], Iter [150/633], LR: 0.005000, Loss: 3.5494, top1: 17.1875\n",
      "Epoch [16/60], Iter [151/633], LR: 0.005000, Loss: 3.5828, top1: 17.1875\n",
      "Epoch [16/60], Iter [152/633], LR: 0.005000, Loss: 3.5276, top1: 15.6250\n",
      "Epoch [16/60], Iter [153/633], LR: 0.005000, Loss: 3.5725, top1: 12.5000\n",
      "Epoch [16/60], Iter [154/633], LR: 0.005000, Loss: 3.5793, top1: 9.3750\n",
      "Epoch [16/60], Iter [155/633], LR: 0.005000, Loss: 3.4762, top1: 25.0000\n",
      "Epoch [16/60], Iter [156/633], LR: 0.005000, Loss: 3.5735, top1: 9.3750\n",
      "Epoch [16/60], Iter [157/633], LR: 0.005000, Loss: 3.5822, top1: 14.0625\n",
      "Epoch [16/60], Iter [158/633], LR: 0.005000, Loss: 3.5794, top1: 12.5000\n",
      "Epoch [16/60], Iter [159/633], LR: 0.005000, Loss: 3.5656, top1: 15.6250\n",
      "Epoch [16/60], Iter [160/633], LR: 0.005000, Loss: 3.5453, top1: 17.1875\n",
      "Epoch [16/60], Iter [161/633], LR: 0.005000, Loss: 3.5751, top1: 18.7500\n",
      "Epoch [16/60], Iter [162/633], LR: 0.005000, Loss: 3.6378, top1: 9.3750\n",
      "Epoch [16/60], Iter [163/633], LR: 0.005000, Loss: 3.5138, top1: 18.7500\n",
      "Epoch [16/60], Iter [164/633], LR: 0.005000, Loss: 3.6083, top1: 12.5000\n",
      "Epoch [16/60], Iter [165/633], LR: 0.005000, Loss: 3.5591, top1: 17.1875\n",
      "Epoch [16/60], Iter [166/633], LR: 0.005000, Loss: 3.5724, top1: 17.1875\n",
      "Epoch [16/60], Iter [167/633], LR: 0.005000, Loss: 3.5794, top1: 14.0625\n",
      "Epoch [16/60], Iter [168/633], LR: 0.005000, Loss: 3.6194, top1: 14.0625\n",
      "Epoch [16/60], Iter [169/633], LR: 0.005000, Loss: 3.5516, top1: 15.6250\n",
      "Epoch [16/60], Iter [170/633], LR: 0.005000, Loss: 3.5664, top1: 15.6250\n",
      "Epoch [16/60], Iter [171/633], LR: 0.005000, Loss: 3.5233, top1: 18.7500\n",
      "Epoch [16/60], Iter [172/633], LR: 0.005000, Loss: 3.5556, top1: 17.1875\n",
      "Epoch [16/60], Iter [173/633], LR: 0.005000, Loss: 3.5639, top1: 15.6250\n",
      "Epoch [16/60], Iter [174/633], LR: 0.005000, Loss: 3.5345, top1: 17.1875\n",
      "Epoch [16/60], Iter [175/633], LR: 0.005000, Loss: 3.5390, top1: 17.1875\n",
      "Epoch [16/60], Iter [176/633], LR: 0.005000, Loss: 3.5377, top1: 15.6250\n",
      "Epoch [16/60], Iter [177/633], LR: 0.005000, Loss: 3.6107, top1: 7.8125\n",
      "Epoch [16/60], Iter [178/633], LR: 0.005000, Loss: 3.6051, top1: 7.8125\n",
      "Epoch [16/60], Iter [179/633], LR: 0.005000, Loss: 3.5639, top1: 14.0625\n",
      "Epoch [16/60], Iter [180/633], LR: 0.005000, Loss: 3.5291, top1: 21.8750\n",
      "Epoch [16/60], Iter [181/633], LR: 0.005000, Loss: 3.5835, top1: 17.1875\n",
      "Epoch [16/60], Iter [182/633], LR: 0.005000, Loss: 3.5951, top1: 10.9375\n",
      "Epoch [16/60], Iter [183/633], LR: 0.005000, Loss: 3.5732, top1: 14.0625\n",
      "Epoch [16/60], Iter [184/633], LR: 0.005000, Loss: 3.5564, top1: 20.3125\n",
      "Epoch [16/60], Iter [185/633], LR: 0.005000, Loss: 3.5741, top1: 12.5000\n",
      "Epoch [16/60], Iter [186/633], LR: 0.005000, Loss: 3.5756, top1: 15.6250\n",
      "Epoch [16/60], Iter [187/633], LR: 0.005000, Loss: 3.5643, top1: 10.9375\n",
      "Epoch [16/60], Iter [188/633], LR: 0.005000, Loss: 3.5916, top1: 10.9375\n",
      "Epoch [16/60], Iter [189/633], LR: 0.005000, Loss: 3.5434, top1: 18.7500\n",
      "Epoch [16/60], Iter [190/633], LR: 0.005000, Loss: 3.5367, top1: 21.8750\n",
      "Epoch [16/60], Iter [191/633], LR: 0.005000, Loss: 3.6109, top1: 10.9375\n",
      "Epoch [16/60], Iter [192/633], LR: 0.005000, Loss: 3.5626, top1: 17.1875\n",
      "Epoch [16/60], Iter [193/633], LR: 0.005000, Loss: 3.6049, top1: 14.0625\n",
      "Epoch [16/60], Iter [194/633], LR: 0.005000, Loss: 3.6102, top1: 6.2500\n",
      "Epoch [16/60], Iter [195/633], LR: 0.005000, Loss: 3.5163, top1: 20.3125\n",
      "Epoch [16/60], Iter [196/633], LR: 0.005000, Loss: 3.5309, top1: 14.0625\n",
      "Epoch [16/60], Iter [197/633], LR: 0.005000, Loss: 3.5561, top1: 18.7500\n",
      "Epoch [16/60], Iter [198/633], LR: 0.005000, Loss: 3.5818, top1: 10.9375\n",
      "Epoch [16/60], Iter [199/633], LR: 0.005000, Loss: 3.5983, top1: 7.8125\n",
      "Epoch [16/60], Iter [200/633], LR: 0.005000, Loss: 3.5629, top1: 21.8750\n",
      "Epoch [16/60], Iter [201/633], LR: 0.005000, Loss: 3.5942, top1: 15.6250\n",
      "Epoch [16/60], Iter [202/633], LR: 0.005000, Loss: 3.5942, top1: 7.8125\n",
      "Epoch [16/60], Iter [203/633], LR: 0.005000, Loss: 3.5443, top1: 21.8750\n",
      "Epoch [16/60], Iter [204/633], LR: 0.005000, Loss: 3.5139, top1: 25.0000\n",
      "Epoch [16/60], Iter [205/633], LR: 0.005000, Loss: 3.5295, top1: 14.0625\n",
      "Epoch [16/60], Iter [206/633], LR: 0.005000, Loss: 3.5584, top1: 20.3125\n",
      "Epoch [16/60], Iter [207/633], LR: 0.005000, Loss: 3.6159, top1: 9.3750\n",
      "Epoch [16/60], Iter [208/633], LR: 0.005000, Loss: 3.5347, top1: 18.7500\n",
      "Epoch [16/60], Iter [209/633], LR: 0.005000, Loss: 3.6057, top1: 12.5000\n",
      "Epoch [16/60], Iter [210/633], LR: 0.005000, Loss: 3.5540, top1: 17.1875\n",
      "Epoch [16/60], Iter [211/633], LR: 0.005000, Loss: 3.5657, top1: 12.5000\n",
      "Epoch [16/60], Iter [212/633], LR: 0.005000, Loss: 3.5458, top1: 14.0625\n",
      "Epoch [16/60], Iter [213/633], LR: 0.005000, Loss: 3.5280, top1: 20.3125\n",
      "Epoch [16/60], Iter [214/633], LR: 0.005000, Loss: 3.5858, top1: 15.6250\n",
      "Epoch [16/60], Iter [215/633], LR: 0.005000, Loss: 3.5775, top1: 7.8125\n",
      "Epoch [16/60], Iter [216/633], LR: 0.005000, Loss: 3.5934, top1: 9.3750\n",
      "Epoch [16/60], Iter [217/633], LR: 0.005000, Loss: 3.5498, top1: 18.7500\n",
      "Epoch [16/60], Iter [218/633], LR: 0.005000, Loss: 3.5468, top1: 20.3125\n",
      "Epoch [16/60], Iter [219/633], LR: 0.005000, Loss: 3.6170, top1: 4.6875\n",
      "Epoch [16/60], Iter [220/633], LR: 0.005000, Loss: 3.5852, top1: 12.5000\n",
      "Epoch [16/60], Iter [221/633], LR: 0.005000, Loss: 3.5793, top1: 10.9375\n",
      "Epoch [16/60], Iter [222/633], LR: 0.005000, Loss: 3.6006, top1: 17.1875\n",
      "Epoch [16/60], Iter [223/633], LR: 0.005000, Loss: 3.5653, top1: 18.7500\n",
      "Epoch [16/60], Iter [224/633], LR: 0.005000, Loss: 3.5082, top1: 17.1875\n",
      "Epoch [16/60], Iter [225/633], LR: 0.005000, Loss: 3.5544, top1: 17.1875\n",
      "Epoch [16/60], Iter [226/633], LR: 0.005000, Loss: 3.6271, top1: 9.3750\n",
      "Epoch [16/60], Iter [227/633], LR: 0.005000, Loss: 3.5364, top1: 21.8750\n",
      "Epoch [16/60], Iter [228/633], LR: 0.005000, Loss: 3.5816, top1: 15.6250\n",
      "Epoch [16/60], Iter [229/633], LR: 0.005000, Loss: 3.5872, top1: 15.6250\n",
      "Epoch [16/60], Iter [230/633], LR: 0.005000, Loss: 3.6100, top1: 14.0625\n",
      "Epoch [16/60], Iter [231/633], LR: 0.005000, Loss: 3.5418, top1: 18.7500\n",
      "Epoch [16/60], Iter [232/633], LR: 0.005000, Loss: 3.5476, top1: 17.1875\n",
      "Epoch [16/60], Iter [233/633], LR: 0.005000, Loss: 3.5480, top1: 17.1875\n",
      "Epoch [16/60], Iter [234/633], LR: 0.005000, Loss: 3.5462, top1: 9.3750\n",
      "Epoch [16/60], Iter [235/633], LR: 0.005000, Loss: 3.5292, top1: 20.3125\n",
      "Epoch [16/60], Iter [236/633], LR: 0.005000, Loss: 3.5538, top1: 17.1875\n",
      "Epoch [16/60], Iter [237/633], LR: 0.005000, Loss: 3.5936, top1: 15.6250\n",
      "Epoch [16/60], Iter [238/633], LR: 0.005000, Loss: 3.5830, top1: 14.0625\n",
      "Epoch [16/60], Iter [239/633], LR: 0.005000, Loss: 3.5426, top1: 15.6250\n",
      "Epoch [16/60], Iter [240/633], LR: 0.005000, Loss: 3.5302, top1: 17.1875\n",
      "Epoch [16/60], Iter [241/633], LR: 0.005000, Loss: 3.5683, top1: 17.1875\n",
      "Epoch [16/60], Iter [242/633], LR: 0.005000, Loss: 3.6157, top1: 7.8125\n",
      "Epoch [16/60], Iter [243/633], LR: 0.005000, Loss: 3.4936, top1: 25.0000\n",
      "Epoch [16/60], Iter [244/633], LR: 0.005000, Loss: 3.5419, top1: 18.7500\n",
      "Epoch [16/60], Iter [245/633], LR: 0.005000, Loss: 3.5136, top1: 21.8750\n",
      "Epoch [16/60], Iter [246/633], LR: 0.005000, Loss: 3.5668, top1: 14.0625\n",
      "Epoch [16/60], Iter [247/633], LR: 0.005000, Loss: 3.5623, top1: 21.8750\n",
      "Epoch [16/60], Iter [248/633], LR: 0.005000, Loss: 3.6373, top1: 9.3750\n",
      "Epoch [16/60], Iter [249/633], LR: 0.005000, Loss: 3.6141, top1: 10.9375\n",
      "Epoch [16/60], Iter [250/633], LR: 0.005000, Loss: 3.5683, top1: 12.5000\n",
      "Epoch [16/60], Iter [251/633], LR: 0.005000, Loss: 3.5804, top1: 12.5000\n",
      "Epoch [16/60], Iter [252/633], LR: 0.005000, Loss: 3.5119, top1: 21.8750\n",
      "Epoch [16/60], Iter [253/633], LR: 0.005000, Loss: 3.6214, top1: 6.2500\n",
      "Epoch [16/60], Iter [254/633], LR: 0.005000, Loss: 3.5338, top1: 17.1875\n",
      "Epoch [16/60], Iter [255/633], LR: 0.005000, Loss: 3.5814, top1: 14.0625\n",
      "Epoch [16/60], Iter [256/633], LR: 0.005000, Loss: 3.5417, top1: 15.6250\n",
      "Epoch [16/60], Iter [257/633], LR: 0.005000, Loss: 3.5883, top1: 14.0625\n",
      "Epoch [16/60], Iter [258/633], LR: 0.005000, Loss: 3.5595, top1: 18.7500\n",
      "Epoch [16/60], Iter [259/633], LR: 0.005000, Loss: 3.5520, top1: 14.0625\n",
      "Epoch [16/60], Iter [260/633], LR: 0.005000, Loss: 3.6016, top1: 14.0625\n",
      "Epoch [16/60], Iter [261/633], LR: 0.005000, Loss: 3.5346, top1: 17.1875\n",
      "Epoch [16/60], Iter [262/633], LR: 0.005000, Loss: 3.5442, top1: 18.7500\n",
      "Epoch [16/60], Iter [263/633], LR: 0.005000, Loss: 3.5958, top1: 14.0625\n",
      "Epoch [16/60], Iter [264/633], LR: 0.005000, Loss: 3.4960, top1: 28.1250\n",
      "Epoch [16/60], Iter [265/633], LR: 0.005000, Loss: 3.5722, top1: 14.0625\n",
      "Epoch [16/60], Iter [266/633], LR: 0.005000, Loss: 3.4962, top1: 20.3125\n",
      "Epoch [16/60], Iter [267/633], LR: 0.005000, Loss: 3.6350, top1: 4.6875\n",
      "Epoch [16/60], Iter [268/633], LR: 0.005000, Loss: 3.6006, top1: 9.3750\n",
      "Epoch [16/60], Iter [269/633], LR: 0.005000, Loss: 3.6021, top1: 10.9375\n",
      "Epoch [16/60], Iter [270/633], LR: 0.005000, Loss: 3.5401, top1: 17.1875\n",
      "Epoch [16/60], Iter [271/633], LR: 0.005000, Loss: 3.5461, top1: 18.7500\n",
      "Epoch [16/60], Iter [272/633], LR: 0.005000, Loss: 3.5984, top1: 15.6250\n",
      "Epoch [16/60], Iter [273/633], LR: 0.005000, Loss: 3.5480, top1: 18.7500\n",
      "Epoch [16/60], Iter [274/633], LR: 0.005000, Loss: 3.5784, top1: 12.5000\n",
      "Epoch [16/60], Iter [275/633], LR: 0.005000, Loss: 3.5251, top1: 18.7500\n",
      "Epoch [16/60], Iter [276/633], LR: 0.005000, Loss: 3.5021, top1: 23.4375\n",
      "Epoch [16/60], Iter [277/633], LR: 0.005000, Loss: 3.5625, top1: 15.6250\n",
      "Epoch [16/60], Iter [278/633], LR: 0.005000, Loss: 3.5172, top1: 25.0000\n",
      "Epoch [16/60], Iter [279/633], LR: 0.005000, Loss: 3.5690, top1: 9.3750\n",
      "Epoch [16/60], Iter [280/633], LR: 0.005000, Loss: 3.5720, top1: 15.6250\n",
      "Epoch [16/60], Iter [281/633], LR: 0.005000, Loss: 3.6226, top1: 10.9375\n",
      "Epoch [16/60], Iter [282/633], LR: 0.005000, Loss: 3.5564, top1: 17.1875\n",
      "Epoch [16/60], Iter [283/633], LR: 0.005000, Loss: 3.5807, top1: 15.6250\n",
      "Epoch [16/60], Iter [284/633], LR: 0.005000, Loss: 3.5707, top1: 15.6250\n",
      "Epoch [16/60], Iter [285/633], LR: 0.005000, Loss: 3.5571, top1: 15.6250\n",
      "Epoch [16/60], Iter [286/633], LR: 0.005000, Loss: 3.5818, top1: 14.0625\n",
      "Epoch [16/60], Iter [287/633], LR: 0.005000, Loss: 3.5564, top1: 20.3125\n",
      "Epoch [16/60], Iter [288/633], LR: 0.005000, Loss: 3.5447, top1: 18.7500\n",
      "Epoch [16/60], Iter [289/633], LR: 0.005000, Loss: 3.5708, top1: 17.1875\n",
      "Epoch [16/60], Iter [290/633], LR: 0.005000, Loss: 3.5894, top1: 7.8125\n",
      "Epoch [16/60], Iter [291/633], LR: 0.005000, Loss: 3.5564, top1: 15.6250\n",
      "Epoch [16/60], Iter [292/633], LR: 0.005000, Loss: 3.5988, top1: 14.0625\n",
      "Epoch [16/60], Iter [293/633], LR: 0.005000, Loss: 3.5729, top1: 12.5000\n",
      "Epoch [16/60], Iter [294/633], LR: 0.005000, Loss: 3.4999, top1: 25.0000\n",
      "Epoch [16/60], Iter [295/633], LR: 0.005000, Loss: 3.5301, top1: 12.5000\n",
      "Epoch [16/60], Iter [296/633], LR: 0.005000, Loss: 3.5492, top1: 14.0625\n",
      "Epoch [16/60], Iter [297/633], LR: 0.005000, Loss: 3.6097, top1: 9.3750\n",
      "Epoch [16/60], Iter [298/633], LR: 0.005000, Loss: 3.5361, top1: 18.7500\n",
      "Epoch [16/60], Iter [299/633], LR: 0.005000, Loss: 3.5535, top1: 17.1875\n",
      "Epoch [16/60], Iter [300/633], LR: 0.005000, Loss: 3.5690, top1: 17.1875\n",
      "Epoch [16/60], Iter [301/633], LR: 0.005000, Loss: 3.5760, top1: 14.0625\n",
      "Epoch [16/60], Iter [302/633], LR: 0.005000, Loss: 3.5939, top1: 14.0625\n",
      "Epoch [16/60], Iter [303/633], LR: 0.005000, Loss: 3.5583, top1: 12.5000\n",
      "Epoch [16/60], Iter [304/633], LR: 0.005000, Loss: 3.5480, top1: 15.6250\n",
      "Epoch [16/60], Iter [305/633], LR: 0.005000, Loss: 3.6086, top1: 9.3750\n",
      "Epoch [16/60], Iter [306/633], LR: 0.005000, Loss: 3.5190, top1: 15.6250\n",
      "Epoch [16/60], Iter [307/633], LR: 0.005000, Loss: 3.5577, top1: 15.6250\n",
      "Epoch [16/60], Iter [308/633], LR: 0.005000, Loss: 3.5875, top1: 12.5000\n",
      "Epoch [16/60], Iter [309/633], LR: 0.005000, Loss: 3.5375, top1: 17.1875\n",
      "Epoch [16/60], Iter [310/633], LR: 0.005000, Loss: 3.5674, top1: 14.0625\n",
      "Epoch [16/60], Iter [311/633], LR: 0.005000, Loss: 3.6076, top1: 12.5000\n",
      "Epoch [16/60], Iter [312/633], LR: 0.005000, Loss: 3.5878, top1: 14.0625\n",
      "Epoch [16/60], Iter [313/633], LR: 0.005000, Loss: 3.6022, top1: 18.7500\n",
      "Epoch [16/60], Iter [314/633], LR: 0.005000, Loss: 3.6231, top1: 10.9375\n",
      "Epoch [16/60], Iter [315/633], LR: 0.005000, Loss: 3.5696, top1: 15.6250\n",
      "Epoch [16/60], Iter [316/633], LR: 0.005000, Loss: 3.5342, top1: 15.6250\n",
      "Epoch [16/60], Iter [317/633], LR: 0.005000, Loss: 3.6066, top1: 15.6250\n",
      "Epoch [16/60], Iter [318/633], LR: 0.005000, Loss: 3.5869, top1: 15.6250\n",
      "Epoch [16/60], Iter [319/633], LR: 0.005000, Loss: 3.5913, top1: 12.5000\n",
      "Epoch [16/60], Iter [320/633], LR: 0.005000, Loss: 3.5879, top1: 18.7500\n",
      "Epoch [16/60], Iter [321/633], LR: 0.005000, Loss: 3.4778, top1: 23.4375\n",
      "Epoch [16/60], Iter [322/633], LR: 0.005000, Loss: 3.6164, top1: 9.3750\n",
      "Epoch [16/60], Iter [323/633], LR: 0.005000, Loss: 3.5710, top1: 17.1875\n",
      "Epoch [16/60], Iter [324/633], LR: 0.005000, Loss: 3.5284, top1: 14.0625\n",
      "Epoch [16/60], Iter [325/633], LR: 0.005000, Loss: 3.6286, top1: 6.2500\n",
      "Epoch [16/60], Iter [326/633], LR: 0.005000, Loss: 3.5675, top1: 12.5000\n",
      "Epoch [16/60], Iter [327/633], LR: 0.005000, Loss: 3.5760, top1: 15.6250\n",
      "Epoch [16/60], Iter [328/633], LR: 0.005000, Loss: 3.5738, top1: 14.0625\n",
      "Epoch [16/60], Iter [329/633], LR: 0.005000, Loss: 3.5631, top1: 14.0625\n",
      "Epoch [16/60], Iter [330/633], LR: 0.005000, Loss: 3.5547, top1: 17.1875\n",
      "Epoch [16/60], Iter [331/633], LR: 0.005000, Loss: 3.5814, top1: 15.6250\n",
      "Epoch [16/60], Iter [332/633], LR: 0.005000, Loss: 3.5685, top1: 15.6250\n",
      "Epoch [16/60], Iter [333/633], LR: 0.005000, Loss: 3.5728, top1: 15.6250\n",
      "Epoch [16/60], Iter [334/633], LR: 0.005000, Loss: 3.5969, top1: 18.7500\n",
      "Epoch [16/60], Iter [335/633], LR: 0.005000, Loss: 3.5859, top1: 14.0625\n",
      "Epoch [16/60], Iter [336/633], LR: 0.005000, Loss: 3.5387, top1: 17.1875\n",
      "Epoch [16/60], Iter [337/633], LR: 0.005000, Loss: 3.5426, top1: 17.1875\n",
      "Epoch [16/60], Iter [338/633], LR: 0.005000, Loss: 3.5240, top1: 21.8750\n",
      "Epoch [16/60], Iter [339/633], LR: 0.005000, Loss: 3.6074, top1: 17.1875\n",
      "Epoch [16/60], Iter [340/633], LR: 0.005000, Loss: 3.5172, top1: 18.7500\n",
      "Epoch [16/60], Iter [341/633], LR: 0.005000, Loss: 3.5759, top1: 9.3750\n",
      "Epoch [16/60], Iter [342/633], LR: 0.005000, Loss: 3.5854, top1: 14.0625\n",
      "Epoch [16/60], Iter [343/633], LR: 0.005000, Loss: 3.5800, top1: 17.1875\n",
      "Epoch [16/60], Iter [344/633], LR: 0.005000, Loss: 3.5816, top1: 12.5000\n",
      "Epoch [16/60], Iter [345/633], LR: 0.005000, Loss: 3.5897, top1: 14.0625\n",
      "Epoch [16/60], Iter [346/633], LR: 0.005000, Loss: 3.5868, top1: 9.3750\n",
      "Epoch [16/60], Iter [347/633], LR: 0.005000, Loss: 3.5879, top1: 12.5000\n",
      "Epoch [16/60], Iter [348/633], LR: 0.005000, Loss: 3.4855, top1: 25.0000\n",
      "Epoch [16/60], Iter [349/633], LR: 0.005000, Loss: 3.6014, top1: 12.5000\n",
      "Epoch [16/60], Iter [350/633], LR: 0.005000, Loss: 3.5713, top1: 10.9375\n",
      "Epoch [16/60], Iter [351/633], LR: 0.005000, Loss: 3.5798, top1: 15.6250\n",
      "Epoch [16/60], Iter [352/633], LR: 0.005000, Loss: 3.5755, top1: 14.0625\n",
      "Epoch [16/60], Iter [353/633], LR: 0.005000, Loss: 3.5747, top1: 17.1875\n",
      "Epoch [16/60], Iter [354/633], LR: 0.005000, Loss: 3.5770, top1: 12.5000\n",
      "Epoch [16/60], Iter [355/633], LR: 0.005000, Loss: 3.5731, top1: 7.8125\n",
      "Epoch [16/60], Iter [356/633], LR: 0.005000, Loss: 3.5147, top1: 25.0000\n",
      "Epoch [16/60], Iter [357/633], LR: 0.005000, Loss: 3.5829, top1: 15.6250\n",
      "Epoch [16/60], Iter [358/633], LR: 0.005000, Loss: 3.5781, top1: 12.5000\n",
      "Epoch [16/60], Iter [359/633], LR: 0.005000, Loss: 3.6095, top1: 10.9375\n",
      "Epoch [16/60], Iter [360/633], LR: 0.005000, Loss: 3.5596, top1: 18.7500\n",
      "Epoch [16/60], Iter [361/633], LR: 0.005000, Loss: 3.5871, top1: 6.2500\n",
      "Epoch [16/60], Iter [362/633], LR: 0.005000, Loss: 3.5738, top1: 18.7500\n",
      "Epoch [16/60], Iter [363/633], LR: 0.005000, Loss: 3.5545, top1: 17.1875\n",
      "Epoch [16/60], Iter [364/633], LR: 0.005000, Loss: 3.6015, top1: 6.2500\n",
      "Epoch [16/60], Iter [365/633], LR: 0.005000, Loss: 3.6011, top1: 10.9375\n",
      "Epoch [16/60], Iter [366/633], LR: 0.005000, Loss: 3.5708, top1: 17.1875\n",
      "Epoch [16/60], Iter [367/633], LR: 0.005000, Loss: 3.5972, top1: 12.5000\n",
      "Epoch [16/60], Iter [368/633], LR: 0.005000, Loss: 3.5347, top1: 18.7500\n",
      "Epoch [16/60], Iter [369/633], LR: 0.005000, Loss: 3.5709, top1: 17.1875\n",
      "Epoch [16/60], Iter [370/633], LR: 0.005000, Loss: 3.5762, top1: 12.5000\n",
      "Epoch [16/60], Iter [371/633], LR: 0.005000, Loss: 3.5807, top1: 15.6250\n",
      "Epoch [16/60], Iter [372/633], LR: 0.005000, Loss: 3.5335, top1: 23.4375\n",
      "Epoch [16/60], Iter [373/633], LR: 0.005000, Loss: 3.5829, top1: 10.9375\n",
      "Epoch [16/60], Iter [374/633], LR: 0.005000, Loss: 3.6099, top1: 14.0625\n",
      "Epoch [16/60], Iter [375/633], LR: 0.005000, Loss: 3.5470, top1: 12.5000\n",
      "Epoch [16/60], Iter [376/633], LR: 0.005000, Loss: 3.5939, top1: 18.7500\n",
      "Epoch [16/60], Iter [377/633], LR: 0.005000, Loss: 3.5591, top1: 12.5000\n",
      "Epoch [16/60], Iter [378/633], LR: 0.005000, Loss: 3.5825, top1: 14.0625\n",
      "Epoch [16/60], Iter [379/633], LR: 0.005000, Loss: 3.5852, top1: 10.9375\n",
      "Epoch [16/60], Iter [380/633], LR: 0.005000, Loss: 3.5830, top1: 9.3750\n",
      "Epoch [16/60], Iter [381/633], LR: 0.005000, Loss: 3.5638, top1: 15.6250\n",
      "Epoch [16/60], Iter [382/633], LR: 0.005000, Loss: 3.6028, top1: 17.1875\n",
      "Epoch [16/60], Iter [383/633], LR: 0.005000, Loss: 3.5677, top1: 17.1875\n",
      "Epoch [16/60], Iter [384/633], LR: 0.005000, Loss: 3.5667, top1: 12.5000\n",
      "Epoch [16/60], Iter [385/633], LR: 0.005000, Loss: 3.5518, top1: 20.3125\n",
      "Epoch [16/60], Iter [386/633], LR: 0.005000, Loss: 3.5391, top1: 23.4375\n",
      "Epoch [16/60], Iter [387/633], LR: 0.005000, Loss: 3.5150, top1: 23.4375\n",
      "Epoch [16/60], Iter [388/633], LR: 0.005000, Loss: 3.5569, top1: 6.2500\n",
      "Epoch [16/60], Iter [389/633], LR: 0.005000, Loss: 3.5528, top1: 17.1875\n",
      "Epoch [16/60], Iter [390/633], LR: 0.005000, Loss: 3.5221, top1: 20.3125\n",
      "Epoch [16/60], Iter [391/633], LR: 0.005000, Loss: 3.6361, top1: 9.3750\n",
      "Epoch [16/60], Iter [392/633], LR: 0.005000, Loss: 3.5843, top1: 12.5000\n",
      "Epoch [16/60], Iter [393/633], LR: 0.005000, Loss: 3.5794, top1: 12.5000\n",
      "Epoch [16/60], Iter [394/633], LR: 0.005000, Loss: 3.5068, top1: 25.0000\n",
      "Epoch [16/60], Iter [395/633], LR: 0.005000, Loss: 3.5985, top1: 15.6250\n",
      "Epoch [16/60], Iter [396/633], LR: 0.005000, Loss: 3.5798, top1: 15.6250\n",
      "Epoch [16/60], Iter [397/633], LR: 0.005000, Loss: 3.5660, top1: 15.6250\n",
      "Epoch [16/60], Iter [398/633], LR: 0.005000, Loss: 3.5382, top1: 17.1875\n",
      "Epoch [16/60], Iter [399/633], LR: 0.005000, Loss: 3.5627, top1: 15.6250\n",
      "Epoch [16/60], Iter [400/633], LR: 0.005000, Loss: 3.5604, top1: 20.3125\n",
      "Epoch [16/60], Iter [401/633], LR: 0.005000, Loss: 3.5272, top1: 26.5625\n",
      "Epoch [16/60], Iter [402/633], LR: 0.005000, Loss: 3.5630, top1: 14.0625\n",
      "Epoch [16/60], Iter [403/633], LR: 0.005000, Loss: 3.5742, top1: 17.1875\n",
      "Epoch [16/60], Iter [404/633], LR: 0.005000, Loss: 3.5541, top1: 17.1875\n",
      "Epoch [16/60], Iter [405/633], LR: 0.005000, Loss: 3.5719, top1: 12.5000\n",
      "Epoch [16/60], Iter [406/633], LR: 0.005000, Loss: 3.5746, top1: 17.1875\n",
      "Epoch [16/60], Iter [407/633], LR: 0.005000, Loss: 3.5946, top1: 9.3750\n",
      "Epoch [16/60], Iter [408/633], LR: 0.005000, Loss: 3.5617, top1: 20.3125\n",
      "Epoch [16/60], Iter [409/633], LR: 0.005000, Loss: 3.5570, top1: 21.8750\n",
      "Epoch [16/60], Iter [410/633], LR: 0.005000, Loss: 3.5971, top1: 10.9375\n",
      "Epoch [16/60], Iter [411/633], LR: 0.005000, Loss: 3.5581, top1: 14.0625\n",
      "Epoch [16/60], Iter [412/633], LR: 0.005000, Loss: 3.5535, top1: 18.7500\n",
      "Epoch [16/60], Iter [413/633], LR: 0.005000, Loss: 3.6026, top1: 9.3750\n",
      "Epoch [16/60], Iter [414/633], LR: 0.005000, Loss: 3.6309, top1: 9.3750\n",
      "Epoch [16/60], Iter [415/633], LR: 0.005000, Loss: 3.6114, top1: 10.9375\n",
      "Epoch [16/60], Iter [416/633], LR: 0.005000, Loss: 3.4869, top1: 26.5625\n",
      "Epoch [16/60], Iter [417/633], LR: 0.005000, Loss: 3.6033, top1: 9.3750\n",
      "Epoch [16/60], Iter [418/633], LR: 0.005000, Loss: 3.5552, top1: 12.5000\n",
      "Epoch [16/60], Iter [419/633], LR: 0.005000, Loss: 3.6211, top1: 9.3750\n",
      "Epoch [16/60], Iter [420/633], LR: 0.005000, Loss: 3.5405, top1: 20.3125\n",
      "Epoch [16/60], Iter [421/633], LR: 0.005000, Loss: 3.5396, top1: 23.4375\n",
      "Epoch [16/60], Iter [422/633], LR: 0.005000, Loss: 3.5926, top1: 10.9375\n",
      "Epoch [16/60], Iter [423/633], LR: 0.005000, Loss: 3.5341, top1: 14.0625\n",
      "Epoch [16/60], Iter [424/633], LR: 0.005000, Loss: 3.5356, top1: 18.7500\n",
      "Epoch [16/60], Iter [425/633], LR: 0.005000, Loss: 3.5067, top1: 21.8750\n",
      "Epoch [16/60], Iter [426/633], LR: 0.005000, Loss: 3.5220, top1: 15.6250\n",
      "Epoch [16/60], Iter [427/633], LR: 0.005000, Loss: 3.5642, top1: 14.0625\n",
      "Epoch [16/60], Iter [428/633], LR: 0.005000, Loss: 3.5816, top1: 14.0625\n",
      "Epoch [16/60], Iter [429/633], LR: 0.005000, Loss: 3.5414, top1: 26.5625\n",
      "Epoch [16/60], Iter [430/633], LR: 0.005000, Loss: 3.6124, top1: 10.9375\n",
      "Epoch [16/60], Iter [431/633], LR: 0.005000, Loss: 3.5631, top1: 14.0625\n",
      "Epoch [16/60], Iter [432/633], LR: 0.005000, Loss: 3.6128, top1: 7.8125\n",
      "Epoch [16/60], Iter [433/633], LR: 0.005000, Loss: 3.5198, top1: 21.8750\n",
      "Epoch [16/60], Iter [434/633], LR: 0.005000, Loss: 3.5547, top1: 18.7500\n",
      "Epoch [16/60], Iter [435/633], LR: 0.005000, Loss: 3.5750, top1: 15.6250\n",
      "Epoch [16/60], Iter [436/633], LR: 0.005000, Loss: 3.5264, top1: 17.1875\n",
      "Epoch [16/60], Iter [437/633], LR: 0.005000, Loss: 3.5502, top1: 15.6250\n",
      "Epoch [16/60], Iter [438/633], LR: 0.005000, Loss: 3.5870, top1: 17.1875\n",
      "Epoch [16/60], Iter [439/633], LR: 0.005000, Loss: 3.5924, top1: 9.3750\n",
      "Epoch [16/60], Iter [440/633], LR: 0.005000, Loss: 3.5928, top1: 9.3750\n",
      "Epoch [16/60], Iter [441/633], LR: 0.005000, Loss: 3.5965, top1: 9.3750\n",
      "Epoch [16/60], Iter [442/633], LR: 0.005000, Loss: 3.5586, top1: 17.1875\n",
      "Epoch [16/60], Iter [443/633], LR: 0.005000, Loss: 3.5636, top1: 15.6250\n",
      "Epoch [16/60], Iter [444/633], LR: 0.005000, Loss: 3.5452, top1: 18.7500\n",
      "Epoch [16/60], Iter [445/633], LR: 0.005000, Loss: 3.5636, top1: 17.1875\n",
      "Epoch [16/60], Iter [446/633], LR: 0.005000, Loss: 3.5140, top1: 20.3125\n",
      "Epoch [16/60], Iter [447/633], LR: 0.005000, Loss: 3.5514, top1: 12.5000\n",
      "Epoch [16/60], Iter [448/633], LR: 0.005000, Loss: 3.5638, top1: 18.7500\n",
      "Epoch [16/60], Iter [449/633], LR: 0.005000, Loss: 3.5486, top1: 10.9375\n",
      "Epoch [16/60], Iter [450/633], LR: 0.005000, Loss: 3.5772, top1: 12.5000\n",
      "Epoch [16/60], Iter [451/633], LR: 0.005000, Loss: 3.5427, top1: 15.6250\n",
      "Epoch [16/60], Iter [452/633], LR: 0.005000, Loss: 3.5251, top1: 14.0625\n",
      "Epoch [16/60], Iter [453/633], LR: 0.005000, Loss: 3.5434, top1: 18.7500\n",
      "Epoch [16/60], Iter [454/633], LR: 0.005000, Loss: 3.5440, top1: 18.7500\n",
      "Epoch [16/60], Iter [455/633], LR: 0.005000, Loss: 3.5398, top1: 14.0625\n",
      "Epoch [16/60], Iter [456/633], LR: 0.005000, Loss: 3.5536, top1: 15.6250\n",
      "Epoch [16/60], Iter [457/633], LR: 0.005000, Loss: 3.5561, top1: 15.6250\n",
      "Epoch [16/60], Iter [458/633], LR: 0.005000, Loss: 3.6090, top1: 6.2500\n",
      "Epoch [16/60], Iter [459/633], LR: 0.005000, Loss: 3.5677, top1: 18.7500\n",
      "Epoch [16/60], Iter [460/633], LR: 0.005000, Loss: 3.5654, top1: 12.5000\n",
      "Epoch [16/60], Iter [461/633], LR: 0.005000, Loss: 3.5740, top1: 15.6250\n",
      "Epoch [16/60], Iter [462/633], LR: 0.005000, Loss: 3.6013, top1: 12.5000\n",
      "Epoch [16/60], Iter [463/633], LR: 0.005000, Loss: 3.5697, top1: 18.7500\n",
      "Epoch [16/60], Iter [464/633], LR: 0.005000, Loss: 3.5358, top1: 20.3125\n",
      "Epoch [16/60], Iter [465/633], LR: 0.005000, Loss: 3.5879, top1: 10.9375\n",
      "Epoch [16/60], Iter [466/633], LR: 0.005000, Loss: 3.5136, top1: 23.4375\n",
      "Epoch [16/60], Iter [467/633], LR: 0.005000, Loss: 3.6223, top1: 9.3750\n",
      "Epoch [16/60], Iter [468/633], LR: 0.005000, Loss: 3.5824, top1: 9.3750\n",
      "Epoch [16/60], Iter [469/633], LR: 0.005000, Loss: 3.5729, top1: 14.0625\n",
      "Epoch [16/60], Iter [470/633], LR: 0.005000, Loss: 3.5535, top1: 17.1875\n",
      "Epoch [16/60], Iter [471/633], LR: 0.005000, Loss: 3.5999, top1: 14.0625\n",
      "Epoch [16/60], Iter [472/633], LR: 0.005000, Loss: 3.5980, top1: 9.3750\n",
      "Epoch [16/60], Iter [473/633], LR: 0.005000, Loss: 3.5304, top1: 17.1875\n",
      "Epoch [16/60], Iter [474/633], LR: 0.005000, Loss: 3.5172, top1: 26.5625\n",
      "Epoch [16/60], Iter [475/633], LR: 0.005000, Loss: 3.6154, top1: 6.2500\n",
      "Epoch [16/60], Iter [476/633], LR: 0.005000, Loss: 3.5450, top1: 17.1875\n",
      "Epoch [16/60], Iter [477/633], LR: 0.005000, Loss: 3.5722, top1: 15.6250\n",
      "Epoch [16/60], Iter [478/633], LR: 0.005000, Loss: 3.5325, top1: 18.7500\n",
      "Epoch [16/60], Iter [479/633], LR: 0.005000, Loss: 3.5155, top1: 20.3125\n",
      "Epoch [16/60], Iter [480/633], LR: 0.005000, Loss: 3.6201, top1: 7.8125\n",
      "Epoch [16/60], Iter [481/633], LR: 0.005000, Loss: 3.5579, top1: 17.1875\n",
      "Epoch [16/60], Iter [482/633], LR: 0.005000, Loss: 3.5832, top1: 15.6250\n",
      "Epoch [16/60], Iter [483/633], LR: 0.005000, Loss: 3.5147, top1: 23.4375\n",
      "Epoch [16/60], Iter [484/633], LR: 0.005000, Loss: 3.5477, top1: 14.0625\n",
      "Epoch [16/60], Iter [485/633], LR: 0.005000, Loss: 3.5951, top1: 17.1875\n",
      "Epoch [16/60], Iter [486/633], LR: 0.005000, Loss: 3.5693, top1: 12.5000\n",
      "Epoch [16/60], Iter [487/633], LR: 0.005000, Loss: 3.6023, top1: 7.8125\n",
      "Epoch [16/60], Iter [488/633], LR: 0.005000, Loss: 3.5451, top1: 14.0625\n",
      "Epoch [16/60], Iter [489/633], LR: 0.005000, Loss: 3.5906, top1: 14.0625\n",
      "Epoch [16/60], Iter [490/633], LR: 0.005000, Loss: 3.5379, top1: 12.5000\n",
      "Epoch [16/60], Iter [491/633], LR: 0.005000, Loss: 3.5724, top1: 18.7500\n",
      "Epoch [16/60], Iter [492/633], LR: 0.005000, Loss: 3.6068, top1: 9.3750\n",
      "Epoch [16/60], Iter [493/633], LR: 0.005000, Loss: 3.5542, top1: 15.6250\n",
      "Epoch [16/60], Iter [494/633], LR: 0.005000, Loss: 3.5833, top1: 10.9375\n",
      "Epoch [16/60], Iter [495/633], LR: 0.005000, Loss: 3.5691, top1: 17.1875\n",
      "Epoch [16/60], Iter [496/633], LR: 0.005000, Loss: 3.6159, top1: 6.2500\n",
      "Epoch [16/60], Iter [497/633], LR: 0.005000, Loss: 3.5771, top1: 9.3750\n",
      "Epoch [16/60], Iter [498/633], LR: 0.005000, Loss: 3.5623, top1: 18.7500\n",
      "Epoch [16/60], Iter [499/633], LR: 0.005000, Loss: 3.5625, top1: 18.7500\n",
      "Epoch [16/60], Iter [500/633], LR: 0.005000, Loss: 3.6142, top1: 10.9375\n",
      "Epoch [16/60], Iter [501/633], LR: 0.005000, Loss: 3.5873, top1: 15.6250\n",
      "Epoch [16/60], Iter [502/633], LR: 0.005000, Loss: 3.6501, top1: 6.2500\n",
      "Epoch [16/60], Iter [503/633], LR: 0.005000, Loss: 3.5957, top1: 12.5000\n",
      "Epoch [16/60], Iter [504/633], LR: 0.005000, Loss: 3.5472, top1: 18.7500\n",
      "Epoch [16/60], Iter [505/633], LR: 0.005000, Loss: 3.5219, top1: 14.0625\n",
      "Epoch [16/60], Iter [506/633], LR: 0.005000, Loss: 3.5410, top1: 21.8750\n",
      "Epoch [16/60], Iter [507/633], LR: 0.005000, Loss: 3.5976, top1: 9.3750\n",
      "Epoch [16/60], Iter [508/633], LR: 0.005000, Loss: 3.5434, top1: 14.0625\n",
      "Epoch [16/60], Iter [509/633], LR: 0.005000, Loss: 3.5474, top1: 15.6250\n",
      "Epoch [16/60], Iter [510/633], LR: 0.005000, Loss: 3.5228, top1: 21.8750\n",
      "Epoch [16/60], Iter [511/633], LR: 0.005000, Loss: 3.5989, top1: 9.3750\n",
      "Epoch [16/60], Iter [512/633], LR: 0.005000, Loss: 3.5824, top1: 15.6250\n",
      "Epoch [16/60], Iter [513/633], LR: 0.005000, Loss: 3.5319, top1: 25.0000\n",
      "Epoch [16/60], Iter [514/633], LR: 0.005000, Loss: 3.5737, top1: 10.9375\n",
      "Epoch [16/60], Iter [515/633], LR: 0.005000, Loss: 3.5801, top1: 10.9375\n",
      "Epoch [16/60], Iter [516/633], LR: 0.005000, Loss: 3.5883, top1: 15.6250\n",
      "Epoch [16/60], Iter [517/633], LR: 0.005000, Loss: 3.5656, top1: 15.6250\n",
      "Epoch [16/60], Iter [518/633], LR: 0.005000, Loss: 3.5137, top1: 20.3125\n",
      "Epoch [16/60], Iter [519/633], LR: 0.005000, Loss: 3.5801, top1: 14.0625\n",
      "Epoch [16/60], Iter [520/633], LR: 0.005000, Loss: 3.5595, top1: 14.0625\n",
      "Epoch [16/60], Iter [521/633], LR: 0.005000, Loss: 3.5420, top1: 21.8750\n",
      "Epoch [16/60], Iter [522/633], LR: 0.005000, Loss: 3.5097, top1: 21.8750\n",
      "Epoch [16/60], Iter [523/633], LR: 0.005000, Loss: 3.5435, top1: 20.3125\n",
      "Epoch [16/60], Iter [524/633], LR: 0.005000, Loss: 3.5155, top1: 20.3125\n",
      "Epoch [16/60], Iter [525/633], LR: 0.005000, Loss: 3.5873, top1: 15.6250\n",
      "Epoch [16/60], Iter [526/633], LR: 0.005000, Loss: 3.6271, top1: 6.2500\n",
      "Epoch [16/60], Iter [527/633], LR: 0.005000, Loss: 3.5517, top1: 18.7500\n",
      "Epoch [16/60], Iter [528/633], LR: 0.005000, Loss: 3.5944, top1: 14.0625\n",
      "Epoch [16/60], Iter [529/633], LR: 0.005000, Loss: 3.5934, top1: 15.6250\n",
      "Epoch [16/60], Iter [530/633], LR: 0.005000, Loss: 3.5241, top1: 18.7500\n",
      "Epoch [16/60], Iter [531/633], LR: 0.005000, Loss: 3.5376, top1: 18.7500\n",
      "Epoch [16/60], Iter [532/633], LR: 0.005000, Loss: 3.6065, top1: 10.9375\n",
      "Epoch [16/60], Iter [533/633], LR: 0.005000, Loss: 3.5748, top1: 18.7500\n",
      "Epoch [16/60], Iter [534/633], LR: 0.005000, Loss: 3.5393, top1: 18.7500\n",
      "Epoch [16/60], Iter [535/633], LR: 0.005000, Loss: 3.6014, top1: 12.5000\n",
      "Epoch [16/60], Iter [536/633], LR: 0.005000, Loss: 3.5551, top1: 17.1875\n",
      "Epoch [16/60], Iter [537/633], LR: 0.005000, Loss: 3.6074, top1: 9.3750\n",
      "Epoch [16/60], Iter [538/633], LR: 0.005000, Loss: 3.5457, top1: 21.8750\n",
      "Epoch [16/60], Iter [539/633], LR: 0.005000, Loss: 3.6005, top1: 17.1875\n",
      "Epoch [16/60], Iter [540/633], LR: 0.005000, Loss: 3.5422, top1: 14.0625\n",
      "Epoch [16/60], Iter [541/633], LR: 0.005000, Loss: 3.5476, top1: 17.1875\n",
      "Epoch [16/60], Iter [542/633], LR: 0.005000, Loss: 3.5772, top1: 10.9375\n",
      "Epoch [16/60], Iter [543/633], LR: 0.005000, Loss: 3.5183, top1: 25.0000\n",
      "Epoch [16/60], Iter [544/633], LR: 0.005000, Loss: 3.5900, top1: 10.9375\n",
      "Epoch [16/60], Iter [545/633], LR: 0.005000, Loss: 3.5766, top1: 14.0625\n",
      "Epoch [16/60], Iter [546/633], LR: 0.005000, Loss: 3.5161, top1: 23.4375\n",
      "Epoch [16/60], Iter [547/633], LR: 0.005000, Loss: 3.5518, top1: 12.5000\n",
      "Epoch [16/60], Iter [548/633], LR: 0.005000, Loss: 3.5751, top1: 15.6250\n",
      "Epoch [16/60], Iter [549/633], LR: 0.005000, Loss: 3.5436, top1: 18.7500\n",
      "Epoch [16/60], Iter [550/633], LR: 0.005000, Loss: 3.5460, top1: 14.0625\n",
      "Epoch [16/60], Iter [551/633], LR: 0.005000, Loss: 3.5808, top1: 14.0625\n",
      "Epoch [16/60], Iter [552/633], LR: 0.005000, Loss: 3.6119, top1: 10.9375\n",
      "Epoch [16/60], Iter [553/633], LR: 0.005000, Loss: 3.5649, top1: 15.6250\n",
      "Epoch [16/60], Iter [554/633], LR: 0.005000, Loss: 3.5565, top1: 17.1875\n",
      "Epoch [16/60], Iter [555/633], LR: 0.005000, Loss: 3.5249, top1: 18.7500\n",
      "Epoch [16/60], Iter [556/633], LR: 0.005000, Loss: 3.5667, top1: 12.5000\n",
      "Epoch [16/60], Iter [557/633], LR: 0.005000, Loss: 3.6101, top1: 6.2500\n",
      "Epoch [16/60], Iter [558/633], LR: 0.005000, Loss: 3.5527, top1: 15.6250\n",
      "Epoch [16/60], Iter [559/633], LR: 0.005000, Loss: 3.6135, top1: 12.5000\n",
      "Epoch [16/60], Iter [560/633], LR: 0.005000, Loss: 3.5812, top1: 9.3750\n",
      "Epoch [16/60], Iter [561/633], LR: 0.005000, Loss: 3.5468, top1: 20.3125\n",
      "Epoch [16/60], Iter [562/633], LR: 0.005000, Loss: 3.5954, top1: 12.5000\n",
      "Epoch [16/60], Iter [563/633], LR: 0.005000, Loss: 3.6166, top1: 9.3750\n",
      "Epoch [16/60], Iter [564/633], LR: 0.005000, Loss: 3.5187, top1: 18.7500\n",
      "Epoch [16/60], Iter [565/633], LR: 0.005000, Loss: 3.5803, top1: 14.0625\n",
      "Epoch [16/60], Iter [566/633], LR: 0.005000, Loss: 3.5397, top1: 12.5000\n",
      "Epoch [16/60], Iter [567/633], LR: 0.005000, Loss: 3.5283, top1: 18.7500\n",
      "Epoch [16/60], Iter [568/633], LR: 0.005000, Loss: 3.5847, top1: 17.1875\n",
      "Epoch [16/60], Iter [569/633], LR: 0.005000, Loss: 3.5395, top1: 23.4375\n",
      "Epoch [16/60], Iter [570/633], LR: 0.005000, Loss: 3.6212, top1: 6.2500\n",
      "Epoch [16/60], Iter [571/633], LR: 0.005000, Loss: 3.5643, top1: 14.0625\n",
      "Epoch [16/60], Iter [572/633], LR: 0.005000, Loss: 3.5149, top1: 21.8750\n",
      "Epoch [16/60], Iter [573/633], LR: 0.005000, Loss: 3.5753, top1: 14.0625\n",
      "Epoch [16/60], Iter [574/633], LR: 0.005000, Loss: 3.6051, top1: 9.3750\n",
      "Epoch [16/60], Iter [575/633], LR: 0.005000, Loss: 3.6018, top1: 9.3750\n",
      "Epoch [16/60], Iter [576/633], LR: 0.005000, Loss: 3.6147, top1: 7.8125\n",
      "Epoch [16/60], Iter [577/633], LR: 0.005000, Loss: 3.5840, top1: 12.5000\n",
      "Epoch [16/60], Iter [578/633], LR: 0.005000, Loss: 3.5452, top1: 14.0625\n",
      "Epoch [16/60], Iter [579/633], LR: 0.005000, Loss: 3.5938, top1: 15.6250\n",
      "Epoch [16/60], Iter [580/633], LR: 0.005000, Loss: 3.5707, top1: 20.3125\n",
      "Epoch [16/60], Iter [581/633], LR: 0.005000, Loss: 3.5858, top1: 6.2500\n",
      "Epoch [16/60], Iter [582/633], LR: 0.005000, Loss: 3.5525, top1: 15.6250\n",
      "Epoch [16/60], Iter [583/633], LR: 0.005000, Loss: 3.5167, top1: 25.0000\n",
      "Epoch [16/60], Iter [584/633], LR: 0.005000, Loss: 3.5821, top1: 15.6250\n",
      "Epoch [16/60], Iter [585/633], LR: 0.005000, Loss: 3.5840, top1: 18.7500\n",
      "Epoch [16/60], Iter [586/633], LR: 0.005000, Loss: 3.5896, top1: 7.8125\n",
      "Epoch [16/60], Iter [587/633], LR: 0.005000, Loss: 3.5353, top1: 17.1875\n",
      "Epoch [16/60], Iter [588/633], LR: 0.005000, Loss: 3.5689, top1: 15.6250\n",
      "Epoch [16/60], Iter [589/633], LR: 0.005000, Loss: 3.5953, top1: 10.9375\n",
      "Epoch [16/60], Iter [590/633], LR: 0.005000, Loss: 3.5632, top1: 18.7500\n",
      "Epoch [16/60], Iter [591/633], LR: 0.005000, Loss: 3.5892, top1: 14.0625\n",
      "Epoch [16/60], Iter [592/633], LR: 0.005000, Loss: 3.4937, top1: 25.0000\n",
      "Epoch [16/60], Iter [593/633], LR: 0.005000, Loss: 3.5372, top1: 20.3125\n",
      "Epoch [16/60], Iter [594/633], LR: 0.005000, Loss: 3.5266, top1: 18.7500\n",
      "Epoch [16/60], Iter [595/633], LR: 0.005000, Loss: 3.6123, top1: 9.3750\n",
      "Epoch [16/60], Iter [596/633], LR: 0.005000, Loss: 3.5617, top1: 12.5000\n",
      "Epoch [16/60], Iter [597/633], LR: 0.005000, Loss: 3.5411, top1: 17.1875\n",
      "Epoch [16/60], Iter [598/633], LR: 0.005000, Loss: 3.5231, top1: 18.7500\n",
      "Epoch [16/60], Iter [599/633], LR: 0.005000, Loss: 3.5582, top1: 17.1875\n",
      "Epoch [16/60], Iter [600/633], LR: 0.005000, Loss: 3.6095, top1: 12.5000\n",
      "Epoch [16/60], Iter [601/633], LR: 0.005000, Loss: 3.5573, top1: 7.8125\n",
      "Epoch [16/60], Iter [602/633], LR: 0.005000, Loss: 3.5938, top1: 14.0625\n",
      "Epoch [16/60], Iter [603/633], LR: 0.005000, Loss: 3.5498, top1: 18.7500\n",
      "Epoch [16/60], Iter [604/633], LR: 0.005000, Loss: 3.5041, top1: 23.4375\n",
      "Epoch [16/60], Iter [605/633], LR: 0.005000, Loss: 3.5527, top1: 14.0625\n",
      "Epoch [16/60], Iter [606/633], LR: 0.005000, Loss: 3.5877, top1: 15.6250\n",
      "Epoch [16/60], Iter [607/633], LR: 0.005000, Loss: 3.5629, top1: 17.1875\n",
      "Epoch [16/60], Iter [608/633], LR: 0.005000, Loss: 3.5727, top1: 12.5000\n",
      "Epoch [16/60], Iter [609/633], LR: 0.005000, Loss: 3.5660, top1: 17.1875\n",
      "Epoch [16/60], Iter [610/633], LR: 0.005000, Loss: 3.5136, top1: 21.8750\n",
      "Epoch [16/60], Iter [611/633], LR: 0.005000, Loss: 3.5663, top1: 18.7500\n",
      "Epoch [16/60], Iter [612/633], LR: 0.005000, Loss: 3.5910, top1: 12.5000\n",
      "Epoch [16/60], Iter [613/633], LR: 0.005000, Loss: 3.5738, top1: 14.0625\n",
      "Epoch [16/60], Iter [614/633], LR: 0.005000, Loss: 3.5686, top1: 15.6250\n",
      "Epoch [16/60], Iter [615/633], LR: 0.005000, Loss: 3.5577, top1: 17.1875\n",
      "Epoch [16/60], Iter [616/633], LR: 0.005000, Loss: 3.5183, top1: 26.5625\n",
      "Epoch [16/60], Iter [617/633], LR: 0.005000, Loss: 3.5443, top1: 18.7500\n",
      "Epoch [16/60], Iter [618/633], LR: 0.005000, Loss: 3.5010, top1: 21.8750\n",
      "Epoch [16/60], Iter [619/633], LR: 0.005000, Loss: 3.5313, top1: 14.0625\n",
      "Epoch [16/60], Iter [620/633], LR: 0.005000, Loss: 3.5178, top1: 17.1875\n",
      "Epoch [16/60], Iter [621/633], LR: 0.005000, Loss: 3.5655, top1: 17.1875\n",
      "Epoch [16/60], Iter [622/633], LR: 0.005000, Loss: 3.5679, top1: 12.5000\n",
      "Epoch [16/60], Iter [623/633], LR: 0.005000, Loss: 3.5582, top1: 20.3125\n",
      "Epoch [16/60], Iter [624/633], LR: 0.005000, Loss: 3.6101, top1: 7.8125\n",
      "Epoch [16/60], Iter [625/633], LR: 0.005000, Loss: 3.6106, top1: 7.8125\n",
      "Epoch [16/60], Iter [626/633], LR: 0.005000, Loss: 3.5925, top1: 10.9375\n",
      "Epoch [16/60], Iter [627/633], LR: 0.005000, Loss: 3.5668, top1: 15.6250\n",
      "Epoch [16/60], Iter [628/633], LR: 0.005000, Loss: 3.5528, top1: 10.9375\n",
      "Epoch [16/60], Iter [629/633], LR: 0.005000, Loss: 3.6344, top1: 10.9375\n",
      "Epoch [16/60], Iter [630/633], LR: 0.005000, Loss: 3.5431, top1: 21.8750\n",
      "Epoch [16/60], Iter [631/633], LR: 0.005000, Loss: 3.5648, top1: 17.1875\n",
      "Epoch [16/60], Iter [632/633], LR: 0.005000, Loss: 3.5963, top1: 10.9375\n",
      "Epoch [16/60], Iter [633/633], LR: 0.005000, Loss: 3.5481, top1: 21.8750\n",
      "Epoch [16/60], Iter [634/633], LR: 0.005000, Loss: 3.5632, top1: 22.5806\n",
      "Epoch [16/60], Val_Loss: 3.5524, Val_top1: 16.9674, best_top1: 15.2509\n",
      "epoch time: 4.442843063672384 min\n",
      "Taking top1 snapshot...\n",
      "Epoch [17/60], Iter [1/633], LR: 0.005000, Loss: 3.6176, top1: 10.9375\n",
      "Epoch [17/60], Iter [2/633], LR: 0.005000, Loss: 3.5274, top1: 18.7500\n",
      "Epoch [17/60], Iter [3/633], LR: 0.005000, Loss: 3.5635, top1: 17.1875\n",
      "Epoch [17/60], Iter [4/633], LR: 0.005000, Loss: 3.6338, top1: 9.3750\n",
      "Epoch [17/60], Iter [5/633], LR: 0.005000, Loss: 3.5233, top1: 18.7500\n",
      "Epoch [17/60], Iter [6/633], LR: 0.005000, Loss: 3.4992, top1: 25.0000\n",
      "Epoch [17/60], Iter [7/633], LR: 0.005000, Loss: 3.5609, top1: 18.7500\n",
      "Epoch [17/60], Iter [8/633], LR: 0.005000, Loss: 3.5836, top1: 17.1875\n",
      "Epoch [17/60], Iter [9/633], LR: 0.005000, Loss: 3.5719, top1: 15.6250\n",
      "Epoch [17/60], Iter [10/633], LR: 0.005000, Loss: 3.5754, top1: 9.3750\n",
      "Epoch [17/60], Iter [11/633], LR: 0.005000, Loss: 3.5544, top1: 14.0625\n",
      "Epoch [17/60], Iter [12/633], LR: 0.005000, Loss: 3.6005, top1: 12.5000\n",
      "Epoch [17/60], Iter [13/633], LR: 0.005000, Loss: 3.5339, top1: 21.8750\n",
      "Epoch [17/60], Iter [14/633], LR: 0.005000, Loss: 3.6172, top1: 6.2500\n",
      "Epoch [17/60], Iter [15/633], LR: 0.005000, Loss: 3.5495, top1: 20.3125\n",
      "Epoch [17/60], Iter [16/633], LR: 0.005000, Loss: 3.5635, top1: 15.6250\n",
      "Epoch [17/60], Iter [17/633], LR: 0.005000, Loss: 3.5193, top1: 15.6250\n",
      "Epoch [17/60], Iter [18/633], LR: 0.005000, Loss: 3.5724, top1: 15.6250\n",
      "Epoch [17/60], Iter [19/633], LR: 0.005000, Loss: 3.5930, top1: 15.6250\n",
      "Epoch [17/60], Iter [20/633], LR: 0.005000, Loss: 3.5576, top1: 15.6250\n",
      "Epoch [17/60], Iter [21/633], LR: 0.005000, Loss: 3.6036, top1: 6.2500\n",
      "Epoch [17/60], Iter [22/633], LR: 0.005000, Loss: 3.5883, top1: 14.0625\n",
      "Epoch [17/60], Iter [23/633], LR: 0.005000, Loss: 3.5503, top1: 18.7500\n",
      "Epoch [17/60], Iter [24/633], LR: 0.005000, Loss: 3.5536, top1: 18.7500\n",
      "Epoch [17/60], Iter [25/633], LR: 0.005000, Loss: 3.5952, top1: 10.9375\n",
      "Epoch [17/60], Iter [26/633], LR: 0.005000, Loss: 3.5116, top1: 23.4375\n",
      "Epoch [17/60], Iter [27/633], LR: 0.005000, Loss: 3.5459, top1: 17.1875\n",
      "Epoch [17/60], Iter [28/633], LR: 0.005000, Loss: 3.5455, top1: 20.3125\n",
      "Epoch [17/60], Iter [29/633], LR: 0.005000, Loss: 3.5898, top1: 12.5000\n",
      "Epoch [17/60], Iter [30/633], LR: 0.005000, Loss: 3.6426, top1: 4.6875\n",
      "Epoch [17/60], Iter [31/633], LR: 0.005000, Loss: 3.5809, top1: 15.6250\n",
      "Epoch [17/60], Iter [32/633], LR: 0.005000, Loss: 3.5375, top1: 17.1875\n",
      "Epoch [17/60], Iter [33/633], LR: 0.005000, Loss: 3.6258, top1: 12.5000\n",
      "Epoch [17/60], Iter [34/633], LR: 0.005000, Loss: 3.5636, top1: 17.1875\n",
      "Epoch [17/60], Iter [35/633], LR: 0.005000, Loss: 3.5562, top1: 20.3125\n",
      "Epoch [17/60], Iter [36/633], LR: 0.005000, Loss: 3.6189, top1: 9.3750\n",
      "Epoch [17/60], Iter [37/633], LR: 0.005000, Loss: 3.5760, top1: 14.0625\n",
      "Epoch [17/60], Iter [38/633], LR: 0.005000, Loss: 3.6303, top1: 6.2500\n",
      "Epoch [17/60], Iter [39/633], LR: 0.005000, Loss: 3.5425, top1: 20.3125\n",
      "Epoch [17/60], Iter [40/633], LR: 0.005000, Loss: 3.6234, top1: 14.0625\n",
      "Epoch [17/60], Iter [41/633], LR: 0.005000, Loss: 3.5751, top1: 15.6250\n",
      "Epoch [17/60], Iter [42/633], LR: 0.005000, Loss: 3.5449, top1: 20.3125\n",
      "Epoch [17/60], Iter [43/633], LR: 0.005000, Loss: 3.6034, top1: 7.8125\n",
      "Epoch [17/60], Iter [44/633], LR: 0.005000, Loss: 3.5669, top1: 15.6250\n",
      "Epoch [17/60], Iter [45/633], LR: 0.005000, Loss: 3.5479, top1: 15.6250\n",
      "Epoch [17/60], Iter [46/633], LR: 0.005000, Loss: 3.5685, top1: 21.8750\n",
      "Epoch [17/60], Iter [47/633], LR: 0.005000, Loss: 3.5859, top1: 14.0625\n",
      "Epoch [17/60], Iter [48/633], LR: 0.005000, Loss: 3.5590, top1: 18.7500\n",
      "Epoch [17/60], Iter [49/633], LR: 0.005000, Loss: 3.5615, top1: 18.7500\n",
      "Epoch [17/60], Iter [50/633], LR: 0.005000, Loss: 3.5766, top1: 12.5000\n",
      "Epoch [17/60], Iter [51/633], LR: 0.005000, Loss: 3.5338, top1: 17.1875\n",
      "Epoch [17/60], Iter [52/633], LR: 0.005000, Loss: 3.5573, top1: 12.5000\n",
      "Epoch [17/60], Iter [53/633], LR: 0.005000, Loss: 3.5882, top1: 15.6250\n",
      "Epoch [17/60], Iter [54/633], LR: 0.005000, Loss: 3.5710, top1: 15.6250\n",
      "Epoch [17/60], Iter [55/633], LR: 0.005000, Loss: 3.5262, top1: 18.7500\n",
      "Epoch [17/60], Iter [56/633], LR: 0.005000, Loss: 3.5439, top1: 18.7500\n",
      "Epoch [17/60], Iter [57/633], LR: 0.005000, Loss: 3.5389, top1: 28.1250\n",
      "Epoch [17/60], Iter [58/633], LR: 0.005000, Loss: 3.6441, top1: 7.8125\n",
      "Epoch [17/60], Iter [59/633], LR: 0.005000, Loss: 3.5737, top1: 9.3750\n",
      "Epoch [17/60], Iter [60/633], LR: 0.005000, Loss: 3.5602, top1: 15.6250\n",
      "Epoch [17/60], Iter [61/633], LR: 0.005000, Loss: 3.6166, top1: 7.8125\n",
      "Epoch [17/60], Iter [62/633], LR: 0.005000, Loss: 3.6029, top1: 10.9375\n",
      "Epoch [17/60], Iter [63/633], LR: 0.005000, Loss: 3.5660, top1: 9.3750\n",
      "Epoch [17/60], Iter [64/633], LR: 0.005000, Loss: 3.5992, top1: 12.5000\n",
      "Epoch [17/60], Iter [65/633], LR: 0.005000, Loss: 3.5406, top1: 18.7500\n",
      "Epoch [17/60], Iter [66/633], LR: 0.005000, Loss: 3.5114, top1: 25.0000\n",
      "Epoch [17/60], Iter [67/633], LR: 0.005000, Loss: 3.5692, top1: 10.9375\n",
      "Epoch [17/60], Iter [68/633], LR: 0.005000, Loss: 3.5761, top1: 10.9375\n",
      "Epoch [17/60], Iter [69/633], LR: 0.005000, Loss: 3.5653, top1: 15.6250\n",
      "Epoch [17/60], Iter [70/633], LR: 0.005000, Loss: 3.5797, top1: 14.0625\n",
      "Epoch [17/60], Iter [71/633], LR: 0.005000, Loss: 3.5474, top1: 17.1875\n",
      "Epoch [17/60], Iter [72/633], LR: 0.005000, Loss: 3.5731, top1: 17.1875\n",
      "Epoch [17/60], Iter [73/633], LR: 0.005000, Loss: 3.5722, top1: 15.6250\n",
      "Epoch [17/60], Iter [74/633], LR: 0.005000, Loss: 3.6128, top1: 14.0625\n",
      "Epoch [17/60], Iter [75/633], LR: 0.005000, Loss: 3.6034, top1: 14.0625\n",
      "Epoch [17/60], Iter [76/633], LR: 0.005000, Loss: 3.5787, top1: 14.0625\n",
      "Epoch [17/60], Iter [77/633], LR: 0.005000, Loss: 3.5477, top1: 15.6250\n",
      "Epoch [17/60], Iter [78/633], LR: 0.005000, Loss: 3.5221, top1: 14.0625\n",
      "Epoch [17/60], Iter [79/633], LR: 0.005000, Loss: 3.5544, top1: 17.1875\n",
      "Epoch [17/60], Iter [80/633], LR: 0.005000, Loss: 3.5778, top1: 17.1875\n",
      "Epoch [17/60], Iter [81/633], LR: 0.005000, Loss: 3.5406, top1: 12.5000\n",
      "Epoch [17/60], Iter [82/633], LR: 0.005000, Loss: 3.5588, top1: 17.1875\n",
      "Epoch [17/60], Iter [83/633], LR: 0.005000, Loss: 3.5668, top1: 10.9375\n",
      "Epoch [17/60], Iter [84/633], LR: 0.005000, Loss: 3.5144, top1: 26.5625\n",
      "Epoch [17/60], Iter [85/633], LR: 0.005000, Loss: 3.5240, top1: 18.7500\n",
      "Epoch [17/60], Iter [86/633], LR: 0.005000, Loss: 3.5504, top1: 18.7500\n",
      "Epoch [17/60], Iter [87/633], LR: 0.005000, Loss: 3.5879, top1: 20.3125\n",
      "Epoch [17/60], Iter [88/633], LR: 0.005000, Loss: 3.6039, top1: 12.5000\n",
      "Epoch [17/60], Iter [89/633], LR: 0.005000, Loss: 3.5397, top1: 21.8750\n",
      "Epoch [17/60], Iter [90/633], LR: 0.005000, Loss: 3.6000, top1: 17.1875\n",
      "Epoch [17/60], Iter [91/633], LR: 0.005000, Loss: 3.5931, top1: 12.5000\n",
      "Epoch [17/60], Iter [92/633], LR: 0.005000, Loss: 3.5898, top1: 10.9375\n",
      "Epoch [17/60], Iter [93/633], LR: 0.005000, Loss: 3.5525, top1: 20.3125\n",
      "Epoch [17/60], Iter [94/633], LR: 0.005000, Loss: 3.5761, top1: 17.1875\n",
      "Epoch [17/60], Iter [95/633], LR: 0.005000, Loss: 3.5586, top1: 10.9375\n",
      "Epoch [17/60], Iter [96/633], LR: 0.005000, Loss: 3.5319, top1: 20.3125\n",
      "Epoch [17/60], Iter [97/633], LR: 0.005000, Loss: 3.5955, top1: 14.0625\n",
      "Epoch [17/60], Iter [98/633], LR: 0.005000, Loss: 3.5496, top1: 15.6250\n",
      "Epoch [17/60], Iter [99/633], LR: 0.005000, Loss: 3.5373, top1: 17.1875\n",
      "Epoch [17/60], Iter [100/633], LR: 0.005000, Loss: 3.5492, top1: 17.1875\n",
      "Epoch [17/60], Iter [101/633], LR: 0.005000, Loss: 3.5504, top1: 14.0625\n",
      "Epoch [17/60], Iter [102/633], LR: 0.005000, Loss: 3.5673, top1: 14.0625\n",
      "Epoch [17/60], Iter [103/633], LR: 0.005000, Loss: 3.5461, top1: 15.6250\n",
      "Epoch [17/60], Iter [104/633], LR: 0.005000, Loss: 3.5772, top1: 18.7500\n",
      "Epoch [17/60], Iter [105/633], LR: 0.005000, Loss: 3.6296, top1: 6.2500\n",
      "Epoch [17/60], Iter [106/633], LR: 0.005000, Loss: 3.5599, top1: 17.1875\n",
      "Epoch [17/60], Iter [107/633], LR: 0.005000, Loss: 3.5968, top1: 10.9375\n",
      "Epoch [17/60], Iter [108/633], LR: 0.005000, Loss: 3.5511, top1: 14.0625\n",
      "Epoch [17/60], Iter [109/633], LR: 0.005000, Loss: 3.5454, top1: 21.8750\n",
      "Epoch [17/60], Iter [110/633], LR: 0.005000, Loss: 3.5589, top1: 14.0625\n",
      "Epoch [17/60], Iter [111/633], LR: 0.005000, Loss: 3.5501, top1: 15.6250\n",
      "Epoch [17/60], Iter [112/633], LR: 0.005000, Loss: 3.5057, top1: 26.5625\n",
      "Epoch [17/60], Iter [113/633], LR: 0.005000, Loss: 3.5253, top1: 14.0625\n",
      "Epoch [17/60], Iter [114/633], LR: 0.005000, Loss: 3.5179, top1: 20.3125\n",
      "Epoch [17/60], Iter [115/633], LR: 0.005000, Loss: 3.5486, top1: 17.1875\n",
      "Epoch [17/60], Iter [116/633], LR: 0.005000, Loss: 3.5408, top1: 18.7500\n",
      "Epoch [17/60], Iter [117/633], LR: 0.005000, Loss: 3.5940, top1: 14.0625\n",
      "Epoch [17/60], Iter [118/633], LR: 0.005000, Loss: 3.4851, top1: 25.0000\n",
      "Epoch [17/60], Iter [119/633], LR: 0.005000, Loss: 3.5713, top1: 15.6250\n",
      "Epoch [17/60], Iter [120/633], LR: 0.005000, Loss: 3.6046, top1: 10.9375\n",
      "Epoch [17/60], Iter [121/633], LR: 0.005000, Loss: 3.5405, top1: 20.3125\n",
      "Epoch [17/60], Iter [122/633], LR: 0.005000, Loss: 3.5579, top1: 23.4375\n",
      "Epoch [17/60], Iter [123/633], LR: 0.005000, Loss: 3.5821, top1: 12.5000\n",
      "Epoch [17/60], Iter [124/633], LR: 0.005000, Loss: 3.5680, top1: 18.7500\n",
      "Epoch [17/60], Iter [125/633], LR: 0.005000, Loss: 3.5691, top1: 15.6250\n",
      "Epoch [17/60], Iter [126/633], LR: 0.005000, Loss: 3.5709, top1: 20.3125\n",
      "Epoch [17/60], Iter [127/633], LR: 0.005000, Loss: 3.5790, top1: 15.6250\n",
      "Epoch [17/60], Iter [128/633], LR: 0.005000, Loss: 3.5435, top1: 17.1875\n",
      "Epoch [17/60], Iter [129/633], LR: 0.005000, Loss: 3.5220, top1: 20.3125\n",
      "Epoch [17/60], Iter [130/633], LR: 0.005000, Loss: 3.5172, top1: 23.4375\n",
      "Epoch [17/60], Iter [131/633], LR: 0.005000, Loss: 3.6038, top1: 14.0625\n",
      "Epoch [17/60], Iter [132/633], LR: 0.005000, Loss: 3.6007, top1: 12.5000\n",
      "Epoch [17/60], Iter [133/633], LR: 0.005000, Loss: 3.5975, top1: 10.9375\n",
      "Epoch [17/60], Iter [134/633], LR: 0.005000, Loss: 3.5732, top1: 15.6250\n",
      "Epoch [17/60], Iter [135/633], LR: 0.005000, Loss: 3.5496, top1: 17.1875\n",
      "Epoch [17/60], Iter [136/633], LR: 0.005000, Loss: 3.5453, top1: 25.0000\n",
      "Epoch [17/60], Iter [137/633], LR: 0.005000, Loss: 3.5743, top1: 10.9375\n",
      "Epoch [17/60], Iter [138/633], LR: 0.005000, Loss: 3.6392, top1: 6.2500\n",
      "Epoch [17/60], Iter [139/633], LR: 0.005000, Loss: 3.5896, top1: 7.8125\n",
      "Epoch [17/60], Iter [140/633], LR: 0.005000, Loss: 3.5430, top1: 17.1875\n",
      "Epoch [17/60], Iter [141/633], LR: 0.005000, Loss: 3.5750, top1: 15.6250\n",
      "Epoch [17/60], Iter [142/633], LR: 0.005000, Loss: 3.5888, top1: 15.6250\n",
      "Epoch [17/60], Iter [143/633], LR: 0.005000, Loss: 3.5891, top1: 14.0625\n",
      "Epoch [17/60], Iter [144/633], LR: 0.005000, Loss: 3.6205, top1: 6.2500\n",
      "Epoch [17/60], Iter [145/633], LR: 0.005000, Loss: 3.5917, top1: 9.3750\n",
      "Epoch [17/60], Iter [146/633], LR: 0.005000, Loss: 3.6084, top1: 10.9375\n",
      "Epoch [17/60], Iter [147/633], LR: 0.005000, Loss: 3.5983, top1: 14.0625\n",
      "Epoch [17/60], Iter [148/633], LR: 0.005000, Loss: 3.5189, top1: 25.0000\n",
      "Epoch [17/60], Iter [149/633], LR: 0.005000, Loss: 3.5700, top1: 10.9375\n",
      "Epoch [17/60], Iter [150/633], LR: 0.005000, Loss: 3.5985, top1: 10.9375\n",
      "Epoch [17/60], Iter [151/633], LR: 0.005000, Loss: 3.5612, top1: 14.0625\n",
      "Epoch [17/60], Iter [152/633], LR: 0.005000, Loss: 3.5611, top1: 18.7500\n",
      "Epoch [17/60], Iter [153/633], LR: 0.005000, Loss: 3.5901, top1: 14.0625\n",
      "Epoch [17/60], Iter [154/633], LR: 0.005000, Loss: 3.5687, top1: 17.1875\n",
      "Epoch [17/60], Iter [155/633], LR: 0.005000, Loss: 3.5846, top1: 10.9375\n",
      "Epoch [17/60], Iter [156/633], LR: 0.005000, Loss: 3.5371, top1: 17.1875\n",
      "Epoch [17/60], Iter [157/633], LR: 0.005000, Loss: 3.5852, top1: 4.6875\n",
      "Epoch [17/60], Iter [158/633], LR: 0.005000, Loss: 3.5247, top1: 25.0000\n",
      "Epoch [17/60], Iter [159/633], LR: 0.005000, Loss: 3.5821, top1: 17.1875\n",
      "Epoch [17/60], Iter [160/633], LR: 0.005000, Loss: 3.6210, top1: 9.3750\n",
      "Epoch [17/60], Iter [161/633], LR: 0.005000, Loss: 3.5700, top1: 14.0625\n",
      "Epoch [17/60], Iter [162/633], LR: 0.005000, Loss: 3.5770, top1: 12.5000\n",
      "Epoch [17/60], Iter [163/633], LR: 0.005000, Loss: 3.5631, top1: 17.1875\n",
      "Epoch [17/60], Iter [164/633], LR: 0.005000, Loss: 3.5670, top1: 17.1875\n",
      "Epoch [17/60], Iter [165/633], LR: 0.005000, Loss: 3.6091, top1: 10.9375\n",
      "Epoch [17/60], Iter [166/633], LR: 0.005000, Loss: 3.5521, top1: 18.7500\n",
      "Epoch [17/60], Iter [167/633], LR: 0.005000, Loss: 3.5571, top1: 17.1875\n",
      "Epoch [17/60], Iter [168/633], LR: 0.005000, Loss: 3.5277, top1: 25.0000\n",
      "Epoch [17/60], Iter [169/633], LR: 0.005000, Loss: 3.5424, top1: 15.6250\n",
      "Epoch [17/60], Iter [170/633], LR: 0.005000, Loss: 3.5940, top1: 15.6250\n",
      "Epoch [17/60], Iter [171/633], LR: 0.005000, Loss: 3.5931, top1: 17.1875\n",
      "Epoch [17/60], Iter [172/633], LR: 0.005000, Loss: 3.5819, top1: 18.7500\n",
      "Epoch [17/60], Iter [173/633], LR: 0.005000, Loss: 3.5624, top1: 14.0625\n",
      "Epoch [17/60], Iter [174/633], LR: 0.005000, Loss: 3.5439, top1: 15.6250\n",
      "Epoch [17/60], Iter [175/633], LR: 0.005000, Loss: 3.5385, top1: 17.1875\n",
      "Epoch [17/60], Iter [176/633], LR: 0.005000, Loss: 3.5765, top1: 12.5000\n",
      "Epoch [17/60], Iter [177/633], LR: 0.005000, Loss: 3.5301, top1: 18.7500\n",
      "Epoch [17/60], Iter [178/633], LR: 0.005000, Loss: 3.5451, top1: 17.1875\n",
      "Epoch [17/60], Iter [179/633], LR: 0.005000, Loss: 3.5598, top1: 14.0625\n",
      "Epoch [17/60], Iter [180/633], LR: 0.005000, Loss: 3.5689, top1: 12.5000\n",
      "Epoch [17/60], Iter [181/633], LR: 0.005000, Loss: 3.5438, top1: 23.4375\n",
      "Epoch [17/60], Iter [182/633], LR: 0.005000, Loss: 3.5070, top1: 23.4375\n",
      "Epoch [17/60], Iter [183/633], LR: 0.005000, Loss: 3.5903, top1: 10.9375\n",
      "Epoch [17/60], Iter [184/633], LR: 0.005000, Loss: 3.5731, top1: 15.6250\n",
      "Epoch [17/60], Iter [185/633], LR: 0.005000, Loss: 3.6124, top1: 14.0625\n",
      "Epoch [17/60], Iter [186/633], LR: 0.005000, Loss: 3.5780, top1: 17.1875\n",
      "Epoch [17/60], Iter [187/633], LR: 0.005000, Loss: 3.6109, top1: 12.5000\n",
      "Epoch [17/60], Iter [188/633], LR: 0.005000, Loss: 3.5449, top1: 23.4375\n",
      "Epoch [17/60], Iter [189/633], LR: 0.005000, Loss: 3.5721, top1: 14.0625\n",
      "Epoch [17/60], Iter [190/633], LR: 0.005000, Loss: 3.5937, top1: 12.5000\n",
      "Epoch [17/60], Iter [191/633], LR: 0.005000, Loss: 3.5506, top1: 15.6250\n",
      "Epoch [17/60], Iter [192/633], LR: 0.005000, Loss: 3.6218, top1: 10.9375\n",
      "Epoch [17/60], Iter [193/633], LR: 0.005000, Loss: 3.6129, top1: 12.5000\n",
      "Epoch [17/60], Iter [194/633], LR: 0.005000, Loss: 3.5737, top1: 15.6250\n",
      "Epoch [17/60], Iter [195/633], LR: 0.005000, Loss: 3.5357, top1: 18.7500\n",
      "Epoch [17/60], Iter [196/633], LR: 0.005000, Loss: 3.5842, top1: 9.3750\n",
      "Epoch [17/60], Iter [197/633], LR: 0.005000, Loss: 3.5782, top1: 17.1875\n",
      "Epoch [17/60], Iter [198/633], LR: 0.005000, Loss: 3.5786, top1: 12.5000\n",
      "Epoch [17/60], Iter [199/633], LR: 0.005000, Loss: 3.5409, top1: 18.7500\n",
      "Epoch [17/60], Iter [200/633], LR: 0.005000, Loss: 3.5820, top1: 14.0625\n",
      "Epoch [17/60], Iter [201/633], LR: 0.005000, Loss: 3.5118, top1: 18.7500\n",
      "Epoch [17/60], Iter [202/633], LR: 0.005000, Loss: 3.5202, top1: 23.4375\n",
      "Epoch [17/60], Iter [203/633], LR: 0.005000, Loss: 3.5936, top1: 15.6250\n",
      "Epoch [17/60], Iter [204/633], LR: 0.005000, Loss: 3.5593, top1: 21.8750\n",
      "Epoch [17/60], Iter [205/633], LR: 0.005000, Loss: 3.5710, top1: 20.3125\n",
      "Epoch [17/60], Iter [206/633], LR: 0.005000, Loss: 3.5569, top1: 18.7500\n",
      "Epoch [17/60], Iter [207/633], LR: 0.005000, Loss: 3.5147, top1: 23.4375\n",
      "Epoch [17/60], Iter [208/633], LR: 0.005000, Loss: 3.5714, top1: 14.0625\n",
      "Epoch [17/60], Iter [209/633], LR: 0.005000, Loss: 3.5776, top1: 14.0625\n",
      "Epoch [17/60], Iter [210/633], LR: 0.005000, Loss: 3.5978, top1: 14.0625\n",
      "Epoch [17/60], Iter [211/633], LR: 0.005000, Loss: 3.5103, top1: 23.4375\n",
      "Epoch [17/60], Iter [212/633], LR: 0.005000, Loss: 3.5177, top1: 25.0000\n",
      "Epoch [17/60], Iter [213/633], LR: 0.005000, Loss: 3.5570, top1: 17.1875\n",
      "Epoch [17/60], Iter [214/633], LR: 0.005000, Loss: 3.5261, top1: 18.7500\n",
      "Epoch [17/60], Iter [215/633], LR: 0.005000, Loss: 3.5863, top1: 12.5000\n",
      "Epoch [17/60], Iter [216/633], LR: 0.005000, Loss: 3.5442, top1: 15.6250\n",
      "Epoch [17/60], Iter [217/633], LR: 0.005000, Loss: 3.5310, top1: 21.8750\n",
      "Epoch [17/60], Iter [218/633], LR: 0.005000, Loss: 3.5532, top1: 10.9375\n",
      "Epoch [17/60], Iter [219/633], LR: 0.005000, Loss: 3.5297, top1: 20.3125\n",
      "Epoch [17/60], Iter [220/633], LR: 0.005000, Loss: 3.5008, top1: 25.0000\n",
      "Epoch [17/60], Iter [221/633], LR: 0.005000, Loss: 3.6097, top1: 9.3750\n",
      "Epoch [17/60], Iter [222/633], LR: 0.005000, Loss: 3.5459, top1: 15.6250\n",
      "Epoch [17/60], Iter [223/633], LR: 0.005000, Loss: 3.5504, top1: 14.0625\n",
      "Epoch [17/60], Iter [224/633], LR: 0.005000, Loss: 3.4873, top1: 26.5625\n",
      "Epoch [17/60], Iter [225/633], LR: 0.005000, Loss: 3.5438, top1: 15.6250\n",
      "Epoch [17/60], Iter [226/633], LR: 0.005000, Loss: 3.5680, top1: 17.1875\n",
      "Epoch [17/60], Iter [227/633], LR: 0.005000, Loss: 3.5308, top1: 15.6250\n",
      "Epoch [17/60], Iter [228/633], LR: 0.005000, Loss: 3.6159, top1: 9.3750\n",
      "Epoch [17/60], Iter [229/633], LR: 0.005000, Loss: 3.6040, top1: 12.5000\n",
      "Epoch [17/60], Iter [230/633], LR: 0.005000, Loss: 3.5691, top1: 10.9375\n",
      "Epoch [17/60], Iter [231/633], LR: 0.005000, Loss: 3.5530, top1: 20.3125\n",
      "Epoch [17/60], Iter [232/633], LR: 0.005000, Loss: 3.4916, top1: 15.6250\n",
      "Epoch [17/60], Iter [233/633], LR: 0.005000, Loss: 3.5865, top1: 9.3750\n",
      "Epoch [17/60], Iter [234/633], LR: 0.005000, Loss: 3.5639, top1: 15.6250\n",
      "Epoch [17/60], Iter [235/633], LR: 0.005000, Loss: 3.5752, top1: 12.5000\n",
      "Epoch [17/60], Iter [236/633], LR: 0.005000, Loss: 3.5701, top1: 20.3125\n",
      "Epoch [17/60], Iter [237/633], LR: 0.005000, Loss: 3.5406, top1: 18.7500\n",
      "Epoch [17/60], Iter [238/633], LR: 0.005000, Loss: 3.5570, top1: 18.7500\n",
      "Epoch [17/60], Iter [239/633], LR: 0.005000, Loss: 3.6313, top1: 10.9375\n",
      "Epoch [17/60], Iter [240/633], LR: 0.005000, Loss: 3.6326, top1: 7.8125\n",
      "Epoch [17/60], Iter [241/633], LR: 0.005000, Loss: 3.5577, top1: 15.6250\n",
      "Epoch [17/60], Iter [242/633], LR: 0.005000, Loss: 3.5422, top1: 15.6250\n",
      "Epoch [17/60], Iter [243/633], LR: 0.005000, Loss: 3.5835, top1: 14.0625\n",
      "Epoch [17/60], Iter [244/633], LR: 0.005000, Loss: 3.5879, top1: 15.6250\n",
      "Epoch [17/60], Iter [245/633], LR: 0.005000, Loss: 3.5363, top1: 21.8750\n",
      "Epoch [17/60], Iter [246/633], LR: 0.005000, Loss: 3.5977, top1: 9.3750\n",
      "Epoch [17/60], Iter [247/633], LR: 0.005000, Loss: 3.5369, top1: 18.7500\n",
      "Epoch [17/60], Iter [248/633], LR: 0.005000, Loss: 3.5689, top1: 12.5000\n",
      "Epoch [17/60], Iter [249/633], LR: 0.005000, Loss: 3.5886, top1: 9.3750\n",
      "Epoch [17/60], Iter [250/633], LR: 0.005000, Loss: 3.5728, top1: 12.5000\n",
      "Epoch [17/60], Iter [251/633], LR: 0.005000, Loss: 3.5972, top1: 9.3750\n",
      "Epoch [17/60], Iter [252/633], LR: 0.005000, Loss: 3.5722, top1: 14.0625\n",
      "Epoch [17/60], Iter [253/633], LR: 0.005000, Loss: 3.5242, top1: 23.4375\n",
      "Epoch [17/60], Iter [254/633], LR: 0.005000, Loss: 3.6070, top1: 17.1875\n",
      "Epoch [17/60], Iter [255/633], LR: 0.005000, Loss: 3.5396, top1: 20.3125\n",
      "Epoch [17/60], Iter [256/633], LR: 0.005000, Loss: 3.6589, top1: 3.1250\n",
      "Epoch [17/60], Iter [257/633], LR: 0.005000, Loss: 3.5609, top1: 14.0625\n",
      "Epoch [17/60], Iter [258/633], LR: 0.005000, Loss: 3.5871, top1: 14.0625\n",
      "Epoch [17/60], Iter [259/633], LR: 0.005000, Loss: 3.6007, top1: 10.9375\n",
      "Epoch [17/60], Iter [260/633], LR: 0.005000, Loss: 3.5631, top1: 15.6250\n",
      "Epoch [17/60], Iter [261/633], LR: 0.005000, Loss: 3.5549, top1: 20.3125\n",
      "Epoch [17/60], Iter [262/633], LR: 0.005000, Loss: 3.5954, top1: 15.6250\n",
      "Epoch [17/60], Iter [263/633], LR: 0.005000, Loss: 3.5983, top1: 6.2500\n",
      "Epoch [17/60], Iter [264/633], LR: 0.005000, Loss: 3.5857, top1: 12.5000\n",
      "Epoch [17/60], Iter [265/633], LR: 0.005000, Loss: 3.5677, top1: 20.3125\n",
      "Epoch [17/60], Iter [266/633], LR: 0.005000, Loss: 3.5673, top1: 12.5000\n",
      "Epoch [17/60], Iter [267/633], LR: 0.005000, Loss: 3.5752, top1: 17.1875\n",
      "Epoch [17/60], Iter [268/633], LR: 0.005000, Loss: 3.5882, top1: 10.9375\n",
      "Epoch [17/60], Iter [269/633], LR: 0.005000, Loss: 3.5847, top1: 10.9375\n",
      "Epoch [17/60], Iter [270/633], LR: 0.005000, Loss: 3.5123, top1: 21.8750\n",
      "Epoch [17/60], Iter [271/633], LR: 0.005000, Loss: 3.5767, top1: 12.5000\n",
      "Epoch [17/60], Iter [272/633], LR: 0.005000, Loss: 3.5465, top1: 17.1875\n",
      "Epoch [17/60], Iter [273/633], LR: 0.005000, Loss: 3.5848, top1: 15.6250\n",
      "Epoch [17/60], Iter [274/633], LR: 0.005000, Loss: 3.5090, top1: 20.3125\n",
      "Epoch [17/60], Iter [275/633], LR: 0.005000, Loss: 3.5999, top1: 12.5000\n",
      "Epoch [17/60], Iter [276/633], LR: 0.005000, Loss: 3.5239, top1: 21.8750\n",
      "Epoch [17/60], Iter [277/633], LR: 0.005000, Loss: 3.5364, top1: 20.3125\n",
      "Epoch [17/60], Iter [278/633], LR: 0.005000, Loss: 3.5796, top1: 12.5000\n",
      "Epoch [17/60], Iter [279/633], LR: 0.005000, Loss: 3.5709, top1: 14.0625\n",
      "Epoch [17/60], Iter [280/633], LR: 0.005000, Loss: 3.5583, top1: 12.5000\n",
      "Epoch [17/60], Iter [281/633], LR: 0.005000, Loss: 3.5991, top1: 12.5000\n",
      "Epoch [17/60], Iter [282/633], LR: 0.005000, Loss: 3.5476, top1: 14.0625\n",
      "Epoch [17/60], Iter [283/633], LR: 0.005000, Loss: 3.5383, top1: 17.1875\n",
      "Epoch [17/60], Iter [284/633], LR: 0.005000, Loss: 3.5528, top1: 18.7500\n",
      "Epoch [17/60], Iter [285/633], LR: 0.005000, Loss: 3.5657, top1: 17.1875\n",
      "Epoch [17/60], Iter [286/633], LR: 0.005000, Loss: 3.6146, top1: 15.6250\n",
      "Epoch [17/60], Iter [287/633], LR: 0.005000, Loss: 3.4919, top1: 25.0000\n",
      "Epoch [17/60], Iter [288/633], LR: 0.005000, Loss: 3.5831, top1: 9.3750\n",
      "Epoch [17/60], Iter [289/633], LR: 0.005000, Loss: 3.5076, top1: 21.8750\n",
      "Epoch [17/60], Iter [290/633], LR: 0.005000, Loss: 3.5618, top1: 21.8750\n",
      "Epoch [17/60], Iter [291/633], LR: 0.005000, Loss: 3.5732, top1: 17.1875\n",
      "Epoch [17/60], Iter [292/633], LR: 0.005000, Loss: 3.5937, top1: 14.0625\n",
      "Epoch [17/60], Iter [293/633], LR: 0.005000, Loss: 3.5797, top1: 17.1875\n",
      "Epoch [17/60], Iter [294/633], LR: 0.005000, Loss: 3.5583, top1: 20.3125\n",
      "Epoch [17/60], Iter [295/633], LR: 0.005000, Loss: 3.5997, top1: 9.3750\n",
      "Epoch [17/60], Iter [296/633], LR: 0.005000, Loss: 3.5357, top1: 14.0625\n",
      "Epoch [17/60], Iter [297/633], LR: 0.005000, Loss: 3.6396, top1: 6.2500\n",
      "Epoch [17/60], Iter [298/633], LR: 0.005000, Loss: 3.5517, top1: 18.7500\n",
      "Epoch [17/60], Iter [299/633], LR: 0.005000, Loss: 3.5209, top1: 14.0625\n",
      "Epoch [17/60], Iter [300/633], LR: 0.005000, Loss: 3.5807, top1: 15.6250\n",
      "Epoch [17/60], Iter [301/633], LR: 0.005000, Loss: 3.5988, top1: 10.9375\n",
      "Epoch [17/60], Iter [302/633], LR: 0.005000, Loss: 3.5704, top1: 15.6250\n",
      "Epoch [17/60], Iter [303/633], LR: 0.005000, Loss: 3.6035, top1: 9.3750\n",
      "Epoch [17/60], Iter [304/633], LR: 0.005000, Loss: 3.6288, top1: 7.8125\n",
      "Epoch [17/60], Iter [305/633], LR: 0.005000, Loss: 3.5969, top1: 9.3750\n",
      "Epoch [17/60], Iter [306/633], LR: 0.005000, Loss: 3.5733, top1: 12.5000\n",
      "Epoch [17/60], Iter [307/633], LR: 0.005000, Loss: 3.5582, top1: 20.3125\n",
      "Epoch [17/60], Iter [308/633], LR: 0.005000, Loss: 3.5563, top1: 23.4375\n",
      "Epoch [17/60], Iter [309/633], LR: 0.005000, Loss: 3.5499, top1: 21.8750\n",
      "Epoch [17/60], Iter [310/633], LR: 0.005000, Loss: 3.5475, top1: 20.3125\n",
      "Epoch [17/60], Iter [311/633], LR: 0.005000, Loss: 3.5835, top1: 18.7500\n",
      "Epoch [17/60], Iter [312/633], LR: 0.005000, Loss: 3.5390, top1: 18.7500\n",
      "Epoch [17/60], Iter [313/633], LR: 0.005000, Loss: 3.5308, top1: 17.1875\n",
      "Epoch [17/60], Iter [314/633], LR: 0.005000, Loss: 3.5363, top1: 17.1875\n",
      "Epoch [17/60], Iter [315/633], LR: 0.005000, Loss: 3.5519, top1: 14.0625\n",
      "Epoch [17/60], Iter [316/633], LR: 0.005000, Loss: 3.5503, top1: 17.1875\n",
      "Epoch [17/60], Iter [317/633], LR: 0.005000, Loss: 3.5952, top1: 10.9375\n",
      "Epoch [17/60], Iter [318/633], LR: 0.005000, Loss: 3.5639, top1: 15.6250\n",
      "Epoch [17/60], Iter [319/633], LR: 0.005000, Loss: 3.5683, top1: 15.6250\n",
      "Epoch [17/60], Iter [320/633], LR: 0.005000, Loss: 3.6005, top1: 9.3750\n",
      "Epoch [17/60], Iter [321/633], LR: 0.005000, Loss: 3.5714, top1: 14.0625\n",
      "Epoch [17/60], Iter [322/633], LR: 0.005000, Loss: 3.5598, top1: 14.0625\n",
      "Epoch [17/60], Iter [323/633], LR: 0.005000, Loss: 3.6104, top1: 7.8125\n",
      "Epoch [17/60], Iter [324/633], LR: 0.005000, Loss: 3.5200, top1: 21.8750\n",
      "Epoch [17/60], Iter [325/633], LR: 0.005000, Loss: 3.5732, top1: 17.1875\n",
      "Epoch [17/60], Iter [326/633], LR: 0.005000, Loss: 3.5900, top1: 12.5000\n",
      "Epoch [17/60], Iter [327/633], LR: 0.005000, Loss: 3.5405, top1: 17.1875\n",
      "Epoch [17/60], Iter [328/633], LR: 0.005000, Loss: 3.5451, top1: 15.6250\n",
      "Epoch [17/60], Iter [329/633], LR: 0.005000, Loss: 3.5738, top1: 17.1875\n",
      "Epoch [17/60], Iter [330/633], LR: 0.005000, Loss: 3.5805, top1: 14.0625\n",
      "Epoch [17/60], Iter [331/633], LR: 0.005000, Loss: 3.5193, top1: 21.8750\n",
      "Epoch [17/60], Iter [332/633], LR: 0.005000, Loss: 3.5365, top1: 18.7500\n",
      "Epoch [17/60], Iter [333/633], LR: 0.005000, Loss: 3.5469, top1: 18.7500\n",
      "Epoch [17/60], Iter [334/633], LR: 0.005000, Loss: 3.5698, top1: 15.6250\n",
      "Epoch [17/60], Iter [335/633], LR: 0.005000, Loss: 3.6182, top1: 9.3750\n",
      "Epoch [17/60], Iter [336/633], LR: 0.005000, Loss: 3.5809, top1: 12.5000\n",
      "Epoch [17/60], Iter [337/633], LR: 0.005000, Loss: 3.5892, top1: 9.3750\n",
      "Epoch [17/60], Iter [338/633], LR: 0.005000, Loss: 3.5072, top1: 23.4375\n",
      "Epoch [17/60], Iter [339/633], LR: 0.005000, Loss: 3.5332, top1: 20.3125\n",
      "Epoch [17/60], Iter [340/633], LR: 0.005000, Loss: 3.5757, top1: 9.3750\n",
      "Epoch [17/60], Iter [341/633], LR: 0.005000, Loss: 3.5097, top1: 23.4375\n",
      "Epoch [17/60], Iter [342/633], LR: 0.005000, Loss: 3.5453, top1: 18.7500\n",
      "Epoch [17/60], Iter [343/633], LR: 0.005000, Loss: 3.5479, top1: 18.7500\n",
      "Epoch [17/60], Iter [344/633], LR: 0.005000, Loss: 3.4571, top1: 25.0000\n",
      "Epoch [17/60], Iter [345/633], LR: 0.005000, Loss: 3.5892, top1: 10.9375\n",
      "Epoch [17/60], Iter [346/633], LR: 0.005000, Loss: 3.4968, top1: 17.1875\n",
      "Epoch [17/60], Iter [347/633], LR: 0.005000, Loss: 3.5936, top1: 14.0625\n",
      "Epoch [17/60], Iter [348/633], LR: 0.005000, Loss: 3.5368, top1: 14.0625\n",
      "Epoch [17/60], Iter [349/633], LR: 0.005000, Loss: 3.5625, top1: 9.3750\n",
      "Epoch [17/60], Iter [350/633], LR: 0.005000, Loss: 3.5687, top1: 12.5000\n",
      "Epoch [17/60], Iter [351/633], LR: 0.005000, Loss: 3.6046, top1: 9.3750\n",
      "Epoch [17/60], Iter [352/633], LR: 0.005000, Loss: 3.5554, top1: 18.7500\n",
      "Epoch [17/60], Iter [353/633], LR: 0.005000, Loss: 3.5449, top1: 14.0625\n",
      "Epoch [17/60], Iter [354/633], LR: 0.005000, Loss: 3.5364, top1: 15.6250\n",
      "Epoch [17/60], Iter [355/633], LR: 0.005000, Loss: 3.6039, top1: 15.6250\n",
      "Epoch [17/60], Iter [356/633], LR: 0.005000, Loss: 3.5799, top1: 15.6250\n",
      "Epoch [17/60], Iter [357/633], LR: 0.005000, Loss: 3.5379, top1: 17.1875\n",
      "Epoch [17/60], Iter [358/633], LR: 0.005000, Loss: 3.5442, top1: 23.4375\n",
      "Epoch [17/60], Iter [359/633], LR: 0.005000, Loss: 3.5531, top1: 17.1875\n",
      "Epoch [17/60], Iter [360/633], LR: 0.005000, Loss: 3.6025, top1: 10.9375\n",
      "Epoch [17/60], Iter [361/633], LR: 0.005000, Loss: 3.5588, top1: 17.1875\n",
      "Epoch [17/60], Iter [362/633], LR: 0.005000, Loss: 3.6064, top1: 14.0625\n",
      "Epoch [17/60], Iter [363/633], LR: 0.005000, Loss: 3.5349, top1: 18.7500\n",
      "Epoch [17/60], Iter [364/633], LR: 0.005000, Loss: 3.5897, top1: 9.3750\n",
      "Epoch [17/60], Iter [365/633], LR: 0.005000, Loss: 3.5570, top1: 9.3750\n",
      "Epoch [17/60], Iter [366/633], LR: 0.005000, Loss: 3.5559, top1: 18.7500\n",
      "Epoch [17/60], Iter [367/633], LR: 0.005000, Loss: 3.6031, top1: 12.5000\n",
      "Epoch [17/60], Iter [368/633], LR: 0.005000, Loss: 3.5241, top1: 17.1875\n",
      "Epoch [17/60], Iter [369/633], LR: 0.005000, Loss: 3.5953, top1: 14.0625\n",
      "Epoch [17/60], Iter [370/633], LR: 0.005000, Loss: 3.5726, top1: 14.0625\n",
      "Epoch [17/60], Iter [371/633], LR: 0.005000, Loss: 3.5604, top1: 18.7500\n",
      "Epoch [17/60], Iter [372/633], LR: 0.005000, Loss: 3.5850, top1: 12.5000\n",
      "Epoch [17/60], Iter [373/633], LR: 0.005000, Loss: 3.6046, top1: 10.9375\n",
      "Epoch [17/60], Iter [374/633], LR: 0.005000, Loss: 3.6471, top1: 9.3750\n",
      "Epoch [17/60], Iter [375/633], LR: 0.005000, Loss: 3.5766, top1: 18.7500\n",
      "Epoch [17/60], Iter [376/633], LR: 0.005000, Loss: 3.5538, top1: 14.0625\n",
      "Epoch [17/60], Iter [377/633], LR: 0.005000, Loss: 3.5807, top1: 15.6250\n",
      "Epoch [17/60], Iter [378/633], LR: 0.005000, Loss: 3.5693, top1: 15.6250\n",
      "Epoch [17/60], Iter [379/633], LR: 0.005000, Loss: 3.5259, top1: 23.4375\n",
      "Epoch [17/60], Iter [380/633], LR: 0.005000, Loss: 3.5843, top1: 10.9375\n",
      "Epoch [17/60], Iter [381/633], LR: 0.005000, Loss: 3.5846, top1: 12.5000\n",
      "Epoch [17/60], Iter [382/633], LR: 0.005000, Loss: 3.5720, top1: 12.5000\n",
      "Epoch [17/60], Iter [383/633], LR: 0.005000, Loss: 3.5326, top1: 15.6250\n",
      "Epoch [17/60], Iter [384/633], LR: 0.005000, Loss: 3.5584, top1: 20.3125\n",
      "Epoch [17/60], Iter [385/633], LR: 0.005000, Loss: 3.5243, top1: 17.1875\n",
      "Epoch [17/60], Iter [386/633], LR: 0.005000, Loss: 3.5747, top1: 12.5000\n",
      "Epoch [17/60], Iter [387/633], LR: 0.005000, Loss: 3.5883, top1: 14.0625\n",
      "Epoch [17/60], Iter [388/633], LR: 0.005000, Loss: 3.5666, top1: 21.8750\n",
      "Epoch [17/60], Iter [389/633], LR: 0.005000, Loss: 3.5953, top1: 9.3750\n",
      "Epoch [17/60], Iter [390/633], LR: 0.005000, Loss: 3.6087, top1: 9.3750\n",
      "Epoch [17/60], Iter [391/633], LR: 0.005000, Loss: 3.5625, top1: 18.7500\n",
      "Epoch [17/60], Iter [392/633], LR: 0.005000, Loss: 3.5590, top1: 20.3125\n",
      "Epoch [17/60], Iter [393/633], LR: 0.005000, Loss: 3.5339, top1: 12.5000\n",
      "Epoch [17/60], Iter [394/633], LR: 0.005000, Loss: 3.6031, top1: 10.9375\n",
      "Epoch [17/60], Iter [395/633], LR: 0.005000, Loss: 3.4721, top1: 25.0000\n",
      "Epoch [17/60], Iter [396/633], LR: 0.005000, Loss: 3.5040, top1: 23.4375\n",
      "Epoch [17/60], Iter [397/633], LR: 0.005000, Loss: 3.5483, top1: 15.6250\n",
      "Epoch [17/60], Iter [398/633], LR: 0.005000, Loss: 3.5659, top1: 15.6250\n",
      "Epoch [17/60], Iter [399/633], LR: 0.005000, Loss: 3.5897, top1: 10.9375\n",
      "Epoch [17/60], Iter [400/633], LR: 0.005000, Loss: 3.5474, top1: 20.3125\n",
      "Epoch [17/60], Iter [401/633], LR: 0.005000, Loss: 3.5225, top1: 17.1875\n",
      "Epoch [17/60], Iter [402/633], LR: 0.005000, Loss: 3.5365, top1: 15.6250\n",
      "Epoch [17/60], Iter [403/633], LR: 0.005000, Loss: 3.6025, top1: 15.6250\n",
      "Epoch [17/60], Iter [404/633], LR: 0.005000, Loss: 3.5251, top1: 15.6250\n",
      "Epoch [17/60], Iter [405/633], LR: 0.005000, Loss: 3.5440, top1: 17.1875\n",
      "Epoch [17/60], Iter [406/633], LR: 0.005000, Loss: 3.5887, top1: 17.1875\n",
      "Epoch [17/60], Iter [407/633], LR: 0.005000, Loss: 3.5718, top1: 17.1875\n",
      "Epoch [17/60], Iter [408/633], LR: 0.005000, Loss: 3.5996, top1: 15.6250\n",
      "Epoch [17/60], Iter [409/633], LR: 0.005000, Loss: 3.5704, top1: 15.6250\n",
      "Epoch [17/60], Iter [410/633], LR: 0.005000, Loss: 3.5475, top1: 15.6250\n",
      "Epoch [17/60], Iter [411/633], LR: 0.005000, Loss: 3.5414, top1: 15.6250\n",
      "Epoch [17/60], Iter [412/633], LR: 0.005000, Loss: 3.5590, top1: 17.1875\n",
      "Epoch [17/60], Iter [413/633], LR: 0.005000, Loss: 3.5235, top1: 23.4375\n",
      "Epoch [17/60], Iter [414/633], LR: 0.005000, Loss: 3.5618, top1: 21.8750\n",
      "Epoch [17/60], Iter [415/633], LR: 0.005000, Loss: 3.6051, top1: 7.8125\n",
      "Epoch [17/60], Iter [416/633], LR: 0.005000, Loss: 3.5123, top1: 14.0625\n",
      "Epoch [17/60], Iter [417/633], LR: 0.005000, Loss: 3.6052, top1: 15.6250\n",
      "Epoch [17/60], Iter [418/633], LR: 0.005000, Loss: 3.5885, top1: 12.5000\n",
      "Epoch [17/60], Iter [419/633], LR: 0.005000, Loss: 3.5920, top1: 12.5000\n",
      "Epoch [17/60], Iter [420/633], LR: 0.005000, Loss: 3.6242, top1: 9.3750\n",
      "Epoch [17/60], Iter [421/633], LR: 0.005000, Loss: 3.6075, top1: 6.2500\n",
      "Epoch [17/60], Iter [422/633], LR: 0.005000, Loss: 3.5856, top1: 10.9375\n",
      "Epoch [17/60], Iter [423/633], LR: 0.005000, Loss: 3.5863, top1: 9.3750\n",
      "Epoch [17/60], Iter [424/633], LR: 0.005000, Loss: 3.5467, top1: 17.1875\n",
      "Epoch [17/60], Iter [425/633], LR: 0.005000, Loss: 3.5545, top1: 14.0625\n",
      "Epoch [17/60], Iter [426/633], LR: 0.005000, Loss: 3.6161, top1: 7.8125\n",
      "Epoch [17/60], Iter [427/633], LR: 0.005000, Loss: 3.5766, top1: 12.5000\n",
      "Epoch [17/60], Iter [428/633], LR: 0.005000, Loss: 3.6027, top1: 7.8125\n",
      "Epoch [17/60], Iter [429/633], LR: 0.005000, Loss: 3.5280, top1: 23.4375\n",
      "Epoch [17/60], Iter [430/633], LR: 0.005000, Loss: 3.6030, top1: 9.3750\n",
      "Epoch [17/60], Iter [431/633], LR: 0.005000, Loss: 3.5557, top1: 12.5000\n",
      "Epoch [17/60], Iter [432/633], LR: 0.005000, Loss: 3.5365, top1: 18.7500\n",
      "Epoch [17/60], Iter [433/633], LR: 0.005000, Loss: 3.5984, top1: 12.5000\n",
      "Epoch [17/60], Iter [434/633], LR: 0.005000, Loss: 3.5472, top1: 18.7500\n",
      "Epoch [17/60], Iter [435/633], LR: 0.005000, Loss: 3.5588, top1: 17.1875\n",
      "Epoch [17/60], Iter [436/633], LR: 0.005000, Loss: 3.6331, top1: 10.9375\n",
      "Epoch [17/60], Iter [437/633], LR: 0.005000, Loss: 3.5737, top1: 9.3750\n",
      "Epoch [17/60], Iter [438/633], LR: 0.005000, Loss: 3.5490, top1: 20.3125\n",
      "Epoch [17/60], Iter [439/633], LR: 0.005000, Loss: 3.5818, top1: 9.3750\n",
      "Epoch [17/60], Iter [440/633], LR: 0.005000, Loss: 3.5922, top1: 14.0625\n",
      "Epoch [17/60], Iter [441/633], LR: 0.005000, Loss: 3.5914, top1: 9.3750\n",
      "Epoch [17/60], Iter [442/633], LR: 0.005000, Loss: 3.5467, top1: 14.0625\n",
      "Epoch [17/60], Iter [443/633], LR: 0.005000, Loss: 3.5288, top1: 17.1875\n",
      "Epoch [17/60], Iter [444/633], LR: 0.005000, Loss: 3.5523, top1: 18.7500\n",
      "Epoch [17/60], Iter [445/633], LR: 0.005000, Loss: 3.5946, top1: 9.3750\n",
      "Epoch [17/60], Iter [446/633], LR: 0.005000, Loss: 3.5701, top1: 14.0625\n",
      "Epoch [17/60], Iter [447/633], LR: 0.005000, Loss: 3.5852, top1: 12.5000\n",
      "Epoch [17/60], Iter [448/633], LR: 0.005000, Loss: 3.5582, top1: 10.9375\n",
      "Epoch [17/60], Iter [449/633], LR: 0.005000, Loss: 3.5707, top1: 15.6250\n",
      "Epoch [17/60], Iter [450/633], LR: 0.005000, Loss: 3.5962, top1: 12.5000\n",
      "Epoch [17/60], Iter [451/633], LR: 0.005000, Loss: 3.5646, top1: 14.0625\n",
      "Epoch [17/60], Iter [452/633], LR: 0.005000, Loss: 3.5611, top1: 21.8750\n",
      "Epoch [17/60], Iter [453/633], LR: 0.005000, Loss: 3.5504, top1: 15.6250\n",
      "Epoch [17/60], Iter [454/633], LR: 0.005000, Loss: 3.6263, top1: 6.2500\n",
      "Epoch [17/60], Iter [455/633], LR: 0.005000, Loss: 3.5425, top1: 18.7500\n",
      "Epoch [17/60], Iter [456/633], LR: 0.005000, Loss: 3.5756, top1: 15.6250\n",
      "Epoch [17/60], Iter [457/633], LR: 0.005000, Loss: 3.5718, top1: 20.3125\n",
      "Epoch [17/60], Iter [458/633], LR: 0.005000, Loss: 3.5228, top1: 20.3125\n",
      "Epoch [17/60], Iter [459/633], LR: 0.005000, Loss: 3.6040, top1: 14.0625\n",
      "Epoch [17/60], Iter [460/633], LR: 0.005000, Loss: 3.5465, top1: 23.4375\n",
      "Epoch [17/60], Iter [461/633], LR: 0.005000, Loss: 3.5618, top1: 10.9375\n",
      "Epoch [17/60], Iter [462/633], LR: 0.005000, Loss: 3.6059, top1: 14.0625\n",
      "Epoch [17/60], Iter [463/633], LR: 0.005000, Loss: 3.5884, top1: 15.6250\n",
      "Epoch [17/60], Iter [464/633], LR: 0.005000, Loss: 3.5834, top1: 17.1875\n",
      "Epoch [17/60], Iter [465/633], LR: 0.005000, Loss: 3.6091, top1: 9.3750\n",
      "Epoch [17/60], Iter [466/633], LR: 0.005000, Loss: 3.5929, top1: 10.9375\n",
      "Epoch [17/60], Iter [467/633], LR: 0.005000, Loss: 3.6193, top1: 15.6250\n",
      "Epoch [17/60], Iter [468/633], LR: 0.005000, Loss: 3.6055, top1: 9.3750\n",
      "Epoch [17/60], Iter [469/633], LR: 0.005000, Loss: 3.5621, top1: 10.9375\n",
      "Epoch [17/60], Iter [470/633], LR: 0.005000, Loss: 3.5437, top1: 21.8750\n",
      "Epoch [17/60], Iter [471/633], LR: 0.005000, Loss: 3.5793, top1: 17.1875\n",
      "Epoch [17/60], Iter [472/633], LR: 0.005000, Loss: 3.5618, top1: 14.0625\n",
      "Epoch [17/60], Iter [473/633], LR: 0.005000, Loss: 3.5330, top1: 20.3125\n",
      "Epoch [17/60], Iter [474/633], LR: 0.005000, Loss: 3.5894, top1: 15.6250\n",
      "Epoch [17/60], Iter [475/633], LR: 0.005000, Loss: 3.5258, top1: 21.8750\n",
      "Epoch [17/60], Iter [476/633], LR: 0.005000, Loss: 3.5506, top1: 14.0625\n",
      "Epoch [17/60], Iter [477/633], LR: 0.005000, Loss: 3.5592, top1: 17.1875\n",
      "Epoch [17/60], Iter [478/633], LR: 0.005000, Loss: 3.5524, top1: 14.0625\n",
      "Epoch [17/60], Iter [479/633], LR: 0.005000, Loss: 3.5439, top1: 20.3125\n",
      "Epoch [17/60], Iter [480/633], LR: 0.005000, Loss: 3.5581, top1: 15.6250\n",
      "Epoch [17/60], Iter [481/633], LR: 0.005000, Loss: 3.5302, top1: 17.1875\n",
      "Epoch [17/60], Iter [482/633], LR: 0.005000, Loss: 3.5553, top1: 9.3750\n",
      "Epoch [17/60], Iter [483/633], LR: 0.005000, Loss: 3.5816, top1: 15.6250\n",
      "Epoch [17/60], Iter [484/633], LR: 0.005000, Loss: 3.5627, top1: 17.1875\n",
      "Epoch [17/60], Iter [485/633], LR: 0.005000, Loss: 3.5286, top1: 17.1875\n",
      "Epoch [17/60], Iter [486/633], LR: 0.005000, Loss: 3.5778, top1: 12.5000\n",
      "Epoch [17/60], Iter [487/633], LR: 0.005000, Loss: 3.5664, top1: 14.0625\n",
      "Epoch [17/60], Iter [488/633], LR: 0.005000, Loss: 3.5350, top1: 23.4375\n",
      "Epoch [17/60], Iter [489/633], LR: 0.005000, Loss: 3.5988, top1: 14.0625\n",
      "Epoch [17/60], Iter [490/633], LR: 0.005000, Loss: 3.5611, top1: 17.1875\n",
      "Epoch [17/60], Iter [491/633], LR: 0.005000, Loss: 3.6006, top1: 10.9375\n",
      "Epoch [17/60], Iter [492/633], LR: 0.005000, Loss: 3.5133, top1: 21.8750\n",
      "Epoch [17/60], Iter [493/633], LR: 0.005000, Loss: 3.5579, top1: 14.0625\n",
      "Epoch [17/60], Iter [494/633], LR: 0.005000, Loss: 3.5512, top1: 17.1875\n",
      "Epoch [17/60], Iter [495/633], LR: 0.005000, Loss: 3.5237, top1: 31.2500\n",
      "Epoch [17/60], Iter [496/633], LR: 0.005000, Loss: 3.5891, top1: 12.5000\n",
      "Epoch [17/60], Iter [497/633], LR: 0.005000, Loss: 3.5763, top1: 9.3750\n",
      "Epoch [17/60], Iter [498/633], LR: 0.005000, Loss: 3.5755, top1: 14.0625\n",
      "Epoch [17/60], Iter [499/633], LR: 0.005000, Loss: 3.5743, top1: 15.6250\n",
      "Epoch [17/60], Iter [500/633], LR: 0.005000, Loss: 3.5395, top1: 17.1875\n",
      "Epoch [17/60], Iter [501/633], LR: 0.005000, Loss: 3.5426, top1: 20.3125\n",
      "Epoch [17/60], Iter [502/633], LR: 0.005000, Loss: 3.5699, top1: 15.6250\n",
      "Epoch [17/60], Iter [503/633], LR: 0.005000, Loss: 3.5843, top1: 12.5000\n",
      "Epoch [17/60], Iter [504/633], LR: 0.005000, Loss: 3.5475, top1: 14.0625\n",
      "Epoch [17/60], Iter [505/633], LR: 0.005000, Loss: 3.6145, top1: 15.6250\n",
      "Epoch [17/60], Iter [506/633], LR: 0.005000, Loss: 3.5887, top1: 9.3750\n",
      "Epoch [17/60], Iter [507/633], LR: 0.005000, Loss: 3.5821, top1: 12.5000\n",
      "Epoch [17/60], Iter [508/633], LR: 0.005000, Loss: 3.5166, top1: 21.8750\n",
      "Epoch [17/60], Iter [509/633], LR: 0.005000, Loss: 3.5498, top1: 17.1875\n",
      "Epoch [17/60], Iter [510/633], LR: 0.005000, Loss: 3.5341, top1: 21.8750\n",
      "Epoch [17/60], Iter [511/633], LR: 0.005000, Loss: 3.5480, top1: 17.1875\n",
      "Epoch [17/60], Iter [512/633], LR: 0.005000, Loss: 3.5725, top1: 17.1875\n",
      "Epoch [17/60], Iter [513/633], LR: 0.005000, Loss: 3.5706, top1: 17.1875\n",
      "Epoch [17/60], Iter [514/633], LR: 0.005000, Loss: 3.5731, top1: 14.0625\n",
      "Epoch [17/60], Iter [515/633], LR: 0.005000, Loss: 3.5403, top1: 20.3125\n",
      "Epoch [17/60], Iter [516/633], LR: 0.005000, Loss: 3.5478, top1: 18.7500\n",
      "Epoch [17/60], Iter [517/633], LR: 0.005000, Loss: 3.5676, top1: 17.1875\n",
      "Epoch [17/60], Iter [518/633], LR: 0.005000, Loss: 3.5767, top1: 14.0625\n",
      "Epoch [17/60], Iter [519/633], LR: 0.005000, Loss: 3.6057, top1: 18.7500\n",
      "Epoch [17/60], Iter [520/633], LR: 0.005000, Loss: 3.5778, top1: 12.5000\n",
      "Epoch [17/60], Iter [521/633], LR: 0.005000, Loss: 3.5621, top1: 23.4375\n",
      "Epoch [17/60], Iter [522/633], LR: 0.005000, Loss: 3.5951, top1: 10.9375\n",
      "Epoch [17/60], Iter [523/633], LR: 0.005000, Loss: 3.5769, top1: 20.3125\n",
      "Epoch [17/60], Iter [524/633], LR: 0.005000, Loss: 3.6247, top1: 12.5000\n",
      "Epoch [17/60], Iter [525/633], LR: 0.005000, Loss: 3.5920, top1: 12.5000\n",
      "Epoch [17/60], Iter [526/633], LR: 0.005000, Loss: 3.5537, top1: 14.0625\n",
      "Epoch [17/60], Iter [527/633], LR: 0.005000, Loss: 3.5824, top1: 14.0625\n",
      "Epoch [17/60], Iter [528/633], LR: 0.005000, Loss: 3.5489, top1: 18.7500\n",
      "Epoch [17/60], Iter [529/633], LR: 0.005000, Loss: 3.5747, top1: 14.0625\n",
      "Epoch [17/60], Iter [530/633], LR: 0.005000, Loss: 3.5823, top1: 10.9375\n",
      "Epoch [17/60], Iter [531/633], LR: 0.005000, Loss: 3.5794, top1: 15.6250\n",
      "Epoch [17/60], Iter [532/633], LR: 0.005000, Loss: 3.5646, top1: 10.9375\n",
      "Epoch [17/60], Iter [533/633], LR: 0.005000, Loss: 3.5465, top1: 23.4375\n",
      "Epoch [17/60], Iter [534/633], LR: 0.005000, Loss: 3.5608, top1: 17.1875\n",
      "Epoch [17/60], Iter [535/633], LR: 0.005000, Loss: 3.5284, top1: 15.6250\n",
      "Epoch [17/60], Iter [536/633], LR: 0.005000, Loss: 3.5841, top1: 14.0625\n",
      "Epoch [17/60], Iter [537/633], LR: 0.005000, Loss: 3.5847, top1: 15.6250\n",
      "Epoch [17/60], Iter [538/633], LR: 0.005000, Loss: 3.6037, top1: 15.6250\n",
      "Epoch [17/60], Iter [539/633], LR: 0.005000, Loss: 3.5583, top1: 15.6250\n",
      "Epoch [17/60], Iter [540/633], LR: 0.005000, Loss: 3.5995, top1: 14.0625\n",
      "Epoch [17/60], Iter [541/633], LR: 0.005000, Loss: 3.5557, top1: 14.0625\n",
      "Epoch [17/60], Iter [542/633], LR: 0.005000, Loss: 3.5674, top1: 20.3125\n",
      "Epoch [17/60], Iter [543/633], LR: 0.005000, Loss: 3.5426, top1: 23.4375\n",
      "Epoch [17/60], Iter [544/633], LR: 0.005000, Loss: 3.5777, top1: 15.6250\n",
      "Epoch [17/60], Iter [545/633], LR: 0.005000, Loss: 3.6082, top1: 6.2500\n",
      "Epoch [17/60], Iter [546/633], LR: 0.005000, Loss: 3.4849, top1: 25.0000\n",
      "Epoch [17/60], Iter [547/633], LR: 0.005000, Loss: 3.5451, top1: 18.7500\n",
      "Epoch [17/60], Iter [548/633], LR: 0.005000, Loss: 3.5809, top1: 15.6250\n",
      "Epoch [17/60], Iter [549/633], LR: 0.005000, Loss: 3.4970, top1: 32.8125\n",
      "Epoch [17/60], Iter [550/633], LR: 0.005000, Loss: 3.5743, top1: 15.6250\n",
      "Epoch [17/60], Iter [551/633], LR: 0.005000, Loss: 3.5565, top1: 14.0625\n",
      "Epoch [17/60], Iter [552/633], LR: 0.005000, Loss: 3.5695, top1: 15.6250\n",
      "Epoch [17/60], Iter [553/633], LR: 0.005000, Loss: 3.5740, top1: 15.6250\n",
      "Epoch [17/60], Iter [554/633], LR: 0.005000, Loss: 3.5720, top1: 14.0625\n",
      "Epoch [17/60], Iter [555/633], LR: 0.005000, Loss: 3.4881, top1: 28.1250\n",
      "Epoch [17/60], Iter [556/633], LR: 0.005000, Loss: 3.5253, top1: 17.1875\n",
      "Epoch [17/60], Iter [557/633], LR: 0.005000, Loss: 3.5938, top1: 14.0625\n",
      "Epoch [17/60], Iter [558/633], LR: 0.005000, Loss: 3.5566, top1: 17.1875\n",
      "Epoch [17/60], Iter [559/633], LR: 0.005000, Loss: 3.6023, top1: 7.8125\n",
      "Epoch [17/60], Iter [560/633], LR: 0.005000, Loss: 3.6234, top1: 7.8125\n",
      "Epoch [17/60], Iter [561/633], LR: 0.005000, Loss: 3.5974, top1: 10.9375\n",
      "Epoch [17/60], Iter [562/633], LR: 0.005000, Loss: 3.5598, top1: 12.5000\n",
      "Epoch [17/60], Iter [563/633], LR: 0.005000, Loss: 3.5252, top1: 17.1875\n",
      "Epoch [17/60], Iter [564/633], LR: 0.005000, Loss: 3.5546, top1: 17.1875\n",
      "Epoch [17/60], Iter [565/633], LR: 0.005000, Loss: 3.6223, top1: 9.3750\n",
      "Epoch [17/60], Iter [566/633], LR: 0.005000, Loss: 3.5837, top1: 15.6250\n",
      "Epoch [17/60], Iter [567/633], LR: 0.005000, Loss: 3.6222, top1: 7.8125\n",
      "Epoch [17/60], Iter [568/633], LR: 0.005000, Loss: 3.5354, top1: 14.0625\n",
      "Epoch [17/60], Iter [569/633], LR: 0.005000, Loss: 3.5898, top1: 15.6250\n",
      "Epoch [17/60], Iter [570/633], LR: 0.005000, Loss: 3.5592, top1: 12.5000\n",
      "Epoch [17/60], Iter [571/633], LR: 0.005000, Loss: 3.6231, top1: 12.5000\n",
      "Epoch [17/60], Iter [572/633], LR: 0.005000, Loss: 3.5554, top1: 15.6250\n",
      "Epoch [17/60], Iter [573/633], LR: 0.005000, Loss: 3.5505, top1: 17.1875\n",
      "Epoch [17/60], Iter [574/633], LR: 0.005000, Loss: 3.5513, top1: 10.9375\n",
      "Epoch [17/60], Iter [575/633], LR: 0.005000, Loss: 3.5624, top1: 17.1875\n",
      "Epoch [17/60], Iter [576/633], LR: 0.005000, Loss: 3.6027, top1: 12.5000\n",
      "Epoch [17/60], Iter [577/633], LR: 0.005000, Loss: 3.5703, top1: 10.9375\n",
      "Epoch [17/60], Iter [578/633], LR: 0.005000, Loss: 3.5914, top1: 12.5000\n",
      "Epoch [17/60], Iter [579/633], LR: 0.005000, Loss: 3.5703, top1: 7.8125\n",
      "Epoch [17/60], Iter [580/633], LR: 0.005000, Loss: 3.5123, top1: 20.3125\n",
      "Epoch [17/60], Iter [581/633], LR: 0.005000, Loss: 3.5644, top1: 14.0625\n",
      "Epoch [17/60], Iter [582/633], LR: 0.005000, Loss: 3.5622, top1: 17.1875\n",
      "Epoch [17/60], Iter [583/633], LR: 0.005000, Loss: 3.5765, top1: 9.3750\n",
      "Epoch [17/60], Iter [584/633], LR: 0.005000, Loss: 3.5721, top1: 17.1875\n",
      "Epoch [17/60], Iter [585/633], LR: 0.005000, Loss: 3.5876, top1: 10.9375\n",
      "Epoch [17/60], Iter [586/633], LR: 0.005000, Loss: 3.5477, top1: 17.1875\n",
      "Epoch [17/60], Iter [587/633], LR: 0.005000, Loss: 3.5581, top1: 17.1875\n",
      "Epoch [17/60], Iter [588/633], LR: 0.005000, Loss: 3.6195, top1: 9.3750\n",
      "Epoch [17/60], Iter [589/633], LR: 0.005000, Loss: 3.5450, top1: 17.1875\n",
      "Epoch [17/60], Iter [590/633], LR: 0.005000, Loss: 3.5439, top1: 17.1875\n",
      "Epoch [17/60], Iter [591/633], LR: 0.005000, Loss: 3.5586, top1: 20.3125\n",
      "Epoch [17/60], Iter [592/633], LR: 0.005000, Loss: 3.5487, top1: 20.3125\n",
      "Epoch [17/60], Iter [593/633], LR: 0.005000, Loss: 3.5860, top1: 15.6250\n",
      "Epoch [17/60], Iter [594/633], LR: 0.005000, Loss: 3.5948, top1: 14.0625\n",
      "Epoch [17/60], Iter [595/633], LR: 0.005000, Loss: 3.6089, top1: 6.2500\n",
      "Epoch [17/60], Iter [596/633], LR: 0.005000, Loss: 3.5938, top1: 12.5000\n",
      "Epoch [17/60], Iter [597/633], LR: 0.005000, Loss: 3.5667, top1: 15.6250\n",
      "Epoch [17/60], Iter [598/633], LR: 0.005000, Loss: 3.5986, top1: 10.9375\n",
      "Epoch [17/60], Iter [599/633], LR: 0.005000, Loss: 3.5386, top1: 20.3125\n",
      "Epoch [17/60], Iter [600/633], LR: 0.005000, Loss: 3.5820, top1: 9.3750\n",
      "Epoch [17/60], Iter [601/633], LR: 0.005000, Loss: 3.6118, top1: 7.8125\n",
      "Epoch [17/60], Iter [602/633], LR: 0.005000, Loss: 3.5796, top1: 14.0625\n",
      "Epoch [17/60], Iter [603/633], LR: 0.005000, Loss: 3.5731, top1: 12.5000\n",
      "Epoch [17/60], Iter [604/633], LR: 0.005000, Loss: 3.5797, top1: 10.9375\n",
      "Epoch [17/60], Iter [605/633], LR: 0.005000, Loss: 3.5660, top1: 15.6250\n",
      "Epoch [17/60], Iter [606/633], LR: 0.005000, Loss: 3.5758, top1: 15.6250\n",
      "Epoch [17/60], Iter [607/633], LR: 0.005000, Loss: 3.5355, top1: 18.7500\n",
      "Epoch [17/60], Iter [608/633], LR: 0.005000, Loss: 3.5809, top1: 15.6250\n",
      "Epoch [17/60], Iter [609/633], LR: 0.005000, Loss: 3.5823, top1: 12.5000\n",
      "Epoch [17/60], Iter [610/633], LR: 0.005000, Loss: 3.5791, top1: 10.9375\n",
      "Epoch [17/60], Iter [611/633], LR: 0.005000, Loss: 3.5441, top1: 14.0625\n",
      "Epoch [17/60], Iter [612/633], LR: 0.005000, Loss: 3.6008, top1: 6.2500\n",
      "Epoch [17/60], Iter [613/633], LR: 0.005000, Loss: 3.5717, top1: 14.0625\n",
      "Epoch [17/60], Iter [614/633], LR: 0.005000, Loss: 3.5779, top1: 10.9375\n",
      "Epoch [17/60], Iter [615/633], LR: 0.005000, Loss: 3.5666, top1: 12.5000\n",
      "Epoch [17/60], Iter [616/633], LR: 0.005000, Loss: 3.5366, top1: 20.3125\n",
      "Epoch [17/60], Iter [617/633], LR: 0.005000, Loss: 3.6132, top1: 14.0625\n",
      "Epoch [17/60], Iter [618/633], LR: 0.005000, Loss: 3.5370, top1: 14.0625\n",
      "Epoch [17/60], Iter [619/633], LR: 0.005000, Loss: 3.5571, top1: 23.4375\n",
      "Epoch [17/60], Iter [620/633], LR: 0.005000, Loss: 3.5921, top1: 15.6250\n",
      "Epoch [17/60], Iter [621/633], LR: 0.005000, Loss: 3.5762, top1: 15.6250\n",
      "Epoch [17/60], Iter [622/633], LR: 0.005000, Loss: 3.5916, top1: 10.9375\n",
      "Epoch [17/60], Iter [623/633], LR: 0.005000, Loss: 3.5406, top1: 21.8750\n",
      "Epoch [17/60], Iter [624/633], LR: 0.005000, Loss: 3.5666, top1: 17.1875\n",
      "Epoch [17/60], Iter [625/633], LR: 0.005000, Loss: 3.5143, top1: 23.4375\n",
      "Epoch [17/60], Iter [626/633], LR: 0.005000, Loss: 3.6093, top1: 7.8125\n",
      "Epoch [17/60], Iter [627/633], LR: 0.005000, Loss: 3.5667, top1: 15.6250\n",
      "Epoch [17/60], Iter [628/633], LR: 0.005000, Loss: 3.5835, top1: 7.8125\n",
      "Epoch [17/60], Iter [629/633], LR: 0.005000, Loss: 3.5580, top1: 14.0625\n",
      "Epoch [17/60], Iter [630/633], LR: 0.005000, Loss: 3.6109, top1: 9.3750\n",
      "Epoch [17/60], Iter [631/633], LR: 0.005000, Loss: 3.5717, top1: 14.0625\n",
      "Epoch [17/60], Iter [632/633], LR: 0.005000, Loss: 3.5571, top1: 21.8750\n",
      "Epoch [17/60], Iter [633/633], LR: 0.005000, Loss: 3.5210, top1: 18.7500\n",
      "Epoch [17/60], Iter [634/633], LR: 0.005000, Loss: 3.5633, top1: 17.7419\n",
      "Epoch [17/60], Val_Loss: 3.5672, Val_top1: 14.8988, best_top1: 16.9674\n",
      "epoch time: 4.339649017651876 min\n",
      "Epoch [18/60], Iter [1/633], LR: 0.005000, Loss: 3.5948, top1: 12.5000\n",
      "Epoch [18/60], Iter [2/633], LR: 0.005000, Loss: 3.5439, top1: 15.6250\n",
      "Epoch [18/60], Iter [3/633], LR: 0.005000, Loss: 3.5638, top1: 14.0625\n",
      "Epoch [18/60], Iter [4/633], LR: 0.005000, Loss: 3.5948, top1: 10.9375\n",
      "Epoch [18/60], Iter [5/633], LR: 0.005000, Loss: 3.5495, top1: 18.7500\n",
      "Epoch [18/60], Iter [6/633], LR: 0.005000, Loss: 3.6189, top1: 9.3750\n",
      "Epoch [18/60], Iter [7/633], LR: 0.005000, Loss: 3.5773, top1: 14.0625\n",
      "Epoch [18/60], Iter [8/633], LR: 0.005000, Loss: 3.6345, top1: 6.2500\n",
      "Epoch [18/60], Iter [9/633], LR: 0.005000, Loss: 3.5914, top1: 14.0625\n",
      "Epoch [18/60], Iter [10/633], LR: 0.005000, Loss: 3.5497, top1: 18.7500\n",
      "Epoch [18/60], Iter [11/633], LR: 0.005000, Loss: 3.5874, top1: 18.7500\n",
      "Epoch [18/60], Iter [12/633], LR: 0.005000, Loss: 3.5697, top1: 17.1875\n",
      "Epoch [18/60], Iter [13/633], LR: 0.005000, Loss: 3.5672, top1: 17.1875\n",
      "Epoch [18/60], Iter [14/633], LR: 0.005000, Loss: 3.5366, top1: 21.8750\n",
      "Epoch [18/60], Iter [15/633], LR: 0.005000, Loss: 3.5943, top1: 15.6250\n",
      "Epoch [18/60], Iter [16/633], LR: 0.005000, Loss: 3.5547, top1: 23.4375\n",
      "Epoch [18/60], Iter [17/633], LR: 0.005000, Loss: 3.5932, top1: 6.2500\n",
      "Epoch [18/60], Iter [18/633], LR: 0.005000, Loss: 3.5493, top1: 18.7500\n",
      "Epoch [18/60], Iter [19/633], LR: 0.005000, Loss: 3.5542, top1: 18.7500\n",
      "Epoch [18/60], Iter [20/633], LR: 0.005000, Loss: 3.6008, top1: 10.9375\n",
      "Epoch [18/60], Iter [21/633], LR: 0.005000, Loss: 3.5891, top1: 12.5000\n",
      "Epoch [18/60], Iter [22/633], LR: 0.005000, Loss: 3.6257, top1: 7.8125\n",
      "Epoch [18/60], Iter [23/633], LR: 0.005000, Loss: 3.5786, top1: 10.9375\n",
      "Epoch [18/60], Iter [24/633], LR: 0.005000, Loss: 3.5684, top1: 12.5000\n",
      "Epoch [18/60], Iter [25/633], LR: 0.005000, Loss: 3.5610, top1: 17.1875\n",
      "Epoch [18/60], Iter [26/633], LR: 0.005000, Loss: 3.6329, top1: 9.3750\n",
      "Epoch [18/60], Iter [27/633], LR: 0.005000, Loss: 3.5627, top1: 20.3125\n",
      "Epoch [18/60], Iter [28/633], LR: 0.005000, Loss: 3.6195, top1: 10.9375\n",
      "Epoch [18/60], Iter [29/633], LR: 0.005000, Loss: 3.5571, top1: 18.7500\n",
      "Epoch [18/60], Iter [30/633], LR: 0.005000, Loss: 3.5478, top1: 15.6250\n",
      "Epoch [18/60], Iter [31/633], LR: 0.005000, Loss: 3.6004, top1: 10.9375\n",
      "Epoch [18/60], Iter [32/633], LR: 0.005000, Loss: 3.5818, top1: 18.7500\n",
      "Epoch [18/60], Iter [33/633], LR: 0.005000, Loss: 3.5916, top1: 14.0625\n",
      "Epoch [18/60], Iter [34/633], LR: 0.005000, Loss: 3.5843, top1: 14.0625\n",
      "Epoch [18/60], Iter [35/633], LR: 0.005000, Loss: 3.5690, top1: 10.9375\n",
      "Epoch [18/60], Iter [36/633], LR: 0.005000, Loss: 3.5950, top1: 14.0625\n",
      "Epoch [18/60], Iter [37/633], LR: 0.005000, Loss: 3.5994, top1: 12.5000\n",
      "Epoch [18/60], Iter [38/633], LR: 0.005000, Loss: 3.5263, top1: 18.7500\n",
      "Epoch [18/60], Iter [39/633], LR: 0.005000, Loss: 3.6073, top1: 10.9375\n",
      "Epoch [18/60], Iter [40/633], LR: 0.005000, Loss: 3.5946, top1: 4.6875\n",
      "Epoch [18/60], Iter [41/633], LR: 0.005000, Loss: 3.5679, top1: 10.9375\n",
      "Epoch [18/60], Iter [42/633], LR: 0.005000, Loss: 3.5689, top1: 15.6250\n",
      "Epoch [18/60], Iter [43/633], LR: 0.005000, Loss: 3.5980, top1: 18.7500\n",
      "Epoch [18/60], Iter [44/633], LR: 0.005000, Loss: 3.5963, top1: 10.9375\n",
      "Epoch [18/60], Iter [45/633], LR: 0.005000, Loss: 3.5871, top1: 10.9375\n",
      "Epoch [18/60], Iter [46/633], LR: 0.005000, Loss: 3.5586, top1: 15.6250\n",
      "Epoch [18/60], Iter [47/633], LR: 0.005000, Loss: 3.5628, top1: 17.1875\n",
      "Epoch [18/60], Iter [48/633], LR: 0.005000, Loss: 3.5880, top1: 14.0625\n",
      "Epoch [18/60], Iter [49/633], LR: 0.005000, Loss: 3.5738, top1: 12.5000\n",
      "Epoch [18/60], Iter [50/633], LR: 0.005000, Loss: 3.5537, top1: 18.7500\n",
      "Epoch [18/60], Iter [51/633], LR: 0.005000, Loss: 3.5466, top1: 15.6250\n",
      "Epoch [18/60], Iter [52/633], LR: 0.005000, Loss: 3.5559, top1: 14.0625\n",
      "Epoch [18/60], Iter [53/633], LR: 0.005000, Loss: 3.5764, top1: 15.6250\n",
      "Epoch [18/60], Iter [54/633], LR: 0.005000, Loss: 3.5598, top1: 18.7500\n",
      "Epoch [18/60], Iter [55/633], LR: 0.005000, Loss: 3.5397, top1: 20.3125\n",
      "Epoch [18/60], Iter [56/633], LR: 0.005000, Loss: 3.5678, top1: 15.6250\n",
      "Epoch [18/60], Iter [57/633], LR: 0.005000, Loss: 3.5994, top1: 17.1875\n",
      "Epoch [18/60], Iter [58/633], LR: 0.005000, Loss: 3.5502, top1: 18.7500\n",
      "Epoch [18/60], Iter [59/633], LR: 0.005000, Loss: 3.6061, top1: 9.3750\n",
      "Epoch [18/60], Iter [60/633], LR: 0.005000, Loss: 3.5508, top1: 21.8750\n",
      "Epoch [18/60], Iter [61/633], LR: 0.005000, Loss: 3.5591, top1: 17.1875\n",
      "Epoch [18/60], Iter [62/633], LR: 0.005000, Loss: 3.5652, top1: 18.7500\n",
      "Epoch [18/60], Iter [63/633], LR: 0.005000, Loss: 3.5266, top1: 17.1875\n",
      "Epoch [18/60], Iter [64/633], LR: 0.005000, Loss: 3.6050, top1: 10.9375\n",
      "Epoch [18/60], Iter [65/633], LR: 0.005000, Loss: 3.6094, top1: 12.5000\n",
      "Epoch [18/60], Iter [66/633], LR: 0.005000, Loss: 3.5280, top1: 17.1875\n",
      "Epoch [18/60], Iter [67/633], LR: 0.005000, Loss: 3.5868, top1: 14.0625\n",
      "Epoch [18/60], Iter [68/633], LR: 0.005000, Loss: 3.6273, top1: 7.8125\n",
      "Epoch [18/60], Iter [69/633], LR: 0.005000, Loss: 3.6171, top1: 7.8125\n",
      "Epoch [18/60], Iter [70/633], LR: 0.005000, Loss: 3.5902, top1: 10.9375\n",
      "Epoch [18/60], Iter [71/633], LR: 0.005000, Loss: 3.5521, top1: 15.6250\n",
      "Epoch [18/60], Iter [72/633], LR: 0.005000, Loss: 3.5768, top1: 14.0625\n",
      "Epoch [18/60], Iter [73/633], LR: 0.005000, Loss: 3.5688, top1: 10.9375\n",
      "Epoch [18/60], Iter [74/633], LR: 0.005000, Loss: 3.5957, top1: 15.6250\n",
      "Epoch [18/60], Iter [75/633], LR: 0.005000, Loss: 3.5557, top1: 17.1875\n",
      "Epoch [18/60], Iter [76/633], LR: 0.005000, Loss: 3.5716, top1: 17.1875\n",
      "Epoch [18/60], Iter [77/633], LR: 0.005000, Loss: 3.5578, top1: 15.6250\n",
      "Epoch [18/60], Iter [78/633], LR: 0.005000, Loss: 3.5788, top1: 15.6250\n",
      "Epoch [18/60], Iter [79/633], LR: 0.005000, Loss: 3.5874, top1: 17.1875\n",
      "Epoch [18/60], Iter [80/633], LR: 0.005000, Loss: 3.5906, top1: 15.6250\n",
      "Epoch [18/60], Iter [81/633], LR: 0.005000, Loss: 3.5522, top1: 17.1875\n",
      "Epoch [18/60], Iter [82/633], LR: 0.005000, Loss: 3.6407, top1: 6.2500\n",
      "Epoch [18/60], Iter [83/633], LR: 0.005000, Loss: 3.5804, top1: 9.3750\n",
      "Epoch [18/60], Iter [84/633], LR: 0.005000, Loss: 3.5591, top1: 20.3125\n",
      "Epoch [18/60], Iter [85/633], LR: 0.005000, Loss: 3.5164, top1: 28.1250\n",
      "Epoch [18/60], Iter [86/633], LR: 0.005000, Loss: 3.5750, top1: 15.6250\n",
      "Epoch [18/60], Iter [87/633], LR: 0.005000, Loss: 3.5961, top1: 17.1875\n",
      "Epoch [18/60], Iter [88/633], LR: 0.005000, Loss: 3.5712, top1: 17.1875\n",
      "Epoch [18/60], Iter [89/633], LR: 0.005000, Loss: 3.5435, top1: 12.5000\n",
      "Epoch [18/60], Iter [90/633], LR: 0.005000, Loss: 3.5678, top1: 17.1875\n",
      "Epoch [18/60], Iter [91/633], LR: 0.005000, Loss: 3.6207, top1: 6.2500\n",
      "Epoch [18/60], Iter [92/633], LR: 0.005000, Loss: 3.5289, top1: 20.3125\n",
      "Epoch [18/60], Iter [93/633], LR: 0.005000, Loss: 3.5983, top1: 14.0625\n",
      "Epoch [18/60], Iter [94/633], LR: 0.005000, Loss: 3.6044, top1: 9.3750\n",
      "Epoch [18/60], Iter [95/633], LR: 0.005000, Loss: 3.5898, top1: 10.9375\n",
      "Epoch [18/60], Iter [96/633], LR: 0.005000, Loss: 3.5561, top1: 17.1875\n",
      "Epoch [18/60], Iter [97/633], LR: 0.005000, Loss: 3.6180, top1: 3.1250\n",
      "Epoch [18/60], Iter [98/633], LR: 0.005000, Loss: 3.5611, top1: 20.3125\n",
      "Epoch [18/60], Iter [99/633], LR: 0.005000, Loss: 3.5792, top1: 9.3750\n",
      "Epoch [18/60], Iter [100/633], LR: 0.005000, Loss: 3.5466, top1: 20.3125\n",
      "Epoch [18/60], Iter [101/633], LR: 0.005000, Loss: 3.5838, top1: 12.5000\n",
      "Epoch [18/60], Iter [102/633], LR: 0.005000, Loss: 3.4998, top1: 21.8750\n",
      "Epoch [18/60], Iter [103/633], LR: 0.005000, Loss: 3.5802, top1: 12.5000\n",
      "Epoch [18/60], Iter [104/633], LR: 0.005000, Loss: 3.6045, top1: 14.0625\n",
      "Epoch [18/60], Iter [105/633], LR: 0.005000, Loss: 3.6055, top1: 7.8125\n",
      "Epoch [18/60], Iter [106/633], LR: 0.005000, Loss: 3.5783, top1: 14.0625\n",
      "Epoch [18/60], Iter [107/633], LR: 0.005000, Loss: 3.5738, top1: 17.1875\n",
      "Epoch [18/60], Iter [108/633], LR: 0.005000, Loss: 3.5752, top1: 15.6250\n",
      "Epoch [18/60], Iter [109/633], LR: 0.005000, Loss: 3.6105, top1: 14.0625\n",
      "Epoch [18/60], Iter [110/633], LR: 0.005000, Loss: 3.5659, top1: 20.3125\n",
      "Epoch [18/60], Iter [111/633], LR: 0.005000, Loss: 3.6260, top1: 6.2500\n",
      "Epoch [18/60], Iter [112/633], LR: 0.005000, Loss: 3.5388, top1: 18.7500\n",
      "Epoch [18/60], Iter [113/633], LR: 0.005000, Loss: 3.6214, top1: 10.9375\n",
      "Epoch [18/60], Iter [114/633], LR: 0.005000, Loss: 3.5644, top1: 17.1875\n",
      "Epoch [18/60], Iter [115/633], LR: 0.005000, Loss: 3.5453, top1: 18.7500\n",
      "Epoch [18/60], Iter [116/633], LR: 0.005000, Loss: 3.5698, top1: 17.1875\n",
      "Epoch [18/60], Iter [117/633], LR: 0.005000, Loss: 3.5864, top1: 9.3750\n",
      "Epoch [18/60], Iter [118/633], LR: 0.005000, Loss: 3.5618, top1: 17.1875\n",
      "Epoch [18/60], Iter [119/633], LR: 0.005000, Loss: 3.5752, top1: 12.5000\n",
      "Epoch [18/60], Iter [120/633], LR: 0.005000, Loss: 3.5930, top1: 14.0625\n",
      "Epoch [18/60], Iter [121/633], LR: 0.005000, Loss: 3.5994, top1: 9.3750\n",
      "Epoch [18/60], Iter [122/633], LR: 0.005000, Loss: 3.5301, top1: 20.3125\n",
      "Epoch [18/60], Iter [123/633], LR: 0.005000, Loss: 3.5734, top1: 14.0625\n",
      "Epoch [18/60], Iter [124/633], LR: 0.005000, Loss: 3.5782, top1: 14.0625\n",
      "Epoch [18/60], Iter [125/633], LR: 0.005000, Loss: 3.5552, top1: 17.1875\n",
      "Epoch [18/60], Iter [126/633], LR: 0.005000, Loss: 3.5586, top1: 10.9375\n",
      "Epoch [18/60], Iter [127/633], LR: 0.005000, Loss: 3.5122, top1: 20.3125\n",
      "Epoch [18/60], Iter [128/633], LR: 0.005000, Loss: 3.5717, top1: 17.1875\n",
      "Epoch [18/60], Iter [129/633], LR: 0.005000, Loss: 3.5766, top1: 17.1875\n",
      "Epoch [18/60], Iter [130/633], LR: 0.005000, Loss: 3.5888, top1: 12.5000\n",
      "Epoch [18/60], Iter [131/633], LR: 0.005000, Loss: 3.5636, top1: 17.1875\n",
      "Epoch [18/60], Iter [132/633], LR: 0.005000, Loss: 3.5552, top1: 17.1875\n",
      "Epoch [18/60], Iter [133/633], LR: 0.005000, Loss: 3.5082, top1: 21.8750\n",
      "Epoch [18/60], Iter [134/633], LR: 0.005000, Loss: 3.5983, top1: 18.7500\n",
      "Epoch [18/60], Iter [135/633], LR: 0.005000, Loss: 3.5243, top1: 28.1250\n",
      "Epoch [18/60], Iter [136/633], LR: 0.005000, Loss: 3.5797, top1: 14.0625\n",
      "Epoch [18/60], Iter [137/633], LR: 0.005000, Loss: 3.6186, top1: 12.5000\n",
      "Epoch [18/60], Iter [138/633], LR: 0.005000, Loss: 3.5701, top1: 12.5000\n",
      "Epoch [18/60], Iter [139/633], LR: 0.005000, Loss: 3.5794, top1: 15.6250\n",
      "Epoch [18/60], Iter [140/633], LR: 0.005000, Loss: 3.5892, top1: 14.0625\n",
      "Epoch [18/60], Iter [141/633], LR: 0.005000, Loss: 3.5720, top1: 17.1875\n",
      "Epoch [18/60], Iter [142/633], LR: 0.005000, Loss: 3.5963, top1: 10.9375\n",
      "Epoch [18/60], Iter [143/633], LR: 0.005000, Loss: 3.5494, top1: 17.1875\n",
      "Epoch [18/60], Iter [144/633], LR: 0.005000, Loss: 3.5921, top1: 12.5000\n",
      "Epoch [18/60], Iter [145/633], LR: 0.005000, Loss: 3.5782, top1: 12.5000\n",
      "Epoch [18/60], Iter [146/633], LR: 0.005000, Loss: 3.5825, top1: 17.1875\n",
      "Epoch [18/60], Iter [147/633], LR: 0.005000, Loss: 3.6049, top1: 10.9375\n",
      "Epoch [18/60], Iter [148/633], LR: 0.005000, Loss: 3.5712, top1: 12.5000\n",
      "Epoch [18/60], Iter [149/633], LR: 0.005000, Loss: 3.5855, top1: 18.7500\n",
      "Epoch [18/60], Iter [150/633], LR: 0.005000, Loss: 3.5588, top1: 17.1875\n",
      "Epoch [18/60], Iter [151/633], LR: 0.005000, Loss: 3.5488, top1: 15.6250\n",
      "Epoch [18/60], Iter [152/633], LR: 0.005000, Loss: 3.5478, top1: 17.1875\n",
      "Epoch [18/60], Iter [153/633], LR: 0.005000, Loss: 3.5989, top1: 9.3750\n",
      "Epoch [18/60], Iter [154/633], LR: 0.005000, Loss: 3.4992, top1: 23.4375\n",
      "Epoch [18/60], Iter [155/633], LR: 0.005000, Loss: 3.5727, top1: 15.6250\n",
      "Epoch [18/60], Iter [156/633], LR: 0.005000, Loss: 3.5735, top1: 20.3125\n",
      "Epoch [18/60], Iter [157/633], LR: 0.005000, Loss: 3.5534, top1: 15.6250\n",
      "Epoch [18/60], Iter [158/633], LR: 0.005000, Loss: 3.5587, top1: 14.0625\n",
      "Epoch [18/60], Iter [159/633], LR: 0.005000, Loss: 3.5375, top1: 17.1875\n",
      "Epoch [18/60], Iter [160/633], LR: 0.005000, Loss: 3.5407, top1: 18.7500\n",
      "Epoch [18/60], Iter [161/633], LR: 0.005000, Loss: 3.5464, top1: 12.5000\n",
      "Epoch [18/60], Iter [162/633], LR: 0.005000, Loss: 3.5805, top1: 20.3125\n",
      "Epoch [18/60], Iter [163/633], LR: 0.005000, Loss: 3.6537, top1: 7.8125\n",
      "Epoch [18/60], Iter [164/633], LR: 0.005000, Loss: 3.5865, top1: 10.9375\n",
      "Epoch [18/60], Iter [165/633], LR: 0.005000, Loss: 3.5975, top1: 12.5000\n",
      "Epoch [18/60], Iter [166/633], LR: 0.005000, Loss: 3.5913, top1: 6.2500\n",
      "Epoch [18/60], Iter [167/633], LR: 0.005000, Loss: 3.5796, top1: 18.7500\n",
      "Epoch [18/60], Iter [168/633], LR: 0.005000, Loss: 3.5891, top1: 18.7500\n",
      "Epoch [18/60], Iter [169/633], LR: 0.005000, Loss: 3.5448, top1: 21.8750\n",
      "Epoch [18/60], Iter [170/633], LR: 0.005000, Loss: 3.5420, top1: 20.3125\n",
      "Epoch [18/60], Iter [171/633], LR: 0.005000, Loss: 3.5776, top1: 10.9375\n",
      "Epoch [18/60], Iter [172/633], LR: 0.005000, Loss: 3.5564, top1: 20.3125\n",
      "Epoch [18/60], Iter [173/633], LR: 0.005000, Loss: 3.5347, top1: 23.4375\n",
      "Epoch [18/60], Iter [174/633], LR: 0.005000, Loss: 3.5422, top1: 14.0625\n",
      "Epoch [18/60], Iter [175/633], LR: 0.005000, Loss: 3.5644, top1: 18.7500\n",
      "Epoch [18/60], Iter [176/633], LR: 0.005000, Loss: 3.5846, top1: 10.9375\n",
      "Epoch [18/60], Iter [177/633], LR: 0.005000, Loss: 3.5575, top1: 20.3125\n",
      "Epoch [18/60], Iter [178/633], LR: 0.005000, Loss: 3.5772, top1: 17.1875\n",
      "Epoch [18/60], Iter [179/633], LR: 0.005000, Loss: 3.6169, top1: 10.9375\n",
      "Epoch [18/60], Iter [180/633], LR: 0.005000, Loss: 3.5617, top1: 14.0625\n",
      "Epoch [18/60], Iter [181/633], LR: 0.005000, Loss: 3.5219, top1: 25.0000\n",
      "Epoch [18/60], Iter [182/633], LR: 0.005000, Loss: 3.5840, top1: 10.9375\n",
      "Epoch [18/60], Iter [183/633], LR: 0.005000, Loss: 3.5391, top1: 23.4375\n",
      "Epoch [18/60], Iter [184/633], LR: 0.005000, Loss: 3.5320, top1: 18.7500\n",
      "Epoch [18/60], Iter [185/633], LR: 0.005000, Loss: 3.5365, top1: 17.1875\n",
      "Epoch [18/60], Iter [186/633], LR: 0.005000, Loss: 3.5713, top1: 9.3750\n",
      "Epoch [18/60], Iter [187/633], LR: 0.005000, Loss: 3.5620, top1: 14.0625\n",
      "Epoch [18/60], Iter [188/633], LR: 0.005000, Loss: 3.5301, top1: 17.1875\n",
      "Epoch [18/60], Iter [189/633], LR: 0.005000, Loss: 3.5036, top1: 20.3125\n",
      "Epoch [18/60], Iter [190/633], LR: 0.005000, Loss: 3.5717, top1: 9.3750\n",
      "Epoch [18/60], Iter [191/633], LR: 0.005000, Loss: 3.5101, top1: 18.7500\n",
      "Epoch [18/60], Iter [192/633], LR: 0.005000, Loss: 3.5922, top1: 14.0625\n",
      "Epoch [18/60], Iter [193/633], LR: 0.005000, Loss: 3.5468, top1: 14.0625\n",
      "Epoch [18/60], Iter [194/633], LR: 0.005000, Loss: 3.5671, top1: 15.6250\n",
      "Epoch [18/60], Iter [195/633], LR: 0.005000, Loss: 3.5628, top1: 17.1875\n",
      "Epoch [18/60], Iter [196/633], LR: 0.005000, Loss: 3.5423, top1: 17.1875\n",
      "Epoch [18/60], Iter [197/633], LR: 0.005000, Loss: 3.5743, top1: 18.7500\n",
      "Epoch [18/60], Iter [198/633], LR: 0.005000, Loss: 3.5710, top1: 10.9375\n",
      "Epoch [18/60], Iter [199/633], LR: 0.005000, Loss: 3.5759, top1: 15.6250\n",
      "Epoch [18/60], Iter [200/633], LR: 0.005000, Loss: 3.5786, top1: 17.1875\n",
      "Epoch [18/60], Iter [201/633], LR: 0.005000, Loss: 3.5532, top1: 17.1875\n",
      "Epoch [18/60], Iter [202/633], LR: 0.005000, Loss: 3.5300, top1: 18.7500\n",
      "Epoch [18/60], Iter [203/633], LR: 0.005000, Loss: 3.5113, top1: 23.4375\n",
      "Epoch [18/60], Iter [204/633], LR: 0.005000, Loss: 3.5759, top1: 15.6250\n",
      "Epoch [18/60], Iter [205/633], LR: 0.005000, Loss: 3.5911, top1: 12.5000\n",
      "Epoch [18/60], Iter [206/633], LR: 0.005000, Loss: 3.5611, top1: 14.0625\n",
      "Epoch [18/60], Iter [207/633], LR: 0.005000, Loss: 3.5500, top1: 17.1875\n",
      "Epoch [18/60], Iter [208/633], LR: 0.005000, Loss: 3.5872, top1: 7.8125\n",
      "Epoch [18/60], Iter [209/633], LR: 0.005000, Loss: 3.5819, top1: 12.5000\n",
      "Epoch [18/60], Iter [210/633], LR: 0.005000, Loss: 3.5733, top1: 10.9375\n",
      "Epoch [18/60], Iter [211/633], LR: 0.005000, Loss: 3.5670, top1: 14.0625\n",
      "Epoch [18/60], Iter [212/633], LR: 0.005000, Loss: 3.5858, top1: 12.5000\n",
      "Epoch [18/60], Iter [213/633], LR: 0.005000, Loss: 3.5720, top1: 14.0625\n",
      "Epoch [18/60], Iter [214/633], LR: 0.005000, Loss: 3.5777, top1: 14.0625\n",
      "Epoch [18/60], Iter [215/633], LR: 0.005000, Loss: 3.5997, top1: 18.7500\n",
      "Epoch [18/60], Iter [216/633], LR: 0.005000, Loss: 3.4949, top1: 21.8750\n",
      "Epoch [18/60], Iter [217/633], LR: 0.005000, Loss: 3.5809, top1: 12.5000\n",
      "Epoch [18/60], Iter [218/633], LR: 0.005000, Loss: 3.5390, top1: 17.1875\n",
      "Epoch [18/60], Iter [219/633], LR: 0.005000, Loss: 3.5344, top1: 20.3125\n",
      "Epoch [18/60], Iter [220/633], LR: 0.005000, Loss: 3.5605, top1: 18.7500\n",
      "Epoch [18/60], Iter [221/633], LR: 0.005000, Loss: 3.5630, top1: 15.6250\n",
      "Epoch [18/60], Iter [222/633], LR: 0.005000, Loss: 3.5636, top1: 10.9375\n",
      "Epoch [18/60], Iter [223/633], LR: 0.005000, Loss: 3.5926, top1: 10.9375\n",
      "Epoch [18/60], Iter [224/633], LR: 0.005000, Loss: 3.5811, top1: 17.1875\n",
      "Epoch [18/60], Iter [225/633], LR: 0.005000, Loss: 3.6118, top1: 7.8125\n",
      "Epoch [18/60], Iter [226/633], LR: 0.005000, Loss: 3.5847, top1: 14.0625\n",
      "Epoch [18/60], Iter [227/633], LR: 0.005000, Loss: 3.5921, top1: 15.6250\n",
      "Epoch [18/60], Iter [228/633], LR: 0.005000, Loss: 3.5669, top1: 15.6250\n",
      "Epoch [18/60], Iter [229/633], LR: 0.005000, Loss: 3.5311, top1: 18.7500\n",
      "Epoch [18/60], Iter [230/633], LR: 0.005000, Loss: 3.5725, top1: 12.5000\n",
      "Epoch [18/60], Iter [231/633], LR: 0.005000, Loss: 3.5828, top1: 21.8750\n",
      "Epoch [18/60], Iter [232/633], LR: 0.005000, Loss: 3.5987, top1: 14.0625\n",
      "Epoch [18/60], Iter [233/633], LR: 0.005000, Loss: 3.5626, top1: 17.1875\n",
      "Epoch [18/60], Iter [234/633], LR: 0.005000, Loss: 3.5633, top1: 18.7500\n",
      "Epoch [18/60], Iter [235/633], LR: 0.005000, Loss: 3.5796, top1: 12.5000\n",
      "Epoch [18/60], Iter [236/633], LR: 0.005000, Loss: 3.5705, top1: 15.6250\n",
      "Epoch [18/60], Iter [237/633], LR: 0.005000, Loss: 3.5480, top1: 21.8750\n",
      "Epoch [18/60], Iter [238/633], LR: 0.005000, Loss: 3.5067, top1: 23.4375\n",
      "Epoch [18/60], Iter [239/633], LR: 0.005000, Loss: 3.6239, top1: 14.0625\n",
      "Epoch [18/60], Iter [240/633], LR: 0.005000, Loss: 3.5747, top1: 14.0625\n",
      "Epoch [18/60], Iter [241/633], LR: 0.005000, Loss: 3.5474, top1: 20.3125\n",
      "Epoch [18/60], Iter [242/633], LR: 0.005000, Loss: 3.5752, top1: 17.1875\n",
      "Epoch [18/60], Iter [243/633], LR: 0.005000, Loss: 3.5378, top1: 23.4375\n",
      "Epoch [18/60], Iter [244/633], LR: 0.005000, Loss: 3.5900, top1: 7.8125\n",
      "Epoch [18/60], Iter [245/633], LR: 0.005000, Loss: 3.5948, top1: 17.1875\n",
      "Epoch [18/60], Iter [246/633], LR: 0.005000, Loss: 3.5298, top1: 17.1875\n",
      "Epoch [18/60], Iter [247/633], LR: 0.005000, Loss: 3.5890, top1: 15.6250\n",
      "Epoch [18/60], Iter [248/633], LR: 0.005000, Loss: 3.6083, top1: 12.5000\n",
      "Epoch [18/60], Iter [249/633], LR: 0.005000, Loss: 3.6026, top1: 10.9375\n",
      "Epoch [18/60], Iter [250/633], LR: 0.005000, Loss: 3.5790, top1: 10.9375\n",
      "Epoch [18/60], Iter [251/633], LR: 0.005000, Loss: 3.5628, top1: 14.0625\n",
      "Epoch [18/60], Iter [252/633], LR: 0.005000, Loss: 3.5661, top1: 12.5000\n",
      "Epoch [18/60], Iter [253/633], LR: 0.005000, Loss: 3.6166, top1: 6.2500\n",
      "Epoch [18/60], Iter [254/633], LR: 0.005000, Loss: 3.5870, top1: 14.0625\n",
      "Epoch [18/60], Iter [255/633], LR: 0.005000, Loss: 3.5880, top1: 15.6250\n",
      "Epoch [18/60], Iter [256/633], LR: 0.005000, Loss: 3.5544, top1: 23.4375\n",
      "Epoch [18/60], Iter [257/633], LR: 0.005000, Loss: 3.5953, top1: 12.5000\n",
      "Epoch [18/60], Iter [258/633], LR: 0.005000, Loss: 3.6248, top1: 6.2500\n",
      "Epoch [18/60], Iter [259/633], LR: 0.005000, Loss: 3.5590, top1: 15.6250\n",
      "Epoch [18/60], Iter [260/633], LR: 0.005000, Loss: 3.4918, top1: 21.8750\n",
      "Epoch [18/60], Iter [261/633], LR: 0.005000, Loss: 3.5715, top1: 12.5000\n",
      "Epoch [18/60], Iter [262/633], LR: 0.005000, Loss: 3.5840, top1: 14.0625\n",
      "Epoch [18/60], Iter [263/633], LR: 0.005000, Loss: 3.5161, top1: 23.4375\n",
      "Epoch [18/60], Iter [264/633], LR: 0.005000, Loss: 3.5793, top1: 18.7500\n",
      "Epoch [18/60], Iter [265/633], LR: 0.005000, Loss: 3.5766, top1: 10.9375\n",
      "Epoch [18/60], Iter [266/633], LR: 0.005000, Loss: 3.5873, top1: 14.0625\n",
      "Epoch [18/60], Iter [267/633], LR: 0.005000, Loss: 3.5617, top1: 18.7500\n",
      "Epoch [18/60], Iter [268/633], LR: 0.005000, Loss: 3.6191, top1: 7.8125\n",
      "Epoch [18/60], Iter [269/633], LR: 0.005000, Loss: 3.5745, top1: 15.6250\n",
      "Epoch [18/60], Iter [270/633], LR: 0.005000, Loss: 3.5704, top1: 15.6250\n",
      "Epoch [18/60], Iter [271/633], LR: 0.005000, Loss: 3.5678, top1: 18.7500\n",
      "Epoch [18/60], Iter [272/633], LR: 0.005000, Loss: 3.5584, top1: 18.7500\n",
      "Epoch [18/60], Iter [273/633], LR: 0.005000, Loss: 3.5817, top1: 15.6250\n",
      "Epoch [18/60], Iter [274/633], LR: 0.005000, Loss: 3.5642, top1: 18.7500\n",
      "Epoch [18/60], Iter [275/633], LR: 0.005000, Loss: 3.5522, top1: 15.6250\n",
      "Epoch [18/60], Iter [276/633], LR: 0.005000, Loss: 3.5987, top1: 14.0625\n",
      "Epoch [18/60], Iter [277/633], LR: 0.005000, Loss: 3.5216, top1: 23.4375\n",
      "Epoch [18/60], Iter [278/633], LR: 0.005000, Loss: 3.5450, top1: 25.0000\n",
      "Epoch [18/60], Iter [279/633], LR: 0.005000, Loss: 3.5952, top1: 10.9375\n",
      "Epoch [18/60], Iter [280/633], LR: 0.005000, Loss: 3.5941, top1: 9.3750\n",
      "Epoch [18/60], Iter [281/633], LR: 0.005000, Loss: 3.5921, top1: 15.6250\n",
      "Epoch [18/60], Iter [282/633], LR: 0.005000, Loss: 3.5041, top1: 21.8750\n",
      "Epoch [18/60], Iter [283/633], LR: 0.005000, Loss: 3.5805, top1: 14.0625\n",
      "Epoch [18/60], Iter [284/633], LR: 0.005000, Loss: 3.5134, top1: 15.6250\n",
      "Epoch [18/60], Iter [285/633], LR: 0.005000, Loss: 3.5397, top1: 18.7500\n",
      "Epoch [18/60], Iter [286/633], LR: 0.005000, Loss: 3.5709, top1: 14.0625\n",
      "Epoch [18/60], Iter [287/633], LR: 0.005000, Loss: 3.5697, top1: 17.1875\n",
      "Epoch [18/60], Iter [288/633], LR: 0.005000, Loss: 3.5708, top1: 14.0625\n",
      "Epoch [18/60], Iter [289/633], LR: 0.005000, Loss: 3.5358, top1: 20.3125\n",
      "Epoch [18/60], Iter [290/633], LR: 0.005000, Loss: 3.5058, top1: 20.3125\n",
      "Epoch [18/60], Iter [291/633], LR: 0.005000, Loss: 3.5466, top1: 20.3125\n",
      "Epoch [18/60], Iter [292/633], LR: 0.005000, Loss: 3.5487, top1: 17.1875\n",
      "Epoch [18/60], Iter [293/633], LR: 0.005000, Loss: 3.5082, top1: 21.8750\n",
      "Epoch [18/60], Iter [294/633], LR: 0.005000, Loss: 3.5086, top1: 23.4375\n",
      "Epoch [18/60], Iter [295/633], LR: 0.005000, Loss: 3.5486, top1: 18.7500\n",
      "Epoch [18/60], Iter [296/633], LR: 0.005000, Loss: 3.5176, top1: 15.6250\n",
      "Epoch [18/60], Iter [297/633], LR: 0.005000, Loss: 3.5458, top1: 14.0625\n",
      "Epoch [18/60], Iter [298/633], LR: 0.005000, Loss: 3.5335, top1: 21.8750\n",
      "Epoch [18/60], Iter [299/633], LR: 0.005000, Loss: 3.5978, top1: 12.5000\n",
      "Epoch [18/60], Iter [300/633], LR: 0.005000, Loss: 3.4997, top1: 20.3125\n",
      "Epoch [18/60], Iter [301/633], LR: 0.005000, Loss: 3.5623, top1: 14.0625\n",
      "Epoch [18/60], Iter [302/633], LR: 0.005000, Loss: 3.5848, top1: 12.5000\n",
      "Epoch [18/60], Iter [303/633], LR: 0.005000, Loss: 3.5672, top1: 14.0625\n",
      "Epoch [18/60], Iter [304/633], LR: 0.005000, Loss: 3.5681, top1: 15.6250\n",
      "Epoch [18/60], Iter [305/633], LR: 0.005000, Loss: 3.5594, top1: 18.7500\n",
      "Epoch [18/60], Iter [306/633], LR: 0.005000, Loss: 3.5370, top1: 18.7500\n",
      "Epoch [18/60], Iter [307/633], LR: 0.005000, Loss: 3.5540, top1: 15.6250\n",
      "Epoch [18/60], Iter [308/633], LR: 0.005000, Loss: 3.5148, top1: 20.3125\n",
      "Epoch [18/60], Iter [309/633], LR: 0.005000, Loss: 3.5998, top1: 12.5000\n",
      "Epoch [18/60], Iter [310/633], LR: 0.005000, Loss: 3.6223, top1: 14.0625\n",
      "Epoch [18/60], Iter [311/633], LR: 0.005000, Loss: 3.6044, top1: 14.0625\n",
      "Epoch [18/60], Iter [312/633], LR: 0.005000, Loss: 3.5738, top1: 10.9375\n",
      "Epoch [18/60], Iter [313/633], LR: 0.005000, Loss: 3.6109, top1: 6.2500\n",
      "Epoch [18/60], Iter [314/633], LR: 0.005000, Loss: 3.5270, top1: 21.8750\n",
      "Epoch [18/60], Iter [315/633], LR: 0.005000, Loss: 3.6068, top1: 10.9375\n",
      "Epoch [18/60], Iter [316/633], LR: 0.005000, Loss: 3.5754, top1: 15.6250\n",
      "Epoch [18/60], Iter [317/633], LR: 0.005000, Loss: 3.5858, top1: 12.5000\n",
      "Epoch [18/60], Iter [318/633], LR: 0.005000, Loss: 3.5553, top1: 14.0625\n",
      "Epoch [18/60], Iter [319/633], LR: 0.005000, Loss: 3.5374, top1: 23.4375\n",
      "Epoch [18/60], Iter [320/633], LR: 0.005000, Loss: 3.6015, top1: 10.9375\n",
      "Epoch [18/60], Iter [321/633], LR: 0.005000, Loss: 3.5649, top1: 14.0625\n",
      "Epoch [18/60], Iter [322/633], LR: 0.005000, Loss: 3.5360, top1: 17.1875\n",
      "Epoch [18/60], Iter [323/633], LR: 0.005000, Loss: 3.5814, top1: 7.8125\n",
      "Epoch [18/60], Iter [324/633], LR: 0.005000, Loss: 3.5493, top1: 25.0000\n",
      "Epoch [18/60], Iter [325/633], LR: 0.005000, Loss: 3.5766, top1: 15.6250\n",
      "Epoch [18/60], Iter [326/633], LR: 0.005000, Loss: 3.5288, top1: 17.1875\n",
      "Epoch [18/60], Iter [327/633], LR: 0.005000, Loss: 3.5201, top1: 21.8750\n",
      "Epoch [18/60], Iter [328/633], LR: 0.005000, Loss: 3.5539, top1: 12.5000\n",
      "Epoch [18/60], Iter [329/633], LR: 0.005000, Loss: 3.5305, top1: 17.1875\n",
      "Epoch [18/60], Iter [330/633], LR: 0.005000, Loss: 3.5593, top1: 18.7500\n",
      "Epoch [18/60], Iter [331/633], LR: 0.005000, Loss: 3.5629, top1: 7.8125\n",
      "Epoch [18/60], Iter [332/633], LR: 0.005000, Loss: 3.5058, top1: 23.4375\n",
      "Epoch [18/60], Iter [333/633], LR: 0.005000, Loss: 3.6117, top1: 7.8125\n",
      "Epoch [18/60], Iter [334/633], LR: 0.005000, Loss: 3.5414, top1: 20.3125\n",
      "Epoch [18/60], Iter [335/633], LR: 0.005000, Loss: 3.5608, top1: 14.0625\n",
      "Epoch [18/60], Iter [336/633], LR: 0.005000, Loss: 3.5568, top1: 10.9375\n",
      "Epoch [18/60], Iter [337/633], LR: 0.005000, Loss: 3.5440, top1: 14.0625\n",
      "Epoch [18/60], Iter [338/633], LR: 0.005000, Loss: 3.5391, top1: 21.8750\n",
      "Epoch [18/60], Iter [339/633], LR: 0.005000, Loss: 3.6011, top1: 10.9375\n",
      "Epoch [18/60], Iter [340/633], LR: 0.005000, Loss: 3.5372, top1: 15.6250\n",
      "Epoch [18/60], Iter [341/633], LR: 0.005000, Loss: 3.5644, top1: 20.3125\n",
      "Epoch [18/60], Iter [342/633], LR: 0.005000, Loss: 3.5093, top1: 26.5625\n",
      "Epoch [18/60], Iter [343/633], LR: 0.005000, Loss: 3.5798, top1: 17.1875\n",
      "Epoch [18/60], Iter [344/633], LR: 0.005000, Loss: 3.5126, top1: 25.0000\n",
      "Epoch [18/60], Iter [345/633], LR: 0.005000, Loss: 3.5647, top1: 15.6250\n",
      "Epoch [18/60], Iter [346/633], LR: 0.005000, Loss: 3.5398, top1: 23.4375\n",
      "Epoch [18/60], Iter [347/633], LR: 0.005000, Loss: 3.5942, top1: 17.1875\n",
      "Epoch [18/60], Iter [348/633], LR: 0.005000, Loss: 3.5565, top1: 15.6250\n",
      "Epoch [18/60], Iter [349/633], LR: 0.005000, Loss: 3.5364, top1: 18.7500\n",
      "Epoch [18/60], Iter [350/633], LR: 0.005000, Loss: 3.5304, top1: 25.0000\n",
      "Epoch [18/60], Iter [351/633], LR: 0.005000, Loss: 3.5463, top1: 18.7500\n",
      "Epoch [18/60], Iter [352/633], LR: 0.005000, Loss: 3.5813, top1: 17.1875\n",
      "Epoch [18/60], Iter [353/633], LR: 0.005000, Loss: 3.5633, top1: 10.9375\n",
      "Epoch [18/60], Iter [354/633], LR: 0.005000, Loss: 3.5728, top1: 17.1875\n",
      "Epoch [18/60], Iter [355/633], LR: 0.005000, Loss: 3.5694, top1: 17.1875\n",
      "Epoch [18/60], Iter [356/633], LR: 0.005000, Loss: 3.6110, top1: 15.6250\n",
      "Epoch [18/60], Iter [357/633], LR: 0.005000, Loss: 3.5722, top1: 12.5000\n",
      "Epoch [18/60], Iter [358/633], LR: 0.005000, Loss: 3.5924, top1: 12.5000\n",
      "Epoch [18/60], Iter [359/633], LR: 0.005000, Loss: 3.5724, top1: 15.6250\n",
      "Epoch [18/60], Iter [360/633], LR: 0.005000, Loss: 3.5867, top1: 17.1875\n",
      "Epoch [18/60], Iter [361/633], LR: 0.005000, Loss: 3.5772, top1: 9.3750\n",
      "Epoch [18/60], Iter [362/633], LR: 0.005000, Loss: 3.6222, top1: 14.0625\n",
      "Epoch [18/60], Iter [363/633], LR: 0.005000, Loss: 3.5500, top1: 17.1875\n",
      "Epoch [18/60], Iter [364/633], LR: 0.005000, Loss: 3.6013, top1: 12.5000\n",
      "Epoch [18/60], Iter [365/633], LR: 0.005000, Loss: 3.6164, top1: 12.5000\n",
      "Epoch [18/60], Iter [366/633], LR: 0.005000, Loss: 3.5868, top1: 14.0625\n",
      "Epoch [18/60], Iter [367/633], LR: 0.005000, Loss: 3.5625, top1: 17.1875\n",
      "Epoch [18/60], Iter [368/633], LR: 0.005000, Loss: 3.5538, top1: 9.3750\n",
      "Epoch [18/60], Iter [369/633], LR: 0.005000, Loss: 3.5630, top1: 17.1875\n",
      "Epoch [18/60], Iter [370/633], LR: 0.005000, Loss: 3.5486, top1: 17.1875\n",
      "Epoch [18/60], Iter [371/633], LR: 0.005000, Loss: 3.5752, top1: 15.6250\n",
      "Epoch [18/60], Iter [372/633], LR: 0.005000, Loss: 3.5575, top1: 17.1875\n",
      "Epoch [18/60], Iter [373/633], LR: 0.005000, Loss: 3.5671, top1: 17.1875\n",
      "Epoch [18/60], Iter [374/633], LR: 0.005000, Loss: 3.5659, top1: 15.6250\n",
      "Epoch [18/60], Iter [375/633], LR: 0.005000, Loss: 3.5781, top1: 14.0625\n",
      "Epoch [18/60], Iter [376/633], LR: 0.005000, Loss: 3.5628, top1: 14.0625\n",
      "Epoch [18/60], Iter [377/633], LR: 0.005000, Loss: 3.5588, top1: 12.5000\n",
      "Epoch [18/60], Iter [378/633], LR: 0.005000, Loss: 3.5717, top1: 12.5000\n",
      "Epoch [18/60], Iter [379/633], LR: 0.005000, Loss: 3.5535, top1: 14.0625\n",
      "Epoch [18/60], Iter [380/633], LR: 0.005000, Loss: 3.5572, top1: 18.7500\n",
      "Epoch [18/60], Iter [381/633], LR: 0.005000, Loss: 3.5636, top1: 10.9375\n",
      "Epoch [18/60], Iter [382/633], LR: 0.005000, Loss: 3.5614, top1: 12.5000\n",
      "Epoch [18/60], Iter [383/633], LR: 0.005000, Loss: 3.5582, top1: 15.6250\n",
      "Epoch [18/60], Iter [384/633], LR: 0.005000, Loss: 3.5305, top1: 20.3125\n",
      "Epoch [18/60], Iter [385/633], LR: 0.005000, Loss: 3.6068, top1: 15.6250\n",
      "Epoch [18/60], Iter [386/633], LR: 0.005000, Loss: 3.5770, top1: 12.5000\n",
      "Epoch [18/60], Iter [387/633], LR: 0.005000, Loss: 3.5889, top1: 14.0625\n",
      "Epoch [18/60], Iter [388/633], LR: 0.005000, Loss: 3.5878, top1: 17.1875\n",
      "Epoch [18/60], Iter [389/633], LR: 0.005000, Loss: 3.6060, top1: 6.2500\n",
      "Epoch [18/60], Iter [390/633], LR: 0.005000, Loss: 3.5773, top1: 12.5000\n",
      "Epoch [18/60], Iter [391/633], LR: 0.005000, Loss: 3.5252, top1: 18.7500\n",
      "Epoch [18/60], Iter [392/633], LR: 0.005000, Loss: 3.5860, top1: 9.3750\n",
      "Epoch [18/60], Iter [393/633], LR: 0.005000, Loss: 3.5552, top1: 17.1875\n",
      "Epoch [18/60], Iter [394/633], LR: 0.005000, Loss: 3.5708, top1: 10.9375\n",
      "Epoch [18/60], Iter [395/633], LR: 0.005000, Loss: 3.5306, top1: 18.7500\n",
      "Epoch [18/60], Iter [396/633], LR: 0.005000, Loss: 3.5729, top1: 18.7500\n",
      "Epoch [18/60], Iter [397/633], LR: 0.005000, Loss: 3.5653, top1: 15.6250\n",
      "Epoch [18/60], Iter [398/633], LR: 0.005000, Loss: 3.5707, top1: 14.0625\n",
      "Epoch [18/60], Iter [399/633], LR: 0.005000, Loss: 3.5562, top1: 14.0625\n",
      "Epoch [18/60], Iter [400/633], LR: 0.005000, Loss: 3.5663, top1: 12.5000\n",
      "Epoch [18/60], Iter [401/633], LR: 0.005000, Loss: 3.5851, top1: 12.5000\n",
      "Epoch [18/60], Iter [402/633], LR: 0.005000, Loss: 3.5934, top1: 15.6250\n",
      "Epoch [18/60], Iter [403/633], LR: 0.005000, Loss: 3.6224, top1: 6.2500\n",
      "Epoch [18/60], Iter [404/633], LR: 0.005000, Loss: 3.6028, top1: 10.9375\n",
      "Epoch [18/60], Iter [405/633], LR: 0.005000, Loss: 3.5281, top1: 21.8750\n",
      "Epoch [18/60], Iter [406/633], LR: 0.005000, Loss: 3.5426, top1: 17.1875\n",
      "Epoch [18/60], Iter [407/633], LR: 0.005000, Loss: 3.5307, top1: 21.8750\n",
      "Epoch [18/60], Iter [408/633], LR: 0.005000, Loss: 3.5591, top1: 15.6250\n",
      "Epoch [18/60], Iter [409/633], LR: 0.005000, Loss: 3.5817, top1: 14.0625\n",
      "Epoch [18/60], Iter [410/633], LR: 0.005000, Loss: 3.5404, top1: 18.7500\n",
      "Epoch [18/60], Iter [411/633], LR: 0.005000, Loss: 3.5650, top1: 10.9375\n",
      "Epoch [18/60], Iter [412/633], LR: 0.005000, Loss: 3.5889, top1: 14.0625\n",
      "Epoch [18/60], Iter [413/633], LR: 0.005000, Loss: 3.5164, top1: 18.7500\n",
      "Epoch [18/60], Iter [414/633], LR: 0.005000, Loss: 3.5559, top1: 14.0625\n",
      "Epoch [18/60], Iter [415/633], LR: 0.005000, Loss: 3.5862, top1: 10.9375\n",
      "Epoch [18/60], Iter [416/633], LR: 0.005000, Loss: 3.5156, top1: 23.4375\n",
      "Epoch [18/60], Iter [417/633], LR: 0.005000, Loss: 3.5949, top1: 14.0625\n",
      "Epoch [18/60], Iter [418/633], LR: 0.005000, Loss: 3.6000, top1: 9.3750\n",
      "Epoch [18/60], Iter [419/633], LR: 0.005000, Loss: 3.5229, top1: 23.4375\n",
      "Epoch [18/60], Iter [420/633], LR: 0.005000, Loss: 3.5767, top1: 10.9375\n",
      "Epoch [18/60], Iter [421/633], LR: 0.005000, Loss: 3.5468, top1: 18.7500\n",
      "Epoch [18/60], Iter [422/633], LR: 0.005000, Loss: 3.5339, top1: 12.5000\n",
      "Epoch [18/60], Iter [423/633], LR: 0.005000, Loss: 3.5623, top1: 17.1875\n",
      "Epoch [18/60], Iter [424/633], LR: 0.005000, Loss: 3.5364, top1: 14.0625\n",
      "Epoch [18/60], Iter [425/633], LR: 0.005000, Loss: 3.5619, top1: 20.3125\n",
      "Epoch [18/60], Iter [426/633], LR: 0.005000, Loss: 3.6022, top1: 14.0625\n",
      "Epoch [18/60], Iter [427/633], LR: 0.005000, Loss: 3.5430, top1: 17.1875\n",
      "Epoch [18/60], Iter [428/633], LR: 0.005000, Loss: 3.5519, top1: 18.7500\n",
      "Epoch [18/60], Iter [429/633], LR: 0.005000, Loss: 3.5364, top1: 25.0000\n",
      "Epoch [18/60], Iter [430/633], LR: 0.005000, Loss: 3.5743, top1: 17.1875\n",
      "Epoch [18/60], Iter [431/633], LR: 0.005000, Loss: 3.5977, top1: 12.5000\n",
      "Epoch [18/60], Iter [432/633], LR: 0.005000, Loss: 3.6151, top1: 9.3750\n",
      "Epoch [18/60], Iter [433/633], LR: 0.005000, Loss: 3.5731, top1: 15.6250\n",
      "Epoch [18/60], Iter [434/633], LR: 0.005000, Loss: 3.5471, top1: 17.1875\n",
      "Epoch [18/60], Iter [435/633], LR: 0.005000, Loss: 3.5824, top1: 12.5000\n",
      "Epoch [18/60], Iter [436/633], LR: 0.005000, Loss: 3.5551, top1: 23.4375\n",
      "Epoch [18/60], Iter [437/633], LR: 0.005000, Loss: 3.6237, top1: 9.3750\n",
      "Epoch [18/60], Iter [438/633], LR: 0.005000, Loss: 3.6203, top1: 14.0625\n",
      "Epoch [18/60], Iter [439/633], LR: 0.005000, Loss: 3.5585, top1: 14.0625\n",
      "Epoch [18/60], Iter [440/633], LR: 0.005000, Loss: 3.5742, top1: 9.3750\n",
      "Epoch [18/60], Iter [441/633], LR: 0.005000, Loss: 3.5645, top1: 18.7500\n",
      "Epoch [18/60], Iter [442/633], LR: 0.005000, Loss: 3.5275, top1: 15.6250\n",
      "Epoch [18/60], Iter [443/633], LR: 0.005000, Loss: 3.5487, top1: 10.9375\n",
      "Epoch [18/60], Iter [444/633], LR: 0.005000, Loss: 3.4978, top1: 23.4375\n",
      "Epoch [18/60], Iter [445/633], LR: 0.005000, Loss: 3.4963, top1: 20.3125\n",
      "Epoch [18/60], Iter [446/633], LR: 0.005000, Loss: 3.5754, top1: 14.0625\n",
      "Epoch [18/60], Iter [447/633], LR: 0.005000, Loss: 3.5716, top1: 18.7500\n",
      "Epoch [18/60], Iter [448/633], LR: 0.005000, Loss: 3.5331, top1: 20.3125\n",
      "Epoch [18/60], Iter [449/633], LR: 0.005000, Loss: 3.5503, top1: 17.1875\n",
      "Epoch [18/60], Iter [450/633], LR: 0.005000, Loss: 3.5813, top1: 14.0625\n",
      "Epoch [18/60], Iter [451/633], LR: 0.005000, Loss: 3.5566, top1: 18.7500\n",
      "Epoch [18/60], Iter [452/633], LR: 0.005000, Loss: 3.5759, top1: 10.9375\n",
      "Epoch [18/60], Iter [453/633], LR: 0.005000, Loss: 3.5400, top1: 23.4375\n",
      "Epoch [18/60], Iter [454/633], LR: 0.005000, Loss: 3.6396, top1: 7.8125\n",
      "Epoch [18/60], Iter [455/633], LR: 0.005000, Loss: 3.5194, top1: 18.7500\n",
      "Epoch [18/60], Iter [456/633], LR: 0.005000, Loss: 3.5256, top1: 23.4375\n",
      "Epoch [18/60], Iter [457/633], LR: 0.005000, Loss: 3.5770, top1: 15.6250\n",
      "Epoch [18/60], Iter [458/633], LR: 0.005000, Loss: 3.5607, top1: 12.5000\n",
      "Epoch [18/60], Iter [459/633], LR: 0.005000, Loss: 3.5485, top1: 18.7500\n",
      "Epoch [18/60], Iter [460/633], LR: 0.005000, Loss: 3.5510, top1: 20.3125\n",
      "Epoch [18/60], Iter [461/633], LR: 0.005000, Loss: 3.5709, top1: 18.7500\n",
      "Epoch [18/60], Iter [462/633], LR: 0.005000, Loss: 3.5636, top1: 15.6250\n",
      "Epoch [18/60], Iter [463/633], LR: 0.005000, Loss: 3.5789, top1: 18.7500\n",
      "Epoch [18/60], Iter [464/633], LR: 0.005000, Loss: 3.5905, top1: 12.5000\n",
      "Epoch [18/60], Iter [465/633], LR: 0.005000, Loss: 3.5560, top1: 21.8750\n",
      "Epoch [18/60], Iter [466/633], LR: 0.005000, Loss: 3.5359, top1: 21.8750\n",
      "Epoch [18/60], Iter [467/633], LR: 0.005000, Loss: 3.5883, top1: 15.6250\n",
      "Epoch [18/60], Iter [468/633], LR: 0.005000, Loss: 3.5721, top1: 17.1875\n",
      "Epoch [18/60], Iter [469/633], LR: 0.005000, Loss: 3.6161, top1: 9.3750\n",
      "Epoch [18/60], Iter [470/633], LR: 0.005000, Loss: 3.5792, top1: 17.1875\n",
      "Epoch [18/60], Iter [471/633], LR: 0.005000, Loss: 3.5423, top1: 25.0000\n",
      "Epoch [18/60], Iter [472/633], LR: 0.005000, Loss: 3.5928, top1: 14.0625\n",
      "Epoch [18/60], Iter [473/633], LR: 0.005000, Loss: 3.5786, top1: 9.3750\n",
      "Epoch [18/60], Iter [474/633], LR: 0.005000, Loss: 3.4819, top1: 25.0000\n",
      "Epoch [18/60], Iter [475/633], LR: 0.005000, Loss: 3.5388, top1: 20.3125\n",
      "Epoch [18/60], Iter [476/633], LR: 0.005000, Loss: 3.5612, top1: 15.6250\n",
      "Epoch [18/60], Iter [477/633], LR: 0.005000, Loss: 3.5619, top1: 14.0625\n",
      "Epoch [18/60], Iter [478/633], LR: 0.005000, Loss: 3.4790, top1: 21.8750\n",
      "Epoch [18/60], Iter [479/633], LR: 0.005000, Loss: 3.4973, top1: 25.0000\n",
      "Epoch [18/60], Iter [480/633], LR: 0.005000, Loss: 3.6004, top1: 14.0625\n",
      "Epoch [18/60], Iter [481/633], LR: 0.005000, Loss: 3.6317, top1: 10.9375\n",
      "Epoch [18/60], Iter [482/633], LR: 0.005000, Loss: 3.5787, top1: 12.5000\n",
      "Epoch [18/60], Iter [483/633], LR: 0.005000, Loss: 3.5264, top1: 26.5625\n",
      "Epoch [18/60], Iter [484/633], LR: 0.005000, Loss: 3.5488, top1: 12.5000\n",
      "Epoch [18/60], Iter [485/633], LR: 0.005000, Loss: 3.5344, top1: 20.3125\n",
      "Epoch [18/60], Iter [486/633], LR: 0.005000, Loss: 3.5622, top1: 18.7500\n",
      "Epoch [18/60], Iter [487/633], LR: 0.005000, Loss: 3.5606, top1: 15.6250\n",
      "Epoch [18/60], Iter [488/633], LR: 0.005000, Loss: 3.5861, top1: 15.6250\n",
      "Epoch [18/60], Iter [489/633], LR: 0.005000, Loss: 3.5617, top1: 15.6250\n",
      "Epoch [18/60], Iter [490/633], LR: 0.005000, Loss: 3.6174, top1: 12.5000\n",
      "Epoch [18/60], Iter [491/633], LR: 0.005000, Loss: 3.5813, top1: 12.5000\n",
      "Epoch [18/60], Iter [492/633], LR: 0.005000, Loss: 3.5796, top1: 14.0625\n",
      "Epoch [18/60], Iter [493/633], LR: 0.005000, Loss: 3.6056, top1: 17.1875\n",
      "Epoch [18/60], Iter [494/633], LR: 0.005000, Loss: 3.5930, top1: 18.7500\n",
      "Epoch [18/60], Iter [495/633], LR: 0.005000, Loss: 3.5433, top1: 12.5000\n",
      "Epoch [18/60], Iter [496/633], LR: 0.005000, Loss: 3.6031, top1: 12.5000\n",
      "Epoch [18/60], Iter [497/633], LR: 0.005000, Loss: 3.5420, top1: 21.8750\n",
      "Epoch [18/60], Iter [498/633], LR: 0.005000, Loss: 3.5509, top1: 14.0625\n",
      "Epoch [18/60], Iter [499/633], LR: 0.005000, Loss: 3.5864, top1: 12.5000\n",
      "Epoch [18/60], Iter [500/633], LR: 0.005000, Loss: 3.5558, top1: 17.1875\n",
      "Epoch [18/60], Iter [501/633], LR: 0.005000, Loss: 3.6019, top1: 9.3750\n",
      "Epoch [18/60], Iter [502/633], LR: 0.005000, Loss: 3.5609, top1: 14.0625\n",
      "Epoch [18/60], Iter [503/633], LR: 0.005000, Loss: 3.5783, top1: 17.1875\n",
      "Epoch [18/60], Iter [504/633], LR: 0.005000, Loss: 3.5406, top1: 25.0000\n",
      "Epoch [18/60], Iter [505/633], LR: 0.005000, Loss: 3.5913, top1: 14.0625\n",
      "Epoch [18/60], Iter [506/633], LR: 0.005000, Loss: 3.5506, top1: 10.9375\n",
      "Epoch [18/60], Iter [507/633], LR: 0.005000, Loss: 3.5732, top1: 9.3750\n",
      "Epoch [18/60], Iter [508/633], LR: 0.005000, Loss: 3.6056, top1: 10.9375\n",
      "Epoch [18/60], Iter [509/633], LR: 0.005000, Loss: 3.5651, top1: 10.9375\n",
      "Epoch [18/60], Iter [510/633], LR: 0.005000, Loss: 3.5399, top1: 14.0625\n",
      "Epoch [18/60], Iter [511/633], LR: 0.005000, Loss: 3.5364, top1: 21.8750\n",
      "Epoch [18/60], Iter [512/633], LR: 0.005000, Loss: 3.5588, top1: 12.5000\n",
      "Epoch [18/60], Iter [513/633], LR: 0.005000, Loss: 3.5923, top1: 12.5000\n",
      "Epoch [18/60], Iter [514/633], LR: 0.005000, Loss: 3.6259, top1: 7.8125\n",
      "Epoch [18/60], Iter [515/633], LR: 0.005000, Loss: 3.5734, top1: 9.3750\n",
      "Epoch [18/60], Iter [516/633], LR: 0.005000, Loss: 3.5614, top1: 15.6250\n",
      "Epoch [18/60], Iter [517/633], LR: 0.005000, Loss: 3.5445, top1: 21.8750\n",
      "Epoch [18/60], Iter [518/633], LR: 0.005000, Loss: 3.6166, top1: 7.8125\n",
      "Epoch [18/60], Iter [519/633], LR: 0.005000, Loss: 3.5700, top1: 17.1875\n",
      "Epoch [18/60], Iter [520/633], LR: 0.005000, Loss: 3.5352, top1: 17.1875\n",
      "Epoch [18/60], Iter [521/633], LR: 0.005000, Loss: 3.5789, top1: 15.6250\n",
      "Epoch [18/60], Iter [522/633], LR: 0.005000, Loss: 3.5514, top1: 15.6250\n",
      "Epoch [18/60], Iter [523/633], LR: 0.005000, Loss: 3.5885, top1: 14.0625\n",
      "Epoch [18/60], Iter [524/633], LR: 0.005000, Loss: 3.5793, top1: 7.8125\n",
      "Epoch [18/60], Iter [525/633], LR: 0.005000, Loss: 3.6220, top1: 7.8125\n",
      "Epoch [18/60], Iter [526/633], LR: 0.005000, Loss: 3.5493, top1: 12.5000\n",
      "Epoch [18/60], Iter [527/633], LR: 0.005000, Loss: 3.5883, top1: 12.5000\n",
      "Epoch [18/60], Iter [528/633], LR: 0.005000, Loss: 3.5547, top1: 15.6250\n",
      "Epoch [18/60], Iter [529/633], LR: 0.005000, Loss: 3.5846, top1: 10.9375\n",
      "Epoch [18/60], Iter [530/633], LR: 0.005000, Loss: 3.5899, top1: 12.5000\n",
      "Epoch [18/60], Iter [531/633], LR: 0.005000, Loss: 3.5389, top1: 15.6250\n",
      "Epoch [18/60], Iter [532/633], LR: 0.005000, Loss: 3.5108, top1: 21.8750\n",
      "Epoch [18/60], Iter [533/633], LR: 0.005000, Loss: 3.5973, top1: 12.5000\n",
      "Epoch [18/60], Iter [534/633], LR: 0.005000, Loss: 3.5666, top1: 15.6250\n",
      "Epoch [18/60], Iter [535/633], LR: 0.005000, Loss: 3.5002, top1: 18.7500\n",
      "Epoch [18/60], Iter [536/633], LR: 0.005000, Loss: 3.5476, top1: 17.1875\n",
      "Epoch [18/60], Iter [537/633], LR: 0.005000, Loss: 3.6084, top1: 14.0625\n",
      "Epoch [18/60], Iter [538/633], LR: 0.005000, Loss: 3.5467, top1: 14.0625\n",
      "Epoch [18/60], Iter [539/633], LR: 0.005000, Loss: 3.5419, top1: 17.1875\n",
      "Epoch [18/60], Iter [540/633], LR: 0.005000, Loss: 3.5617, top1: 20.3125\n",
      "Epoch [18/60], Iter [541/633], LR: 0.005000, Loss: 3.6004, top1: 10.9375\n",
      "Epoch [18/60], Iter [542/633], LR: 0.005000, Loss: 3.5902, top1: 12.5000\n",
      "Epoch [18/60], Iter [543/633], LR: 0.005000, Loss: 3.5723, top1: 12.5000\n",
      "Epoch [18/60], Iter [544/633], LR: 0.005000, Loss: 3.5752, top1: 17.1875\n",
      "Epoch [18/60], Iter [545/633], LR: 0.005000, Loss: 3.5313, top1: 21.8750\n",
      "Epoch [18/60], Iter [546/633], LR: 0.005000, Loss: 3.6158, top1: 9.3750\n",
      "Epoch [18/60], Iter [547/633], LR: 0.005000, Loss: 3.6078, top1: 12.5000\n",
      "Epoch [18/60], Iter [548/633], LR: 0.005000, Loss: 3.5375, top1: 20.3125\n",
      "Epoch [18/60], Iter [549/633], LR: 0.005000, Loss: 3.5967, top1: 12.5000\n",
      "Epoch [18/60], Iter [550/633], LR: 0.005000, Loss: 3.5705, top1: 14.0625\n",
      "Epoch [18/60], Iter [551/633], LR: 0.005000, Loss: 3.5801, top1: 18.7500\n",
      "Epoch [18/60], Iter [552/633], LR: 0.005000, Loss: 3.5355, top1: 18.7500\n",
      "Epoch [18/60], Iter [553/633], LR: 0.005000, Loss: 3.5189, top1: 26.5625\n",
      "Epoch [18/60], Iter [554/633], LR: 0.005000, Loss: 3.6030, top1: 7.8125\n",
      "Epoch [18/60], Iter [555/633], LR: 0.005000, Loss: 3.5634, top1: 14.0625\n",
      "Epoch [18/60], Iter [556/633], LR: 0.005000, Loss: 3.5518, top1: 12.5000\n",
      "Epoch [18/60], Iter [557/633], LR: 0.005000, Loss: 3.5917, top1: 9.3750\n",
      "Epoch [18/60], Iter [558/633], LR: 0.005000, Loss: 3.5871, top1: 10.9375\n",
      "Epoch [18/60], Iter [559/633], LR: 0.005000, Loss: 3.5715, top1: 14.0625\n",
      "Epoch [18/60], Iter [560/633], LR: 0.005000, Loss: 3.5978, top1: 7.8125\n",
      "Epoch [18/60], Iter [561/633], LR: 0.005000, Loss: 3.5484, top1: 15.6250\n",
      "Epoch [18/60], Iter [562/633], LR: 0.005000, Loss: 3.5673, top1: 18.7500\n",
      "Epoch [18/60], Iter [563/633], LR: 0.005000, Loss: 3.5731, top1: 9.3750\n",
      "Epoch [18/60], Iter [564/633], LR: 0.005000, Loss: 3.5872, top1: 14.0625\n",
      "Epoch [18/60], Iter [565/633], LR: 0.005000, Loss: 3.5743, top1: 15.6250\n",
      "Epoch [18/60], Iter [566/633], LR: 0.005000, Loss: 3.5887, top1: 18.7500\n",
      "Epoch [18/60], Iter [567/633], LR: 0.005000, Loss: 3.5494, top1: 12.5000\n",
      "Epoch [18/60], Iter [568/633], LR: 0.005000, Loss: 3.5787, top1: 12.5000\n",
      "Epoch [18/60], Iter [569/633], LR: 0.005000, Loss: 3.5595, top1: 18.7500\n",
      "Epoch [18/60], Iter [570/633], LR: 0.005000, Loss: 3.5409, top1: 12.5000\n",
      "Epoch [18/60], Iter [571/633], LR: 0.005000, Loss: 3.5945, top1: 10.9375\n",
      "Epoch [18/60], Iter [572/633], LR: 0.005000, Loss: 3.5679, top1: 15.6250\n",
      "Epoch [18/60], Iter [573/633], LR: 0.005000, Loss: 3.5404, top1: 18.7500\n",
      "Epoch [18/60], Iter [574/633], LR: 0.005000, Loss: 3.6022, top1: 10.9375\n",
      "Epoch [18/60], Iter [575/633], LR: 0.005000, Loss: 3.5724, top1: 23.4375\n",
      "Epoch [18/60], Iter [576/633], LR: 0.005000, Loss: 3.5515, top1: 21.8750\n",
      "Epoch [18/60], Iter [577/633], LR: 0.005000, Loss: 3.5727, top1: 14.0625\n",
      "Epoch [18/60], Iter [578/633], LR: 0.005000, Loss: 3.5100, top1: 23.4375\n",
      "Epoch [18/60], Iter [579/633], LR: 0.005000, Loss: 3.5291, top1: 21.8750\n",
      "Epoch [18/60], Iter [580/633], LR: 0.005000, Loss: 3.5577, top1: 17.1875\n",
      "Epoch [18/60], Iter [581/633], LR: 0.005000, Loss: 3.5566, top1: 18.7500\n",
      "Epoch [18/60], Iter [582/633], LR: 0.005000, Loss: 3.5606, top1: 18.7500\n",
      "Epoch [18/60], Iter [583/633], LR: 0.005000, Loss: 3.5646, top1: 15.6250\n",
      "Epoch [18/60], Iter [584/633], LR: 0.005000, Loss: 3.5708, top1: 9.3750\n",
      "Epoch [18/60], Iter [585/633], LR: 0.005000, Loss: 3.5664, top1: 10.9375\n",
      "Epoch [18/60], Iter [586/633], LR: 0.005000, Loss: 3.5214, top1: 21.8750\n",
      "Epoch [18/60], Iter [587/633], LR: 0.005000, Loss: 3.6084, top1: 9.3750\n",
      "Epoch [18/60], Iter [588/633], LR: 0.005000, Loss: 3.5421, top1: 25.0000\n",
      "Epoch [18/60], Iter [589/633], LR: 0.005000, Loss: 3.5650, top1: 17.1875\n",
      "Epoch [18/60], Iter [590/633], LR: 0.005000, Loss: 3.6337, top1: 9.3750\n",
      "Epoch [18/60], Iter [591/633], LR: 0.005000, Loss: 3.5638, top1: 20.3125\n",
      "Epoch [18/60], Iter [592/633], LR: 0.005000, Loss: 3.5625, top1: 17.1875\n",
      "Epoch [18/60], Iter [593/633], LR: 0.005000, Loss: 3.5232, top1: 20.3125\n",
      "Epoch [18/60], Iter [594/633], LR: 0.005000, Loss: 3.5945, top1: 15.6250\n",
      "Epoch [18/60], Iter [595/633], LR: 0.005000, Loss: 3.6043, top1: 7.8125\n",
      "Epoch [18/60], Iter [596/633], LR: 0.005000, Loss: 3.5476, top1: 21.8750\n",
      "Epoch [18/60], Iter [597/633], LR: 0.005000, Loss: 3.4965, top1: 29.6875\n",
      "Epoch [18/60], Iter [598/633], LR: 0.005000, Loss: 3.5253, top1: 21.8750\n",
      "Epoch [18/60], Iter [599/633], LR: 0.005000, Loss: 3.5585, top1: 15.6250\n",
      "Epoch [18/60], Iter [600/633], LR: 0.005000, Loss: 3.5880, top1: 10.9375\n",
      "Epoch [18/60], Iter [601/633], LR: 0.005000, Loss: 3.5468, top1: 20.3125\n",
      "Epoch [18/60], Iter [602/633], LR: 0.005000, Loss: 3.5568, top1: 21.8750\n",
      "Epoch [18/60], Iter [603/633], LR: 0.005000, Loss: 3.5832, top1: 14.0625\n",
      "Epoch [18/60], Iter [604/633], LR: 0.005000, Loss: 3.5452, top1: 23.4375\n",
      "Epoch [18/60], Iter [605/633], LR: 0.005000, Loss: 3.5527, top1: 15.6250\n",
      "Epoch [18/60], Iter [606/633], LR: 0.005000, Loss: 3.5392, top1: 17.1875\n",
      "Epoch [18/60], Iter [607/633], LR: 0.005000, Loss: 3.5962, top1: 12.5000\n",
      "Epoch [18/60], Iter [608/633], LR: 0.005000, Loss: 3.6095, top1: 6.2500\n",
      "Epoch [18/60], Iter [609/633], LR: 0.005000, Loss: 3.6059, top1: 14.0625\n",
      "Epoch [18/60], Iter [610/633], LR: 0.005000, Loss: 3.5565, top1: 15.6250\n",
      "Epoch [18/60], Iter [611/633], LR: 0.005000, Loss: 3.5553, top1: 17.1875\n",
      "Epoch [18/60], Iter [612/633], LR: 0.005000, Loss: 3.5733, top1: 12.5000\n",
      "Epoch [18/60], Iter [613/633], LR: 0.005000, Loss: 3.5626, top1: 15.6250\n",
      "Epoch [18/60], Iter [614/633], LR: 0.005000, Loss: 3.5668, top1: 12.5000\n",
      "Epoch [18/60], Iter [615/633], LR: 0.005000, Loss: 3.5450, top1: 23.4375\n",
      "Epoch [18/60], Iter [616/633], LR: 0.005000, Loss: 3.4824, top1: 25.0000\n",
      "Epoch [18/60], Iter [617/633], LR: 0.005000, Loss: 3.6024, top1: 10.9375\n",
      "Epoch [18/60], Iter [618/633], LR: 0.005000, Loss: 3.5404, top1: 25.0000\n",
      "Epoch [18/60], Iter [619/633], LR: 0.005000, Loss: 3.5542, top1: 17.1875\n",
      "Epoch [18/60], Iter [620/633], LR: 0.005000, Loss: 3.6075, top1: 7.8125\n",
      "Epoch [18/60], Iter [621/633], LR: 0.005000, Loss: 3.5246, top1: 23.4375\n",
      "Epoch [18/60], Iter [622/633], LR: 0.005000, Loss: 3.5594, top1: 15.6250\n",
      "Epoch [18/60], Iter [623/633], LR: 0.005000, Loss: 3.5600, top1: 17.1875\n",
      "Epoch [18/60], Iter [624/633], LR: 0.005000, Loss: 3.5573, top1: 17.1875\n",
      "Epoch [18/60], Iter [625/633], LR: 0.005000, Loss: 3.5548, top1: 20.3125\n",
      "Epoch [18/60], Iter [626/633], LR: 0.005000, Loss: 3.6108, top1: 4.6875\n",
      "Epoch [18/60], Iter [627/633], LR: 0.005000, Loss: 3.5474, top1: 17.1875\n",
      "Epoch [18/60], Iter [628/633], LR: 0.005000, Loss: 3.5241, top1: 18.7500\n",
      "Epoch [18/60], Iter [629/633], LR: 0.005000, Loss: 3.5757, top1: 15.6250\n",
      "Epoch [18/60], Iter [630/633], LR: 0.005000, Loss: 3.5994, top1: 12.5000\n",
      "Epoch [18/60], Iter [631/633], LR: 0.005000, Loss: 3.5439, top1: 17.1875\n",
      "Epoch [18/60], Iter [632/633], LR: 0.005000, Loss: 3.5901, top1: 12.5000\n",
      "Epoch [18/60], Iter [633/633], LR: 0.005000, Loss: 3.5496, top1: 10.9375\n",
      "Epoch [18/60], Iter [634/633], LR: 0.005000, Loss: 3.5889, top1: 11.2903\n",
      "Epoch [18/60], Val_Loss: 3.5533, Val_top1: 16.1092, best_top1: 16.9674\n",
      "epoch time: 4.392608511447906 min\n",
      "Epoch [19/60], Iter [1/633], LR: 0.005000, Loss: 3.5335, top1: 17.1875\n",
      "Epoch [19/60], Iter [2/633], LR: 0.005000, Loss: 3.5553, top1: 15.6250\n",
      "Epoch [19/60], Iter [3/633], LR: 0.005000, Loss: 3.5465, top1: 21.8750\n",
      "Epoch [19/60], Iter [4/633], LR: 0.005000, Loss: 3.6018, top1: 14.0625\n",
      "Epoch [19/60], Iter [5/633], LR: 0.005000, Loss: 3.5335, top1: 17.1875\n",
      "Epoch [19/60], Iter [6/633], LR: 0.005000, Loss: 3.6135, top1: 9.3750\n",
      "Epoch [19/60], Iter [7/633], LR: 0.005000, Loss: 3.5570, top1: 17.1875\n",
      "Epoch [19/60], Iter [8/633], LR: 0.005000, Loss: 3.5580, top1: 15.6250\n",
      "Epoch [19/60], Iter [9/633], LR: 0.005000, Loss: 3.5901, top1: 15.6250\n",
      "Epoch [19/60], Iter [10/633], LR: 0.005000, Loss: 3.5399, top1: 20.3125\n",
      "Epoch [19/60], Iter [11/633], LR: 0.005000, Loss: 3.5737, top1: 20.3125\n",
      "Epoch [19/60], Iter [12/633], LR: 0.005000, Loss: 3.6007, top1: 9.3750\n",
      "Epoch [19/60], Iter [13/633], LR: 0.005000, Loss: 3.5663, top1: 14.0625\n",
      "Epoch [19/60], Iter [14/633], LR: 0.005000, Loss: 3.5662, top1: 17.1875\n",
      "Epoch [19/60], Iter [15/633], LR: 0.005000, Loss: 3.5498, top1: 14.0625\n",
      "Epoch [19/60], Iter [16/633], LR: 0.005000, Loss: 3.6076, top1: 14.0625\n",
      "Epoch [19/60], Iter [17/633], LR: 0.005000, Loss: 3.5278, top1: 18.7500\n",
      "Epoch [19/60], Iter [18/633], LR: 0.005000, Loss: 3.5214, top1: 25.0000\n",
      "Epoch [19/60], Iter [19/633], LR: 0.005000, Loss: 3.6024, top1: 12.5000\n",
      "Epoch [19/60], Iter [20/633], LR: 0.005000, Loss: 3.5588, top1: 17.1875\n",
      "Epoch [19/60], Iter [21/633], LR: 0.005000, Loss: 3.5855, top1: 15.6250\n",
      "Epoch [19/60], Iter [22/633], LR: 0.005000, Loss: 3.5910, top1: 12.5000\n",
      "Epoch [19/60], Iter [23/633], LR: 0.005000, Loss: 3.5645, top1: 15.6250\n",
      "Epoch [19/60], Iter [24/633], LR: 0.005000, Loss: 3.5574, top1: 12.5000\n",
      "Epoch [19/60], Iter [25/633], LR: 0.005000, Loss: 3.5590, top1: 15.6250\n",
      "Epoch [19/60], Iter [26/633], LR: 0.005000, Loss: 3.6225, top1: 10.9375\n",
      "Epoch [19/60], Iter [27/633], LR: 0.005000, Loss: 3.5978, top1: 9.3750\n",
      "Epoch [19/60], Iter [28/633], LR: 0.005000, Loss: 3.5221, top1: 15.6250\n",
      "Epoch [19/60], Iter [29/633], LR: 0.005000, Loss: 3.5430, top1: 26.5625\n",
      "Epoch [19/60], Iter [30/633], LR: 0.005000, Loss: 3.5659, top1: 14.0625\n",
      "Epoch [19/60], Iter [31/633], LR: 0.005000, Loss: 3.5808, top1: 9.3750\n",
      "Epoch [19/60], Iter [32/633], LR: 0.005000, Loss: 3.5748, top1: 15.6250\n",
      "Epoch [19/60], Iter [33/633], LR: 0.005000, Loss: 3.5678, top1: 17.1875\n",
      "Epoch [19/60], Iter [34/633], LR: 0.005000, Loss: 3.5760, top1: 12.5000\n",
      "Epoch [19/60], Iter [35/633], LR: 0.005000, Loss: 3.5612, top1: 17.1875\n",
      "Epoch [19/60], Iter [36/633], LR: 0.005000, Loss: 3.5497, top1: 14.0625\n",
      "Epoch [19/60], Iter [37/633], LR: 0.005000, Loss: 3.5828, top1: 15.6250\n",
      "Epoch [19/60], Iter [38/633], LR: 0.005000, Loss: 3.5838, top1: 6.2500\n",
      "Epoch [19/60], Iter [39/633], LR: 0.005000, Loss: 3.5625, top1: 12.5000\n",
      "Epoch [19/60], Iter [40/633], LR: 0.005000, Loss: 3.5410, top1: 15.6250\n",
      "Epoch [19/60], Iter [41/633], LR: 0.005000, Loss: 3.5287, top1: 23.4375\n",
      "Epoch [19/60], Iter [42/633], LR: 0.005000, Loss: 3.5857, top1: 14.0625\n",
      "Epoch [19/60], Iter [43/633], LR: 0.005000, Loss: 3.6028, top1: 9.3750\n",
      "Epoch [19/60], Iter [44/633], LR: 0.005000, Loss: 3.5589, top1: 14.0625\n",
      "Epoch [19/60], Iter [45/633], LR: 0.005000, Loss: 3.5581, top1: 18.7500\n",
      "Epoch [19/60], Iter [46/633], LR: 0.005000, Loss: 3.6089, top1: 9.3750\n",
      "Epoch [19/60], Iter [47/633], LR: 0.005000, Loss: 3.5700, top1: 12.5000\n",
      "Epoch [19/60], Iter [48/633], LR: 0.005000, Loss: 3.5883, top1: 7.8125\n",
      "Epoch [19/60], Iter [49/633], LR: 0.005000, Loss: 3.5406, top1: 17.1875\n",
      "Epoch [19/60], Iter [50/633], LR: 0.005000, Loss: 3.5639, top1: 20.3125\n",
      "Epoch [19/60], Iter [51/633], LR: 0.005000, Loss: 3.5182, top1: 23.4375\n",
      "Epoch [19/60], Iter [52/633], LR: 0.005000, Loss: 3.6016, top1: 14.0625\n",
      "Epoch [19/60], Iter [53/633], LR: 0.005000, Loss: 3.5582, top1: 20.3125\n",
      "Epoch [19/60], Iter [54/633], LR: 0.005000, Loss: 3.5906, top1: 9.3750\n",
      "Epoch [19/60], Iter [55/633], LR: 0.005000, Loss: 3.5919, top1: 15.6250\n",
      "Epoch [19/60], Iter [56/633], LR: 0.005000, Loss: 3.5967, top1: 12.5000\n",
      "Epoch [19/60], Iter [57/633], LR: 0.005000, Loss: 3.5943, top1: 9.3750\n",
      "Epoch [19/60], Iter [58/633], LR: 0.005000, Loss: 3.5351, top1: 14.0625\n",
      "Epoch [19/60], Iter [59/633], LR: 0.005000, Loss: 3.5860, top1: 7.8125\n",
      "Epoch [19/60], Iter [60/633], LR: 0.005000, Loss: 3.5374, top1: 21.8750\n",
      "Epoch [19/60], Iter [61/633], LR: 0.005000, Loss: 3.5345, top1: 23.4375\n",
      "Epoch [19/60], Iter [62/633], LR: 0.005000, Loss: 3.6024, top1: 12.5000\n",
      "Epoch [19/60], Iter [63/633], LR: 0.005000, Loss: 3.5976, top1: 14.0625\n",
      "Epoch [19/60], Iter [64/633], LR: 0.005000, Loss: 3.5767, top1: 10.9375\n",
      "Epoch [19/60], Iter [65/633], LR: 0.005000, Loss: 3.5817, top1: 18.7500\n",
      "Epoch [19/60], Iter [66/633], LR: 0.005000, Loss: 3.5704, top1: 12.5000\n",
      "Epoch [19/60], Iter [67/633], LR: 0.005000, Loss: 3.6033, top1: 12.5000\n",
      "Epoch [19/60], Iter [68/633], LR: 0.005000, Loss: 3.5998, top1: 9.3750\n",
      "Epoch [19/60], Iter [69/633], LR: 0.005000, Loss: 3.5176, top1: 18.7500\n",
      "Epoch [19/60], Iter [70/633], LR: 0.005000, Loss: 3.5053, top1: 23.4375\n",
      "Epoch [19/60], Iter [71/633], LR: 0.005000, Loss: 3.6196, top1: 9.3750\n",
      "Epoch [19/60], Iter [72/633], LR: 0.005000, Loss: 3.5657, top1: 10.9375\n",
      "Epoch [19/60], Iter [73/633], LR: 0.005000, Loss: 3.5378, top1: 14.0625\n",
      "Epoch [19/60], Iter [74/633], LR: 0.005000, Loss: 3.5999, top1: 10.9375\n",
      "Epoch [19/60], Iter [75/633], LR: 0.005000, Loss: 3.5480, top1: 20.3125\n",
      "Epoch [19/60], Iter [76/633], LR: 0.005000, Loss: 3.5841, top1: 14.0625\n",
      "Epoch [19/60], Iter [77/633], LR: 0.005000, Loss: 3.6054, top1: 10.9375\n",
      "Epoch [19/60], Iter [78/633], LR: 0.005000, Loss: 3.5625, top1: 12.5000\n",
      "Epoch [19/60], Iter [79/633], LR: 0.005000, Loss: 3.5523, top1: 18.7500\n",
      "Epoch [19/60], Iter [80/633], LR: 0.005000, Loss: 3.5804, top1: 12.5000\n",
      "Epoch [19/60], Iter [81/633], LR: 0.005000, Loss: 3.5706, top1: 15.6250\n",
      "Epoch [19/60], Iter [82/633], LR: 0.005000, Loss: 3.5898, top1: 14.0625\n",
      "Epoch [19/60], Iter [83/633], LR: 0.005000, Loss: 3.5378, top1: 17.1875\n",
      "Epoch [19/60], Iter [84/633], LR: 0.005000, Loss: 3.5865, top1: 14.0625\n",
      "Epoch [19/60], Iter [85/633], LR: 0.005000, Loss: 3.6167, top1: 15.6250\n",
      "Epoch [19/60], Iter [86/633], LR: 0.005000, Loss: 3.5029, top1: 17.1875\n",
      "Epoch [19/60], Iter [87/633], LR: 0.005000, Loss: 3.5655, top1: 17.1875\n",
      "Epoch [19/60], Iter [88/633], LR: 0.005000, Loss: 3.5438, top1: 20.3125\n",
      "Epoch [19/60], Iter [89/633], LR: 0.005000, Loss: 3.5708, top1: 10.9375\n",
      "Epoch [19/60], Iter [90/633], LR: 0.005000, Loss: 3.5912, top1: 14.0625\n",
      "Epoch [19/60], Iter [91/633], LR: 0.005000, Loss: 3.5674, top1: 10.9375\n",
      "Epoch [19/60], Iter [92/633], LR: 0.005000, Loss: 3.5207, top1: 25.0000\n",
      "Epoch [19/60], Iter [93/633], LR: 0.005000, Loss: 3.5625, top1: 17.1875\n",
      "Epoch [19/60], Iter [94/633], LR: 0.005000, Loss: 3.5854, top1: 10.9375\n",
      "Epoch [19/60], Iter [95/633], LR: 0.005000, Loss: 3.5730, top1: 12.5000\n",
      "Epoch [19/60], Iter [96/633], LR: 0.005000, Loss: 3.5781, top1: 12.5000\n",
      "Epoch [19/60], Iter [97/633], LR: 0.005000, Loss: 3.5743, top1: 14.0625\n",
      "Epoch [19/60], Iter [98/633], LR: 0.005000, Loss: 3.5855, top1: 9.3750\n",
      "Epoch [19/60], Iter [99/633], LR: 0.005000, Loss: 3.5407, top1: 15.6250\n",
      "Epoch [19/60], Iter [100/633], LR: 0.005000, Loss: 3.5841, top1: 12.5000\n",
      "Epoch [19/60], Iter [101/633], LR: 0.005000, Loss: 3.6038, top1: 10.9375\n",
      "Epoch [19/60], Iter [102/633], LR: 0.005000, Loss: 3.5560, top1: 21.8750\n",
      "Epoch [19/60], Iter [103/633], LR: 0.005000, Loss: 3.5500, top1: 15.6250\n",
      "Epoch [19/60], Iter [104/633], LR: 0.005000, Loss: 3.5350, top1: 23.4375\n",
      "Epoch [19/60], Iter [105/633], LR: 0.005000, Loss: 3.5594, top1: 12.5000\n",
      "Epoch [19/60], Iter [106/633], LR: 0.005000, Loss: 3.5358, top1: 20.3125\n",
      "Epoch [19/60], Iter [107/633], LR: 0.005000, Loss: 3.6034, top1: 14.0625\n",
      "Epoch [19/60], Iter [108/633], LR: 0.005000, Loss: 3.6159, top1: 9.3750\n",
      "Epoch [19/60], Iter [109/633], LR: 0.005000, Loss: 3.5285, top1: 17.1875\n",
      "Epoch [19/60], Iter [110/633], LR: 0.005000, Loss: 3.6179, top1: 9.3750\n",
      "Epoch [19/60], Iter [111/633], LR: 0.005000, Loss: 3.6033, top1: 12.5000\n",
      "Epoch [19/60], Iter [112/633], LR: 0.005000, Loss: 3.5660, top1: 17.1875\n",
      "Epoch [19/60], Iter [113/633], LR: 0.005000, Loss: 3.5593, top1: 17.1875\n",
      "Epoch [19/60], Iter [114/633], LR: 0.005000, Loss: 3.5523, top1: 15.6250\n",
      "Epoch [19/60], Iter [115/633], LR: 0.005000, Loss: 3.5848, top1: 12.5000\n",
      "Epoch [19/60], Iter [116/633], LR: 0.005000, Loss: 3.5199, top1: 21.8750\n",
      "Epoch [19/60], Iter [117/633], LR: 0.005000, Loss: 3.5114, top1: 26.5625\n",
      "Epoch [19/60], Iter [118/633], LR: 0.005000, Loss: 3.5667, top1: 25.0000\n",
      "Epoch [19/60], Iter [119/633], LR: 0.005000, Loss: 3.5694, top1: 14.0625\n",
      "Epoch [19/60], Iter [120/633], LR: 0.005000, Loss: 3.5805, top1: 7.8125\n",
      "Epoch [19/60], Iter [121/633], LR: 0.005000, Loss: 3.5897, top1: 14.0625\n",
      "Epoch [19/60], Iter [122/633], LR: 0.005000, Loss: 3.5832, top1: 14.0625\n",
      "Epoch [19/60], Iter [123/633], LR: 0.005000, Loss: 3.5776, top1: 18.7500\n",
      "Epoch [19/60], Iter [124/633], LR: 0.005000, Loss: 3.5588, top1: 17.1875\n",
      "Epoch [19/60], Iter [125/633], LR: 0.005000, Loss: 3.5856, top1: 12.5000\n",
      "Epoch [19/60], Iter [126/633], LR: 0.005000, Loss: 3.5761, top1: 17.1875\n",
      "Epoch [19/60], Iter [127/633], LR: 0.005000, Loss: 3.5686, top1: 12.5000\n",
      "Epoch [19/60], Iter [128/633], LR: 0.005000, Loss: 3.5863, top1: 9.3750\n",
      "Epoch [19/60], Iter [129/633], LR: 0.005000, Loss: 3.5686, top1: 14.0625\n",
      "Epoch [19/60], Iter [130/633], LR: 0.005000, Loss: 3.6007, top1: 10.9375\n",
      "Epoch [19/60], Iter [131/633], LR: 0.005000, Loss: 3.5570, top1: 21.8750\n",
      "Epoch [19/60], Iter [132/633], LR: 0.005000, Loss: 3.5815, top1: 14.0625\n",
      "Epoch [19/60], Iter [133/633], LR: 0.005000, Loss: 3.5442, top1: 18.7500\n",
      "Epoch [19/60], Iter [134/633], LR: 0.005000, Loss: 3.5467, top1: 14.0625\n",
      "Epoch [19/60], Iter [135/633], LR: 0.005000, Loss: 3.5656, top1: 14.0625\n",
      "Epoch [19/60], Iter [136/633], LR: 0.005000, Loss: 3.5304, top1: 21.8750\n",
      "Epoch [19/60], Iter [137/633], LR: 0.005000, Loss: 3.5650, top1: 17.1875\n",
      "Epoch [19/60], Iter [138/633], LR: 0.005000, Loss: 3.5299, top1: 21.8750\n",
      "Epoch [19/60], Iter [139/633], LR: 0.005000, Loss: 3.5985, top1: 10.9375\n",
      "Epoch [19/60], Iter [140/633], LR: 0.005000, Loss: 3.6026, top1: 7.8125\n",
      "Epoch [19/60], Iter [141/633], LR: 0.005000, Loss: 3.5762, top1: 17.1875\n",
      "Epoch [19/60], Iter [142/633], LR: 0.005000, Loss: 3.5344, top1: 21.8750\n",
      "Epoch [19/60], Iter [143/633], LR: 0.005000, Loss: 3.5594, top1: 15.6250\n",
      "Epoch [19/60], Iter [144/633], LR: 0.005000, Loss: 3.5254, top1: 15.6250\n",
      "Epoch [19/60], Iter [145/633], LR: 0.005000, Loss: 3.5603, top1: 14.0625\n",
      "Epoch [19/60], Iter [146/633], LR: 0.005000, Loss: 3.5961, top1: 9.3750\n",
      "Epoch [19/60], Iter [147/633], LR: 0.005000, Loss: 3.5281, top1: 14.0625\n",
      "Epoch [19/60], Iter [148/633], LR: 0.005000, Loss: 3.6026, top1: 12.5000\n",
      "Epoch [19/60], Iter [149/633], LR: 0.005000, Loss: 3.5774, top1: 15.6250\n",
      "Epoch [19/60], Iter [150/633], LR: 0.005000, Loss: 3.6079, top1: 7.8125\n",
      "Epoch [19/60], Iter [151/633], LR: 0.005000, Loss: 3.5728, top1: 18.7500\n",
      "Epoch [19/60], Iter [152/633], LR: 0.005000, Loss: 3.5928, top1: 14.0625\n",
      "Epoch [19/60], Iter [153/633], LR: 0.005000, Loss: 3.5408, top1: 17.1875\n",
      "Epoch [19/60], Iter [154/633], LR: 0.005000, Loss: 3.5904, top1: 18.7500\n",
      "Epoch [19/60], Iter [155/633], LR: 0.005000, Loss: 3.5081, top1: 21.8750\n",
      "Epoch [19/60], Iter [156/633], LR: 0.005000, Loss: 3.5078, top1: 18.7500\n",
      "Epoch [19/60], Iter [157/633], LR: 0.005000, Loss: 3.5572, top1: 17.1875\n",
      "Epoch [19/60], Iter [158/633], LR: 0.005000, Loss: 3.5279, top1: 18.7500\n",
      "Epoch [19/60], Iter [159/633], LR: 0.005000, Loss: 3.5644, top1: 12.5000\n",
      "Epoch [19/60], Iter [160/633], LR: 0.005000, Loss: 3.5903, top1: 17.1875\n",
      "Epoch [19/60], Iter [161/633], LR: 0.005000, Loss: 3.5570, top1: 18.7500\n",
      "Epoch [19/60], Iter [162/633], LR: 0.005000, Loss: 3.5825, top1: 12.5000\n",
      "Epoch [19/60], Iter [163/633], LR: 0.005000, Loss: 3.5483, top1: 15.6250\n",
      "Epoch [19/60], Iter [164/633], LR: 0.005000, Loss: 3.5296, top1: 23.4375\n",
      "Epoch [19/60], Iter [165/633], LR: 0.005000, Loss: 3.5489, top1: 18.7500\n",
      "Epoch [19/60], Iter [166/633], LR: 0.005000, Loss: 3.5785, top1: 14.0625\n",
      "Epoch [19/60], Iter [167/633], LR: 0.005000, Loss: 3.5861, top1: 14.0625\n",
      "Epoch [19/60], Iter [168/633], LR: 0.005000, Loss: 3.5139, top1: 17.1875\n",
      "Epoch [19/60], Iter [169/633], LR: 0.005000, Loss: 3.5669, top1: 18.7500\n",
      "Epoch [19/60], Iter [170/633], LR: 0.005000, Loss: 3.5698, top1: 21.8750\n",
      "Epoch [19/60], Iter [171/633], LR: 0.005000, Loss: 3.5336, top1: 18.7500\n",
      "Epoch [19/60], Iter [172/633], LR: 0.005000, Loss: 3.5895, top1: 18.7500\n",
      "Epoch [19/60], Iter [173/633], LR: 0.005000, Loss: 3.5925, top1: 7.8125\n",
      "Epoch [19/60], Iter [174/633], LR: 0.005000, Loss: 3.5477, top1: 12.5000\n",
      "Epoch [19/60], Iter [175/633], LR: 0.005000, Loss: 3.5349, top1: 23.4375\n",
      "Epoch [19/60], Iter [176/633], LR: 0.005000, Loss: 3.5900, top1: 10.9375\n",
      "Epoch [19/60], Iter [177/633], LR: 0.005000, Loss: 3.5500, top1: 18.7500\n",
      "Epoch [19/60], Iter [178/633], LR: 0.005000, Loss: 3.5697, top1: 14.0625\n",
      "Epoch [19/60], Iter [179/633], LR: 0.005000, Loss: 3.5152, top1: 20.3125\n",
      "Epoch [19/60], Iter [180/633], LR: 0.005000, Loss: 3.5488, top1: 7.8125\n",
      "Epoch [19/60], Iter [181/633], LR: 0.005000, Loss: 3.5893, top1: 10.9375\n",
      "Epoch [19/60], Iter [182/633], LR: 0.005000, Loss: 3.5820, top1: 12.5000\n",
      "Epoch [19/60], Iter [183/633], LR: 0.005000, Loss: 3.5565, top1: 15.6250\n",
      "Epoch [19/60], Iter [184/633], LR: 0.005000, Loss: 3.5602, top1: 20.3125\n",
      "Epoch [19/60], Iter [185/633], LR: 0.005000, Loss: 3.5720, top1: 12.5000\n",
      "Epoch [19/60], Iter [186/633], LR: 0.005000, Loss: 3.5095, top1: 21.8750\n",
      "Epoch [19/60], Iter [187/633], LR: 0.005000, Loss: 3.5413, top1: 17.1875\n",
      "Epoch [19/60], Iter [188/633], LR: 0.005000, Loss: 3.5742, top1: 20.3125\n",
      "Epoch [19/60], Iter [189/633], LR: 0.005000, Loss: 3.5669, top1: 20.3125\n",
      "Epoch [19/60], Iter [190/633], LR: 0.005000, Loss: 3.5690, top1: 17.1875\n",
      "Epoch [19/60], Iter [191/633], LR: 0.005000, Loss: 3.5636, top1: 14.0625\n",
      "Epoch [19/60], Iter [192/633], LR: 0.005000, Loss: 3.5506, top1: 18.7500\n",
      "Epoch [19/60], Iter [193/633], LR: 0.005000, Loss: 3.5539, top1: 14.0625\n",
      "Epoch [19/60], Iter [194/633], LR: 0.005000, Loss: 3.5918, top1: 10.9375\n",
      "Epoch [19/60], Iter [195/633], LR: 0.005000, Loss: 3.5603, top1: 23.4375\n",
      "Epoch [19/60], Iter [196/633], LR: 0.005000, Loss: 3.5448, top1: 18.7500\n",
      "Epoch [19/60], Iter [197/633], LR: 0.005000, Loss: 3.5739, top1: 14.0625\n",
      "Epoch [19/60], Iter [198/633], LR: 0.005000, Loss: 3.5282, top1: 21.8750\n",
      "Epoch [19/60], Iter [199/633], LR: 0.005000, Loss: 3.5294, top1: 23.4375\n",
      "Epoch [19/60], Iter [200/633], LR: 0.005000, Loss: 3.5468, top1: 17.1875\n",
      "Epoch [19/60], Iter [201/633], LR: 0.005000, Loss: 3.5549, top1: 17.1875\n",
      "Epoch [19/60], Iter [202/633], LR: 0.005000, Loss: 3.5662, top1: 10.9375\n",
      "Epoch [19/60], Iter [203/633], LR: 0.005000, Loss: 3.5657, top1: 17.1875\n",
      "Epoch [19/60], Iter [204/633], LR: 0.005000, Loss: 3.5990, top1: 10.9375\n",
      "Epoch [19/60], Iter [205/633], LR: 0.005000, Loss: 3.5918, top1: 7.8125\n",
      "Epoch [19/60], Iter [206/633], LR: 0.005000, Loss: 3.5805, top1: 10.9375\n",
      "Epoch [19/60], Iter [207/633], LR: 0.005000, Loss: 3.5694, top1: 15.6250\n",
      "Epoch [19/60], Iter [208/633], LR: 0.005000, Loss: 3.5848, top1: 15.6250\n",
      "Epoch [19/60], Iter [209/633], LR: 0.005000, Loss: 3.5961, top1: 12.5000\n",
      "Epoch [19/60], Iter [210/633], LR: 0.005000, Loss: 3.5840, top1: 15.6250\n",
      "Epoch [19/60], Iter [211/633], LR: 0.005000, Loss: 3.5491, top1: 14.0625\n",
      "Epoch [19/60], Iter [212/633], LR: 0.005000, Loss: 3.5630, top1: 10.9375\n",
      "Epoch [19/60], Iter [213/633], LR: 0.005000, Loss: 3.5447, top1: 21.8750\n",
      "Epoch [19/60], Iter [214/633], LR: 0.005000, Loss: 3.5585, top1: 21.8750\n",
      "Epoch [19/60], Iter [215/633], LR: 0.005000, Loss: 3.5898, top1: 12.5000\n",
      "Epoch [19/60], Iter [216/633], LR: 0.005000, Loss: 3.6079, top1: 7.8125\n",
      "Epoch [19/60], Iter [217/633], LR: 0.005000, Loss: 3.5760, top1: 14.0625\n",
      "Epoch [19/60], Iter [218/633], LR: 0.005000, Loss: 3.5845, top1: 14.0625\n",
      "Epoch [19/60], Iter [219/633], LR: 0.005000, Loss: 3.5487, top1: 18.7500\n",
      "Epoch [19/60], Iter [220/633], LR: 0.005000, Loss: 3.5365, top1: 17.1875\n",
      "Epoch [19/60], Iter [221/633], LR: 0.005000, Loss: 3.5263, top1: 21.8750\n",
      "Epoch [19/60], Iter [222/633], LR: 0.005000, Loss: 3.5420, top1: 20.3125\n",
      "Epoch [19/60], Iter [223/633], LR: 0.005000, Loss: 3.5618, top1: 14.0625\n",
      "Epoch [19/60], Iter [224/633], LR: 0.005000, Loss: 3.5411, top1: 17.1875\n",
      "Epoch [19/60], Iter [225/633], LR: 0.005000, Loss: 3.5183, top1: 20.3125\n",
      "Epoch [19/60], Iter [226/633], LR: 0.005000, Loss: 3.5708, top1: 15.6250\n",
      "Epoch [19/60], Iter [227/633], LR: 0.005000, Loss: 3.5616, top1: 17.1875\n",
      "Epoch [19/60], Iter [228/633], LR: 0.005000, Loss: 3.5249, top1: 18.7500\n",
      "Epoch [19/60], Iter [229/633], LR: 0.005000, Loss: 3.5714, top1: 17.1875\n",
      "Epoch [19/60], Iter [230/633], LR: 0.005000, Loss: 3.6103, top1: 9.3750\n",
      "Epoch [19/60], Iter [231/633], LR: 0.005000, Loss: 3.5454, top1: 15.6250\n",
      "Epoch [19/60], Iter [232/633], LR: 0.005000, Loss: 3.5784, top1: 17.1875\n",
      "Epoch [19/60], Iter [233/633], LR: 0.005000, Loss: 3.5951, top1: 10.9375\n",
      "Epoch [19/60], Iter [234/633], LR: 0.005000, Loss: 3.6118, top1: 10.9375\n",
      "Epoch [19/60], Iter [235/633], LR: 0.005000, Loss: 3.5975, top1: 12.5000\n",
      "Epoch [19/60], Iter [236/633], LR: 0.005000, Loss: 3.5322, top1: 23.4375\n",
      "Epoch [19/60], Iter [237/633], LR: 0.005000, Loss: 3.5507, top1: 17.1875\n",
      "Epoch [19/60], Iter [238/633], LR: 0.005000, Loss: 3.5509, top1: 15.6250\n",
      "Epoch [19/60], Iter [239/633], LR: 0.005000, Loss: 3.5683, top1: 17.1875\n",
      "Epoch [19/60], Iter [240/633], LR: 0.005000, Loss: 3.6142, top1: 10.9375\n",
      "Epoch [19/60], Iter [241/633], LR: 0.005000, Loss: 3.5769, top1: 18.7500\n",
      "Epoch [19/60], Iter [242/633], LR: 0.005000, Loss: 3.5646, top1: 14.0625\n",
      "Epoch [19/60], Iter [243/633], LR: 0.005000, Loss: 3.5562, top1: 17.1875\n",
      "Epoch [19/60], Iter [244/633], LR: 0.005000, Loss: 3.5760, top1: 17.1875\n",
      "Epoch [19/60], Iter [245/633], LR: 0.005000, Loss: 3.6040, top1: 14.0625\n",
      "Epoch [19/60], Iter [246/633], LR: 0.005000, Loss: 3.5785, top1: 15.6250\n",
      "Epoch [19/60], Iter [247/633], LR: 0.005000, Loss: 3.5610, top1: 17.1875\n",
      "Epoch [19/60], Iter [248/633], LR: 0.005000, Loss: 3.5543, top1: 14.0625\n",
      "Epoch [19/60], Iter [249/633], LR: 0.005000, Loss: 3.5583, top1: 14.0625\n",
      "Epoch [19/60], Iter [250/633], LR: 0.005000, Loss: 3.5501, top1: 18.7500\n",
      "Epoch [19/60], Iter [251/633], LR: 0.005000, Loss: 3.5994, top1: 6.2500\n",
      "Epoch [19/60], Iter [252/633], LR: 0.005000, Loss: 3.5828, top1: 10.9375\n",
      "Epoch [19/60], Iter [253/633], LR: 0.005000, Loss: 3.5493, top1: 18.7500\n",
      "Epoch [19/60], Iter [254/633], LR: 0.005000, Loss: 3.5716, top1: 14.0625\n",
      "Epoch [19/60], Iter [255/633], LR: 0.005000, Loss: 3.5637, top1: 9.3750\n",
      "Epoch [19/60], Iter [256/633], LR: 0.005000, Loss: 3.5297, top1: 17.1875\n",
      "Epoch [19/60], Iter [257/633], LR: 0.005000, Loss: 3.5902, top1: 15.6250\n",
      "Epoch [19/60], Iter [258/633], LR: 0.005000, Loss: 3.5713, top1: 17.1875\n",
      "Epoch [19/60], Iter [259/633], LR: 0.005000, Loss: 3.5509, top1: 12.5000\n",
      "Epoch [19/60], Iter [260/633], LR: 0.005000, Loss: 3.5634, top1: 14.0625\n",
      "Epoch [19/60], Iter [261/633], LR: 0.005000, Loss: 3.5554, top1: 17.1875\n",
      "Epoch [19/60], Iter [262/633], LR: 0.005000, Loss: 3.5906, top1: 12.5000\n",
      "Epoch [19/60], Iter [263/633], LR: 0.005000, Loss: 3.5190, top1: 15.6250\n",
      "Epoch [19/60], Iter [264/633], LR: 0.005000, Loss: 3.5818, top1: 10.9375\n",
      "Epoch [19/60], Iter [265/633], LR: 0.005000, Loss: 3.5477, top1: 20.3125\n",
      "Epoch [19/60], Iter [266/633], LR: 0.005000, Loss: 3.5656, top1: 21.8750\n",
      "Epoch [19/60], Iter [267/633], LR: 0.005000, Loss: 3.5900, top1: 10.9375\n",
      "Epoch [19/60], Iter [268/633], LR: 0.005000, Loss: 3.5880, top1: 10.9375\n",
      "Epoch [19/60], Iter [269/633], LR: 0.005000, Loss: 3.6115, top1: 7.8125\n",
      "Epoch [19/60], Iter [270/633], LR: 0.005000, Loss: 3.5590, top1: 14.0625\n",
      "Epoch [19/60], Iter [271/633], LR: 0.005000, Loss: 3.5374, top1: 12.5000\n",
      "Epoch [19/60], Iter [272/633], LR: 0.005000, Loss: 3.5693, top1: 12.5000\n",
      "Epoch [19/60], Iter [273/633], LR: 0.005000, Loss: 3.5703, top1: 12.5000\n",
      "Epoch [19/60], Iter [274/633], LR: 0.005000, Loss: 3.5642, top1: 10.9375\n",
      "Epoch [19/60], Iter [275/633], LR: 0.005000, Loss: 3.5698, top1: 12.5000\n",
      "Epoch [19/60], Iter [276/633], LR: 0.005000, Loss: 3.5598, top1: 17.1875\n",
      "Epoch [19/60], Iter [277/633], LR: 0.005000, Loss: 3.5539, top1: 15.6250\n",
      "Epoch [19/60], Iter [278/633], LR: 0.005000, Loss: 3.5926, top1: 15.6250\n",
      "Epoch [19/60], Iter [279/633], LR: 0.005000, Loss: 3.5970, top1: 18.7500\n",
      "Epoch [19/60], Iter [280/633], LR: 0.005000, Loss: 3.5177, top1: 23.4375\n",
      "Epoch [19/60], Iter [281/633], LR: 0.005000, Loss: 3.5226, top1: 17.1875\n",
      "Epoch [19/60], Iter [282/633], LR: 0.005000, Loss: 3.5822, top1: 14.0625\n",
      "Epoch [19/60], Iter [283/633], LR: 0.005000, Loss: 3.5594, top1: 14.0625\n",
      "Epoch [19/60], Iter [284/633], LR: 0.005000, Loss: 3.5929, top1: 12.5000\n",
      "Epoch [19/60], Iter [285/633], LR: 0.005000, Loss: 3.5312, top1: 21.8750\n",
      "Epoch [19/60], Iter [286/633], LR: 0.005000, Loss: 3.5839, top1: 14.0625\n",
      "Epoch [19/60], Iter [287/633], LR: 0.005000, Loss: 3.5797, top1: 12.5000\n",
      "Epoch [19/60], Iter [288/633], LR: 0.005000, Loss: 3.5042, top1: 23.4375\n",
      "Epoch [19/60], Iter [289/633], LR: 0.005000, Loss: 3.5103, top1: 20.3125\n",
      "Epoch [19/60], Iter [290/633], LR: 0.005000, Loss: 3.5574, top1: 17.1875\n",
      "Epoch [19/60], Iter [291/633], LR: 0.005000, Loss: 3.5674, top1: 17.1875\n",
      "Epoch [19/60], Iter [292/633], LR: 0.005000, Loss: 3.5963, top1: 7.8125\n",
      "Epoch [19/60], Iter [293/633], LR: 0.005000, Loss: 3.5501, top1: 17.1875\n",
      "Epoch [19/60], Iter [294/633], LR: 0.005000, Loss: 3.5105, top1: 20.3125\n",
      "Epoch [19/60], Iter [295/633], LR: 0.005000, Loss: 3.5179, top1: 20.3125\n",
      "Epoch [19/60], Iter [296/633], LR: 0.005000, Loss: 3.5487, top1: 18.7500\n",
      "Epoch [19/60], Iter [297/633], LR: 0.005000, Loss: 3.5837, top1: 14.0625\n",
      "Epoch [19/60], Iter [298/633], LR: 0.005000, Loss: 3.5763, top1: 10.9375\n",
      "Epoch [19/60], Iter [299/633], LR: 0.005000, Loss: 3.5752, top1: 12.5000\n",
      "Epoch [19/60], Iter [300/633], LR: 0.005000, Loss: 3.6145, top1: 6.2500\n",
      "Epoch [19/60], Iter [301/633], LR: 0.005000, Loss: 3.5385, top1: 18.7500\n",
      "Epoch [19/60], Iter [302/633], LR: 0.005000, Loss: 3.5844, top1: 15.6250\n",
      "Epoch [19/60], Iter [303/633], LR: 0.005000, Loss: 3.5489, top1: 17.1875\n",
      "Epoch [19/60], Iter [304/633], LR: 0.005000, Loss: 3.5404, top1: 21.8750\n",
      "Epoch [19/60], Iter [305/633], LR: 0.005000, Loss: 3.5510, top1: 12.5000\n",
      "Epoch [19/60], Iter [306/633], LR: 0.005000, Loss: 3.5765, top1: 12.5000\n",
      "Epoch [19/60], Iter [307/633], LR: 0.005000, Loss: 3.5500, top1: 15.6250\n",
      "Epoch [19/60], Iter [308/633], LR: 0.005000, Loss: 3.5253, top1: 18.7500\n",
      "Epoch [19/60], Iter [309/633], LR: 0.005000, Loss: 3.5777, top1: 18.7500\n",
      "Epoch [19/60], Iter [310/633], LR: 0.005000, Loss: 3.5405, top1: 15.6250\n",
      "Epoch [19/60], Iter [311/633], LR: 0.005000, Loss: 3.5765, top1: 15.6250\n",
      "Epoch [19/60], Iter [312/633], LR: 0.005000, Loss: 3.5649, top1: 14.0625\n",
      "Epoch [19/60], Iter [313/633], LR: 0.005000, Loss: 3.5576, top1: 17.1875\n",
      "Epoch [19/60], Iter [314/633], LR: 0.005000, Loss: 3.5926, top1: 15.6250\n",
      "Epoch [19/60], Iter [315/633], LR: 0.005000, Loss: 3.5264, top1: 18.7500\n",
      "Epoch [19/60], Iter [316/633], LR: 0.005000, Loss: 3.5233, top1: 21.8750\n",
      "Epoch [19/60], Iter [317/633], LR: 0.005000, Loss: 3.5973, top1: 7.8125\n",
      "Epoch [19/60], Iter [318/633], LR: 0.005000, Loss: 3.5510, top1: 18.7500\n",
      "Epoch [19/60], Iter [319/633], LR: 0.005000, Loss: 3.5425, top1: 20.3125\n",
      "Epoch [19/60], Iter [320/633], LR: 0.005000, Loss: 3.5837, top1: 12.5000\n",
      "Epoch [19/60], Iter [321/633], LR: 0.005000, Loss: 3.5726, top1: 17.1875\n",
      "Epoch [19/60], Iter [322/633], LR: 0.005000, Loss: 3.5767, top1: 17.1875\n",
      "Epoch [19/60], Iter [323/633], LR: 0.005000, Loss: 3.5531, top1: 17.1875\n",
      "Epoch [19/60], Iter [324/633], LR: 0.005000, Loss: 3.5705, top1: 20.3125\n",
      "Epoch [19/60], Iter [325/633], LR: 0.005000, Loss: 3.5823, top1: 12.5000\n",
      "Epoch [19/60], Iter [326/633], LR: 0.005000, Loss: 3.5700, top1: 15.6250\n",
      "Epoch [19/60], Iter [327/633], LR: 0.005000, Loss: 3.5992, top1: 9.3750\n",
      "Epoch [19/60], Iter [328/633], LR: 0.005000, Loss: 3.5662, top1: 12.5000\n",
      "Epoch [19/60], Iter [329/633], LR: 0.005000, Loss: 3.5417, top1: 14.0625\n",
      "Epoch [19/60], Iter [330/633], LR: 0.005000, Loss: 3.5492, top1: 21.8750\n",
      "Epoch [19/60], Iter [331/633], LR: 0.005000, Loss: 3.6184, top1: 7.8125\n",
      "Epoch [19/60], Iter [332/633], LR: 0.005000, Loss: 3.5408, top1: 14.0625\n",
      "Epoch [19/60], Iter [333/633], LR: 0.005000, Loss: 3.5355, top1: 18.7500\n",
      "Epoch [19/60], Iter [334/633], LR: 0.005000, Loss: 3.5600, top1: 15.6250\n",
      "Epoch [19/60], Iter [335/633], LR: 0.005000, Loss: 3.5740, top1: 14.0625\n",
      "Epoch [19/60], Iter [336/633], LR: 0.005000, Loss: 3.6072, top1: 10.9375\n",
      "Epoch [19/60], Iter [337/633], LR: 0.005000, Loss: 3.5502, top1: 14.0625\n",
      "Epoch [19/60], Iter [338/633], LR: 0.005000, Loss: 3.5978, top1: 10.9375\n",
      "Epoch [19/60], Iter [339/633], LR: 0.005000, Loss: 3.5587, top1: 20.3125\n",
      "Epoch [19/60], Iter [340/633], LR: 0.005000, Loss: 3.5324, top1: 21.8750\n",
      "Epoch [19/60], Iter [341/633], LR: 0.005000, Loss: 3.4903, top1: 25.0000\n",
      "Epoch [19/60], Iter [342/633], LR: 0.005000, Loss: 3.5301, top1: 20.3125\n",
      "Epoch [19/60], Iter [343/633], LR: 0.005000, Loss: 3.5674, top1: 18.7500\n",
      "Epoch [19/60], Iter [344/633], LR: 0.005000, Loss: 3.5638, top1: 15.6250\n",
      "Epoch [19/60], Iter [345/633], LR: 0.005000, Loss: 3.5389, top1: 21.8750\n",
      "Epoch [19/60], Iter [346/633], LR: 0.005000, Loss: 3.5695, top1: 12.5000\n",
      "Epoch [19/60], Iter [347/633], LR: 0.005000, Loss: 3.5478, top1: 20.3125\n",
      "Epoch [19/60], Iter [348/633], LR: 0.005000, Loss: 3.5439, top1: 17.1875\n",
      "Epoch [19/60], Iter [349/633], LR: 0.005000, Loss: 3.5437, top1: 18.7500\n",
      "Epoch [19/60], Iter [350/633], LR: 0.005000, Loss: 3.6119, top1: 12.5000\n",
      "Epoch [19/60], Iter [351/633], LR: 0.005000, Loss: 3.5853, top1: 10.9375\n",
      "Epoch [19/60], Iter [352/633], LR: 0.005000, Loss: 3.5838, top1: 9.3750\n",
      "Epoch [19/60], Iter [353/633], LR: 0.005000, Loss: 3.5879, top1: 12.5000\n",
      "Epoch [19/60], Iter [354/633], LR: 0.005000, Loss: 3.6075, top1: 9.3750\n",
      "Epoch [19/60], Iter [355/633], LR: 0.005000, Loss: 3.5884, top1: 10.9375\n",
      "Epoch [19/60], Iter [356/633], LR: 0.005000, Loss: 3.5635, top1: 14.0625\n",
      "Epoch [19/60], Iter [357/633], LR: 0.005000, Loss: 3.5561, top1: 14.0625\n",
      "Epoch [19/60], Iter [358/633], LR: 0.005000, Loss: 3.5923, top1: 14.0625\n",
      "Epoch [19/60], Iter [359/633], LR: 0.005000, Loss: 3.5435, top1: 17.1875\n",
      "Epoch [19/60], Iter [360/633], LR: 0.005000, Loss: 3.5269, top1: 20.3125\n",
      "Epoch [19/60], Iter [361/633], LR: 0.005000, Loss: 3.5716, top1: 14.0625\n",
      "Epoch [19/60], Iter [362/633], LR: 0.005000, Loss: 3.5640, top1: 14.0625\n",
      "Epoch [19/60], Iter [363/633], LR: 0.005000, Loss: 3.5477, top1: 20.3125\n",
      "Epoch [19/60], Iter [364/633], LR: 0.005000, Loss: 3.5699, top1: 10.9375\n",
      "Epoch [19/60], Iter [365/633], LR: 0.005000, Loss: 3.5785, top1: 12.5000\n",
      "Epoch [19/60], Iter [366/633], LR: 0.005000, Loss: 3.6005, top1: 9.3750\n",
      "Epoch [19/60], Iter [367/633], LR: 0.005000, Loss: 3.5463, top1: 20.3125\n",
      "Epoch [19/60], Iter [368/633], LR: 0.005000, Loss: 3.5562, top1: 15.6250\n",
      "Epoch [19/60], Iter [369/633], LR: 0.005000, Loss: 3.5577, top1: 12.5000\n",
      "Epoch [19/60], Iter [370/633], LR: 0.005000, Loss: 3.5414, top1: 12.5000\n",
      "Epoch [19/60], Iter [371/633], LR: 0.005000, Loss: 3.5315, top1: 12.5000\n",
      "Epoch [19/60], Iter [372/633], LR: 0.005000, Loss: 3.6227, top1: 10.9375\n",
      "Epoch [19/60], Iter [373/633], LR: 0.005000, Loss: 3.5880, top1: 10.9375\n",
      "Epoch [19/60], Iter [374/633], LR: 0.005000, Loss: 3.5581, top1: 18.7500\n",
      "Epoch [19/60], Iter [375/633], LR: 0.005000, Loss: 3.4940, top1: 21.8750\n",
      "Epoch [19/60], Iter [376/633], LR: 0.005000, Loss: 3.5781, top1: 18.7500\n",
      "Epoch [19/60], Iter [377/633], LR: 0.005000, Loss: 3.6470, top1: 6.2500\n",
      "Epoch [19/60], Iter [378/633], LR: 0.005000, Loss: 3.6103, top1: 14.0625\n",
      "Epoch [19/60], Iter [379/633], LR: 0.005000, Loss: 3.5891, top1: 14.0625\n",
      "Epoch [19/60], Iter [380/633], LR: 0.005000, Loss: 3.5796, top1: 15.6250\n",
      "Epoch [19/60], Iter [381/633], LR: 0.005000, Loss: 3.6086, top1: 10.9375\n",
      "Epoch [19/60], Iter [382/633], LR: 0.005000, Loss: 3.5582, top1: 15.6250\n",
      "Epoch [19/60], Iter [383/633], LR: 0.005000, Loss: 3.5388, top1: 14.0625\n",
      "Epoch [19/60], Iter [384/633], LR: 0.005000, Loss: 3.5872, top1: 10.9375\n",
      "Epoch [19/60], Iter [385/633], LR: 0.005000, Loss: 3.5971, top1: 14.0625\n",
      "Epoch [19/60], Iter [386/633], LR: 0.005000, Loss: 3.5424, top1: 18.7500\n",
      "Epoch [19/60], Iter [387/633], LR: 0.005000, Loss: 3.5292, top1: 18.7500\n",
      "Epoch [19/60], Iter [388/633], LR: 0.005000, Loss: 3.6000, top1: 14.0625\n",
      "Epoch [19/60], Iter [389/633], LR: 0.005000, Loss: 3.5951, top1: 9.3750\n",
      "Epoch [19/60], Iter [390/633], LR: 0.005000, Loss: 3.5897, top1: 14.0625\n",
      "Epoch [19/60], Iter [391/633], LR: 0.005000, Loss: 3.5291, top1: 15.6250\n",
      "Epoch [19/60], Iter [392/633], LR: 0.005000, Loss: 3.5751, top1: 14.0625\n",
      "Epoch [19/60], Iter [393/633], LR: 0.005000, Loss: 3.5629, top1: 20.3125\n",
      "Epoch [19/60], Iter [394/633], LR: 0.005000, Loss: 3.5785, top1: 12.5000\n",
      "Epoch [19/60], Iter [395/633], LR: 0.005000, Loss: 3.5498, top1: 17.1875\n",
      "Epoch [19/60], Iter [396/633], LR: 0.005000, Loss: 3.5866, top1: 17.1875\n",
      "Epoch [19/60], Iter [397/633], LR: 0.005000, Loss: 3.6075, top1: 10.9375\n",
      "Epoch [19/60], Iter [398/633], LR: 0.005000, Loss: 3.5522, top1: 21.8750\n",
      "Epoch [19/60], Iter [399/633], LR: 0.005000, Loss: 3.6284, top1: 7.8125\n",
      "Epoch [19/60], Iter [400/633], LR: 0.005000, Loss: 3.6228, top1: 9.3750\n",
      "Epoch [19/60], Iter [401/633], LR: 0.005000, Loss: 3.5179, top1: 23.4375\n",
      "Epoch [19/60], Iter [402/633], LR: 0.005000, Loss: 3.5510, top1: 15.6250\n",
      "Epoch [19/60], Iter [403/633], LR: 0.005000, Loss: 3.5749, top1: 12.5000\n",
      "Epoch [19/60], Iter [404/633], LR: 0.005000, Loss: 3.5745, top1: 14.0625\n",
      "Epoch [19/60], Iter [405/633], LR: 0.005000, Loss: 3.5559, top1: 14.0625\n",
      "Epoch [19/60], Iter [406/633], LR: 0.005000, Loss: 3.4959, top1: 20.3125\n",
      "Epoch [19/60], Iter [407/633], LR: 0.005000, Loss: 3.5735, top1: 12.5000\n",
      "Epoch [19/60], Iter [408/633], LR: 0.005000, Loss: 3.5441, top1: 23.4375\n",
      "Epoch [19/60], Iter [409/633], LR: 0.005000, Loss: 3.5869, top1: 7.8125\n",
      "Epoch [19/60], Iter [410/633], LR: 0.005000, Loss: 3.5757, top1: 17.1875\n",
      "Epoch [19/60], Iter [411/633], LR: 0.005000, Loss: 3.5831, top1: 15.6250\n",
      "Epoch [19/60], Iter [412/633], LR: 0.005000, Loss: 3.5684, top1: 18.7500\n",
      "Epoch [19/60], Iter [413/633], LR: 0.005000, Loss: 3.5795, top1: 15.6250\n",
      "Epoch [19/60], Iter [414/633], LR: 0.005000, Loss: 3.5285, top1: 14.0625\n",
      "Epoch [19/60], Iter [415/633], LR: 0.005000, Loss: 3.5363, top1: 18.7500\n",
      "Epoch [19/60], Iter [416/633], LR: 0.005000, Loss: 3.5452, top1: 18.7500\n",
      "Epoch [19/60], Iter [417/633], LR: 0.005000, Loss: 3.5625, top1: 10.9375\n",
      "Epoch [19/60], Iter [418/633], LR: 0.005000, Loss: 3.5596, top1: 17.1875\n",
      "Epoch [19/60], Iter [419/633], LR: 0.005000, Loss: 3.5221, top1: 21.8750\n",
      "Epoch [19/60], Iter [420/633], LR: 0.005000, Loss: 3.5962, top1: 14.0625\n",
      "Epoch [19/60], Iter [421/633], LR: 0.005000, Loss: 3.5511, top1: 18.7500\n",
      "Epoch [19/60], Iter [422/633], LR: 0.005000, Loss: 3.5777, top1: 6.2500\n",
      "Epoch [19/60], Iter [423/633], LR: 0.005000, Loss: 3.5618, top1: 15.6250\n",
      "Epoch [19/60], Iter [424/633], LR: 0.005000, Loss: 3.5574, top1: 10.9375\n",
      "Epoch [19/60], Iter [425/633], LR: 0.005000, Loss: 3.5538, top1: 17.1875\n",
      "Epoch [19/60], Iter [426/633], LR: 0.005000, Loss: 3.5378, top1: 17.1875\n",
      "Epoch [19/60], Iter [427/633], LR: 0.005000, Loss: 3.5632, top1: 17.1875\n",
      "Epoch [19/60], Iter [428/633], LR: 0.005000, Loss: 3.6101, top1: 7.8125\n",
      "Epoch [19/60], Iter [429/633], LR: 0.005000, Loss: 3.5657, top1: 15.6250\n",
      "Epoch [19/60], Iter [430/633], LR: 0.005000, Loss: 3.5502, top1: 17.1875\n",
      "Epoch [19/60], Iter [431/633], LR: 0.005000, Loss: 3.5800, top1: 14.0625\n",
      "Epoch [19/60], Iter [432/633], LR: 0.005000, Loss: 3.6191, top1: 10.9375\n",
      "Epoch [19/60], Iter [433/633], LR: 0.005000, Loss: 3.5567, top1: 12.5000\n",
      "Epoch [19/60], Iter [434/633], LR: 0.005000, Loss: 3.5793, top1: 15.6250\n",
      "Epoch [19/60], Iter [435/633], LR: 0.005000, Loss: 3.5724, top1: 17.1875\n",
      "Epoch [19/60], Iter [436/633], LR: 0.005000, Loss: 3.5899, top1: 7.8125\n",
      "Epoch [19/60], Iter [437/633], LR: 0.005000, Loss: 3.5949, top1: 14.0625\n",
      "Epoch [19/60], Iter [438/633], LR: 0.005000, Loss: 3.5878, top1: 14.0625\n",
      "Epoch [19/60], Iter [439/633], LR: 0.005000, Loss: 3.5737, top1: 17.1875\n",
      "Epoch [19/60], Iter [440/633], LR: 0.005000, Loss: 3.5631, top1: 20.3125\n",
      "Epoch [19/60], Iter [441/633], LR: 0.005000, Loss: 3.5551, top1: 18.7500\n",
      "Epoch [19/60], Iter [442/633], LR: 0.005000, Loss: 3.5732, top1: 15.6250\n",
      "Epoch [19/60], Iter [443/633], LR: 0.005000, Loss: 3.5277, top1: 18.7500\n",
      "Epoch [19/60], Iter [444/633], LR: 0.005000, Loss: 3.5095, top1: 21.8750\n",
      "Epoch [19/60], Iter [445/633], LR: 0.005000, Loss: 3.5801, top1: 15.6250\n",
      "Epoch [19/60], Iter [446/633], LR: 0.005000, Loss: 3.5284, top1: 15.6250\n",
      "Epoch [19/60], Iter [447/633], LR: 0.005000, Loss: 3.5603, top1: 14.0625\n",
      "Epoch [19/60], Iter [448/633], LR: 0.005000, Loss: 3.6161, top1: 7.8125\n",
      "Epoch [19/60], Iter [449/633], LR: 0.005000, Loss: 3.5908, top1: 15.6250\n",
      "Epoch [19/60], Iter [450/633], LR: 0.005000, Loss: 3.5322, top1: 12.5000\n",
      "Epoch [19/60], Iter [451/633], LR: 0.005000, Loss: 3.5879, top1: 10.9375\n",
      "Epoch [19/60], Iter [452/633], LR: 0.005000, Loss: 3.5039, top1: 21.8750\n",
      "Epoch [19/60], Iter [453/633], LR: 0.005000, Loss: 3.5213, top1: 17.1875\n",
      "Epoch [19/60], Iter [454/633], LR: 0.005000, Loss: 3.6010, top1: 12.5000\n",
      "Epoch [19/60], Iter [455/633], LR: 0.005000, Loss: 3.5314, top1: 15.6250\n",
      "Epoch [19/60], Iter [456/633], LR: 0.005000, Loss: 3.5573, top1: 15.6250\n",
      "Epoch [19/60], Iter [457/633], LR: 0.005000, Loss: 3.6172, top1: 9.3750\n",
      "Epoch [19/60], Iter [458/633], LR: 0.005000, Loss: 3.5267, top1: 15.6250\n",
      "Epoch [19/60], Iter [459/633], LR: 0.005000, Loss: 3.5980, top1: 10.9375\n",
      "Epoch [19/60], Iter [460/633], LR: 0.005000, Loss: 3.5493, top1: 15.6250\n",
      "Epoch [19/60], Iter [461/633], LR: 0.005000, Loss: 3.5153, top1: 29.6875\n",
      "Epoch [19/60], Iter [462/633], LR: 0.005000, Loss: 3.5438, top1: 20.3125\n",
      "Epoch [19/60], Iter [463/633], LR: 0.005000, Loss: 3.6062, top1: 7.8125\n",
      "Epoch [19/60], Iter [464/633], LR: 0.005000, Loss: 3.5873, top1: 12.5000\n",
      "Epoch [19/60], Iter [465/633], LR: 0.005000, Loss: 3.6003, top1: 15.6250\n",
      "Epoch [19/60], Iter [466/633], LR: 0.005000, Loss: 3.5563, top1: 14.0625\n",
      "Epoch [19/60], Iter [467/633], LR: 0.005000, Loss: 3.5810, top1: 12.5000\n",
      "Epoch [19/60], Iter [468/633], LR: 0.005000, Loss: 3.5714, top1: 15.6250\n",
      "Epoch [19/60], Iter [469/633], LR: 0.005000, Loss: 3.5402, top1: 18.7500\n",
      "Epoch [19/60], Iter [470/633], LR: 0.005000, Loss: 3.5488, top1: 15.6250\n",
      "Epoch [19/60], Iter [471/633], LR: 0.005000, Loss: 3.5874, top1: 17.1875\n",
      "Epoch [19/60], Iter [472/633], LR: 0.005000, Loss: 3.5320, top1: 18.7500\n",
      "Epoch [19/60], Iter [473/633], LR: 0.005000, Loss: 3.5580, top1: 18.7500\n",
      "Epoch [19/60], Iter [474/633], LR: 0.005000, Loss: 3.5669, top1: 17.1875\n",
      "Epoch [19/60], Iter [475/633], LR: 0.005000, Loss: 3.5083, top1: 31.2500\n",
      "Epoch [19/60], Iter [476/633], LR: 0.005000, Loss: 3.5233, top1: 23.4375\n",
      "Epoch [19/60], Iter [477/633], LR: 0.005000, Loss: 3.5562, top1: 17.1875\n",
      "Epoch [19/60], Iter [478/633], LR: 0.005000, Loss: 3.5552, top1: 17.1875\n",
      "Epoch [19/60], Iter [479/633], LR: 0.005000, Loss: 3.6030, top1: 15.6250\n",
      "Epoch [19/60], Iter [480/633], LR: 0.005000, Loss: 3.5890, top1: 10.9375\n",
      "Epoch [19/60], Iter [481/633], LR: 0.005000, Loss: 3.5538, top1: 14.0625\n",
      "Epoch [19/60], Iter [482/633], LR: 0.005000, Loss: 3.5670, top1: 14.0625\n",
      "Epoch [19/60], Iter [483/633], LR: 0.005000, Loss: 3.5680, top1: 15.6250\n",
      "Epoch [19/60], Iter [484/633], LR: 0.005000, Loss: 3.5577, top1: 18.7500\n",
      "Epoch [19/60], Iter [485/633], LR: 0.005000, Loss: 3.5459, top1: 14.0625\n",
      "Epoch [19/60], Iter [486/633], LR: 0.005000, Loss: 3.5850, top1: 12.5000\n",
      "Epoch [19/60], Iter [487/633], LR: 0.005000, Loss: 3.5689, top1: 17.1875\n",
      "Epoch [19/60], Iter [488/633], LR: 0.005000, Loss: 3.5945, top1: 12.5000\n",
      "Epoch [19/60], Iter [489/633], LR: 0.005000, Loss: 3.5142, top1: 17.1875\n",
      "Epoch [19/60], Iter [490/633], LR: 0.005000, Loss: 3.5691, top1: 12.5000\n",
      "Epoch [19/60], Iter [491/633], LR: 0.005000, Loss: 3.5795, top1: 17.1875\n",
      "Epoch [19/60], Iter [492/633], LR: 0.005000, Loss: 3.5953, top1: 14.0625\n",
      "Epoch [19/60], Iter [493/633], LR: 0.005000, Loss: 3.5622, top1: 15.6250\n",
      "Epoch [19/60], Iter [494/633], LR: 0.005000, Loss: 3.5280, top1: 17.1875\n",
      "Epoch [19/60], Iter [495/633], LR: 0.005000, Loss: 3.5293, top1: 21.8750\n",
      "Epoch [19/60], Iter [496/633], LR: 0.005000, Loss: 3.6076, top1: 10.9375\n",
      "Epoch [19/60], Iter [497/633], LR: 0.005000, Loss: 3.5350, top1: 12.5000\n",
      "Epoch [19/60], Iter [498/633], LR: 0.005000, Loss: 3.5546, top1: 15.6250\n",
      "Epoch [19/60], Iter [499/633], LR: 0.005000, Loss: 3.5408, top1: 14.0625\n",
      "Epoch [19/60], Iter [500/633], LR: 0.005000, Loss: 3.5536, top1: 18.7500\n",
      "Epoch [19/60], Iter [501/633], LR: 0.005000, Loss: 3.5433, top1: 17.1875\n",
      "Epoch [19/60], Iter [502/633], LR: 0.005000, Loss: 3.5529, top1: 17.1875\n",
      "Epoch [19/60], Iter [503/633], LR: 0.005000, Loss: 3.5527, top1: 18.7500\n",
      "Epoch [19/60], Iter [504/633], LR: 0.005000, Loss: 3.6593, top1: 7.8125\n",
      "Epoch [19/60], Iter [505/633], LR: 0.005000, Loss: 3.5252, top1: 20.3125\n",
      "Epoch [19/60], Iter [506/633], LR: 0.005000, Loss: 3.5556, top1: 12.5000\n",
      "Epoch [19/60], Iter [507/633], LR: 0.005000, Loss: 3.5653, top1: 15.6250\n",
      "Epoch [19/60], Iter [508/633], LR: 0.005000, Loss: 3.5842, top1: 7.8125\n",
      "Epoch [19/60], Iter [509/633], LR: 0.005000, Loss: 3.6022, top1: 12.5000\n",
      "Epoch [19/60], Iter [510/633], LR: 0.005000, Loss: 3.6097, top1: 12.5000\n",
      "Epoch [19/60], Iter [511/633], LR: 0.005000, Loss: 3.5505, top1: 12.5000\n",
      "Epoch [19/60], Iter [512/633], LR: 0.005000, Loss: 3.6037, top1: 12.5000\n",
      "Epoch [19/60], Iter [513/633], LR: 0.005000, Loss: 3.5807, top1: 9.3750\n",
      "Epoch [19/60], Iter [514/633], LR: 0.005000, Loss: 3.5382, top1: 15.6250\n",
      "Epoch [19/60], Iter [515/633], LR: 0.005000, Loss: 3.5492, top1: 20.3125\n",
      "Epoch [19/60], Iter [516/633], LR: 0.005000, Loss: 3.6295, top1: 14.0625\n",
      "Epoch [19/60], Iter [517/633], LR: 0.005000, Loss: 3.5242, top1: 20.3125\n",
      "Epoch [19/60], Iter [518/633], LR: 0.005000, Loss: 3.5679, top1: 12.5000\n",
      "Epoch [19/60], Iter [519/633], LR: 0.005000, Loss: 3.5503, top1: 17.1875\n",
      "Epoch [19/60], Iter [520/633], LR: 0.005000, Loss: 3.5625, top1: 15.6250\n",
      "Epoch [19/60], Iter [521/633], LR: 0.005000, Loss: 3.6334, top1: 6.2500\n",
      "Epoch [19/60], Iter [522/633], LR: 0.005000, Loss: 3.5757, top1: 15.6250\n",
      "Epoch [19/60], Iter [523/633], LR: 0.005000, Loss: 3.5391, top1: 21.8750\n",
      "Epoch [19/60], Iter [524/633], LR: 0.005000, Loss: 3.5828, top1: 15.6250\n",
      "Epoch [19/60], Iter [525/633], LR: 0.005000, Loss: 3.5685, top1: 6.2500\n",
      "Epoch [19/60], Iter [526/633], LR: 0.005000, Loss: 3.5511, top1: 14.0625\n",
      "Epoch [19/60], Iter [527/633], LR: 0.005000, Loss: 3.5901, top1: 10.9375\n",
      "Epoch [19/60], Iter [528/633], LR: 0.005000, Loss: 3.5753, top1: 15.6250\n",
      "Epoch [19/60], Iter [529/633], LR: 0.005000, Loss: 3.5921, top1: 14.0625\n",
      "Epoch [19/60], Iter [530/633], LR: 0.005000, Loss: 3.5486, top1: 17.1875\n",
      "Epoch [19/60], Iter [531/633], LR: 0.005000, Loss: 3.5442, top1: 20.3125\n",
      "Epoch [19/60], Iter [532/633], LR: 0.005000, Loss: 3.5637, top1: 14.0625\n",
      "Epoch [19/60], Iter [533/633], LR: 0.005000, Loss: 3.5516, top1: 12.5000\n",
      "Epoch [19/60], Iter [534/633], LR: 0.005000, Loss: 3.5851, top1: 14.0625\n",
      "Epoch [19/60], Iter [535/633], LR: 0.005000, Loss: 3.5925, top1: 12.5000\n",
      "Epoch [19/60], Iter [536/633], LR: 0.005000, Loss: 3.5842, top1: 17.1875\n",
      "Epoch [19/60], Iter [537/633], LR: 0.005000, Loss: 3.5263, top1: 23.4375\n",
      "Epoch [19/60], Iter [538/633], LR: 0.005000, Loss: 3.4849, top1: 29.6875\n",
      "Epoch [19/60], Iter [539/633], LR: 0.005000, Loss: 3.5125, top1: 20.3125\n",
      "Epoch [19/60], Iter [540/633], LR: 0.005000, Loss: 3.5419, top1: 14.0625\n",
      "Epoch [19/60], Iter [541/633], LR: 0.005000, Loss: 3.5831, top1: 12.5000\n",
      "Epoch [19/60], Iter [542/633], LR: 0.005000, Loss: 3.5437, top1: 20.3125\n",
      "Epoch [19/60], Iter [543/633], LR: 0.005000, Loss: 3.5716, top1: 12.5000\n",
      "Epoch [19/60], Iter [544/633], LR: 0.005000, Loss: 3.5001, top1: 26.5625\n",
      "Epoch [19/60], Iter [545/633], LR: 0.005000, Loss: 3.5565, top1: 10.9375\n",
      "Epoch [19/60], Iter [546/633], LR: 0.005000, Loss: 3.5699, top1: 12.5000\n",
      "Epoch [19/60], Iter [547/633], LR: 0.005000, Loss: 3.5627, top1: 17.1875\n",
      "Epoch [19/60], Iter [548/633], LR: 0.005000, Loss: 3.5632, top1: 17.1875\n",
      "Epoch [19/60], Iter [549/633], LR: 0.005000, Loss: 3.4936, top1: 25.0000\n",
      "Epoch [19/60], Iter [550/633], LR: 0.005000, Loss: 3.6180, top1: 14.0625\n",
      "Epoch [19/60], Iter [551/633], LR: 0.005000, Loss: 3.6043, top1: 12.5000\n",
      "Epoch [19/60], Iter [552/633], LR: 0.005000, Loss: 3.5740, top1: 12.5000\n",
      "Epoch [19/60], Iter [553/633], LR: 0.005000, Loss: 3.5899, top1: 12.5000\n",
      "Epoch [19/60], Iter [554/633], LR: 0.005000, Loss: 3.6063, top1: 12.5000\n",
      "Epoch [19/60], Iter [555/633], LR: 0.005000, Loss: 3.5917, top1: 14.0625\n",
      "Epoch [19/60], Iter [556/633], LR: 0.005000, Loss: 3.5756, top1: 10.9375\n",
      "Epoch [19/60], Iter [557/633], LR: 0.005000, Loss: 3.5654, top1: 14.0625\n",
      "Epoch [19/60], Iter [558/633], LR: 0.005000, Loss: 3.5151, top1: 25.0000\n",
      "Epoch [19/60], Iter [559/633], LR: 0.005000, Loss: 3.5356, top1: 14.0625\n",
      "Epoch [19/60], Iter [560/633], LR: 0.005000, Loss: 3.5909, top1: 17.1875\n",
      "Epoch [19/60], Iter [561/633], LR: 0.005000, Loss: 3.5953, top1: 9.3750\n",
      "Epoch [19/60], Iter [562/633], LR: 0.005000, Loss: 3.5941, top1: 10.9375\n",
      "Epoch [19/60], Iter [563/633], LR: 0.005000, Loss: 3.5877, top1: 14.0625\n",
      "Epoch [19/60], Iter [564/633], LR: 0.005000, Loss: 3.5743, top1: 14.0625\n",
      "Epoch [19/60], Iter [565/633], LR: 0.005000, Loss: 3.5516, top1: 18.7500\n",
      "Epoch [19/60], Iter [566/633], LR: 0.005000, Loss: 3.5602, top1: 15.6250\n",
      "Epoch [19/60], Iter [567/633], LR: 0.005000, Loss: 3.5895, top1: 10.9375\n",
      "Epoch [19/60], Iter [568/633], LR: 0.005000, Loss: 3.5245, top1: 20.3125\n",
      "Epoch [19/60], Iter [569/633], LR: 0.005000, Loss: 3.5432, top1: 15.6250\n",
      "Epoch [19/60], Iter [570/633], LR: 0.005000, Loss: 3.5240, top1: 15.6250\n",
      "Epoch [19/60], Iter [571/633], LR: 0.005000, Loss: 3.5329, top1: 21.8750\n",
      "Epoch [19/60], Iter [572/633], LR: 0.005000, Loss: 3.5862, top1: 9.3750\n",
      "Epoch [19/60], Iter [573/633], LR: 0.005000, Loss: 3.5735, top1: 15.6250\n",
      "Epoch [19/60], Iter [574/633], LR: 0.005000, Loss: 3.5874, top1: 9.3750\n",
      "Epoch [19/60], Iter [575/633], LR: 0.005000, Loss: 3.6089, top1: 10.9375\n",
      "Epoch [19/60], Iter [576/633], LR: 0.005000, Loss: 3.5620, top1: 12.5000\n",
      "Epoch [19/60], Iter [577/633], LR: 0.005000, Loss: 3.5327, top1: 18.7500\n",
      "Epoch [19/60], Iter [578/633], LR: 0.005000, Loss: 3.5746, top1: 18.7500\n",
      "Epoch [19/60], Iter [579/633], LR: 0.005000, Loss: 3.5294, top1: 21.8750\n",
      "Epoch [19/60], Iter [580/633], LR: 0.005000, Loss: 3.5957, top1: 18.7500\n",
      "Epoch [19/60], Iter [581/633], LR: 0.005000, Loss: 3.5703, top1: 20.3125\n",
      "Epoch [19/60], Iter [582/633], LR: 0.005000, Loss: 3.5545, top1: 18.7500\n",
      "Epoch [19/60], Iter [583/633], LR: 0.005000, Loss: 3.5653, top1: 18.7500\n",
      "Epoch [19/60], Iter [584/633], LR: 0.005000, Loss: 3.5513, top1: 14.0625\n",
      "Epoch [19/60], Iter [585/633], LR: 0.005000, Loss: 3.5847, top1: 14.0625\n",
      "Epoch [19/60], Iter [586/633], LR: 0.005000, Loss: 3.5268, top1: 15.6250\n",
      "Epoch [19/60], Iter [587/633], LR: 0.005000, Loss: 3.5493, top1: 15.6250\n",
      "Epoch [19/60], Iter [588/633], LR: 0.005000, Loss: 3.5733, top1: 15.6250\n",
      "Epoch [19/60], Iter [589/633], LR: 0.005000, Loss: 3.5786, top1: 14.0625\n",
      "Epoch [19/60], Iter [590/633], LR: 0.005000, Loss: 3.5407, top1: 17.1875\n",
      "Epoch [19/60], Iter [591/633], LR: 0.005000, Loss: 3.5270, top1: 21.8750\n",
      "Epoch [19/60], Iter [592/633], LR: 0.005000, Loss: 3.5529, top1: 15.6250\n",
      "Epoch [19/60], Iter [593/633], LR: 0.005000, Loss: 3.5726, top1: 20.3125\n",
      "Epoch [19/60], Iter [594/633], LR: 0.005000, Loss: 3.5264, top1: 20.3125\n",
      "Epoch [19/60], Iter [595/633], LR: 0.005000, Loss: 3.6069, top1: 10.9375\n",
      "Epoch [19/60], Iter [596/633], LR: 0.005000, Loss: 3.5639, top1: 17.1875\n",
      "Epoch [19/60], Iter [597/633], LR: 0.005000, Loss: 3.5463, top1: 17.1875\n",
      "Epoch [19/60], Iter [598/633], LR: 0.005000, Loss: 3.5357, top1: 18.7500\n",
      "Epoch [19/60], Iter [599/633], LR: 0.005000, Loss: 3.5878, top1: 14.0625\n",
      "Epoch [19/60], Iter [600/633], LR: 0.005000, Loss: 3.6061, top1: 10.9375\n",
      "Epoch [19/60], Iter [601/633], LR: 0.005000, Loss: 3.5629, top1: 14.0625\n",
      "Epoch [19/60], Iter [602/633], LR: 0.005000, Loss: 3.6179, top1: 10.9375\n",
      "Epoch [19/60], Iter [603/633], LR: 0.005000, Loss: 3.5524, top1: 14.0625\n",
      "Epoch [19/60], Iter [604/633], LR: 0.005000, Loss: 3.5868, top1: 15.6250\n",
      "Epoch [19/60], Iter [605/633], LR: 0.005000, Loss: 3.5689, top1: 18.7500\n",
      "Epoch [19/60], Iter [606/633], LR: 0.005000, Loss: 3.5273, top1: 21.8750\n",
      "Epoch [19/60], Iter [607/633], LR: 0.005000, Loss: 3.5542, top1: 7.8125\n",
      "Epoch [19/60], Iter [608/633], LR: 0.005000, Loss: 3.5851, top1: 9.3750\n",
      "Epoch [19/60], Iter [609/633], LR: 0.005000, Loss: 3.5641, top1: 15.6250\n",
      "Epoch [19/60], Iter [610/633], LR: 0.005000, Loss: 3.5628, top1: 14.0625\n",
      "Epoch [19/60], Iter [611/633], LR: 0.005000, Loss: 3.5432, top1: 21.8750\n",
      "Epoch [19/60], Iter [612/633], LR: 0.005000, Loss: 3.5703, top1: 12.5000\n",
      "Epoch [19/60], Iter [613/633], LR: 0.005000, Loss: 3.5129, top1: 17.1875\n",
      "Epoch [19/60], Iter [614/633], LR: 0.005000, Loss: 3.5952, top1: 15.6250\n",
      "Epoch [19/60], Iter [615/633], LR: 0.005000, Loss: 3.5714, top1: 10.9375\n",
      "Epoch [19/60], Iter [616/633], LR: 0.005000, Loss: 3.5486, top1: 15.6250\n",
      "Epoch [19/60], Iter [617/633], LR: 0.005000, Loss: 3.5614, top1: 14.0625\n",
      "Epoch [19/60], Iter [618/633], LR: 0.005000, Loss: 3.5919, top1: 14.0625\n",
      "Epoch [19/60], Iter [619/633], LR: 0.005000, Loss: 3.5363, top1: 17.1875\n",
      "Epoch [19/60], Iter [620/633], LR: 0.005000, Loss: 3.6175, top1: 9.3750\n",
      "Epoch [19/60], Iter [621/633], LR: 0.005000, Loss: 3.5677, top1: 14.0625\n",
      "Epoch [19/60], Iter [622/633], LR: 0.005000, Loss: 3.5607, top1: 14.0625\n",
      "Epoch [19/60], Iter [623/633], LR: 0.005000, Loss: 3.6372, top1: 1.5625\n",
      "Epoch [19/60], Iter [624/633], LR: 0.005000, Loss: 3.5935, top1: 15.6250\n",
      "Epoch [19/60], Iter [625/633], LR: 0.005000, Loss: 3.5731, top1: 7.8125\n",
      "Epoch [19/60], Iter [626/633], LR: 0.005000, Loss: 3.5306, top1: 20.3125\n",
      "Epoch [19/60], Iter [627/633], LR: 0.005000, Loss: 3.5672, top1: 14.0625\n",
      "Epoch [19/60], Iter [628/633], LR: 0.005000, Loss: 3.6019, top1: 12.5000\n",
      "Epoch [19/60], Iter [629/633], LR: 0.005000, Loss: 3.5506, top1: 20.3125\n",
      "Epoch [19/60], Iter [630/633], LR: 0.005000, Loss: 3.5841, top1: 17.1875\n",
      "Epoch [19/60], Iter [631/633], LR: 0.005000, Loss: 3.5945, top1: 12.5000\n",
      "Epoch [19/60], Iter [632/633], LR: 0.005000, Loss: 3.4828, top1: 28.1250\n",
      "Epoch [19/60], Iter [633/633], LR: 0.005000, Loss: 3.6016, top1: 14.0625\n",
      "Epoch [19/60], Iter [634/633], LR: 0.005000, Loss: 3.5284, top1: 20.9677\n",
      "Epoch [19/60], Val_Loss: 3.5494, Val_top1: 16.2192, best_top1: 16.9674\n",
      "epoch time: 4.398421967029572 min\n",
      "Epoch [20/60], Iter [1/633], LR: 0.005000, Loss: 3.5179, top1: 18.7500\n",
      "Epoch [20/60], Iter [2/633], LR: 0.005000, Loss: 3.5690, top1: 15.6250\n",
      "Epoch [20/60], Iter [3/633], LR: 0.005000, Loss: 3.5883, top1: 12.5000\n",
      "Epoch [20/60], Iter [4/633], LR: 0.005000, Loss: 3.5683, top1: 15.6250\n",
      "Epoch [20/60], Iter [5/633], LR: 0.005000, Loss: 3.5800, top1: 12.5000\n",
      "Epoch [20/60], Iter [6/633], LR: 0.005000, Loss: 3.6125, top1: 12.5000\n",
      "Epoch [20/60], Iter [7/633], LR: 0.005000, Loss: 3.5780, top1: 10.9375\n",
      "Epoch [20/60], Iter [8/633], LR: 0.005000, Loss: 3.5561, top1: 15.6250\n",
      "Epoch [20/60], Iter [9/633], LR: 0.005000, Loss: 3.5890, top1: 14.0625\n",
      "Epoch [20/60], Iter [10/633], LR: 0.005000, Loss: 3.6102, top1: 9.3750\n",
      "Epoch [20/60], Iter [11/633], LR: 0.005000, Loss: 3.4984, top1: 20.3125\n",
      "Epoch [20/60], Iter [12/633], LR: 0.005000, Loss: 3.6095, top1: 10.9375\n",
      "Epoch [20/60], Iter [13/633], LR: 0.005000, Loss: 3.5498, top1: 20.3125\n",
      "Epoch [20/60], Iter [14/633], LR: 0.005000, Loss: 3.6248, top1: 9.3750\n",
      "Epoch [20/60], Iter [15/633], LR: 0.005000, Loss: 3.5726, top1: 12.5000\n",
      "Epoch [20/60], Iter [16/633], LR: 0.005000, Loss: 3.5496, top1: 20.3125\n",
      "Epoch [20/60], Iter [17/633], LR: 0.005000, Loss: 3.6177, top1: 9.3750\n",
      "Epoch [20/60], Iter [18/633], LR: 0.005000, Loss: 3.5418, top1: 17.1875\n",
      "Epoch [20/60], Iter [19/633], LR: 0.005000, Loss: 3.5459, top1: 23.4375\n",
      "Epoch [20/60], Iter [20/633], LR: 0.005000, Loss: 3.5462, top1: 18.7500\n",
      "Epoch [20/60], Iter [21/633], LR: 0.005000, Loss: 3.5575, top1: 18.7500\n",
      "Epoch [20/60], Iter [22/633], LR: 0.005000, Loss: 3.5702, top1: 14.0625\n",
      "Epoch [20/60], Iter [23/633], LR: 0.005000, Loss: 3.5664, top1: 12.5000\n",
      "Epoch [20/60], Iter [24/633], LR: 0.005000, Loss: 3.5284, top1: 21.8750\n",
      "Epoch [20/60], Iter [25/633], LR: 0.005000, Loss: 3.5682, top1: 17.1875\n",
      "Epoch [20/60], Iter [26/633], LR: 0.005000, Loss: 3.5187, top1: 18.7500\n",
      "Epoch [20/60], Iter [27/633], LR: 0.005000, Loss: 3.5536, top1: 15.6250\n",
      "Epoch [20/60], Iter [28/633], LR: 0.005000, Loss: 3.5310, top1: 18.7500\n",
      "Epoch [20/60], Iter [29/633], LR: 0.005000, Loss: 3.5242, top1: 15.6250\n",
      "Epoch [20/60], Iter [30/633], LR: 0.005000, Loss: 3.5694, top1: 15.6250\n",
      "Epoch [20/60], Iter [31/633], LR: 0.005000, Loss: 3.5715, top1: 15.6250\n",
      "Epoch [20/60], Iter [32/633], LR: 0.005000, Loss: 3.5254, top1: 17.1875\n",
      "Epoch [20/60], Iter [33/633], LR: 0.005000, Loss: 3.5533, top1: 12.5000\n",
      "Epoch [20/60], Iter [34/633], LR: 0.005000, Loss: 3.5933, top1: 10.9375\n",
      "Epoch [20/60], Iter [35/633], LR: 0.005000, Loss: 3.5686, top1: 10.9375\n",
      "Epoch [20/60], Iter [36/633], LR: 0.005000, Loss: 3.5108, top1: 15.6250\n",
      "Epoch [20/60], Iter [37/633], LR: 0.005000, Loss: 3.6002, top1: 15.6250\n",
      "Epoch [20/60], Iter [38/633], LR: 0.005000, Loss: 3.5484, top1: 17.1875\n",
      "Epoch [20/60], Iter [39/633], LR: 0.005000, Loss: 3.5321, top1: 23.4375\n",
      "Epoch [20/60], Iter [40/633], LR: 0.005000, Loss: 3.5799, top1: 17.1875\n",
      "Epoch [20/60], Iter [41/633], LR: 0.005000, Loss: 3.5136, top1: 23.4375\n",
      "Epoch [20/60], Iter [42/633], LR: 0.005000, Loss: 3.5873, top1: 10.9375\n",
      "Epoch [20/60], Iter [43/633], LR: 0.005000, Loss: 3.6234, top1: 6.2500\n",
      "Epoch [20/60], Iter [44/633], LR: 0.005000, Loss: 3.5578, top1: 17.1875\n",
      "Epoch [20/60], Iter [45/633], LR: 0.005000, Loss: 3.5702, top1: 17.1875\n",
      "Epoch [20/60], Iter [46/633], LR: 0.005000, Loss: 3.5299, top1: 25.0000\n",
      "Epoch [20/60], Iter [47/633], LR: 0.005000, Loss: 3.5487, top1: 12.5000\n",
      "Epoch [20/60], Iter [48/633], LR: 0.005000, Loss: 3.5810, top1: 14.0625\n",
      "Epoch [20/60], Iter [49/633], LR: 0.005000, Loss: 3.6023, top1: 7.8125\n",
      "Epoch [20/60], Iter [50/633], LR: 0.005000, Loss: 3.5122, top1: 20.3125\n",
      "Epoch [20/60], Iter [51/633], LR: 0.005000, Loss: 3.5697, top1: 14.0625\n",
      "Epoch [20/60], Iter [52/633], LR: 0.005000, Loss: 3.6081, top1: 10.9375\n",
      "Epoch [20/60], Iter [53/633], LR: 0.005000, Loss: 3.6523, top1: 7.8125\n",
      "Epoch [20/60], Iter [54/633], LR: 0.005000, Loss: 3.5835, top1: 10.9375\n",
      "Epoch [20/60], Iter [55/633], LR: 0.005000, Loss: 3.5658, top1: 14.0625\n",
      "Epoch [20/60], Iter [56/633], LR: 0.005000, Loss: 3.5131, top1: 17.1875\n",
      "Epoch [20/60], Iter [57/633], LR: 0.005000, Loss: 3.6064, top1: 15.6250\n",
      "Epoch [20/60], Iter [58/633], LR: 0.005000, Loss: 3.5606, top1: 12.5000\n",
      "Epoch [20/60], Iter [59/633], LR: 0.005000, Loss: 3.5455, top1: 15.6250\n",
      "Epoch [20/60], Iter [60/633], LR: 0.005000, Loss: 3.5868, top1: 14.0625\n",
      "Epoch [20/60], Iter [61/633], LR: 0.005000, Loss: 3.5884, top1: 10.9375\n",
      "Epoch [20/60], Iter [62/633], LR: 0.005000, Loss: 3.5584, top1: 17.1875\n",
      "Epoch [20/60], Iter [63/633], LR: 0.005000, Loss: 3.5745, top1: 12.5000\n",
      "Epoch [20/60], Iter [64/633], LR: 0.005000, Loss: 3.5447, top1: 17.1875\n",
      "Epoch [20/60], Iter [65/633], LR: 0.005000, Loss: 3.5681, top1: 15.6250\n",
      "Epoch [20/60], Iter [66/633], LR: 0.005000, Loss: 3.5379, top1: 15.6250\n",
      "Epoch [20/60], Iter [67/633], LR: 0.005000, Loss: 3.5208, top1: 17.1875\n",
      "Epoch [20/60], Iter [68/633], LR: 0.005000, Loss: 3.5288, top1: 15.6250\n",
      "Epoch [20/60], Iter [69/633], LR: 0.005000, Loss: 3.5677, top1: 17.1875\n",
      "Epoch [20/60], Iter [70/633], LR: 0.005000, Loss: 3.5235, top1: 21.8750\n",
      "Epoch [20/60], Iter [71/633], LR: 0.005000, Loss: 3.5801, top1: 9.3750\n",
      "Epoch [20/60], Iter [72/633], LR: 0.005000, Loss: 3.5989, top1: 10.9375\n",
      "Epoch [20/60], Iter [73/633], LR: 0.005000, Loss: 3.5999, top1: 15.6250\n",
      "Epoch [20/60], Iter [74/633], LR: 0.005000, Loss: 3.5424, top1: 21.8750\n",
      "Epoch [20/60], Iter [75/633], LR: 0.005000, Loss: 3.5960, top1: 12.5000\n",
      "Epoch [20/60], Iter [76/633], LR: 0.005000, Loss: 3.5609, top1: 17.1875\n",
      "Epoch [20/60], Iter [77/633], LR: 0.005000, Loss: 3.5636, top1: 12.5000\n",
      "Epoch [20/60], Iter [78/633], LR: 0.005000, Loss: 3.5328, top1: 17.1875\n",
      "Epoch [20/60], Iter [79/633], LR: 0.005000, Loss: 3.5406, top1: 15.6250\n",
      "Epoch [20/60], Iter [80/633], LR: 0.005000, Loss: 3.5744, top1: 14.0625\n",
      "Epoch [20/60], Iter [81/633], LR: 0.005000, Loss: 3.5147, top1: 14.0625\n",
      "Epoch [20/60], Iter [82/633], LR: 0.005000, Loss: 3.5871, top1: 10.9375\n",
      "Epoch [20/60], Iter [83/633], LR: 0.005000, Loss: 3.6007, top1: 6.2500\n",
      "Epoch [20/60], Iter [84/633], LR: 0.005000, Loss: 3.5682, top1: 9.3750\n",
      "Epoch [20/60], Iter [85/633], LR: 0.005000, Loss: 3.5552, top1: 12.5000\n",
      "Epoch [20/60], Iter [86/633], LR: 0.005000, Loss: 3.5752, top1: 12.5000\n",
      "Epoch [20/60], Iter [87/633], LR: 0.005000, Loss: 3.5647, top1: 14.0625\n",
      "Epoch [20/60], Iter [88/633], LR: 0.005000, Loss: 3.5251, top1: 17.1875\n",
      "Epoch [20/60], Iter [89/633], LR: 0.005000, Loss: 3.5678, top1: 15.6250\n",
      "Epoch [20/60], Iter [90/633], LR: 0.005000, Loss: 3.6006, top1: 14.0625\n",
      "Epoch [20/60], Iter [91/633], LR: 0.005000, Loss: 3.5750, top1: 17.1875\n",
      "Epoch [20/60], Iter [92/633], LR: 0.005000, Loss: 3.5465, top1: 20.3125\n",
      "Epoch [20/60], Iter [93/633], LR: 0.005000, Loss: 3.5357, top1: 20.3125\n",
      "Epoch [20/60], Iter [94/633], LR: 0.005000, Loss: 3.5920, top1: 10.9375\n",
      "Epoch [20/60], Iter [95/633], LR: 0.005000, Loss: 3.5508, top1: 21.8750\n",
      "Epoch [20/60], Iter [96/633], LR: 0.005000, Loss: 3.5741, top1: 14.0625\n",
      "Epoch [20/60], Iter [97/633], LR: 0.005000, Loss: 3.5591, top1: 14.0625\n",
      "Epoch [20/60], Iter [98/633], LR: 0.005000, Loss: 3.5860, top1: 14.0625\n",
      "Epoch [20/60], Iter [99/633], LR: 0.005000, Loss: 3.6170, top1: 10.9375\n",
      "Epoch [20/60], Iter [100/633], LR: 0.005000, Loss: 3.5615, top1: 10.9375\n",
      "Epoch [20/60], Iter [101/633], LR: 0.005000, Loss: 3.5428, top1: 21.8750\n",
      "Epoch [20/60], Iter [102/633], LR: 0.005000, Loss: 3.5677, top1: 15.6250\n",
      "Epoch [20/60], Iter [103/633], LR: 0.005000, Loss: 3.5447, top1: 18.7500\n",
      "Epoch [20/60], Iter [104/633], LR: 0.005000, Loss: 3.5504, top1: 15.6250\n",
      "Epoch [20/60], Iter [105/633], LR: 0.005000, Loss: 3.5911, top1: 9.3750\n",
      "Epoch [20/60], Iter [106/633], LR: 0.005000, Loss: 3.5524, top1: 20.3125\n",
      "Epoch [20/60], Iter [107/633], LR: 0.005000, Loss: 3.5450, top1: 18.7500\n",
      "Epoch [20/60], Iter [108/633], LR: 0.005000, Loss: 3.5505, top1: 21.8750\n",
      "Epoch [20/60], Iter [109/633], LR: 0.005000, Loss: 3.5804, top1: 9.3750\n",
      "Epoch [20/60], Iter [110/633], LR: 0.005000, Loss: 3.5388, top1: 18.7500\n",
      "Epoch [20/60], Iter [111/633], LR: 0.005000, Loss: 3.6286, top1: 12.5000\n",
      "Epoch [20/60], Iter [112/633], LR: 0.005000, Loss: 3.5551, top1: 17.1875\n",
      "Epoch [20/60], Iter [113/633], LR: 0.005000, Loss: 3.5045, top1: 21.8750\n",
      "Epoch [20/60], Iter [114/633], LR: 0.005000, Loss: 3.5880, top1: 12.5000\n",
      "Epoch [20/60], Iter [115/633], LR: 0.005000, Loss: 3.4925, top1: 21.8750\n",
      "Epoch [20/60], Iter [116/633], LR: 0.005000, Loss: 3.5262, top1: 21.8750\n",
      "Epoch [20/60], Iter [117/633], LR: 0.005000, Loss: 3.5620, top1: 15.6250\n",
      "Epoch [20/60], Iter [118/633], LR: 0.005000, Loss: 3.5785, top1: 17.1875\n",
      "Epoch [20/60], Iter [119/633], LR: 0.005000, Loss: 3.5656, top1: 12.5000\n",
      "Epoch [20/60], Iter [120/633], LR: 0.005000, Loss: 3.5559, top1: 17.1875\n",
      "Epoch [20/60], Iter [121/633], LR: 0.005000, Loss: 3.5559, top1: 12.5000\n",
      "Epoch [20/60], Iter [122/633], LR: 0.005000, Loss: 3.5988, top1: 7.8125\n",
      "Epoch [20/60], Iter [123/633], LR: 0.005000, Loss: 3.5477, top1: 15.6250\n",
      "Epoch [20/60], Iter [124/633], LR: 0.005000, Loss: 3.6235, top1: 7.8125\n",
      "Epoch [20/60], Iter [125/633], LR: 0.005000, Loss: 3.5542, top1: 18.7500\n",
      "Epoch [20/60], Iter [126/633], LR: 0.005000, Loss: 3.5104, top1: 20.3125\n",
      "Epoch [20/60], Iter [127/633], LR: 0.005000, Loss: 3.5561, top1: 17.1875\n",
      "Epoch [20/60], Iter [128/633], LR: 0.005000, Loss: 3.5587, top1: 15.6250\n",
      "Epoch [20/60], Iter [129/633], LR: 0.005000, Loss: 3.5436, top1: 18.7500\n",
      "Epoch [20/60], Iter [130/633], LR: 0.005000, Loss: 3.5371, top1: 15.6250\n",
      "Epoch [20/60], Iter [131/633], LR: 0.005000, Loss: 3.5699, top1: 12.5000\n",
      "Epoch [20/60], Iter [132/633], LR: 0.005000, Loss: 3.5525, top1: 14.0625\n",
      "Epoch [20/60], Iter [133/633], LR: 0.005000, Loss: 3.5481, top1: 14.0625\n",
      "Epoch [20/60], Iter [134/633], LR: 0.005000, Loss: 3.5589, top1: 17.1875\n",
      "Epoch [20/60], Iter [135/633], LR: 0.005000, Loss: 3.5495, top1: 14.0625\n",
      "Epoch [20/60], Iter [136/633], LR: 0.005000, Loss: 3.5423, top1: 20.3125\n",
      "Epoch [20/60], Iter [137/633], LR: 0.005000, Loss: 3.5761, top1: 17.1875\n",
      "Epoch [20/60], Iter [138/633], LR: 0.005000, Loss: 3.5164, top1: 28.1250\n",
      "Epoch [20/60], Iter [139/633], LR: 0.005000, Loss: 3.5948, top1: 12.5000\n",
      "Epoch [20/60], Iter [140/633], LR: 0.005000, Loss: 3.5908, top1: 9.3750\n",
      "Epoch [20/60], Iter [141/633], LR: 0.005000, Loss: 3.5756, top1: 14.0625\n",
      "Epoch [20/60], Iter [142/633], LR: 0.005000, Loss: 3.5174, top1: 20.3125\n",
      "Epoch [20/60], Iter [143/633], LR: 0.005000, Loss: 3.5275, top1: 18.7500\n",
      "Epoch [20/60], Iter [144/633], LR: 0.005000, Loss: 3.5947, top1: 9.3750\n",
      "Epoch [20/60], Iter [145/633], LR: 0.005000, Loss: 3.5503, top1: 18.7500\n",
      "Epoch [20/60], Iter [146/633], LR: 0.005000, Loss: 3.5373, top1: 17.1875\n",
      "Epoch [20/60], Iter [147/633], LR: 0.005000, Loss: 3.5691, top1: 12.5000\n",
      "Epoch [20/60], Iter [148/633], LR: 0.005000, Loss: 3.5722, top1: 10.9375\n",
      "Epoch [20/60], Iter [149/633], LR: 0.005000, Loss: 3.5415, top1: 15.6250\n",
      "Epoch [20/60], Iter [150/633], LR: 0.005000, Loss: 3.5661, top1: 15.6250\n",
      "Epoch [20/60], Iter [151/633], LR: 0.005000, Loss: 3.5736, top1: 17.1875\n",
      "Epoch [20/60], Iter [152/633], LR: 0.005000, Loss: 3.5675, top1: 12.5000\n",
      "Epoch [20/60], Iter [153/633], LR: 0.005000, Loss: 3.5542, top1: 18.7500\n",
      "Epoch [20/60], Iter [154/633], LR: 0.005000, Loss: 3.5578, top1: 15.6250\n",
      "Epoch [20/60], Iter [155/633], LR: 0.005000, Loss: 3.6241, top1: 10.9375\n",
      "Epoch [20/60], Iter [156/633], LR: 0.005000, Loss: 3.5602, top1: 17.1875\n",
      "Epoch [20/60], Iter [157/633], LR: 0.005000, Loss: 3.5399, top1: 21.8750\n",
      "Epoch [20/60], Iter [158/633], LR: 0.005000, Loss: 3.5469, top1: 17.1875\n"
     ]
    }
   ],
   "source": [
    "def create_weights(dataset, label_list):\n",
    "    label2name = {}\n",
    "    with open(label_list, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        name, label = line.strip().split(',')\n",
    "        label2name[int(label)] = name\n",
    "    name2num = {}\n",
    "    path = os.path.join(config.data_root, 'designer_image_train_v2_cropped', 'designer_image_train_v2_cropped')\n",
    "    for name in os.listdir(path):\n",
    "        name2num[name] = len(os.listdir(os.path.join(path, name)))\n",
    "\n",
    "    weights = []\n",
    "    for im, label in dataset:\n",
    "        name = label2name[label]\n",
    "        num = name2num[name]\n",
    "        weights.append(1./num)\n",
    "\n",
    "    return weights\n",
    "    \n",
    "\n",
    "def train():\n",
    "    # model\n",
    "    if config.model == 'ResNet18_plus_plus':\n",
    "        backbone = models.resnet18(pretrained=True)\n",
    "        model = ResNet18_plus_plus(backbone, num_classes=config.num_classes)\n",
    "    \n",
    "    if config.model == 'ResNet18':\n",
    "        backbone = models.resnet18(pretrained=True)\n",
    "        model = ResNet18(backbone, num_classes=config.num_classes)\n",
    "    elif config.model == 'ResNet34':\n",
    "        backbone = models.resnet34(pretrained=True)\n",
    "        model = ResNet34(backbone, num_classes=config.num_classes)\n",
    "    elif config.model == 'ResNet50':\n",
    "        backbone = models.resnet50(pretrained=True)\n",
    "        model = ResNet50(backbone, num_classes=config.num_classes)\n",
    "    elif config.model == 'ResNet101':\n",
    "        backbone = models.resnet101(pretrained=True)\n",
    "        model = ResNet101(backbone, num_classes=config.num_classes)\n",
    "    elif config.model == 'ResNet152':\n",
    "        backbone = models.resnet152(pretrained=True)\n",
    "        model = ResNet152(backbone, num_classes=config.num_classes)\n",
    "    elif config.model == 'se_resnet50':\n",
    "        backbone = pretrainedmodels.__dict__['se_resnet50'](pretrained='imagenet')\n",
    "        model = se_resnet50(backbone, num_classes=config.num_classes)\n",
    "    else:\n",
    "        print('ERROR: No model {}!!!'.format(config.model))\n",
    "    print(model)\n",
    "    # model = torch.nn.DataParallel(model)\n",
    "    model.cuda()\n",
    "    \n",
    "    # freeze layers\n",
    "    if config.freeze:\n",
    "        for p in model.backbone.layer1.parameters(): p.requires_grad = False\n",
    "        for p in model.backbone.layer2.parameters(): p.requires_grad = False\n",
    "        for p in model.backbone.layer3.parameters(): p.requires_grad = False\n",
    "        #for p in model.backbone.layer4.parameters(): p.requires_grad = False\n",
    "\n",
    "\n",
    "    # loss\n",
    "    # criterion = nn.CrossEntropyLoss().cuda()\n",
    "    criterion = FocalLoss(config.num_classes, alpha=None, gamma=2, size_average=True)\n",
    "    # criterion = LabelSmoothing(config.num_classes, 0, 0.1)\n",
    "\n",
    "    # train data\n",
    "    transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ColorJitter(0.05, 0.05, 0.05),\n",
    "                                    transforms.RandomRotation(10),\n",
    "                                    transforms.Resize((config.width, config.height)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                         std=[0.229, 0.224, 0.225])])\n",
    "    dst_train = iDesignerDataset('./data/train.txt', transform=transform)\n",
    "#     weights = create_weights(dst_train, './data/label_list.txt')\n",
    "#     sampler = WeightedRandomSampler(weights, num_samples=len(dst_train), replacement=True)\n",
    "    # 会根据每个样本的权重选取数据，在样本比例不均衡的问题中，可用它来进行重采样。\n",
    "#     dataloader_train = DataLoader(dst_train, shuffle=False, batch_size=config.batch_size, \n",
    "#                                   num_workers=config.num_workers, sampler=sampler)\n",
    "\n",
    "    dataloader_train = DataLoader(dst_train, shuffle=True, batch_size=int(config.batch_size), \n",
    "                                  num_workers=config.num_workers)\n",
    "\n",
    "    # validation data\n",
    "    transform = transforms.Compose([transforms.Resize((config.width, config.height)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                         std=[0.229, 0.224, 0.225])])\n",
    "    dst_valid = iDesignerDataset('./data/valid.txt', transform=transform)\n",
    "\n",
    "    dataloader_valid = DataLoader(dst_valid, shuffle=False, batch_size=int(config.batch_size/2), num_workers=config.num_workers)\n",
    "\n",
    "    # log\n",
    "    if not os.path.exists('./log'):\n",
    "        os.makedirs('./log')\n",
    "    log = open('./log/log.txt', 'a')\n",
    "\n",
    "    log.write('-'*30+'\\n')\n",
    "    log.write('model:{}\\nnum_classes:{}\\nnum_epoch:{}\\nim_width:{}\\nim_height:{}\\niter_smooth:{}\\n'.format(\n",
    "               config.model, config.num_classes, config.num_epochs,\n",
    "               config.width, config.height, config.iter_smooth))\n",
    "\n",
    "    # load checkpoint\n",
    "    if config.resume:\n",
    "        print('resume checkpoint...')\n",
    "        model = torch.load(os.path.join('./checkpoints', config.checkpoint))\n",
    "\n",
    "    # visdom\n",
    "    # vis = visdom.Visdom(env='kaggle_idesigner')\n",
    "\n",
    "    # optimizer\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=0.0002)\n",
    "    # optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    #                              lr=0.00001, betas=(0.9, 0.999), weight_decay=0.0002)\n",
    "    # optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    #                       lr=lr, momentum=1e-1, weight_decay=1e-4)\n",
    "\n",
    "    # adjust lr\n",
    "    # lr = half_lr(config.lr, epoch)\n",
    "    # lr = step_lr(epoch)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [40, 100, 150, 200], gamma=0.1)\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # train\n",
    "    sum = 0\n",
    "    train_loss_sum = 0\n",
    "    train_top1_sum = 0\n",
    "    max_val_top1_acc = 0\n",
    "    iters = 0\n",
    "    for epoch in range(config.num_epochs):\n",
    "        val_loss, val_top1  = eval(model, dataloader_valid, criterion)\n",
    "        ep_start = time.time()\n",
    "        lr = step_lr(epoch)\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                                     lr=lr, betas=(0.9, 0.999), weight_decay=0.0002)\n",
    "        model.train()\n",
    "        for i, (ims, label) in enumerate(dataloader_train):\n",
    "            input = Variable(ims).cuda()\n",
    "            target = Variable(label).cuda().long()\n",
    "\n",
    "            output = model(input)\n",
    "\n",
    "            if config.smooth_label:\n",
    "                smoothed_target = label_smoothing(output, target).cuda()\n",
    "                loss = F.kl_div(output, smoothed_target).cuda()\n",
    "            \n",
    "            # OHEM: online hard example mining\n",
    "            if not config.OHEM and not config.smooth_label:\n",
    "                loss = criterion(output, target)\n",
    "            elif config.OHEM:\n",
    "                if epoch < 50:\n",
    "                    loss = criterion(output, target)\n",
    "                else:\n",
    "                    loss = F.cross_entropy(output, target, reduce=False).cuda()\n",
    "                    OHEM, _ = loss.topk(int(config.num_classes*config.OHEM_ratio), dim=0, \n",
    "                                        largest=True, sorted=True)\n",
    "                    loss = OHEM.mean()\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #lr_scheduler.step()\n",
    "\n",
    "            acc = accuracy(output.data, target.data, topk=(1,))\n",
    "            train_loss_sum += loss.data.cpu().numpy()\n",
    "            train_top1_sum += acc[0]\n",
    "            sum += 1\n",
    "\n",
    "\n",
    "            if (i+1) % config.iter_smooth == 0:\n",
    "                iters += 1\n",
    "#                 vis.line(X=torch.FloatTensor([iters]), Y=torch.FloatTensor([train_loss_sum/sum]),\n",
    "#                          win='train_loss', update='append')\n",
    "#                 vis.line(X=torch.FloatTensor([iters]), Y=torch.FloatTensor([train_top1_sum/sum]),\n",
    "#                          win='train_acc_top1', update='append')\n",
    "\n",
    "                print('Epoch [%d/%d], Iter [%d/%d], LR: %.6f, Loss: %.4f, top1: %.4f'\n",
    "                       %(epoch+1, config.num_epochs, i+1, len(dst_train)//config.batch_size, lr,\n",
    "                        train_loss_sum/sum, train_top1_sum/sum))\n",
    "                log.write('Epoch [%d/%d], Iter [%d/%d], Loss: %.4f, top1: %.4f\\n'\n",
    "                           %(epoch+1, config.num_epochs, i+1, len(dst_train)//config.batch_size, \n",
    "                            train_loss_sum/sum, train_top1_sum/sum))\n",
    "                sum = 0\n",
    "                train_loss_sum = 0\n",
    "                train_top1_sum = 0\n",
    " \n",
    "        epoch_time = (time.time() - ep_start) / 60.\n",
    "        if epoch % 1 == 0 and epoch < config.num_epochs:\n",
    "            # eval\n",
    "            val_time_start = time.time()\n",
    "            val_loss, val_top1  = eval(model, dataloader_valid, criterion)\n",
    "            val_time = (time.time() - val_time_start) / 60.\n",
    "\n",
    "#             vis.line(X=torch.FloatTensor([epoch]), Y=torch.FloatTensor([val_loss]),\n",
    "#                      win='val_loss', update='append')\n",
    "#             vis.line(X=torch.FloatTensor([epoch]), Y=torch.FloatTensor([val_top1]),\n",
    "#                      win='val_acc_top1', update='append')\n",
    "\n",
    "            print('Epoch [%d/%d], Val_Loss: %.4f, Val_top1: %.4f, best_top1: %.4f'\n",
    "                   %(epoch+1, config.num_epochs, val_loss, val_top1, max_val_top1_acc))\n",
    "            print('epoch time: {} min'.format(epoch_time))\n",
    "            if val_top1[0].data > max_val_top1_acc:\n",
    "                max_val_top1_acc = val_top1[0].data\n",
    "                print('Taking top1 snapshot...')\n",
    "                if not os.path.exists('./checkpoints'):\n",
    "                    os.makedirs('./checkpoints')\n",
    "                torch.save(model, '{}/{}.pth'.format('checkpoints', config.model))\n",
    "\n",
    "            log.write('Epoch [%d/%d], Val_Loss: %.4f, Val_top1: %.4f, best_top1: %.4f\\n'\n",
    "                       %(epoch+1, config.num_epochs, val_loss, val_top1, max_val_top1_acc))\n",
    "        torch.save(model, '{}/{}_last.pth'.format('checkpoints', config.model))\n",
    "\n",
    "    log.write('-'*30+'\\n')\n",
    "    log.close()\n",
    "\n",
    "# validation\n",
    "def eval(model, dataloader_valid, criterion):\n",
    "    sum = 0\n",
    "    val_loss_sum = 0\n",
    "    val_top1_sum = 0\n",
    "    model.eval().cuda()\n",
    "    for ims, label in dataloader_valid:\n",
    "        input_val = Variable(ims).cuda()\n",
    "        target_val = Variable(label).cuda()\n",
    "        output_val = model(input_val)\n",
    "        loss = criterion(output_val, target_val)\n",
    "        acc_val = accuracy(output_val.data, target_val.data, topk=(1,))\n",
    "        \n",
    "        sum += 1\n",
    "        val_loss_sum += loss.data.cpu().numpy()\n",
    "        val_top1_sum += acc_val[0]\n",
    "    avg_loss = val_loss_sum / sum\n",
    "    avg_top1 = val_top1_sum / sum\n",
    "    return avg_loss, avg_top1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    transform = transforms.Compose([transforms.Resize((config.width, config.height)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                         std=[0.229, 0.224, 0.225])])\n",
    "    dst_train = iDesignerDataset('./data/train.txt', transform=transform)\n",
    "    weights = create_weights(dst_train, './data/label_list.txt')\n",
    "    print weights, len(weights)\n",
    "    '''\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18.pth  ResNet18_last.pth\r\n"
     ]
    }
   ],
   "source": [
    "ls checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/167 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|██████████| 167/167 [00:26<00:00,  5.28it/s]\n",
      "  0%|          | 0/167 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|██████████| 167/167 [00:21<00:00,  7.76it/s]\n",
      "  0%|          | 0/167 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [00:21<00:00,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def read_label(path):\n",
    "    data = open(path, 'r')\n",
    "    label2name = {}\n",
    "    for line in data.readlines():\n",
    "        name, label = line.strip().split('  ')\n",
    "        label2name[int(label)] = name\n",
    "    return label2name\n",
    "\n",
    "\n",
    "def inference():\n",
    "    # model\n",
    "    # load checkpoint\n",
    "    model = torch.load(os.path.join('./checkpoints', config.checkpoint))\n",
    "    # print model\n",
    "    # model = torch.nn.DataParallel(model)\n",
    "    model.cuda()\n",
    "    \n",
    "    # validation data\n",
    "    transform = transforms.Compose([transforms.Resize((config.width, config.height)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                         std=[0.229, 0.224, 0.225])])\n",
    "    # label2name\n",
    "    label2name = read_label('./data/label_list.txt')\n",
    "    \n",
    "    # TTA raw invert CenterCrop\n",
    "    augments = [0, 1, 2]\n",
    "    tta0, tta1, tta2 = {}, {}, {}\n",
    "    for idx in range(len(augments)):\n",
    "        print('TTA {}'.format(idx))\n",
    "        dst_test = iDesignerTestDataset(os.path.join(config.data_root, 'sample.csv'), \n",
    "                                 transform=transform, augment=augments[idx])\n",
    "        dataloader_test = DataLoader(dst_test, shuffle=False, batch_size=int(config.batch_size/2), \n",
    "                                      num_workers=config.num_workers)\n",
    "\n",
    "        sum = 0\n",
    "        model.eval()\n",
    "        results = []\n",
    "        probs_all = []\n",
    "        for ims, im_names in tqdm(dataloader_test):\n",
    "            input = Variable(ims).cuda()\n",
    "            output = model(input)\n",
    "\n",
    "            _, preds = output.topk(1, 1, True, True)\n",
    "            preds = preds.cpu().detach().numpy()\n",
    "            for pred, im_name in zip(preds, im_names):\n",
    "                top5_name = [label2name[p] for p in pred]\n",
    "                results.append({'Id':im_name, 'Category':''.join(top5_name)})\n",
    "\n",
    "            # TTA\n",
    "            probs = F.softmax(output)\n",
    "            probs = probs.cpu().detach().numpy()\n",
    "            for prob, im_name in zip(probs, im_names):\n",
    "                if idx == 0:\n",
    "                    tta0[im_name] = prob\n",
    "                elif idx == 1:\n",
    "                    tta1[im_name] = prob\n",
    "                elif idx == 2:\n",
    "                    tta2[im_name] = prob\n",
    "                else:\n",
    "                    print('Error: No  other TTA method!!!')\n",
    "                    break\n",
    "        # save no TTA\n",
    "        df = pd.DataFrame(results, columns=['Id', 'Category'])\n",
    "        df.to_csv('./data/result_no_TTA.csv', index=False)\n",
    "    # endemble TTA\n",
    "    #print tta0\n",
    "    tta_results = []\n",
    "    for key in tta0.keys():\n",
    "        prob = (tta0[key] + tta1[key] + tta2[key]) / 3.\n",
    "        top1_idx = prob.argsort()[-1]\n",
    "        top1_name = label2name[top1_idx]\n",
    "        tta_results.append({'Id':key, 'Category':top1_name})\n",
    "    # save TTA result\n",
    "    tta_df = pd.DataFrame(tta_results, columns=['Id', 'Category'])\n",
    "    tta_df.to_csv('./data/result_3TTA.csv', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    inference()\n",
    "    print('finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# see csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv_len:  5326\n",
      "['Id,Category\\n', '8820.png,jason wu\\n', '348.png,alexander wang\\n', '1804.png,chado ralph rucci\\n', '4968.png,dries van noten\\n', '8834.png,jil sander\\n', '6831.png,missoni\\n', '12907.png,carolina herrera\\n', '3791.png,versace\\n', '4798.png,givenchy\\n', '7285.png,giorgio armani\\n', '10862.png,yves saint laurent\\n', '4954.png,yves saint laurent\\n', '5492.png,missoni\\n', '3785.png,chado ralph rucci\\n', '8808.png,yves saint laurent\\n', '12913.png,carolina herrera\\n', '9525.png,derek lam\\n', '11229.png,marni\\n', '10137.png,alexander mcqueen\\n', '5479.png,missoni\\n', '6170.png,missoni\\n', '8149.png,bottega veneta\\n', '10123.png,chado ralph rucci\\n', '14345.png,marc by marc jacobs\\n', '5323.png,ralph lauren\\n', '1145.png,armani prive\\n', '9519.png,carolina herrera\\n', '7534.png,marni\\n', '7252.png,marc by marc jacobs\\n', '1623.png,bottega veneta\\n', '13364.png,yves saint laurent\\n', '5445.png,missoni\\n', '3034.png,jason wu\\n', '5451.png,missoni\\n', '11567.png,givenchy\\n', '8175.png,ralph lauren\\n', '6158.png,vera wang\\n', '1637.png,marc by marc jacobs\\n', '1151.png,armani prive\\n', '13416.png,yves saint laurent\\n', '11201.png,jil sander\\n', '3746.png,emporio armani\\n', '2458.png,armani prive\\n', '10490.png,vera wang\\n', '5860.png,jil sander\\n', '14184.png,jil sander\\n', '7905.png,carolina herrera\\n', '8388.png,jil sander\\n', '5684.png,jil sander\\n', '3587.png,etro\\n', '5848.png,jil sander\\n', '14635.png,dolce & gabbana\\n', '11995.png,vera wang\\n', '10453.png,vera wang\\n', '10335.png,chado ralph rucci\\n', '4203.png,marc jacobs\\n', '837.png,giorgio armani\\n', '8439.png,givenchy\\n', '823.png,armani prive\\n', '9733.png,jason wu\\n', '10321.png,chado ralph rucci\\n', '2666.png,vera wang\\n', '4217.png,dries van noten\\n', '3578.png,emporio armani\\n', '4571.png,alexander wang\\n', '11981.png,emporio armani\\n', '10447.png,alexander wang\\n', '2100.png,jil sander\\n', '14621.png,giorgio armani\\n', '3236.png,jason wu\\n', '1421.png,marc by marc jacobs\\n', '14609.png,etro\\n', '7050.png,jil sander\\n', '7736.png,marc jacobs\\n', '8405.png,jason wu\\n', '13600.png,yves saint laurent\\n', '2896.png,armani prive\\n', '3550.png,jil sander\\n', '2882.png,carolina herrera\\n', '13614.png,yves saint laurent\\n', '1353.png,etro\\n', '13172.png,ralph lauren\\n', '3222.png,jason wu\\n', '11765.png,marc jacobs\\n', '14782.png,dolce & gabbana\\n', '980.png,armani prive\\n', '12495.png,derek lam\\n', '7865.png,armani prive\\n', '9690.png,derek lam\\n', '10282.png,chado ralph rucci\\n', '994.png,armani prive\\n', '1596.png,miu miu\\n', '2921.png,miu miu\\n', '7859.png,derek lam\\n', '9860.png,marc jacobs\\n', '5082.png,marni\\n', '14741.png,etro\\n', '11639.png,missoni\\n', '3418.png,dries van noten\\n', '5069.png,derek lam\\n', '10241.png,chado ralph rucci\\n', '9653.png,jason wu\\n', '14027.png,alexander wang\\n', '13990.png,alexander wang\\n', '957.png,armani prive\\n', '4363.png,dries van noten\\n', '2712.png,emporio armani\\n', '2074.png,fendi\\n', '4405.png,chado ralph rucci\\n', '14755.png,dolce & gabbana\\n', '6212.png,missoni\\n', '7124.png,jil sander\\n', '8217.png,dries van noten\\n', '13012.png,carolina herrera\\n', '13774.png,jil sander\\n', '11163.png,missoni\\n', '5041.png,dries van noten\\n', '13760.png,miu miu\\n', '1227.png,marc by marc jacobs\\n', '7656.png,alexander wang\\n', '12318.png,giorgio armani\\n', '7130.png,jil sander\\n', '14769.png,vera wang\\n', '13006.png,carolina herrera\\n', '1541.png,missoni\\n', '5727.png,jil sander\\n', '3356.png,emporio armani\\n', '11611.png,alexander mcqueen\\n', '10080.png,alexander wang\\n', '9492.png,etro\\n', '13589.png,yves saint laurent\\n', '4808.png,alexander wang\\n', '8798.png,etro\\n', '10094.png,missoni\\n', '8968.png,marc jacobs\\n', '566.png,bottega veneta\\n', '4834.png,emporio armani\\n', '1958.png,carolina herrera\\n', '2289.png,alexander wang\\n', '3197.png,chado ralph rucci\\n', '3829.png,emporio armani\\n', '572.png,bottega veneta\\n', '10043.png,marc by marc jacobs\\n', '4175.png,jason wu\\n', '6004.png,bottega veneta\\n', '12132.png,derek lam\\n', '14557.png,givenchy\\n', '5519.png,bottega veneta\\n', '2276.png,armani prive\\n', '10731.png,dries van noten\\n', '2510.png,yves saint laurent\\n', '8983.png,carolina herrera\\n', '5257.png,marni\\n', '11361.png,dolce & gabbana\\n', '3626.png,miu miu\\n', '12668.png,derek lam\\n', '14219.png,jason wu\\n', '13576.png,yves saint laurent\\n', '7326.png,marc jacobs\\n', '11407.png,versace\\n', '3140.png,jason wu\\n', '5531.png,carolina herrera\\n', '10719.png,derek lam\\n', '11413.png,marc jacobs\\n', '1743.png,carolina herrera\\n', '6992.png,chado ralph rucci\\n', '8767.png,emporio armani\\n', '7454.png,alexander wang\\n', '3632.png,emporio armani\\n', '7333.png,emporio armani\\n', '13205.png,carolina herrera\\n', '1742.png,jason wu\\n', '13563.png,yves saint laurent\\n', '9478.png,jil sander\\n', '13577.png,yves saint laurent\\n', '1030.png,armani prive\\n', '14218.png,emporio armani\\n', '8772.png,chado ralph rucci\\n', '12669.png,marc jacobs\\n', '6987.png,jil sander\\n', '4148.png,missoni\\n', '5256.png,miu miu\\n', '2539.png,dries van noten\\n', '3141.png,miu miu\\n', '8014.png,marc jacobs\\n', '6039.png,lanvin\\n', '13211.png,miu miu\\n', '2277.png,alexander wang\\n', '5518.png,dries van noten\\n', '12127.png,gucci\\n', '1018.png,marni\\n', '9444.png,emporio armani\\n', '12641.png,jason wu\\n', '14224.png,gucci\\n', '6005.png,alexander wang\\n', '4612.png,missoni\\n', '3196.png,marc jacobs\\n', '2288.png,alexander wang\\n', '4821.png,derek lam\\n', '1959.png,jason wu\\n', '7496.png,jil sander\\n', '12866.png,chado ralph rucci\\n', '8969.png,christian dior\\n', '567.png,bottega veneta\\n', '12872.png,vera wang\\n', '1795.png,vera wang\\n', '14595.png,dolce & gabbana\\n', '229.png,dries van noten\\n', '1965.png,marc by marc jacobs\\n', '3814.png,emporio armani\\n', '8799.png,giorgio armani\\n', '9487.png,missoni\\n', '13588.png,yves saint laurent\\n', '8564.png,etro\\n', '13761.png,marni\\n', '10268.png,chado ralph rucci\\n', '5040.png,dries van noten\\n', '11176.png,chado ralph rucci\\n', '3431.png,jil sander\\n', '4438.png,etro\\n', '2049.png,vera wang\\n', '1540.png,tommy hilfiger\\n', '13007.png,jason wu\\n', '8202.png,armani prive\\n', '1554.png,bottega veneta\\n', '3343.png,vera wang\\n', '5732.png,gucci\\n', '8570.png,givenchy\\n', '13775.png,marc by marc jacobs\\n', '4362.png,jil sander\\n', '13985.png,bottega veneta\\n', '956.png,alexander wang\\n', '9646.png,derek lam\\n', '14754.png,vera wang\\n', '10532.png,vera wang\\n', '11638.png,alexander wang\\n', '12331.png,marc by marc jacobs\\n', '942.png,armani prive\\n', '13749.png,bottega veneta\\n', '2707.png,giambattista valli\\n', '2934.png,bottega veneta\\n', '771.png,yves saint laurent\\n', '7694.png,vera wang\\n', '5929.png,jil sander\\n', '3380.png,etro\\n', '9685.png,marni\\n', '995.png,marc jacobs\\n', '2908.png,jason wu\\n', '11189.png,fendi\\n', '5901.png,jil sander\\n', '14797.png,carolina herrera\\n', '14783.png,vera wang\\n', '10283.png,dries van noten\\n', '759.png,giorgio armani\\n', '1352.png,missoni\\n', '11002.png,marni\\n', '3545.png,yves saint laurent\\n', '1434.png,marc by marc jacobs\\n', '7051.png,jil sander\\n', '14608.png,dolce & gabbana\\n', '1420.png,marc by marc jacobs\\n', '2129.png,chado ralph rucci\\n', '11770.png,vera wang\\n', '3237.png,vera wang\\n', '2897.png,vera wang\\n', '5120.png,marc by marc jacobs\\n', '13601.png,yves saint laurent\\n', '7737.png,marc jacobs\\n', '10320.png,alexander wang\\n', '6401.png,marc jacobs\\n', '62.png,marni\\n', '6367.png,etro\\n', '14620.png,dolce & gabbana\\n', '2101.png,jason wu\\n', '11980.png,chado ralph rucci\\n', '4570.png,marc jacobs\\n', '11758.png,marni\\n', '2115.png,armani prive\\n', '10452.png,alexander wang\\n', '11994.png,etro\\n', '188.png,bottega veneta\\n', '9726.png,emporio armani\\n', '836.png,marc jacobs\\n', '10334.png,chado ralph rucci\\n', '2840.png,vera wang\\n', '605.png,giorgio armani\\n', '6398.png,vera wang\\n', '7086.png,jil sander\\n', '5691.png,jil sander\\n', '5685.png,marc by marc jacobs\\n', '7092.png,jil sander\\n', '13832.png,christian dior\\n', '639.png,giorgio armani\\n', '10485.png,emporio armani\\n', '13198.png,chado ralph rucci\\n', '5861.png,yves saint laurent\\n', '10491.png,vera wang\\n', '11957.png,ralph lauren\\n', '13826.png,marc jacobs\\n', '7247.png,balmain\\n', '3021.png,jason wu\\n', '4996.png,dries van noten\\n', '2459.png,gucci\\n', '5336.png,marni\\n', '3747.png,emporio armani\\n', '11200.png,marni\\n', '12709.png,jason wu\\n', '7521.png,missoni\\n', '13417.png,jason wu\\n', '9518.png,alexander wang\\n', '3753.png,emporio armani\\n', '4982.png,emporio armani\\n', '7253.png,miu miu\\n', '8160.png,chado ralph rucci\\n', '4772.png,marc by marc jacobs\\n', '10644.png,derek lam\\n', '2303.png,alexander wang\\n', '14422.png,dolce & gabbana\\n', '8148.png,vera wang\\n', '9530.png,marc jacobs\\n', '10122.png,chado ralph rucci\\n', '10136.png,chado ralph rucci\\n', '9524.png,alexander wang\\n', '6617.png,chado ralph rucci\\n', '12047.png,missoni\\n', '3009.png,jason wu\\n', '10888.png,derek lam\\n', '5478.png,missoni\\n', '2317.png,alexander wang\\n', '5493.png,missoni\\n', '361.png,bottega veneta\\n', '12912.png,dries van noten\\n', '407.png,carolina herrera\\n', '3948.png,jil sander\\n', '12906.png,carolina herrera\\n', '413.png,bottega veneta\\n', '7290.png,bottega veneta\\n', '375.png,bottega veneta\\n', '5487.png,missoni\\n', '1811.png,fendi\\n', '9295.png,dries van noten\\n', '4969.png,dries van noten\\n', '10687.png,vera wang\\n', '11599.png,carolina herrera\\n', '14393.png,bottega veneta\\n', '9281.png,etro\\n', '3962.png,yves saint laurent\\n', '12092.png,derek lam\\n', '1813.png,chado ralph rucci\\n', '13398.png,yves saint laurent\\n', '12086.png,vera wang\\n', '1807.png,jil sander\\n', '14391.png,alexander wang\\n', '2498.png,vera wang\\n', '6826.png,armani prive\\n', '363.png,bottega veneta\\n', '10861.png,emporio armani\\n', '4957.png,dries van noten\\n', '5491.png,yves saint laurent\\n', '6832.png,bottega veneta\\n', '13429.png,yves saint laurent\\n', '9532.png,alexander wang\\n', '14346.png,gucci\\n', '12737.png,givenchy\\n', '7279.png,etro\\n', '9254.png,missoni\\n', '10646.png,vera wang\\n', '2301.png,emporio armani\\n', '4770.png,derek lam\\n', '10652.png,vera wang\\n', '14434.png,marc by marc jacobs\\n', '9526.png,chado ralph rucci\\n', '6615.png,chado ralph rucci\\n', '8638.png,jason wu\\n', '12723.png,jason wu\\n', '4002.png,emporio armani\\n', '2473.png,chado ralph rucci\\n', '5334.png,givenchy\\n', '4994.png,dries van noten\\n', '5452.png,emporio armani\\n', '3023.png,jason wu\\n', '14408.png,derek lam\\n', '13367.png,ralph lauren\\n', '5320.png,bottega veneta\\n', '10108.png,chado ralph rucci\\n', '13401.png,gucci\\n', '1146.png,bottega veneta\\n', '8604.png,bottega veneta\\n', '12290.png,jil sander\\n', '14839.png,carolina herrera\\n', '10487.png,missoni\\n', '11799.png,marc jacobs\\n', '14187.png,fendi\\n', '13824.png,carolina herrera\\n', '10493.png,vera wang\\n', '7084.png,jil sander\\n', '607.png,giorgio armani\\n', '9917.png,jil sander\\n', '2842.png,miu miu\\n', '1387.png,marc by marc jacobs\\n', '11969.png,chado ralph rucci\\n', '4599.png,givenchy\\n', '2103.png,ralph lauren\\n', '10444.png,vera wang\\n', '9730.png,derek lam\\n', '2665.png,derek lam\\n', '10322.png,missoni\\n', '4214.png,derek lam\\n', '2671.png,yves saint laurent\\n', '7709.png,chado ralph rucci\\n', '6417.png,chado ralph rucci\\n', '9724.png,derek lam\\n', '1378.png,marc by marc jacobs\\n', '14636.png,jil sander\\n', '5678.png,jil sander\\n', '2117.png,alexander wang\\n', '8374.png,marni\\n', '4228.png,derek lam\\n', '2881.png,yves saint laurent\\n', '1350.png,jil sander\\n', '13617.png,armani prive\\n', '8406.png,givenchy\\n', '9718.png,derek lam\\n', '11772.png,carolina herrera\\n', '3235.png,vera wang\\n', '1422.png,marc by marc jacobs\\n', '5903.png,miu miu\\n', '9687.png,derek lam\\n', '13788.png,dolce & gabbana\\n', '7866.png,derek lam\\n', '9693.png,miu miu\\n', '14781.png,dolce & gabbana\\n', '2088.png,alexander wang\\n', '5095.png,marc by marc jacobs\\n', '9877.png,emporio armani\\n', '767.png,armani prive\\n', '10530.png,vera wang\\n', '6211.png,derek lam\\n', '13039.png,carolina herrera\\n', '1218.png,marc by marc jacobs\\n', '14030.png,vera wang\\n', '9644.png,derek lam\\n', '13987.png,dries van noten\\n', '954.png,armani prive\\n', '14024.png,dries van noten\\n', '13993.png,gucci\\n', '12455.png,missoni\\n', '12333.png,derek lam\\n', '6205.png,jil sander\\n', '2063.png,alexander wang\\n', '10524.png,etro\\n', '1542.png,missoni\\n', '5724.png,jil sander\\n', '11612.png,miu miu\\n', '3355.png,etro\\n', '1224.png,marc by marc jacobs\\n', '13763.png,jason wu\\n', '968.png,armani prive\\n', '7655.png,jil sander\\n', '1230.png,jil sander\\n', '7899.png,jil sander\\n', '8572.png,givenchy\\n', '11160.png,missoni\\n', '3427.png,alexander wang\\n', '5730.png,jil sander\\n', '11606.png,derek lam\\n', '12858.png,fendi\\n', '8943.png,missoni\\n', '1973.png,vera wang\\n', '3802.png,emporio armani\\n', '5283.png,miu miu\\n', '571.png,bottega veneta\\n', '1783.png,alexander wang\\n', '4837.png,jil sander\\n', '3180.png,emporio armani\\n', '10901.png,yves saint laurent\\n', '1797.png,alexander wang\\n', '12870.png,missoni\\n', '565.png,carolina herrera\\n', '8758.png,jason wu\\n', '6775.png,chado ralph rucci\\n', '4604.png,jil sander\\n', '4610.png,carolina herrera\\n', '2261.png,alexander wang\\n', '1768.png,giambattista valli\\n', '6007.png,missoni\\n', '12131.png,dries van noten\\n', '13549.png,yves saint laurent\\n', '4176.png,vera wang\\n', '3619.png,emporio armani\\n', '6991.png,jil sander\\n', '6749.png,carolina herrera\\n', '10068.png,marc jacobs\\n', '5240.png,marc by marc jacobs\\n', '3157.png,jason wu\\n', '5526.png,miu miu\\n', '2249.png,marc jacobs\\n', '13207.png,carolina herrera\\n', '14568.png,jason wu\\n', '1998.png,yves saint laurent\\n', '13213.png,bottega veneta\\n', '1754.png,missoni\\n', '9308.png,marc by marc jacobs\\n', '7325.png,carolina herrera\\n', '13575.png,yves saint laurent\\n', '1032.png,armani prive\\n', '13212.png,ralph lauren\\n', '8771.png,chado ralph rucci\\n', '7442.png,missoni\\n', '3624.png,emporio armani\\n', '10069.png,marc jacobs\\n', '6990.png,jil sander\\n', '8003.png,chado ralph rucci\\n', '1741.png,carolina herrera\\n', '3156.png,jason wu\\n', '7318.png,jason wu\\n', '6006.png,marni\\n', '2260.png,alexander wang\\n', '10727.png,miu miu\\n', '4611.png,chado ralph rucci\\n', '10041.png,chado ralph rucci\\n', '13548.png,yves saint laurent\\n', '8995.png,jil sander\\n', '6774.png,gucci\\n', '9447.png,jil sander\\n', '8759.png,vera wang\\n', '10055.png,missoni\\n', '6012.png,ralph lauren\\n', '7481.png,jil sander\\n', '6947.png,jil sander\\n', '564.png,bottega veneta\\n', '12871.png,dries van noten\\n', '6953.png,giambattista valli\\n', '570.png,jil sander\\n', '3195.png,dries van noten\\n', '4822.png,dries van noten\\n', '1782.png,chado ralph rucci\\n', '558.png,bottega veneta\\n', '3817.png,emporio armani\\n', '10096.png,chado ralph rucci\\n', '11388.png,chado ralph rucci\\n', '12859.png,marc jacobs\\n', '4349.png,derek lam\\n', '14019.png,armani prive\\n', '13776.png,vera wang\\n', '13010.png,chado ralph rucci\\n', '8215.png,miu miu\\n', '3340.png,yves saint laurent\\n', '11607.png,chado ralph rucci\\n', '10519.png,emporio armani\\n', '11613.png,miu miu\\n', '8201.png,miu miu\\n', '8567.png,marc by marc jacobs\\n', '3432.png,alexander wang\\n', '11175.png,marc jacobs\\n', '9889.png,yves saint laurent\\n', '12454.png,jil sander\\n', '14025.png,dries van noten\\n', '9651.png,etro\\n', '10243.png,chado ralph rucci\\n', '6204.png,dries van noten\\n', '8229.png,carolina herrera\\n', '12326.png,missoni\\n', '10257.png,chado ralph rucci\\n', '14031.png,dries van noten\\n', '1219.png,marc by marc jacobs\\n', '2923.png,giambattista valli\\n', '1594.png,missoni\\n', '1580.png,jil sander\\n', '5080.png,miu miu\\n', '9862.png,jil sander\\n', '10280.png,emporio armani\\n', '14780.png,etro\\n', '5902.png,jil sander\\n', '14794.png,vera wang\\n', '9686.png,derek lam\\n', '7873.png,derek lam\\n', '996.png,armani prive\\n', '13945.png,carolina herrera\\n', '11015.png,missoni\\n', '5123.png,yves saint laurent\\n', '13602.png,yves saint laurent\\n', '1345.png,bottega veneta\\n', '49.png,marni\\n', '7734.png,marc jacobs\\n', '8407.png,derek lam\\n', '7052.png,jil sander\\n', '13164.png,ralph lauren\\n', '3234.png,jason wu\\n', '11773.png,miu miu\\n', '3220.png,jason wu\\n', '6358.png,chado ralph rucci\\n', '14179.png,jil sander\\n', '8413.png,givenchy\\n', '11001.png,missoni\\n', '4229.png,chado ralph rucci\\n', '835.png,giorgio armani\\n', '7708.png,marc by marc jacobs\\n', '10337.png,missoni\\n', '10451.png,vera wang\\n', '2116.png,alexander wang\\n', '12246.png,jil sander\\n', '9043.png,missoni\\n', '12252.png,emporio armani\\n', '9057.png,alexander wang\\n', '10323.png,miu miu\\n', '14145.png,giorgio armani\\n', '9731.png,jason wu\\n', '612.png,marc jacobs\\n', '3591.png,emporio armani\\n', '4598.png,carolina herrera\\n', '11968.png,jason wu\\n', '174.png,marni\\n', '7091.png,missoni\\n', '9916.png,miu miu\\n', '606.png,derek lam\\n', '5862.png,jil sander\\n', '9094.png,alexander wang\\n', '12291.png,bottega veneta\\n', '14186.png,gucci\\n', '4759.png,chado ralph rucci\\n', '1621.png,bottega veneta\\n', '12078.png,dries van noten\\n', '6628.png,carolina herrera\\n', '1147.png,armani prive\\n', '3750.png,emporio armani\\n', '11203.png,alexander wang\\n', '13414.png,yves saint laurent\\n', '8177.png,miu miu\\n', '389.png,bottega veneta\\n', '6172.png,missoni\\n', '10653.png,missoni\\n', '10135.png,emporio armani\\n', '12722.png,emporio armani\\n', '8639.png,carolina herrera\\n', '14353.png,alexander wang\\n', '6600.png,bottega veneta\\n', '14347.png,givenchy\\n', '3778.png,carolina herrera\\n', '11559.png,marc jacobs\\n', '4771.png,dries van noten\\n', '9255.png,alexander wang\\n', '7278.png,carolina herrera\\n', '376.png,bottega veneta\\n', '5484.png,jason wu\\n', '4942.png,armani prive\\n', '3793.png,emporio armani\\n', '12905.png,carolina herrera\\n', '1190.png,marc by marc jacobs\\n', '404.png,bottega veneta\\n', '6827.png,missoni\\n', '5490.png,missoni\\n', '4956.png,dries van noten\\n', '7287.png,carolina herrera\\n', '1806.png,jil sander\\n', '9282.png,missoni\\n', '12939.png,marc by marc jacobs\\n', '3963.png,jason wu\\n', '8836.png,derek lam\\n', '1812.png,gucci\\n', '12093.png,vera wang\\n', '7283.png,carolina herrera\\n', '5494.png,missoni\\n', '4952.png,missoni\\n', '12915.png,jason wu\\n', '400.png,bottega veneta\\n', '12901.png,derek lam\\n', '414.png,bottega veneta\\n', '5480.png,miu miu\\n', '4946.png,jil sander\\n', '10680.png,etro\\n', '1816.png,marc by marc jacobs\\n', '12097.png,missoni\\n', '14380.png,etro\\n', '3973.png,yves saint laurent\\n', '1802.png,derek lam\\n', '8198.png,jason wu\\n', '5457.png,carolina herrera\\n', '2338.png,dries van noten\\n', '4991.png,marc jacobs\\n', '4749.png,derek lam\\n', '13410.png,missoni\\n', '1157.png,armani prive\\n', '3740.png,emporio armani\\n', '11207.png,marni\\n', '10119.png,jason wu\\n', '3754.png,missoni\\n', '1143.png,armani prive\\n', '8167.png,miu miu\\n', '1625.png,tommy hilfiger\\n', '5443.png,missoni\\n', '14425.png,givenchy\\n', '6162.png,dries van noten\\n', '10643.png,vera wang\\n', '2304.png,jil sander\\n', '4013.png,chado ralph rucci\\n', '2462.png,chado ralph rucci\\n', '9537.png,alexander wang\\n', '12732.png,dries van noten\\n', '14357.png,jil sander\\n', '9523.png,derek lam\\n', '12726.png,derek lam\\n', '2310.png,etro\\n', '11549.png,alexander wang\\n', '4761.png,dries van noten\\n', '7268.png,missoni\\n', '9245.png,armani prive\\n', '1396.png,gucci\\n', '9912.png,jil sander\\n', '2847.png,bottega veneta\\n', '5696.png,lanvin\\n', '7081.png,jil sander\\n', '14814.png,dolce & gabbana\\n', '5682.png,jil sander\\n', '3595.png,emporio armani\\n', '2853.png,giambattista valli\\n', '7903.png,yves saint laurent\\n', '13835.png,chado ralph rucci\\n', '158.png,chado ralph rucci\\n', '10482.png,jason wu\\n', '11944.png,miu miu\\n', '5872.png,jil sander\\n', '12281.png,armani prive\\n', '3542.png,missoni\\n', '8417.png,givenchy\\n', '1355.png,marc by marc jacobs\\n', '9709.png,derek lam\\n', '59.png,jil sander\\n', '1433.png,marc by marc jacobs\\n', '13174.png,chado ralph rucci\\n', '7042.png,jil sander\\n', '3230.png,jason wu\\n', '5641.png,missoni\\n', '6348.png,giorgio armani\\n', '1427.png,marc by marc jacobs\\n', '8365.png,miu miu\\n', '7056.png,jil sander\\n', '13606.png,alexander wang\\n', '2648.png,ralph lauren\\n', '11011.png,carolina herrera\\n', '2890.png,bottega veneta\\n', '825.png,giorgio armani\\n', '12530.png,emporio armani\\n', '65.png,givenchy\\n', '10327.png,chado ralph rucci\\n', '4577.png,jil sander\\n', '14633.png,emporio armani\\n', '12242.png,jason wu\\n', '11993.png,chado ralph rucci\\n', '4205.png,marc jacobs\\n', '831.png,giorgio armani\\n', '71.png,bottega veneta\\n', '776.png,giorgio armani\\n', '3387.png,giambattista valli\\n', '2099.png,alexander wang\\n', '2927.png,vera wang\\n', '9872.png,derek lam\\n', '13969.png,alexander wang\\n', '5906.png,jil sander\\n', '11830.png,missoni\\n', '11824.png,marc jacobs\\n', '14784.png,dolce & gabbana\\n', '13955.png,gucci\\n', '9696.png,missoni\\n', '7863.png,marc jacobs\\n', '10284.png,jason wu\\n', '4359.png,ralph lauren\\n', '5047.png,miu miu\\n', '8205.png,jil sander\\n', '7136.png,jason wu\\n', '13000.png,chado ralph rucci\\n', '8211.png,jason wu\\n', '979.png,armani prive\\n', '3422.png,etro\\n', '13982.png,gucci\\n', '951.png,armani prive\\n', '2714.png,jason wu\\n', '8239.png,miu miu\\n', '14753.png,dolce & gabbana\\n', '4417.png,alexander wang\\n', '11159.png,bottega veneta\\n', '9655.png,derek lam\\n', '1209.png,marc by marc jacobs\\n', '12450.png,jil sander\\n', '13996.png,jason wu\\n', '945.png,giorgio armani\\n', '4198.png,dries van noten\\n', '7491.png,alexander wang\\n', '6957.png,jil sander\\n', '560.png,marc jacobs\\n', '5292.png,marc by marc jacobs\\n', '1792.png,marni\\n', '548.png,etro\\n', '8946.png,marc jacobs\\n', '1976.png,yves saint laurent\\n', '3152.png,jason wu\\n', '5523.png,missoni\\n', '9319.png,vera wang\\n', '1745.png,miu miu\\n', '7452.png,yves saint laurent\\n', '1023.png,armani prive\\n', '3634.png,emporio armani\\n', '11373.png,alexander wang\\n', '10079.png,jil sander\\n', '6980.png,jil sander\\n', '6758.png,alexander wang\\n', '14579.png,dolce & gabbana\\n', '1751.png,miu miu\\n', '1989.png,dries van noten\\n', '11401.png,jil sander\\n', '2258.png,bottega veneta\\n', '9325.png,chado ralph rucci\\n', '6016.png,marni\\n', '7308.png,balmain\\n', '2270.png,alexander wang\\n', '3608.png,jason wu\\n', '4167.png,dolce & gabbana\\n', '13558.png,yves saint laurent\\n', '10045.png,marc jacobs\\n', '2264.png,alexander wang\\n', '9331.png,etro\\n', '14545.png,alexander wang\\n', '6002.png,jil sander\\n', '4172.png,marc by marc jacobs\\n', '12653.png,jason wu\\n', '6003.png,alexander wang\\n', '2265.png,alexander wang\\n', '10722.png,dolce & gabbana\\n', '2271.png,miu miu\\n', '12121.png,chado ralph rucci\\n', '14236.png,etro\\n', '13559.png,yves saint laurent\\n', '4166.png,jason wu\\n', '10050.png,dolce & gabbana\\n', '6759.png,carolina herrera\\n', '1036.png,armani prive\\n', '7447.png,chado ralph rucci\\n', '3621.png,emporio armani\\n', '10078.png,marc jacobs\\n', '5536.png,yves saint laurent\\n', '2259.png,alexander wang\\n', '3147.png,jason wu\\n', '7321.png,missoni\\n', '13217.png,ralph lauren\\n', '13203.png,marni\\n', '5522.png,vera wang\\n', '3153.png,jason wu\\n', '3635.png,emporio armani\\n', '5244.png,versace\\n', '1022.png,armani prive\\n', '13565.png,yves saint laurent\\n', '6995.png,jil sander\\n', '8760.png,armani prive\\n', '10087.png,dries van noten\\n', '10939.png,versace\\n', '1977.png,fendi\\n', '3812.png,emporio armani\\n', '561.png,bottega veneta\\n', '207.png,chado ralph rucci\\n', '1793.png,marc by marc jacobs\\n', '10905.png,vera wang\\n', '3190.png,jason wu\\n', '10911.png,vera wang\\n', '4827.png,emporio armani\\n', '1787.png,vera wang\\n', '213.png,giorgio armani\\n', '2067.png,etro\\n', '6201.png,marc by marc jacobs\\n', '12337.png,chado ralph rucci\\n', '10246.png,chado ralph rucci\\n', '11158.png,alexander wang\\n', '950.png,armani prive\\n', '9640.png,derek lam\\n', '14034.png,dries van noten\\n', '6573.png,dolce & gabbana\\n', '14752.png,chado ralph rucci\\n', '10534.png,yves saint laurent\\n', '2073.png,chado ralph rucci\\n', '8210.png,miu miu\\n', '5734.png,jil sander\\n', '7645.png,etro\\n', '978.png,dries van noten\\n', '1234.png,miu miu\\n', '8562.png,marc by marc jacobs\\n', '5046.png,chado ralph rucci\\n', '2729.png,bottega veneta\\n', '11170.png,giorgio armani\\n', '4358.png,chado ralph rucci\\n', '3351.png,etro\\n', '10508.png,vera wang\\n', '1546.png,miu miu\\n', '11825.png,chado ralph rucci\\n', '987.png,armani prive\\n', '8589.png,jil sander\\n', '13798.png,chado ralph rucci\\n', '7876.png,marc jacobs\\n', '12486.png,yves saint laurent\\n', '11831.png,carolina herrera\\n', '5907.png,jil sander\\n', '14791.png,versace\\n', '11819.png,etro\\n', '11992.png,chado ralph rucci\\n', '2113.png,alexander wang\\n', '9720.png,derek lam\\n', '14154.png,jason wu\\n', '12525.png,ralph lauren\\n', '2661.png,alexander wang\\n', '9734.png,derek lam\\n', '12531.png,dries van noten\\n', '824.png,giorgio armani\\n', '7719.png,jil sander\\n', '12257.png,armani prive\\n', '14626.png,dries van noten\\n', '11986.png,vera wang\\n', '3231.png,jason wu\\n', '11776.png,carolina herrera\\n', '2891.png,giambattista valli\\n', '4238.png,jason wu\\n', '14168.png,gucci\\n', '13607.png,emporio armani\\n', '1340.png,miu miu\\n', '12519.png,marc by marc jacobs\\n', '7731.png,alexander wang\\n', '8402.png,giorgio armani\\n', '9708.png,jason wu\\n', '13613.png,yves saint laurent\\n', '818.png,giorgio armani\\n', '2885.png,giambattista valli\\n', '3543.png,marc jacobs\\n', '11004.png,missoni\\n', '5654.png,derek lam\\n', '8370.png,yves saint laurent\\n', '9085.png,chado ralph rucci\\n', '14829.png,carolina herrera\\n', '12280.png,emporio armani\\n', '5867.png,jason wu\\n', '11951.png,marc jacobs\\n', '14197.png,missoni\\n', '13834.png,giorgio armani\\n', '5873.png,jil sander\\n', '5683.png,jil sander\\n', '617.png,chado ralph rucci\\n', '3594.png,emporio armani\\n', '2846.png,chado ralph rucci\\n', '3580.png,emporio armani\\n', '11979.png,miu miu\\n', '10130.png,miu miu\\n', '4006.png,chado ralph rucci\\n', '14356.png,emporio armani\\n', '6177.png,marni\\n', '4760.png,jason wu\\n', '2305.png,alexander wang\\n', '8628.png,derek lam\\n', '6605.png,vera wang\\n', '14342.png,dries van noten\\n', '9536.png,dries van noten\\n', '2463.png,gucci\\n', '10124.png,chado ralph rucci\\n', '8600.png,miu miu\\n', '1142.png,armani prive\\n', '11212.png,marni\\n', '3755.png,emporio armani\\n', '3033.png,jason wu\\n', '4984.png,dries van noten\\n', '9278.png,marni\\n', '1624.png,missoni\\n', '13363.png,ralph lauren\\n', '8166.png,emporio armani\\n', '7255.png,marc jacobs\\n', '13377.png,alexander mcqueen\\n', '4748.png,marc by marc jacobs\\n', '3027.png,jason wu\\n', '4990.png,marc jacobs\\n', '5456.png,emporio armani\\n', '10118.png,chado ralph rucci\\n', '3741.png,emporio armani\\n', '8827.png,derek lam\\n', '10695.png,missoni\\n', '12082.png,dolce & gabbana\\n', '9287.png,alexander wang\\n', '12096.png,missoni\\n', '1817.png,vera wang\\n', '9293.png,marc jacobs\\n', '13388.png,yves saint laurent\\n', '10681.png,derek lam\\n', '8833.png,yves saint laurent\\n', '2488.png,vera wang\\n', '1181.png,armani prive\\n', '12900.png,dries van noten\\n', '6836.png,emporio armani\\n', '4947.png,derek lam\\n', '10871.png,vera wang\\n', '1195.png,yves saint laurent\\n', '10873.png,derek lam\\n', '4945.png,carolina herrera\\n', '371.png,bottega veneta\\n', '12902.png,dries van noten\\n', '8819.png,alexander mcqueen\\n', '417.png,bottega veneta\\n', '3958.png,armani prive\\n', '12916.png,miu miu\\n', '365.png,bottega veneta\\n', '9285.png,armani prive\\n', '3970.png,emporio armani\\n', '6808.png,emporio armani\\n', '14397.png,marni\\n', '8825.png,chado ralph rucci\\n', '14383.png,armani prive\\n', '3964.png,yves saint laurent\\n', '8164.png,miu miu\\n', '13361.png,jason wu\\n', '5440.png,missoni\\n', '10668.png,vera wang\\n', '3031.png,jason wu\\n', '4038.png,versace\\n', '11210.png,jil sander\\n', '1140.png,armani prive\\n', '9508.png,marc jacobs\\n', '1154.png,armani prive\\n', '13413.png,yves saint laurent\\n', '8616.png,chado ralph rucci\\n', '5332.png,dolce & gabbana\\n', '2313.png,marc jacobs\\n', '10654.png,vera wang\\n', '12043.png,alexander wang\\n', '9246.png,yves saint laurent\\n', '6613.png,vera wang\\n', '2475.png,derek lam\\n', '10126.png,marc jacobs\\n', '14426.png,vera wang\\n', '5468.png,yves saint laurent\\n', '2307.png,alexander wang\\n', '10640.png,vera wang\\n', '4776.png,chado ralph rucci\\n', '3596.png,vera wang\\n', '2688.png,giambattista valli\\n', '615.png,dolce & gabbana\\n', '173.png,jil sander\\n', '5681.png,jil sander\\n', '5859.png,jil sander\\n', '14803.png,dolce & gabbana\\n', '3582.png,emporio armani\\n', '7914.png,missoni\\n', '14195.png,vera wang\\n', '13822.png,carolina herrera\\n', '5865.png,jil sander\\n', '5871.png,dries van noten\\n', '14181.png,gucci\\n', '13836.png,missoni\\n', '13605.png,yves saint laurent\\n', '11012.png,dolce & gabbana\\n', '2893.png,bottega veneta\\n', '11774.png,armani prive\\n', '9078.png,jason wu\\n', '7055.png,jil sander\\n', '8366.png,lanvin\\n', '13177.png,ralph lauren\\n', '3227.png,emporio armani\\n', '8.png,marni\\n', '2139.png,alexander wang\\n', '10318.png,chado ralph rucci\\n', '11006.png,marni\\n', '13611.png,yves saint laurent\\n', '10330.png,chado ralph rucci\\n', '3569.png,emporio armani\\n', '4206.png,dolce & gabbana\\n', '13639.png,yves saint laurent\\n', '12241.png,gucci\\n', '11748.png,emporio armani\\n', '10456.png,vera wang\\n', '2105.png,alexander wang\\n', '14624.png,dolce & gabbana\\n', '198.png,yves saint laurent\\n', '12255.png,alexander wang\\n', '12533.png,derek lam\\n', '9736.png,jason wu\\n', '6405.png,chado ralph rucci\\n', '10324.png,chado ralph rucci\\n', '2663.png,vera wang\\n', '4212.png,dries van noten\\n', '2924.png,yves saint laurent\\n', '761.png,carolina herrera\\n', '3384.png,emporio armani\\n', '3390.png,vera wang\\n', '9695.png,chado ralph rucci\\n', '11199.png,alexander wang\\n', '11827.png,alexander wang\\n', '5905.png,jil sander\\n', '12484.png,giorgio armani\\n', '749.png,giorgio armani\\n', '8574.png,dolce & gabbana\\n', '3421.png,derek lam\\n', '10278.png,chado ralph rucci\\n', '13017.png,carolina herrera\\n', '7135.png,bottega veneta\\n', '11172.png,alexander wang\\n', '5044.png,dries van noten\\n', '14022.png,gucci\\n', '6565.png,carolina herrera\\n', '946.png,armani prive\\n', '13995.png,gucci\\n', '9130.png,christian dior\\n', '6203.png,vera wang\\n', '2065.png,miu miu\\n', '7109.png,miu miu\\n', '14036.png,gucci\\n', '9642.png,derek lam\\n', '6571.png,dolce & gabbana\\n', '13759.png,jil sander\\n', '12447.png,jil sander\\n', '952.png,armani prive\\n', '4366.png,carolina herrera\\n', '10250.png,chado ralph rucci\\n', '4831.png,marc by marc jacobs\\n', '10907.png,bottega veneta\\n', '3186.png,dries van noten\\n', '1949.png,alexander wang\\n', '6798.png,carolina herrera\\n', '3838.png,chado ralph rucci\\n', '8979.png,marc by marc jacobs\\n', '4825.png,missoni\\n', '10913.png,emporio armani\\n', '9497.png,chado ralph rucci\\n', '13598.png,chado ralph rucci\\n', '8945.png,miu miu\\n', '9483.png,yves saint laurent\\n', '10091.png,alexander wang\\n', '4819.png,jason wu\\n', '5534.png,missoni\\n', '5252.png,miu miu\\n', '11364.png,balmain\\n', '1034.png,bottega veneta\\n', '13573.png,vera wang\\n', '7451.png,giambattista valli\\n', '14208.png,armani prive\\n', '3637.png,emporio armani\\n', '10708.png,giorgio armani\\n', '5508.png,vera wang\\n', '6001.png,marni\\n', '9332.png,carolina herrera\\n', '12137.png,emporio armani\\n', '12651.png,dries van noten\\n', '1008.png,armani prive\\n', '12889.png,carolina herrera\\n', '14220.png,etro\\n', '10046.png,derek lam\\n', '4170.png,armani prive\\n', '588.png,bottega veneta\\n', '8986.png,christian dior\\n', '12123.png,etro\\n', '8038.png,carolina herrera\\n', '2273.png,vera wang\\n', '10734.png,jason wu\\n', '6772.png,carolina herrera\\n', '2272.png,ralph lauren\\n', '6014.png,derek lam\\n', '14547.png,alexander wang\\n', '5509.png,missoni\\n', '2266.png,jason wu\\n', '11359.png,chado ralph rucci\\n', '2500.png,vera wang\\n', '11371.png,vera wang\\n', '12678.png,derek lam\\n', '6996.png,jil sander\\n', '8763.png,jason wu\\n', '1747.png,yves saint laurent\\n', '6028.png,jil sander\\n', '3150.png,jason wu\\n', '11417.png,carolina herrera\\n', '3144.png,miu miu\\n', '8011.png,vera wang\\n', '13214.png,ralph lauren\\n', '8777.png,dolce & gabbana\\n', '11365.png,marc jacobs\\n', '3811.png,emporio armani\\n', '6969.png,jil sander\\n', '8944.png,vera wang\\n', '14590.png,etro\\n', '238.png,bottega veneta\\n', '8788.png,alexander mcqueen\\n', '9496.png,marc by marc jacobs\\n', '3805.png,emporio armani\\n', '6955.png,jil sander\\n', '8978.png,christian dior\\n', '4824.png,dries van noten\\n', '1790.png,marc by marc jacobs\\n', '1948.png,missoni\\n', '10906.png,chado ralph rucci\\n', '2299.png,alexander wang\\n', '4830.png,dries van noten\\n', '6216.png,alexander wang\\n', '10537.png,fendi\\n', '2716.png,chado ralph rucci\\n', '13980.png,jason wu\\n', '12446.png,alexander wang\\n', '9643.png,alexander wang\\n', '13994.png,christian dior\\n', '8549.png,miu miu\\n', '10245.png,vera wang\\n', '11615.png,emporio armani\\n', '3352.png,etro\\n', '5723.png,jil sander\\n', '9119.png,missoni\\n', '8207.png,yves saint laurent\\n', '1223.png,marc by marc jacobs\\n', '11173.png,marni\\n', '3434.png,etro\\n', '11167.png,bottega veneta\\n', '3420.png,marc jacobs\\n', '8575.png,marc jacobs\\n', '8213.png,miu miu\\n', '11601.png,dries van noten\\n', '5737.png,jil sander\\n', '748.png,marc by marc jacobs\\n', '990.png,armani prive\\n', '10292.png,chado ralph rucci\\n', '12491.png,yves saint laurent\\n', '14786.png,vera wang\\n', '5910.png,jil sander\\n', '1586.png,vera wang\\n', '3391.png,gucci\\n', '4398.png,carolina herrera\\n', '7691.png,missoni\\n', '2104.png,alexander wang\\n', '10443.png,vera wang\\n', '2662.png,gucci\\n', '10325.png,chado ralph rucci\\n', '9737.png,derek lam\\n', '827.png,alexander wang\\n', '8429.png,vera wang\\n', '833.png,giorgio armani\\n', '4207.png,etro\\n', '3568.png,missoni\\n', '10331.png,chado ralph rucci\\n', '10457.png,derek lam\\n', '11749.png,chado ralph rucci\\n', '12240.png,bottega veneta\\n', '1419.png,jil sander\\n', '14631.png,dolce & gabbana\\n', '5657.png,bottega veneta\\n', '11761.png,emporio armani\\n', '13176.png,alexander mcqueen\\n', '14619.png,dolce & gabbana\\n', '6438.png,bottega veneta\\n', '1357.png,jil sander\\n', '10319.png,chado ralph rucci\\n', '13604.png,yves saint laurent\\n', '3232.png,etro\\n', '5870.png,jil sander\\n', '10480.png,vera wang\\n', '7915.png,marc jacobs\\n', '9086.png,dries van noten\\n', '8398.png,vera wang\\n', '5864.png,marc by marc jacobs\\n', '10494.png,chado ralph rucci\\n', '166.png,armani prive\\n', '2845.png,alexander wang\\n', '3583.png,emporio armani\\n', '9910.png,jil sander\\n', '98.png,armani prive\\n', '7929.png,marc jacobs\\n', '2689.png,giambattista valli\\n', '2851.png,giambattista valli\\n', '3597.png,emporio armani\\n', '5858.png,armani prive\\n', '5680.png,armani prive\\n', '172.png,marc jacobs\\n', '7097.png,jil sander\\n', '1169.png,armani prive\\n', '9535.png,chado ralph rucci\\n', '6606.png,carolina herrera\\n', '4777.png,jason wu\\n', '10899.png,alexander wang\\n', '3018.png,jason wu\\n', '14427.png,alexander wang\\n', '12042.png,marni\\n', '10655.png,vera wang\\n', '4005.png,jil sander\\n', '9521.png,derek lam\\n', '3742.png,chado ralph rucci\\n', '13412.png,yves saint laurent\\n', '3024.png,emporio armani\\n', '11563.png,marni\\n', '5455.png,missoni\\n', '4987.png,dries van noten\\n', '6148.png,dries van noten\\n', '1627.png,marc by marc jacobs\\n', '8165.png,miu miu\\n', '14369.png,jil sander\\n', '5327.png,alexander wang\\n', '4039.png,miu miu\\n', '3965.png,yves saint laurent\\n', '8830.png,emporio armani\\n', '14382.png,yves saint laurent\\n', '358.png,jil sander\\n', '10696.png,jil sander\\n', '6821.png,missoni\\n', '4950.png,dries van noten\\n', '4788.png,chado ralph rucci\\n', '7281.png,marc by marc jacobs\\n', '364.png,miu miu\\n', '4944.png,dries van noten\\n', '10829.png,nina ricci\\n', '14497.png,gucci\\n', '12780.png,carolina herrera\\n', '8843.png,armani prive\\n', '9585.png,derek lam\\n', '10197.png,chado ralph rucci\\n', '12794.png,fendi\\n', '459.png,bottega veneta\\n', '8857.png,marc jacobs\\n', '14483.png,alexander wang\\n', '1873.png,chado ralph rucci\\n', '1683.png,marc by marc jacobs\\n', '317.png,bottega veneta\\n', '10815.png,etro\\n', '3094.png,jason wu\\n', '6852.png,jil sander\\n', '471.png,bottega veneta\\n', '12964.png,derek lam\\n', '4089.png,miu miu\\n', '4937.png,dries van noten\\n', '1697.png,chado ralph rucci\\n', '6113.png,marni\\n', '10632.png,vera wang\\n', '4704.png,marc jacobs\\n', '9546.png,yves saint laurent\\n', '6675.png,carolina herrera\\n', '8658.png,chado ralph rucci\\n', '8894.png,gucci\\n', '13449.png,yves saint laurent\\n', '14326.png,missoni\\n', '9552.png,yves saint laurent\\n', '6661.png,miu miu\\n', '10626.png,vera wang\\n', '2361.png,alexander wang\\n', '3057.png,bottega veneta\\n', '7231.png,jason wu\\n', '14468.png,givenchy\\n', '13461.png,yves saint laurent\\n', '8664.png,marc jacobs\\n', '3731.png,emporio armani\\n', '10168.png,chado ralph rucci\\n', '3725.png,emporio armani\\n', '13475.png,missoni\\n', '7225.png,miu miu\\n', '9208.png,alexander wang\\n', '5432.png,missoni\\n', '3043.png,alexander wang\\n', '11504.png,chado ralph rucci\\n', '10395.png,chado ralph rucci\\n', '13844.png,marc by marc jacobs\\n', '8499.png,ralph lauren\\n', '14695.png,miu miu\\n', '11935.png,vera wang\\n', '11921.png,vera wang\\n', '14681.png,derek lam\\n', '9793.png,chado ralph rucci\\n', '13850.png,marni\\n', '11909.png,missoni\\n', '115.png,marc jacobs\\n', '667.png,jason wu\\n', '7782.png,marc jacobs\\n', '854.png,alexander wang\\n', '6477.png,vera wang\\n', '14130.png,chado ralph rucci\\n', '4506.png,marni\\n', '2177.png,alexander wang\\n', '10430.png,jason wu\\n', '13139.png,dolce & gabbana\\n', '9022.png,alexander wang\\n', '14642.png,marc jacobs\\n', '9036.png,alexander wang\\n', '8328.png,alexander wang\\n', '2163.png,alexander wang\\n', '10424.png,vera wang\\n', '10342.png,chado ralph rucci\\n', '12555.png,jason wu\\n', '840.png,giorgio armani\\n', '3533.png,etro\\n', '868.png,derek lam\\n', '9778.png,derek lam\\n', '1324.png,marc by marc jacobs\\n', '1442.png,marc by marc jacobs\\n', '8300.png,yves saint laurent\\n', '3255.png,jason wu\\n', '5624.png,carolina herrera\\n', '1456.png,missoni\\n', '8314.png,jason wu\\n', '7741.png,chado ralph rucci\\n', '7999.png,marc jacobs\\n', '11060.png,marni\\n', '3527.png,armani prive\\n', '7806.png,marc jacobs\\n', '14087.png,dolce & gabbana\\n', '10587.png,derek lam\\n', '5963.png,marni\\n', '11855.png,yves saint laurent\\n', '713.png,marc by marc jacobs\\n', '9630.png,derek lam\\n', '4314.png,carolina herrera\\n', '2765.png,miu miu\\n', '10222.png,chado ralph rucci\\n', '2003.png,marc jacobs\\n', '10544.png,vera wang\\n', '12353.png,jil sander\\n', '14722.png,ralph lauren\\n', '14736.png,dolce & gabbana\\n', '13059.png,carolina herrera\\n', '2017.png,dolce & gabbana\\n', '10550.png,yves saint laurent\\n', '11896.png,yves saint laurent\\n', '5778.png,yves saint laurent\\n', '11128.png,jil sander\\n', '9624.png,alexander wang\\n', '14050.png,dries van noten\\n', '12421.png,miu miu\\n', '11100.png,missoni\\n', '3447.png,bottega veneta\\n', '5036.png,emporio armani\\n', '1250.png,marc by marc jacobs\\n', '7621.png,jil sander\\n', '13071.png,carolina herrera\\n', '5750.png,marc by marc jacobs\\n', '5988.png,alexander wang\\n', '11114.png,lanvin\\n', '2995.png,jil sander\\n', '8089.png,chado ralph rucci\\n', '539.png,missoni\\n', '14291.png,tommy hilfiger\\n', '12838.png,carolina herrera\\n', '12186.png,armani prive\\n', '5591.png,missoni\\n', '2598.png,derek lam\\n', '12810.png,missoni\\n', '505.png,miu miu\\n', '1085.png,armani prive\\n', '6932.png,jil sander\\n', '3692.png,emporio armani\\n', '4843.png,missoni\\n', '7392.png,missoni\\n', '277.png,chado ralph rucci\\n', '6067.png,givenchy\\n', '7379.png,derek lam\\n', '10746.png,dolce & gabbana\\n', '4116.png,yves saint laurent\\n', '14246.png,gucci\\n', '13529.png,yves saint laurent\\n', '8738.png,chado ralph rucci\\n', '6715.png,yves saint laurent\\n', '2215.png,alexander wang\\n', '9340.png,etro\\n', '14534.png,givenchy\\n', '6073.png,derek lam\\n', '12145.png,vera wang\\n', '3123.png,bottega veneta\\n', '11464.png,missoni\\n', '1052.png,armani prive\\n', '3645.png,etro\\n', '5220.png,etro\\n', '3651.png,emporio armani\\n', '8704.png,marc jacobs\\n', '1046.png,alexander wang\\n', '7351.png,alexander wang\\n', '12179.png,jil sander\\n', '2229.png,miu miu\\n', '13500.png,armani prive\\n', '8705.png,jason wu\\n', '3650.png,emporio armani\\n', '4881.png,missoni\\n', '5547.png,missoni\\n', '3136.png,jason wu\\n', '7344.png,yves saint laurent\\n', '9369.png,armani prive\\n', '1735.png,emporio armani\\n', '13272.png,ralph lauren\\n', '4895.png,dries van noten\\n', '5553.png,missoni\\n', '11303.png,dolce & gabbana\\n', '7422.png,dolce & gabbana\\n', '4103.png,alexander wang\\n', '6714.png,vera wang\\n', '9427.png,etro\\n', '8739.png,miu miu\\n', '14535.png,alexander wang\\n', '289.png,givenchy\\n', '2214.png,alexander wang\\n', '2200.png,miu miu\\n', '12150.png,dolce & gabbana\\n', '9433.png,givenchy\\n', '4117.png,armani prive\\n', '2566.png,missoni\\n', '3693.png,emporio armani\\n', '6933.png,jil sander\\n', '1084.png,armani prive\\n', '7393.png,alexander wang\\n', '10960.png,jil sander\\n', '4856.png,yves saint laurent\\n', '6099.png,ralph lauren\\n', '262.png,bottega veneta\\n', '6927.png,giorgio armani\\n', '1090.png,armani prive\\n', '12811.png,derek lam\\n', '12839.png,derek lam\\n', '9382.png,alexander wang\\n', '12187.png,carolina herrera\\n', '12193.png,missoni\\n', '8088.png,carolina herrera\\n', '538.png,giambattista valli\\n', '1523.png,alexander wang\\n', '7152.png,dolce & gabbana\\n', '8261.png,emporio armani\\n', '3334.png,ralph lauren\\n', '5023.png,dries van noten\\n', '2994.png,jason wu\\n', '3452.png,jason wu\\n', '8507.png,givenchy\\n', '9619.png,derek lam\\n', '1245.png,jil sander\\n', '12408.png,giambattista valli\\n', '5037.png,chado ralph rucci\\n', '3320.png,jil sander\\n', '11667.png,etro\\n', '13070.png,carolina herrera\\n', '11897.png,vera wang\\n', '13058.png,carolina herrera\\n', '14737.png,emporio armani\\n', '6270.png,giorgio armani\\n', '12346.png,miu miu\\n', '935.png,armani prive\\n', '14051.png,gucci\\n', '9625.png,missoni\\n', '6516.png,marc jacobs\\n', '1279.png,marc by marc jacobs\\n', '10237.png,chado ralph rucci\\n', '2770.png,bottega veneta\\n', '2764.png,giambattista valli\\n', '9631.png,derek lam\\n', '9157.png,missoni\\n', '14723.png,dolce & gabbana\\n', '6264.png,miu miu\\n', '8249.png,miu miu\\n', '12352.png,alexander wang\\n', '11883.png,bottega veneta\\n', '10545.png,vera wang\\n', '11868.png,marc by marc jacobs\\n', '712.png,dolce & gabbana\\n', '1286.png,gucci\\n', '9802.png,derek lam\\n', '2957.png,bottega veneta\\n', '3485.png,etro\\n', '706.png,armani prive\\n', '11854.png,giorgio armani\\n', '5962.png,armani prive\\n', '7807.png,jil sander\\n', '10586.png,vera wang\\n', '5976.png,marc by marc jacobs\\n', '9194.png,missoni\\n', '13110.png,ralph lauren\\n', '10419.png,emporio armani\\n', '5631.png,missoni\\n', '3240.png,jason wu\\n', '3526.png,etro\\n', '11061.png,missoni\\n', '4249.png,chado ralph rucci\\n', '2638.png,alexander wang\\n', '7998.png,marc jacobs\\n', '8473.png,marc by marc jacobs\\n', '13662.png,yves saint laurent\\n', '11075.png,lanvin\\n', '11713.png,derek lam\\n', '7032.png,marc by marc jacobs\\n', '8301.png,miu miu\\n', '2162.png,emporio armani\\n', '12232.png,jil sander\\n', '8329.png,miu miu\\n', '14643.png,carolina herrera\\n', '9751.png,yves saint laurent\\n', '855.png,giorgio armani\\n', '13886.png,missoni\\n', '7768.png,marc jacobs\\n', '12540.png,marc jacobs\\n', '6310.png,dries van noten\\n', '5619.png,carolina herrera\\n', '1494.png,tommy hilfiger\\n', '9976.png,miu miu\\n', '5194.png,emporio armani\\n', '9962.png,jil sander\\n', '1480.png,jil sander\\n', '114.png,marni\\n', '3297.png,etro\\n', '2189.png,alexander wang\\n', '10380.png,chado ralph rucci\\n', '882.png,giorgio armani\\n', '13851.png,marc jacobs\\n', '7967.png,missoni\\n', '896.png,armani prive\\n', '13845.png,marc jacobs\\n', '11934.png,alexander wang\\n', '128.png,jason wu\\n', '13474.png,alexander wang\\n', '3724.png,emporio armani\\n', '5433.png,yves saint laurent\\n', '1655.png,tommy hilfiger\\n', '13312.png,carolina herrera\\n', '8117.png,miu miu\\n', '14469.png,alexander wang\\n', '1899.png,vera wang\\n', '8103.png,miu miu\\n', '4739.png,marc by marc jacobs\\n', '1127.png,armani prive\\n', '3718.png,emporio armani\\n', '12756.png,marc jacobs\\n', '9553.png,emporio armani\\n', '13448.png,yves saint laurent\\n', '12030.png,dries van noten\\n', '4711.png,gucci\\n', '2360.png,emporio armani\\n', '2374.png,bottega veneta\\n', '10633.png,vera wang\\n', '9221.png,jason wu\\n', '12024.png,miu miu\\n', '12742.png,fendi\\n', '6674.png,bottega veneta\\n', '9547.png,dries van noten\\n', '8881.png,alexander mcqueen\\n', '10155.png,alexander wang\\n', '12971.png,yves saint laurent\\n', '464.png,bottega veneta\\n', '302.png,marc by marc jacobs\\n', '3081.png,jason wu\\n', '4922.png,jason wu\\n', '3095.png,jason wu\\n', '10814.png,chado ralph rucci\\n', '316.png,bottega veneta\\n', '9590.png,jason wu\\n', '458.png,bottega veneta\\n', '8856.png,chado ralph rucci\\n', '10182.png,chado ralph rucci\\n', '3903.png,jason wu\\n', '1872.png,jason wu\\n', '14496.png,carolina herrera\\n', '11288.png,alexander wang\\n', '10196.png,chado ralph rucci\\n', '8842.png,fendi\\n', '10180.png,chado ralph rucci\\n', '13489.png,etro\\n', '8854.png,marc by marc jacobs\\n', '12783.png,carolina herrera\\n', '3915.png,chado ralph rucci\\n', '10194.png,chado ralph rucci\\n', '1864.png,missoni\\n', '3083.png,jason wu\\n', '4934.png,miu miu\\n', '8868.png,yves saint laurent\\n', '3929.png,givenchy\\n', '6851.png,missoni\\n', '472.png,bottega veneta\\n', '1680.png,bottega veneta\\n', '314.png,dries van noten\\n', '1858.png,vera wang\\n', '10816.png,marc by marc jacobs\\n', '4920.png,jason wu\\n', '2389.png,jason wu\\n', '2362.png,dries van noten\\n', '10625.png,emporio armani\\n', '4713.png,carolina herrera\\n', '6104.png,jil sander\\n', '499.png,bottega veneta\\n', '2404.png,dolce & gabbana\\n', '10143.png,chado ralph rucci\\n', '10157.png,chado ralph rucci\\n', '8883.png,chado ralph rucci\\n', '6676.png,missoni\\n', '12998.png,jil sander\\n', '9545.png,derek lam\\n', '14331.png,jil sander\\n', '9223.png,marc jacobs\\n', '2376.png,alexander wang\\n', '10631.png,missoni\\n', '5419.png,carolina herrera\\n', '7226.png,marc jacobs\\n', '1657.png,lanvin\\n', '13310.png,miu miu\\n', '10619.png,vera wang\\n', '11507.png,jil sander\\n', '1131.png,miu miu\\n', '13476.png,chado ralph rucci\\n', '14319.png,marc by marc jacobs\\n', '6886.png,jil sander\\n', '13462.png,alexander wang\\n', '9579.png,derek lam\\n', '8667.png,vera wang\\n', '7554.png,giambattista valli\\n', '3054.png,jason wu\\n', '1643.png,chado ralph rucci\\n', '13304.png,ralph lauren\\n', '10382.png,jil sander\\n', '11922.png,chado ralph rucci\\n', '11936.png,derek lam\\n', '11088.png,alexander wang\\n', '7971.png,armani prive\\n', '13847.png,missoni\\n', '7959.png,carolina herrera\\n', '664.png,giorgio armani\\n', '9974.png,jil sander\\n', '670.png,giorgio armani\\n', '10341.png,chado ralph rucci\\n', '3518.png,jil sander\\n', '843.png,giorgio armani\\n', '13890.png,marc jacobs\\n', '9753.png,derek lam\\n', '13648.png,yves saint laurent\\n', '14641.png,dolce & gabbana\\n', '9035.png,christian dior\\n', '4511.png,marc by marc jacobs\\n', '2160.png,alexander wang\\n', '4505.png,giorgio armani\\n', '2174.png,alexander wang\\n', '9021.png,missoni\\n', '12224.png,lanvin\\n', '9747.png,chado ralph rucci\\n', '6474.png,vera wang\\n', '10355.png,chado ralph rucci\\n', '2612.png,jason wu\\n', '13674.png,yves saint laurent\\n', '11705.png,carolina herrera\\n', '5633.png,marc by marc jacobs\\n', '7030.png,jil sander\\n', '12218.png,miu miu\\n', '3256.png,jason wu\\n', '5141.png,alexander wang\\n', '11077.png,marc jacobs\\n', '7756.png,marc jacobs\\n', '1327.png,jil sander\\n', '14090.png,gucci\\n', '11856.png,carolina herrera\\n', '13099.png,missoni\\n', '7805.png,carolina herrera\\n', '1284.png,jil sander\\n', '710.png,alexander wang\\n', '5948.png,chado ralph rucci\\n', '6299.png,yves saint laurent\\n', '7839.png,derek lam\\n', '704.png,giorgio armani\\n', '1290.png,marc by marc jacobs\\n', '2799.png,miu miu\\n', '10235.png,chado ralph rucci\\n', '2772.png,dries van noten\\n', '14053.png,gucci\\n', '12344.png,marc by marc jacobs\\n', '14735.png,jil sander\\n', '10553.png,etro\\n', '4465.png,jil sander\\n', '10547.png,jason wu\\n', '2000.png,marc jacobs\\n', '11881.png,marc by marc jacobs\\n', '11659.png,marni\\n', '12350.png,emporio armani\\n', '7178.png,missoni\\n', '9155.png,etro\\n', '14047.png,gucci\\n', '923.png,armani prive\\n', '12436.png,carolina herrera\\n', '5009.png,dries van noten\\n', '2766.png,alexander wang\\n', '1247.png,giorgio armani\\n', '6528.png,jason wu\\n', '2996.png,jason wu\\n', '5021.png,dries van noten\\n', '10209.png,dries van noten\\n', '5747.png,jil sander\\n', '3336.png,etro\\n', '11671.png,etro\\n', '1521.png,chado ralph rucci\\n', '7144.png,missoni\\n', '11103.png,marc jacobs\\n', '2982.png,jason wu\\n', '7622.png,yves saint laurent\\n', '8511.png,tommy hilfiger\\n', '248.png,chado ralph rucci\\n', '9380.png,alexander wang\\n', '3875.png,marc jacobs\\n', '14292.png,vera wang\\n', '14286.png,alexander wang\\n', '8934.png,missoni\\n', '4868.png,vera wang\\n', '12191.png,chado ralph rucci\\n', '9394.png,giambattista valli\\n', '4840.png,chado ralph rucci\\n', '7391.png,vera wang\\n', '274.png,tommy hilfiger\\n', '12807.png,carolina herrera\\n', '6931.png,jil sander\\n', '3108.png,jason wu\\n', '10989.png,emporio armani\\n', '5579.png,missoni\\n', '2216.png,alexander wang\\n', '9343.png,versace\\n', '14537.png,alexander wang\\n', '12620.png,jason wu\\n', '14251.png,marc jacobs\\n', '1079.png,armani prive\\n', '4101.png,jil sander\\n', '2564.png,missoni\\n', '9431.png,missoni\\n', '8049.png,chado ralph rucci\\n', '2202.png,chado ralph rucci\\n', '1723.png,bottega veneta\\n', '13264.png,marc jacobs\\n', '8061.png,bottega veneta\\n', '7352.png,marc jacobs\\n', '3134.png,jason wu\\n', '4883.png,chado ralph rucci\\n', '11315.png,ralph lauren\\n', '3652.png,emporio armani\\n', '7434.png,marc jacobs\\n', '8713.png,chado ralph rucci\\n', '7420.png,jil sander\\n', '1051.png,armani prive\\n', '13516.png,vera wang\\n', '2558.png,marni\\n', '5237.png,miu miu\\n', '4129.png,yves saint laurent\\n', '3646.png,emporio armani\\n', '3120.png,bottega veneta\\n', '5551.png,missoni\\n', '4897.png,dries van noten\\n', '1737.png,bottega veneta\\n', '6058.png,marni\\n', '8075.png,miu miu\\n', '4128.png,yves saint laurent\\n', '13517.png,chado ralph rucci\\n', '8712.png,miu miu\\n', '6059.png,alexander wang\\n', '13271.png,chado ralph rucci\\n', '10778.png,dries van noten\\n', '5550.png,yves saint laurent\\n', '3121.png,jason wu\\n', '4882.png,dries van noten\\n', '5544.png,missoni\\n', '3135.png,alexander wang\\n', '11472.png,missoni\\n', '7353.png,marni\\n', '1722.png,jil sander\\n', '13503.png,vera wang\\n', '1044.png,armani prive\\n', '8706.png,carolina herrera\\n', '3653.png,emporio armani\\n', '11314.png,marc jacobs\\n', '6703.png,bottega veneta\\n', '10744.png,carolina herrera\\n', '8048.png,miu miu\\n', '9342.png,etro\\n', '2217.png,alexander wang\\n', '5578.png,missoni\\n', '2571.png,derek lam\\n', '1093.png,armani prive\\n', '3684.png,dolce & gabbana\\n', '10963.png,carolina herrera\\n', '4855.png,vera wang\\n', '5593.png,missoni\\n', '261.png,giambattista valli\\n', '275.png,jason wu\\n', '1939.png,alexander wang\\n', '4841.png,emporio armani\\n', '5587.png,missoni\\n', '3848.png,emporio armani\\n', '6930.png,marc by marc jacobs\\n', '1087.png,armani prive\\n', '8935.png,miu miu\\n', '14287.png,miu miu\\n', '1911.png,missoni\\n', '11499.png,bottega veneta\\n', '1905.png,vera wang\\n', '14293.png,alexander wang\\n', '5752.png,jil sander\\n', '8276.png,miu miu\\n', '8510.png,dolce & gabbana\\n', '1252.png,marc by marc jacobs\\n', '11102.png,miu miu\\n', '2997.png,jason wu\\n', '11116.png,alexander wang\\n', '8504.png,gucci\\n', '14708.png,carolina herrera\\n', '8262.png,missoni\\n', '7151.png,dolce & gabbana\\n', '5746.png,carolina herrera\\n', '2029.png,emporio armani\\n', '1508.png,bottega veneta\\n', '9154.png,dolce & gabbana\\n', '14720.png,dries van noten\\n', '7179.png,armani prive\\n', '12351.png,alexander wang\\n', '11658.png,dolce & gabbana\\n', '11880.png,carolina herrera\\n', '10546.png,vera wang\\n', '2767.png,marc jacobs\\n', '10220.png,chado ralph rucci\\n', '5008.png,miu miu\\n', '12437.png,missoni\\n', '6501.png,emporio armani\\n', '9632.png,derek lam\\n', '14046.png,emporio armani\\n', '936.png,armani prive\\n', '8538.png,giorgio armani\\n', '14052.png,gucci\\n', '9626.png,derek lam\\n', '10234.png,chado ralph rucci\\n', '4302.png,missoni\\n', '11894.png,etro\\n', '10552.png,alexander wang\\n', '9140.png,carolina herrera\\n', '5791.png,missoni\\n', '5949.png,marni\\n', '2798.png,dries van noten\\n', '705.png,dolce & gabbana\\n', '1285.png,marc jacobs\\n', '9801.png,chado ralph rucci\\n', '2954.png,giambattista valli\\n', '7192.png,derek lam\\n', '5975.png,missoni\\n', '7804.png,marc jacobs\\n', '2968.png,giambattista valli\\n', '14091.png,gucci\\n', '7810.png,bottega veneta\\n', '5961.png,chado ralph rucci\\n', '2149.png,alexander wang\\n', '14668.png,carolina herrera\\n', '1440.png,marc by marc jacobs\\n', '13661.png,yves saint laurent\\n', '11076.png,bottega veneta\\n', '11062.png,missoni\\n', '1332.png,marc jacobs\\n', '8316.png,miu miu\\n', '7025.png,alexander wang\\n', '9008.png,chado ralph rucci\\n', '11704.png,carolina herrera\\n', '3243.png,jason wu\\n', '12225.png,chado ralph rucci\\n', '14654.png,dolce & gabbana\\n', '9020.png,christian dior\\n', '2175.png,gucci\\n', '10432.png,etro\\n', '10354.png,chado ralph rucci\\n', '13885.png,carolina herrera\\n', '856.png,giorgio armani\\n', '8458.png,givenchy\\n', '14126.png,givenchy\\n', '842.png,giorgio armani\\n', '5168.png,marc by marc jacobs\\n', '10340.png,chado ralph rucci\\n', '2161.png,alexander wang\\n', '4510.png,versace\\n', '11738.png,etro\\n', '14640.png,dolce & gabbana\\n', '2834.png,marc by marc jacobs\\n', '671.png,giorgio armani\\n', '2820.png,versace\\n', '5801.png,jil sander\\n', '11937.png,dolce & gabbana\\n', '13846.png,dolce & gabbana\\n', '895.png,armani prive\\n', '12580.png,chado ralph rucci\\n', '10397.png,emporio armani\\n', '2808.png,bottega veneta\\n', '7964.png,alexander mcqueen\\n', '9791.png,derek lam\\n', '5815.png,jil sander\\n', '11923.png,chado ralph rucci\\n', '5342.png,armani prive\\n', '11274.png,missoni\\n', '6893.png,dries van noten\\n', '8666.png,jason wu\\n', '13463.png,jil sander\\n', '1124.png,armani prive\\n', '1642.png,marc jacobs\\n', '7233.png,gucci\\n', '3055.png,miu miu\\n', '5424.png,missoni\\n', '5430.png,armani prive\\n', '10618.png,vera wang\\n', '13311.png,alexander mcqueen\\n', '1656.png,marc by marc jacobs\\n', '7227.png,alexander mcqueen\\n', '8672.png,dries van noten\\n', '13477.png,yves saint laurent\\n', '12741.png,bottega veneta\\n', '14330.png,alexander wang\\n', '1118.png,armani prive\\n', '4706.png,armani prive\\n', '10630.png,carolina herrera\\n', '2377.png,armani prive\\n', '9236.png,jil sander\\n', '14442.png,marc by marc jacobs\\n', '2363.png,alexander wang\\n', '12755.png,vera wang\\n', '9550.png,derek lam\\n', '498.png,giambattista valli\\n', '473.png,bottega veneta\\n', '4921.png,giambattista valli\\n', '3096.png,jason wu\\n', '10817.png,vera wang\\n', '1681.png,jil sander\\n', '1695.png,missoni\\n', '10803.png,vera wang\\n', '12972.png,carolina herrera\\n', '10195.png,chado ralph rucci\\n', '3914.png,jason wu\\n', '9587.png,derek lam\\n', '8699.png,chado ralph rucci\\n', '14481.png,givenchy\\n', '9593.png,derek lam\\n', '13488.png,derek lam\\n', '6878.png,alexander wang\\n', '10181.png,chado ralph rucci\\n', '3900.png,missoni\\n', '12976.png,yves saint laurent\\n', '4931.png,emporio armani\\n', '305.png,bottega veneta\\n', '1685.png,bottega veneta\\n', '3092.png,jason wu\\n', '10813.png,marc by marc jacobs\\n', '6854.png,yves saint laurent\\n', '12962.png,carolina herrera\\n', '10185.png,chado ralph rucci\\n', '3904.png,versace\\n', '9597.png,derek lam\\n', '8851.png,emporio armani\\n', '339.png,bottega veneta\\n', '1875.png,marc by marc jacobs\\n', '14491.png,jil sander\\n', '1861.png,marc by marc jacobs\\n', '6868.png,marc by marc jacobs\\n', '13498.png,yves saint laurent\\n', '8845.png,alexander mcqueen\\n', '3910.png,missoni\\n', '3723.png,emporio armani\\n', '1134.png,armani prive\\n', '1652.png,dries van noten\\n', '5434.png,carolina herrera\\n', '10608.png,vera wang\\n', '3051.png,miu miu\\n', '8104.png,givenchy\\n', '6129.png,carolina herrera\\n', '13467.png,yves saint laurent\\n', '8662.png,derek lam\\n', '11270.png,lanvin\\n', '3737.png,emporio armani\\n', '1108.png,armani prive\\n', '10146.png,marni\\n', '10620.png,missoni\\n', '13329.png,ralph lauren\\n', '6101.png,vera wang\\n', '2373.png,vera wang\\n', '2415.png,etro\\n', '10152.png,chado ralph rucci\\n', '8886.png,derek lam\\n', '12745.png,emporio armani\\n', '5193.png,alexander wang\\n', '661.png,giorgio armani\\n', '7784.png,marni\\n', '5811.png,jil sander\\n', '14687.png,carolina herrera\\n', '649.png,giorgio armani\\n', '14693.png,miu miu\\n', '11933.png,vera wang\\n', '5805.png,jil sander\\n', '5636.png,missoni\\n', '12209.png,alexander wang\\n', '8312.png,miu miu\\n', '7747.png,jil sander\\n', '13671.png,yves saint laurent\\n', '10378.png,chado ralph rucci\\n', '3521.png,etro\\n', '11066.png,christian dior\\n', '3535.png,dries van noten\\n', '11072.png,giorgio armani\\n', '8460.png,alexander wang\\n', '1322.png,jil sander\\n', '9018.png,bottega veneta\\n', '13103.png,carolina herrera\\n', '8306.png,chado ralph rucci\\n', '3253.png,jason wu\\n', '11714.png,marc jacobs\\n', '5622.png,missoni\\n', '10422.png,missoni\\n', '12553.png,yves saint laurent\\n', '14122.png,gucci\\n', '13659.png,yves saint laurent\\n', '9742.png,derek lam\\n', '12.png,chado ralph rucci\\n', '3509.png,emporio armani\\n', '4500.png,vera wang\\n', '2171.png,alexander wang\\n', '9024.png,miu miu\\n', '14650.png,dolce & gabbana\\n', '5959.png,carolina herrera\\n', '2788.png,alexander wang\\n', '3496.png,etro\\n', '7828.png,marc jacobs\\n', '1281.png,marc by marc jacobs\\n', '715.png,chado ralph rucci\\n', '1295.png,marc by marc jacobs\\n', '3482.png,etro\\n', '11853.png,marc by marc jacobs\\n', '8299.png,carolina herrera\\n', '12382.png,dries van noten\\n', '729.png,alexander wang\\n', '2978.png,jason wu\\n', '14081.png,gucci\\n', '9193.png,chado ralph rucci\\n', '10581.png,derek lam\\n', '11674.png,dries van noten\\n', '8266.png,miu miu\\n', '9178.png,alexander wang\\n', '13705.png,marni\\n', '1242.png,jil sander\\n', '8500.png,marc jacobs\\n', '3455.png,marc jacobs\\n', '2987.png,emporio armani\\n', '10218.png,chado ralph rucci\\n', '13711.png,chado ralph rucci\\n', '1256.png,dolce & gabbana\\n', '7627.png,chado ralph rucci\\n', '12369.png,missoni\\n', '5756.png,jil sander\\n', '11660.png,vera wang\\n', '10556.png,derek lam\\n', '2011.png,chado ralph rucci\\n', '4306.png,vera wang\\n', '5018.png,jason wu\\n', '12427.png,miu miu\\n', '14042.png,missoni\\n', '6505.png,vera wang\\n', '926.png,armani prive\\n', '10224.png,chado ralph rucci\\n', '11884.png,derek lam\\n', '10542.png,vera wang\\n', '14724.png,emporio armani\\n', '517.png,bottega veneta\\n', '5583.png,dries van noten\\n', '7380.png,carolina herrera\\n', '265.png,missoni\\n', '503.png,bottega veneta\\n', '12816.png,carolina herrera\\n', '9385.png,missoni\\n', '11489.png,marc jacobs\\n', '10783.png,armani prive\\n', '1915.png,chado ralph rucci\\n', '9391.png,missoni\\n', '8931.png,missoni\\n', '5226.png,jason wu\\n', '8702.png,derek lam\\n', '1040.png,armani prive\\n', '13507.png,yves saint laurent\\n', '14268.png,alexander wang\\n', '1726.png,bottega veneta\\n', '13261.png,chado ralph rucci\\n', '7357.png,carolina herrera\\n', '11476.png,derek lam\\n', '4886.png,jason wu\\n', '11462.png,marc jacobs\\n', '4892.png,yves saint laurent\\n', '13275.png,giorgio armani\\n', '7343.png,emporio armani\\n', '12625.png,yves saint laurent\\n', '10032.png,chado ralph rucci\\n', '2213.png,alexander wang\\n', '6075.png,armani prive\\n', '12143.png,miu miu\\n', '14526.png,marc by marc jacobs\\n', '9352.png,miu miu\\n', '3119.png,derek lam\\n', '1068.png,missoni\\n', '6707.png,carolina herrera\\n', '14240.png,jason wu\\n', '10741.png,jason wu\\n', '2206.png,dries van noten\\n', '3118.png,jason wu\\n', '9353.png,dries van noten\\n', '1069.png,chado ralph rucci\\n', '7418.png,bottega veneta\\n', '4105.png,gucci\\n', '9347.png,missoni\\n', '6074.png,missoni\\n', '5555.png,missoni\\n', '11463.png,carolina herrera\\n', '3642.png,giambattista valli\\n', '13506.png,yves saint laurent\\n', '3656.png,emporio armani\\n', '5227.png,yves saint laurent\\n', '5541.png,missoni\\n', '4887.png,marc jacobs\\n', '7356.png,miu miu\\n', '1914.png,gucci\\n', '8930.png,missoni\\n', '3871.png,miu miu\\n', '4878.png,jason wu\\n', '10796.png,jil sander\\n', '9384.png,alexander wang\\n', '264.png,dries van noten\\n', '1928.png,chado ralph rucci\\n', '12817.png,carolina herrera\\n', '3859.png,carolina herrera\\n', '12803.png,carolina herrera\\n', '8918.png,miu miu\\n', '10972.png,jason wu\\n', '5582.png,missoni\\n', '10225.png,chado ralph rucci\\n', '927.png,marc jacobs\\n', '9637.png,derek lam\\n', '14043.png,yves saint laurent\\n', '14725.png,ralph lauren\\n', '12354.png,chado ralph rucci\\n', '10543.png,derek lam\\n', '11885.png,etro\\n', '11891.png,emporio armani\\n', '6276.png,carolina herrera\\n', '1519.png,vera wang\\n', '12340.png,missoni\\n', '933.png,armani prive\\n', '13738.png,marni\\n', '6510.png,missoni\\n', '9623.png,derek lam\\n', '10231.png,jil sander\\n', '13710.png,miu miu\\n', '5031.png,jason wu\\n', '11107.png,missoni\\n', '3440.png,etro\\n', '4449.png,marc jacobs\\n', '11661.png,etro\\n', '13076.png,carolina herrera\\n', '7140.png,marc jacobs\\n', '1525.png,giambattista valli\\n', '11675.png,etro\\n', '3332.png,etro\\n', '3454.png,marc jacobs\\n', '13704.png,dolce & gabbana\\n', '7801.png,tommy hilfiger\\n', '13937.png,chado ralph rucci\\n', '2979.png,chado ralph rucci\\n', '11846.png,marc by marc jacobs\\n', '12397.png,alexander wang\\n', '13089.png,armani prive\\n', '8298.png,miu miu\\n', '11852.png,derek lam\\n', '5964.png,dolce & gabbana\\n', '7815.png,marc jacobs\\n', '728.png,giorgio armani\\n', '9838.png,jil sander\\n', '2945.png,bottega veneta\\n', '1294.png,missoni\\n', '5958.png,derek lam\\n', '7197.png,bottega veneta\\n', '714.png,giorgio armani\\n', '9804.png,dolce & gabbana\\n', '3508.png,emporio armani\\n', '10351.png,marc by marc jacobs\\n', '9743.png,derek lam\\n', '13658.png,etro\\n', '853.png,giorgio armani\\n', '2170.png,alexander wang\\n', '11729.png,etro\\n', '4501.png,dries van noten\\n', '2164.png,alexander wang\\n', '6302.png,lanvin\\n', '9031.png,alexander wang\\n', '14645.png,dolce & gabbana\\n', '9757.png,miu miu\\n', '14123.png,missoni\\n', '10345.png,chado ralph rucci\\n', '3534.png,etro\\n', '5145.png,marc by marc jacobs\\n', '11715.png,miu miu\\n', '8307.png,miu miu\\n', '9019.png,christian dior\\n', '1451.png,marc by marc jacobs\\n', '13116.png,yves saint laurent\\n', '4529.png,chado ralph rucci\\n', '11067.png,jil sander\\n', '13670.png,yves saint laurent\\n', '7746.png,marc jacobs\\n', '12585.png,versace\\n', '9958.png,dries van noten\\n', '890.png,armani prive\\n', '648.png,giorgio armani\\n', '10392.png,chado ralph rucci\\n', '14686.png,dolce & gabbana\\n', '10386.png,carolina herrera\\n', '2819.png,yves saint laurent\\n', '12591.png,giambattista valli\\n', '884.png,giorgio armani\\n', '5186.png,jil sander\\n', '4298.png,jil sander\\n', '3285.png,etro\\n', '1492.png,missoni\\n', '660.png,armani prive\\n', '2372.png,alexander wang\\n', '6114.png,alexander wang\\n', '489.png,jil sander\\n', '8887.png,chado ralph rucci\\n', '9541.png,missoni\\n', '10153.png,armani prive\\n', '4065.png,yves saint laurent\\n', '1109.png,armani prive\\n', '6100.png,chado ralph rucci\\n', '12036.png,marc jacobs\\n', '3078.png,jason wu\\n', '10621.png,vera wang\\n', '2366.png,alexander wang\\n', '8105.png,miu miu\\n', '3050.png,jason wu\\n', '5421.png,missoni\\n', '3736.png,emporio armani\\n', '1121.png,armani prive\\n', '8677.png,jason wu\\n', '13472.png,yves saint laurent\\n', '3722.png,emporio armani\\n', '8844.png,chado ralph rucci\\n', '9582.png,ralph lauren\\n', '6869.png,jil sander\\n', '8850.png,jason wu\\n', '8688.png,chado ralph rucci\\n', '3905.png,alexander wang\\n', '338.png,bottega veneta\\n', '310.png,bottega veneta\\n', '1684.png,gucci\\n', '12963.png,derek lam\\n', '3939.png,bottega veneta\\n', '462.png,bottega veneta\\n', '12977.png,jason wu\\n', '7587.png,bottega veneta\\n', '1690.png,tommy hilfiger\\n', '4930.png,dries van noten\\n', '3087.png,jason wu\\n', '5386.png,missoni\\n', '7591.png,gucci\\n', '3091.png,jason wu\\n', '4926.png,dries van noten\\n', '10804.png,vera wang\\n', '6843.png,dolce & gabbana\\n', '12975.png,chado ralph rucci\\n', '448.png,bottega veneta\\n', '1862.png,miu miu\\n', '10838.png,miu miu\\n', '3907.png,chado ralph rucci\\n', '11298.png,armani prive\\n', '9594.png,chado ralph rucci\\n', '8661.png,chado ralph rucci\\n', '3734.png,emporio armani\\n', '5423.png,missoni\\n', '11515.png,miu miu\\n', '7234.png,bottega veneta\\n', '8107.png,miu miu\\n', '13302.png,missoni\\n', '12008.png,emporio armani\\n', '14479.png,tommy hilfiger\\n', '10179.png,chado ralph rucci\\n', '13470.png,yves saint laurent\\n', '8675.png,dolce & gabbana\\n', '4067.png,yves saint laurent\\n', '3708.png,emporio armani\\n', '10151.png,armani prive\\n', '9543.png,derek lam\\n', '14337.png,vera wang\\n', '1679.png,tommy hilfiger\\n', '10637.png,vera wang\\n', '4701.png,carolina herrera\\n', '2364.png,alexander wang\\n', '14323.png,marc by marc jacobs\\n', '9557.png,derek lam\\n', '4073.png,alexander wang\\n', '10145.png,chado ralph rucci\\n', '676.png,carolina herrera\\n', '9972.png,yves saint laurent\\n', '11918.png,vera wang\\n', '3287.png,etro\\n', '14848.png,dries van noten\\n', '14690.png,dolce & gabbana\\n', '11930.png,dries van noten\\n', '10390.png,chado ralph rucci\\n', '7977.png,yves saint laurent\\n', '9782.png,derek lam\\n', '892.png,armani prive\\n', '9796.png,alexander wang\\n', '13855.png,miu miu\\n', '8488.png,vera wang\\n', '10384.png,chado ralph rucci\\n', '5812.png,jil sander\\n', '1447.png,marc jacobs\\n', '8305.png,marni\\n', '3250.png,jason wu\\n', '3536.png,etro\\n', '8463.png,giorgio armani\\n', '7750.png,bottega veneta\\n', '1321.png,marc by marc jacobs\\n', '14109.png,jason wu\\n', '8477.png,bottega veneta\\n', '879.png,giorgio armani\\n', '7744.png,marc jacobs\\n', '13672.png,yves saint laurent\\n', '39.png,armani prive\\n', '11703.png,chado ralph rucci\\n', '5635.png,missoni\\n', '13114.png,alexander wang\\n', '8311.png,miu miu\\n', '10435.png,vera wang\\n', '9027.png,missoni\\n', '14653.png,marc by marc jacobs\\n', '8339.png,armani prive\\n', '9999.png,missoni\\n', '689.png,chado ralph rucci\\n', '9741.png,derek lam\\n', '14135.png,gucci\\n', '10353.png,chado ralph rucci\\n', '7778.png,etro\\n', '1309.png,marc by marc jacobs\\n', '6466.png,armani prive\\n', '4517.png,carolina herrera\\n', '2166.png,marc jacobs\\n', '7181.png,marc by marc jacobs\\n', '13909.png,missoni\\n', '1296.png,marc by marc jacobs\\n', '2947.png,missoni\\n', '3481.png,marc jacobs\\n', '2953.png,marc jacobs\\n', '716.png,giorgio armani\\n', '5782.png,jil sander\\n', '13921.png,marni\\n', '10596.png,vera wang\\n', '1533.png,miu miu\\n', '5755.png,jil sander\\n', '3324.png,armani prive\\n', '2984.png,dries van noten\\n', '1255.png,jil sander\\n', '919.png,chado ralph rucci\\n', '14069.png,miu miu\\n', '12418.png,missoni\\n', '2990.png,jason wu\\n', '2748.png,bottega veneta\\n', '5027.png,armani prive\\n', '5999.png,lanvin\\n', '11677.png,etro\\n', '3330.png,etro\\n', '7156.png,bottega veneta\\n', '13060.png,marc jacobs\\n', '11887.png,jason wu\\n', '5769.png,marc jacobs\\n', '3318.png,etro\\n', '9635.png,derek lam\\n', '925.png,armani prive\\n', '11139.png,missoni\\n', '2760.png,chado ralph rucci\\n', '10227.png,chado ralph rucci\\n', '9621.png,jil sander\\n', '6274.png,jil sander\\n', '2012.png,armani prive\\n', '3683.png,alexander wang\\n', '12815.png,carolina herrera\\n', '4852.png,dries van noten\\n', '4846.png,jil sander\\n', '5580.png,missoni\\n', '12801.png,marc by marc jacobs\\n', '8932.png,missoni\\n', '14280.png,alexander wang\\n', '12197.png,miu miu\\n', '1902.png,derek lam\\n', '8098.png,miu miu\\n', '528.png,bottega veneta\\n', '1057.png,emporio armani\\n', '5231.png,etro\\n', '2238.png,alexander wang\\n', '13276.png,jason wu\\n', '13262.png,ralph lauren\\n', '1725.png,yves saint laurent\\n', '7354.png,missoni\\n', '8067.png,miu miu\\n', '3132.png,jil sander\\n', '3654.png,missoni\\n', '10025.png,jil sander\\n', '4113.png,versace\\n', '9437.png,etro\\n', '14525.png,jil sander\\n', '299.png,givenchy\\n', '11449.png,bottega veneta\\n', '1719.png,tommy hilfiger\\n', '12626.png,derek lam\\n', '9423.png,miu miu\\n', '6710.png,dolce & gabbana\\n', '12141.png,marc by marc jacobs\\n', '1718.png,chado ralph rucci\\n', '14530.png,alexander wang\\n', '2211.png,alexander wang\\n', '10756.png,yves saint laurent\\n', '4660.png,carolina herrera\\n', '4106.png,yves saint laurent\\n', '3669.png,emporio armani\\n', '14256.png,gucci\\n', '13539.png,chado ralph rucci\\n', '12627.png,jason wu\\n', '6705.png,dolce & gabbana\\n', '2205.png,marc jacobs\\n', '10742.png,jil sander\\n', '8066.png,miu miu\\n', '13505.png,yves saint laurent\\n', '7433.png,etro\\n', '10018.png,marni\\n', '3899.png,jil sander\\n', '1730.png,jil sander\\n', '13277.png,ralph lauren\\n', '5556.png,chado ralph rucci\\n', '11460.png,yves saint laurent\\n', '3127.png,jason wu\\n', '9387.png,miu miu\\n', '8099.png,jil sander\\n', '8927.png,missoni\\n', '12828.png,carolina herrera\\n', '10959.png,etro\\n', '273.png,miu miu\\n', '6088.png,derek lam\\n', '10971.png,chado ralph rucci\\n', '3696.png,emporio armani\\n', '12800.png,carolina herrera\\n', '1081.png,armani prive\\n', '515.png,bottega veneta\\n', '6922.png,jil sander\\n', '12814.png,carolina herrera\\n', '3682.png,emporio armani\\n', '5595.png,missoni\\n', '9620.png,missoni\\n', '2775.png,bottega veneta\\n', '4462.png,carolina herrera\\n', '2013.png,fendi\\n', '14732.png,dolce & gabbana\\n', '9146.png,missoni\\n', '8258.png,miu miu\\n', '3319.png,marc by marc jacobs\\n', '5768.png,derek lam\\n', '2761.png,chado ralph rucci\\n', '11138.png,missoni\\n', '1268.png,marc by marc jacobs\\n', '14040.png,jil sander\\n', '4338.png,marc jacobs\\n', '7157.png,emporio armani\\n', '5754.png,jil sander\\n', '1532.png,marc jacobs\\n', '8270.png,miu miu\\n', '918.png,armani prive\\n', '2985.png,jason wu\\n', '7816.png,marc jacobs\\n', '11689.png,marc jacobs\\n', '5967.png,carolina herrera\\n', '10583.png,vera wang\\n', '12394.png,bottega veneta\\n', '7802.png,marc jacobs\\n', '14083.png,etro\\n', '717.png,giorgio armani\\n', '7194.png,emporio armani\\n', '7180.png,carolina herrera\\n', '5797.png,jil sander\\n', '4489.png,chado ralph rucci\\n', '3480.png,gucci\\n', '2946.png,giambattista valli\\n', '1297.png,marc by marc jacobs\\n', '13908.png,emporio armani\\n', '2601.png,ralph lauren\\n', '10420.png,vera wang\\n', '2167.png,jason wu\\n', '14646.png,dolce & gabbana\\n', '12223.png,chado ralph rucci\\n', '6315.png,marc by marc jacobs\\n', '4264.png,yves saint laurent\\n', '10.png,dolce & gabbana\\n', '9998.png,yves saint laurent\\n', '12545.png,marc jacobs\\n', '850.png,giorgio armani\\n', '13883.png,miu miu\\n', '3523.png,jil sander\\n', '11064.png,missoni\\n', '13673.png,giambattista valli\\n', '1334.png,jil sander\\n', '878.png,giorgio armani\\n', '13115.png,ralph lauren\\n', '5634.png,missoni\\n', '3245.png,jason wu\\n', '5620.png,missoni\\n', '8304.png,miu miu\\n', '13101.png,ralph lauren\\n', '1446.png,dolce & gabbana\\n', '8462.png,bottega veneta\\n', '3537.png,jason wu\\n', '2629.png,miu miu\\n', '139.png,jil sander\\n', '5813.png,dries van noten\\n', '14691.png,carolina herrera\\n', '9783.png,jason wu\\n', '10391.png,chado ralph rucci\\n', '13868.png,lanvin\\n', '11919.png,missoni\\n', '105.png,bottega veneta\\n', '3292.png,yves saint laurent\\n', '2832.png,giambattista valli\\n', '677.png,derek lam\\n', '6103.png,marni\\n', '14444.png,miu miu\\n', '12035.png,gucci\\n', '2403.png,giambattista valli\\n', '10144.png,chado ralph rucci\\n', '6665.png,chado ralph rucci\\n', '9556.png,dolce & gabbana\\n', '12747.png,emporio armani\\n', '8884.png,jason wu\\n', '14336.png,missoni\\n', '9542.png,jil sander\\n', '4700.png,bottega veneta\\n', '10636.png,emporio armani\\n', '9224.png,marni\\n', '3047.png,etro\\n', '1650.png,missoni\\n', '12009.png,dries van noten\\n', '8112.png,alexander wang\\n', '7221.png,bottega veneta\\n', '1136.png,armani prive\\n', '13471.png,yves saint laurent\\n', '5350.png,marni\\n', '10178.png,emporio armani\\n', '3721.png,emporio armani\\n', '3735.png,emporio armani\\n', '8660.png,emporio armani\\n', '6895.png,jil sander\\n', '1122.png,armani prive\\n', '13303.png,alexander wang\\n', '8106.png,miu miu\\n', '5422.png,missoni\\n', '1877.png,carolina herrera\\n', '14487.png,bottega veneta\\n', '9595.png,yves saint laurent\\n', '10187.png,marni\\n', '8847.png,dolce & gabbana\\n', '12784.png,carolina herrera\\n', '1863.png,chado ralph rucci\\n', '1693.png,yves saint laurent\\n', '4933.png,chado ralph rucci\\n', '3084.png,jason wu\\n', '5393.png,derek lam\\n', '6842.png,dolce & gabbana\\n', '6856.png,jil sander\\n', '5387.png,missoni\\n', '313.png,tommy hilfiger\\n', '1687.png,marc by marc jacobs\\n', '10808.png,vera wang\\n', '3089.png,jason wu\\n', '2397.png,emporio armani\\n', '3937.png,yves saint laurent\\n', '7589.png,chado ralph rucci\\n', '12979.png,carolina herrera\\n', '8876.png,miu miu\\n', '478.png,bottega veneta\\n', '3923.png,chado ralph rucci\\n', '336.png,bottega veneta\\n', '8686.png,jason wu\\n', '9598.png,derek lam\\n', '8692.png,chado ralph rucci\\n', '12789.png,bottega veneta\\n', '12951.png,derek lam\\n', '10820.png,vera wang\\n', '10613.png,emporio armani\\n', '2354.png,carolina herrera\\n', '14475.png,bottega veneta\\n', '9201.png,marc jacobs\\n', '9567.png,derek lam\\n', '10175.png,derek lam\\n', '2432.png,alexander wang\\n', '3738.png,emporio armani\\n', '10161.png,chado ralph rucci\\n', '2426.png,dries van noten\\n', '13468.png,marc by marc jacobs\\n', '9573.png,derek lam\\n', '12010.png,fendi\\n', '1649.png,missoni\\n', '10607.png,vera wang\\n', '2340.png,alexander wang\\n', '7210.png,jason wu\\n', '14449.png,marc jacobs\\n', '11257.png,missoni\\n', '5361.png,vera wang\\n', '13440.png,etro\\n', '1107.png,bottega veneta\\n', '7576.png,missoni\\n', '12992.png,carolina herrera\\n', '3704.png,emporio armani\\n', '3062.png,jason wu\\n', '11525.png,marc jacobs\\n', '8137.png,carolina herrera\\n', '11914.png,vera wang\\n', '1488.png,armani prive\\n', '11900.png,vera wang\\n', '2181.png,alexander wang\\n', '4296.png,vera wang\\n', '652.png,missoni\\n', '9942.png,marc by marc jacobs\\n', '8484.png,missoni\\n', '14844.png,alexander wang\\n', '13695.png,carolina herrera\\n', '14111.png,dries van noten\\n', '1339.png,missoni\\n', '12206.png,alexander wang\\n', '3248.png,bottega veneta\\n', '4527.png,armani prive\\n', '5639.png,missoni\\n', '9017.png,jason wu\\n', '8309.png,miu miu\\n', '6442.png,miu miu\\n', '9771.png,miu miu\\n', '14105.png,etro\\n', '849.png,giorgio armani\\n', '8447.png,giorgio armani\\n', '7774.png,missoni\\n', '9759.png,derek lam\\n', '691.png,giorgio armani\\n', '13642.png,yves saint laurent\\n', '3512.png,vera wang\\n', '8321.png,bottega veneta\\n', '1477.png,emporio armani\\n', '13130.png,ralph lauren\\n', '3260.png,etro\\n', '2618.png,dries van noten\\n', '7760.png,derek lam\\n', '14139.png,jil sander\\n', '685.png,derek lam\\n', '13656.png,missoni\\n', '5942.png,jil sander\\n', '726.png,derek lam\\n', '13093.png,dolce & gabbana\\n', '11690.png,bottega veneta\\n', '8282.png,miu miu\\n', '732.png,giorgio armani\\n', '6522.png,vera wang\\n', '9611.png,chado ralph rucci\\n', '14065.png,gucci\\n', '901.png,alexander wang\\n', '8269.png,miu miu\\n', '14703.png,dolce & gabbana\\n', '2022.png,marc by marc jacobs\\n', '10571.png,chado ralph rucci\\n', '5759.png,miu miu\\n', '4447.png,jason wu\\n', '5981.png,marc by marc jacobs\\n', '14717.png,vera wang\\n', '9163.png,missoni\\n', '915.png,armani prive\\n', '12400.png,marc jacobs\\n', '11109.png,giorgio armani\\n', '4321.png,vera wang\\n', '2750.png,giambattista valli\\n', '8533.png,miu miu\\n', '11121.png,marc by marc jacobs\\n', '3466.png,etro\\n', '11647.png,jil sander\\n', '8255.png,miu miu\\n', '1517.png,yves saint laurent\\n', '13050.png,carolina herrera\\n', '8241.png,miu miu\\n', '1503.png,gucci\\n', '5765.png,jil sander\\n', '5003.png,chado ralph rucci\\n', '13722.png,marc jacobs\\n', '929.png,alexander wang\\n', '8916.png,alexander wang\\n', '518.png,bottega veneta\\n', '12819.png,carolina herrera\\n', '8902.png,emporio armani\\n', '5598.png,missoni\\n', '4686.png,derek lam\\n', '6091.png,armani prive\\n', '4876.png,alexander wang\\n', '8094.png,miu miu\\n', '13291.png,ralph lauren\\n', '12831.png,jil sander\\n', '14298.png,chado ralph rucci\\n', '6907.png,bottega veneta\\n', '530.png,marc by marc jacobs\\n', '256.png,marc jacobs\\n', '10767.png,derek lam\\n', '2220.png,lanvin\\n', '6720.png,fendi\\n', '10001.png,jil sander\\n', '4137.png,missoni\\n', '3894.png,alexander wang\\n', '6734.png,dolce & gabbana\\n', '9361.png,chado ralph rucci\\n', '6052.png,missoni\\n', '281.png,marc jacobs\\n', '9349.png,missoni\\n', '11445.png,yves saint laurent\\n', '3664.png,emporio armani\\n', '11323.png,marc by marc jacobs\\n', '7402.png,emporio armani\\n', '1073.png,armani prive\\n', '6708.png,tommy hilfiger\\n', '3116.png,jason wu\\n', '13246.png,carolina herrera\\n', '7370.png,carolina herrera\\n', '8043.png,miu miu\\n', '8724.png,alexander mcqueen\\n', '1700.png,carolina herrera\\n', '13247.png,alexander mcqueen\\n', '5566.png,missoni\\n', '10996.png,missoni\\n', '10982.png,derek lam\\n', '11444.png,dries van noten\\n', '3103.png,derek lam\\n', '8056.png,bottega veneta\\n', '280.png,carolina herrera\\n', '13253.png,marc jacobs\\n', '13535.png,yves saint laurent\\n', '3665.png,emporio armani\\n', '9406.png,missoni\\n', '2553.png,vera wang\\n', '2235.png,carolina herrera\\n', '10772.png,jason wu\\n', '12171.png,dries van noten\\n', '4888.png,miu miu\\n', '11478.png,derek lam\\n', '3659.png,emporio armani\\n', '4136.png,vera wang\\n', '9412.png,bottega veneta\\n', '12824.png,carolina herrera\\n', '10955.png,missoni\\n', '8081.png,marc by marc jacobs\\n', '13290.png,alexander mcqueen\\n', '6906.png,bottega veneta\\n', '1099.png,armani prive\\n', '12818.png,carolina herrera\\n', '5599.png,missoni\\n', '519.png,bottega veneta\\n', '3842.png,emporio armani\\n', '2584.png,armani prive\\n', '8240.png,carolina herrera\\n', '9638.png,derek lam\\n', '1264.png,miu miu\\n', '5002.png,dries van noten\\n', '2779.png,yves saint laurent\\n', '8532.png,missoni\\n', '14058.png,gucci\\n', '6279.png,marc jacobs\\n', '7167.png,missoni\\n', '8254.png,carolina herrera\\n', '3301.png,emporio armani\\n', '11646.png,jil sander\\n', '14716.png,jason wu\\n', '3329.png,chado ralph rucci\\n', '4446.png,marc jacobs\\n', '2037.png,fendi\\n', '10216.png,miu miu\\n', '2751.png,jason wu\\n', '12415.png,dries van noten\\n', '9610.png,derek lam\\n', '4452.png,derek lam\\n', '10564.png,bottega veneta\\n', '2023.png,derek lam\\n', '14702.png,jason wu\\n', '8268.png,marc by marc jacobs\\n', '13086.png,ralph lauren\\n', '2976.png,marni\\n', '727.png,giorgio armani\\n', '8297.png,miu miu\\n', '13092.png,alexander mcqueen\\n', '6292.png,miu miu\\n', '2792.png,marc jacobs\\n', '3498.png,vera wang\\n', '7826.png,jason wu\\n', '7198.png,miu miu\\n', '11861.png,missoni\\n', '5957.png,marni\\n', '13657.png,yves saint laurent\\n', '5176.png,alexander wang\\n', '3513.png,etro\\n', '11054.png,lanvin\\n', '690.png,giorgio armani\\n', '9758.png,vera wang\\n', '7013.png,jil sander\\n', '3275.png,etro\\n', '9016.png,miu miu\\n', '6325.png,giambattista valli\\n', '10404.png,etro\\n', '4254.png,derek lam\\n', '14104.png,chado ralph rucci\\n', '9770.png,derek lam\\n', '20.png,jil sander\\n', '860.png,giorgio armani\\n', '1338.png,marc by marc jacobs\\n', '874.png,giorgio armani\\n', '11068.png,jason wu\\n', '6331.png,alexander wang\\n', '11083.png,etro\\n', '2802.png,derek lam\\n', '8491.png,armani prive\\n', '9957.png,alexander wang\\n', '10389.png,chado ralph rucci\\n', '14689.png,carolina herrera\\n', '135.png,armani prive\\n', '2180.png,alexander wang\\n', '1489.png,tommy hilfiger\\n', '11242.png,chado ralph rucci\\n', '8650.png,jason wu\\n', '12993.png,carolina herrera\\n', '8888.png,chado ralph rucci\\n', '13455.png,yves saint laurent\\n', '1674.png,missoni\\n', '9228.png,etro\\n', '7205.png,etro\\n', '3063.png,jason wu\\n', '5412.png,missoni\\n', '5406.png,vera wang\\n', '2369.png,alexander wang\\n', '1660.png,marc by marc jacobs\\n', '13327.png,etro\\n', '14448.png,christian dior\\n', '12039.png,chado ralph rucci\\n', '8644.png,chado ralph rucci\\n', '6669.png,miu miu\\n', '13441.png,yves saint laurent\\n', '10148.png,dries van noten\\n', '3711.png,vera wang\\n', '6641.png,missoni\\n', '13469.png,yves saint laurent\\n', '4056.png,derek lam\\n', '3739.png,emporio armani\\n', '2341.png,alexander wang\\n', '10606.png,chado ralph rucci\\n', '1648.png,giambattista valli\\n', '6127.png,missoni\\n', '14460.png,versace\\n', '7239.png,bottega veneta\\n', '1890.png,marc by marc jacobs\\n', '12011.png,dries van noten\\n', '6133.png,marni\\n', '9200.png,jil sander\\n', '1884.png,emporio armani\\n', '2355.png,alexander wang\\n', '2433.png,versace\\n', '8678.png,chado ralph rucci\\n', '445.png,bottega veneta\\n', '13496.png,yves saint laurent\\n', '12788.png,carolina herrera\\n', '8693.png,missoni\\n', '11281.png,marni\\n', '4903.png,dries van noten\\n', '9599.png,derek lam\\n', '12944.png,carolina herrera\\n', '451.png,carolina herrera\\n', '4095.png,jason wu\\n', '8877.png,carolina herrera\\n', '1853.png,chado ralph rucci\\n', '1847.png,bottega veneta\\n', '6696.png,marc jacobs\\n', '3936.png,jason wu\\n', '4929.png,dries van noten\\n', '1689.png,missoni\\n', '3920.png,marc by marc jacobs\\n', '5389.png,jil sander\\n', '3934.png,emporio armani\\n', '321.png,bottega veneta\\n', '11283.png,marni\\n', '8691.png,alexander mcqueen\\n', '447.png,bottega veneta\\n', '13494.png,alexander wang\\n', '4901.png,chado ralph rucci\\n', '14489.png,marc by marc jacobs\\n', '1879.png,fendi\\n', '8108.png,jil sander\\n', '4054.png,missoni\\n', '12775.png,jason wu\\n', '1138.png,armani prive\\n', '9564.png,derek lam\\n', '4040.png,vera wang\\n', '10176.png,chado ralph rucci\\n', '2357.png,alexander wang\\n', '5438.png,missoni\\n', '4726.png,dolce & gabbana\\n', '1886.png,marc jacobs\\n', '13319.png,derek lam\\n', '5410.png,missoni\\n', '3061.png,jason wu\\n', '7207.png,gucci\\n', '13331.png,ralph lauren\\n', '1110.png,dries van noten\\n', '11240.png,missoni\\n', '11254.png,missoni\\n', '9558.png,derek lam\\n', '12985.png,carolina herrera\\n', '8646.png,chado ralph rucci\\n', '13325.png,missoni\\n', '3075.png,jason wu\\n', '6482.png,givenchy\\n', '679.png,chado ralph rucci\\n', '9969.png,derek lam\\n', '13872.png,chado ralph rucci\\n', '11917.png,emporio armani\\n', '2196.png,alexander wang\\n', '5821.png,jil sander\\n', '6496.png,miu miu\\n', '13696.png,carolina herrera\\n', '8493.png,givenchy\\n', '2800.png,dries van noten\\n', '14853.png,marc jacobs\\n', '9799.png,derek lam\\n', '651.png,giorgio armani\\n', '889.png,armani prive\\n', '9772.png,missoni\\n', '6441.png,giambattista valli\\n', '7987.png,marc jacobs\\n', '13669.png,yves saint laurent\\n', '11718.png,jil sander\\n', '6327.png,alexander wang\\n', '10374.png,vera wang\\n', '12563.png,derek lam\\n', '11042.png,missoni\\n', '8450.png,alexander wang\\n', '13655.png,yves saint laurent\\n', '13133.png,ralph lauren\\n', '5612.png,carolina herrera\\n', '5606.png,missoni\\n', '13127.png,tommy hilfiger\\n', '14648.png,dolce & gabbana\\n', '7011.png,jil sander\\n', '8322.png,carolina herrera\\n', '7777.png,chado ralph rucci\\n', '1306.png,marc by marc jacobs\\n', '11056.png,missoni\\n', '2948.png,bottega veneta\\n', '1299.png,yves saint laurent\\n', '6290.png,giambattista valli\\n', '5799.png,jil sander\\n', '5955.png,marni\\n', '13912.png,carolina herrera\\n', '9809.png,chado ralph rucci\\n', '11693.png,marc jacobs\\n', '8281.png,alexander wang\\n', '13090.png,dries van noten\\n', '11687.png,armani prive\\n', '2960.png,giambattista valli\\n', '7818.png,marc jacobs\\n', '9606.png,derek lam\\n', '10214.png,chado ralph rucci\\n', '10572.png,vera wang\\n', '5982.png,gucci\\n', '12365.png,alexander wang\\n', '14714.png,vera wang\\n', '9160.png,christian dior\\n', '6253.png,miu miu\\n', '12371.png,alexander wang\\n', '7159.png,marni\\n', '5996.png,carolina herrera\\n', '3459.png,etro\\n', '5028.png,jason wu\\n', '10200.png,armani prive\\n', '2747.png,giambattista valli\\n', '9612.png,derek lam\\n', '14066.png,jason wu\\n', '6521.png,marc jacobs\\n', '5000.png,dries van noten\\n', '6509.png,emporio armani\\n', '7617.png,lanvin\\n', '13047.png,carolina herrera\\n', '1500.png,missoni\\n', '2009.png,fendi\\n', '5766.png,jil sander\\n', '11650.png,vera wang\\n', '5772.png,jil sander\\n', '9148.png,alexander wang\\n', '1514.png,missoni\\n', '3465.png,dries van noten\\n', '5014.png,emporio armani\\n', '1925.png,vera wang\\n', '269.png,alexander wang\\n', '6092.png,missoni\\n', '3854.png,emporio armani\\n', '3698.png,emporio armani\\n', '2586.png,ralph lauren\\n', '6938.png,marc by marc jacobs\\n', '7398.png,ralph lauren\\n', '1931.png,chado ralph rucci\\n', '4849.png,dries van noten\\n', '1919.png,derek lam\\n', '12198.png,missoni\\n', '4861.png,chado ralph rucci\\n', '12826.png,giorgio armani\\n', '533.png,bottega veneta\\n', '527.png,bottega veneta\\n', '6904.png,jil sander\\n', '11485.png,chado ralph rucci\\n', '9389.png,derek lam\\n', '3129.png,jason wu\\n', '4646.png,yves saint laurent\\n', '11308.png,miu miu\\n', '12601.png,emporio armani\\n', '7429.png,miu miu\\n', '14270.png,miu miu\\n', '1058.png,yves saint laurent\\n', '8068.png,jil sander\\n', '3115.png,marc jacobs\\n', '8040.png,miu miu\\n', '8726.png,emporio armani\\n', '7415.png,dolce & gabbana\\n', '9438.png,dolce & gabbana\\n', '13523.png,yves saint laurent\\n', '3673.png,derek lam\\n', '2579.png,dolce & gabbana\\n', '12629.png,giorgio armani\\n', '7401.png,dolce & gabbana\\n', '14258.png,missoni\\n', '1716.png,marc by marc jacobs\\n', '8054.png,bottega veneta\\n', '13536.png,yves saint laurent\\n', '1071.png,armani prive\\n', '7400.png,emporio armani\\n', '8733.png,marni\\n', '3666.png,emporio armani\\n', '11321.png,vera wang\\n', '5571.png,chado ralph rucci\\n', '3100.png,bottega veneta\\n', '13250.png,derek lam\\n', '1717.png,missoni\\n', '8041.png,jason wu\\n', '1703.png,alexander wang\\n', '5565.png,bottega veneta\\n', '1065.png,armani prive\\n', '7414.png,fendi\\n', '4135.png,chado ralph rucci\\n', '6722.png,alexander mcqueen\\n', '8069.png,miu miu\\n', '14503.png,jason wu\\n', '9377.png,jil sander\\n', '6044.png,armani prive\\n', '2222.png,dolce & gabbana\\n', '3128.png,miu miu\\n', '12166.png,miu miu\\n', '6050.png,emporio armani\\n', '13278.png,bottega veneta\\n', '1059.png,alexander wang\\n', '14271.png,vera wang\\n', '6736.png,miu miu\\n', '3896.png,missoni\\n', '2550.png,etro\\n', '12833.png,carolina herrera\\n', '8096.png,bottega veneta\\n', '11484.png,missoni\\n', '4860.png,vera wang\\n', '254.png,dries van noten\\n', '8082.png,miu miu\\n', '3869.png,chado ralph rucci\\n', '6939.png,jil sander\\n', '4690.png,jason wu\\n', '6087.png,missoni\\n', '6093.png,givenchy\\n', '2593.png,chado ralph rucci\\n', '11645.png,vera wang\\n', '5773.png,jil sander\\n', '5015.png,chado ralph rucci\\n', '11123.png,missoni\\n', '3464.png,marc jacobs\\n', '7602.png,ralph lauren\\n', '1267.png,marc by marc jacobs\\n', '10229.png,etro\\n', '4479.png,gucci\\n', '11889.png,derek lam\\n', '2008.png,dries van noten\\n', '1501.png,missoni\\n', '8243.png,alexander wang\\n', '11679.png,dries van noten\\n', '2020.png,fendi\\n', '14701.png,dolce & gabbana\\n', '7158.png,missoni\\n', '12370.png,marni\\n', '14067.png,marni\\n', '13708.png,jil sander\\n', '2752.png,bottega veneta\\n', '10215.png,chado ralph rucci\\n', '12402.png,miu miu\\n', '8519.png,miu miu\\n', '6534.png,carolina herrera\\n', '9607.png,derek lam\\n', '14073.png,gucci\\n', '9161.png,missoni\\n', '11686.png,dolce & gabbana\\n', '13091.png,givenchy\\n', '11862.png,marni\\n', '5954.png,alexander wang\\n', '2785.png,alexander wang\\n', '7825.png,chado ralph rucci\\n', '718.png,chado ralph rucci\\n', '9808.png,givenchy\\n', '7831.png,missoni\\n', '1298.png,miu miu\\n', '2791.png,marc by marc jacobs\\n', '5798.png,jil sander\\n', '11876.png,miu miu\\n', '5940.png,bottega veneta\\n', '4486.png,carolina herrera\\n', '8323.png,miu miu\\n', '7010.png,jil sander\\n', '14649.png,jason wu\\n', '13126.png,ralph lauren\\n', '3276.png,etro\\n', '3510.png,etro\\n', '10349.png,chado ralph rucci\\n', '693.png,giorgio armani\\n', '6468.png,tommy hilfiger\\n', '8445.png,marni\\n', '13898.png,jil sander\\n', '7776.png,marc jacobs\\n', '687.png,giorgio armani\\n', '7762.png,marc jacobs\\n', '11043.png,marni\\n', '3504.png,etro\\n', '5613.png,missoni\\n', '8337.png,armani prive\\n', '7004.png,jil sander\\n', '1475.png,jil sander\\n', '13132.png,lanvin\\n', '2154.png,alexander wang\\n', '12204.png,ralph lauren\\n', '9001.png,christian dior\\n', '7992.png,jil sander\\n', '14113.png,dries van noten\\n', '877.png,giorgio armani\\n', '10375.png,chado ralph rucci\\n', '2626.png,derek lam\\n', '13668.png,yves saint laurent\\n', '23.png,marc by marc jacobs\\n', '14107.png,giambattista valli\\n', '863.png,bottega veneta\\n', '14661.png,dolce & gabbana\\n', '9015.png,carolina herrera\\n', '10407.png,vera wang\\n', '650.png,giorgio armani\\n', '11094.png,alexander wang\\n', '2815.png,bottega veneta\\n', '2801.png,armani prive\\n', '12589.png,miu miu\\n', '644.png,giorgio armani\\n', '7979.png,miu miu\\n', '5808.png,marc by marc jacobs\\n', '7789.png,versace\\n', '13873.png,armani prive\\n', '7574.png,alexander wang\\n', '8647.png,etro\\n', '12984.png,carolina herrera\\n', '491.png,bottega veneta\\n', '5363.png,missoni\\n', '11255.png,alexander wang\\n', '3074.png,chado ralph rucci\\n', '5405.png,missoni\\n', '13324.png,ralph lauren\\n', '1663.png,miu miu\\n', '8121.png,miu miu\\n', '13330.png,chado ralph rucci\\n', '5411.png,dries van noten\\n', '5377.png,missoni\\n', '3706.png,nina ricci\\n', '11241.png,jil sander\\n', '8653.png,alexander mcqueen\\n', '485.png,chado ralph rucci\\n', '10177.png,chado ralph rucci\\n', '4041.png,giambattista valli\\n', '11269.png,missoni\\n', '1139.png,armani prive\\n', '13318.png,jason wu\\n', '14477.png,givenchy\\n', '1887.png,giambattista valli\\n', '12006.png,chado ralph rucci\\n', '3048.png,chado ralph rucci\\n', '4733.png,carolina herrera\\n', '10605.png,emporio armani\\n', '2342.png,chado ralph rucci\\n', '9217.png,carolina herrera\\n', '8109.png,miu miu\\n', '12012.png,vera wang\\n', '12774.png,dries van noten\\n', '10163.png,alexander mcqueen\\n', '12947.png,dries van noten\\n', '13481.png,etro\\n', '334.png,gucci\\n', '10822.png,chado ralph rucci\\n', '13495.png,yves saint laurent\\n', '2381.png,jil sander\\n', '3921.png,etro\\n', '6681.png,christian dior\\n', '3919.png,giambattista valli\\n', '8694.png,derek lam\\n', '12957.png,carolina herrera\\n', '13491.png,yves saint laurent\\n', '324.png,bottega veneta\\n', '4910.png,dries van noten\\n', '4904.png,dries van noten\\n', '8680.png,alexander mcqueen\\n', '12943.png,carolina herrera\\n', '456.png,bottega veneta\\n', '8858.png,jason wu\\n', '13485.png,chado ralph rucci\\n', '11292.png,alexander wang\\n', '4092.png,jil sander\\n', '1840.png,missoni\\n', '2391.png,versace\\n', '4938.png,dries van noten\\n', '6849.png,jil sander\\n', '8864.png,alexander mcqueen\\n', '12994.png,marc by marc jacobs\\n', '1115.png,armani prive\\n', '13452.png,yves saint laurent\\n', '11245.png,missoni\\n', '5415.png,miu miu\\n', '3064.png,jason wu\\n', '8125.png,miu miu\\n', '7216.png,jil sander\\n', '13320.png,jason wu\\n', '5401.png,missoni\\n', '10629.png,vera wang\\n', '11537.png,missoni\\n', '4079.png,miu miu\\n', '11251.png,missoni\\n', '3716.png,emporio armani\\n', '2408.png,etro\\n', '1101.png,armani prive\\n', '8643.png,chado ralph rucci\\n', '10167.png,derek lam\\n', '9575.png,derek lam\\n', '13308.png,ralph lauren\\n', '5429.png,missoni\\n', '4737.png,carolina herrera\\n', '6134.png,carolina herrera\\n', '14473.png,jil sander\\n', '6652.png,carolina herrera\\n', '14315.png,givenchy\\n', '9561.png,derek lam\\n', '4045.png,alexander wang\\n', '2434.png,yves saint laurent\\n', '10173.png,chado ralph rucci\\n', '126.png,marc jacobs\\n', '640.png,giorgio armani\\n', '9788.png,derek lam\\n', '898.png,alexander wang\\n', '8496.png,carolina herrera\\n', '2811.png,bottega veneta\\n', '654.png,alexander wang\\n', '14856.png,vera wang\\n', '5818.png,marc by marc jacobs\\n', '5830.png,marc by marc jacobs\\n', '3299.png,etro\\n', '6487.png,bottega veneta\\n', '13877.png,alexander wang\\n', '668.png,giorgio armani\\n', '2193.png,alexander wang\\n', '11912.png,carolina herrera\\n', '5824.png,jil sander\\n', '14659.png,vera wang\\n', '7000.png,jil sander\\n', '8333.png,jason wu\\n', '10359.png,chado ralph rucci\\n', '3500.png,etro\\n', '13650.png,yves saint laurent\\n', '683.png,yves saint laurent\\n', '9987.png,miu miu\\n', '13644.png,yves saint laurent\\n', '1303.png,marc by marc jacobs\\n', '697.png,giorgio armani\\n', '5165.png,missoni\\n', '3514.png,jil sander\\n', '3272.png,etro\\n', '11735.png,dries van noten\\n', '10403.png,vera wang\\n', '2144.png,alexander wang\\n', '8469.png,chado ralph rucci\\n', '10365.png,armani prive\\n', '5159.png,armani prive\\n', '2636.png,yves saint laurent\\n', '4247.png,etro\\n', '7996.png,vera wang\\n', '13678.png,yves saint laurent\\n', '33.png,carolina herrera\\n', '1459.png,marc by marc jacobs\\n', '12200.png,miu miu\\n', '7028.png,jil sander\\n', '11709.png,carolina herrera\\n', '5978.png,emporio armani\\n', '10588.png,missoni\\n', '8284.png,missoni\\n', '14088.png,gucci\\n', '7809.png,jil sander\\n', '734.png,missoni\\n', '2971.png,giambattista valli\\n', '13095.png,etro\\n', '8290.png,carolina herrera\\n', '5944.png,jil sander\\n', '4482.png,alexander wang\\n', '11872.png,marc jacobs\\n', '7821.png,carolina herrera\\n', '2781.png,versace\\n', '5950.png,jil sander\\n', '5788.png,gucci\\n', '6281.png,alexander wang\\n', '7174.png,marni\\n', '8247.png,miu miu\\n', '13042.png,marc by marc jacobs\\n', '1505.png,tommy hilfiger\\n', '5763.png,marc by marc jacobs\\n', '5005.png,jil sander\\n', '6518.png,tommy hilfiger\\n', '13730.png,bottega veneta\\n', '1277.png,missoni\\n', '3460.png,emporio armani\\n', '5777.png,jil sander\\n', '2018.png,bottega veneta\\n', '11641.png,armani prive\\n', '1511.png,jason wu\\n', '11669.png,yves saint laurent\\n', '14711.png,dolce & gabbana\\n', '13718.png,carolina herrera\\n', '2756.png,derek lam\\n', '10205.png,chado ralph rucci\\n', '9617.png,giorgio armani\\n', '12412.png,alexander wang\\n', '8509.png,jason wu\\n', '907.png,missoni\\n', '9171.png,missoni\\n', '6242.png,lanvin\\n', '10563.png,emporio armani\\n', '2024.png,bottega veneta\\n', '5993.png,carolina herrera\\n', '8938.png,alexander wang\\n', '12823.png,carolina herrera\\n', '8086.png,alexander wang\\n', '4864.png,miu miu\\n', '12189.png,vera wang\\n', '8092.png,miu miu\\n', '1908.png,fendi\\n', '13297.png,marc by marc jacobs\\n', '522.png,marc jacobs\\n', '8904.png,marc jacobs\\n', '3689.png,emporio armani\\n', '3851.png,emporio armani\\n', '1920.png,chado ralph rucci\\n', '6083.png,alexander wang\\n', '278.png,yves saint laurent\\n', '3845.png,emporio armani\\n', '2583.png,missoni\\n', '8910.png,christian dior\\n', '8723.png,alexander mcqueen\\n', '7410.png,carolina herrera\\n', '12638.png,yves saint laurent\\n', '14249.png,gucci\\n', '3676.png,giorgio armani\\n', '4119.png,armani prive\\n', '11457.png,giambattista valli\\n', '10749.png,jason wu\\n', '5561.png,missoni\\n', '6068.png,marc jacobs\\n', '287.png,givenchy\\n', '1713.png,givenchy\\n', '11325.png,emporio armani\\n', '3662.png,emporio armani\\n', '8737.png,vera wang\\n', '7404.png,christian dior\\n', '2554.png,marc jacobs\\n', '4125.png,alexander wang\\n', '6732.png,dolce & gabbana\\n', '14275.png,bottega veneta\\n', '9401.png,missoni\\n', '12162.png,emporio armani\\n', '8079.png,carolina herrera\\n', '2232.png,alexander wang\\n', '3138.png,jil sander\\n', '5549.png,missoni\\n', '12610.png,marc jacobs\\n', '1049.png,armani prive\\n', '6726.png,carolina herrera\\n', '14261.png,carolina herrera\\n', '3886.png,yves saint laurent\\n', '14506.png,marc by marc jacobs\\n', '2227.png,marc by marc jacobs\\n', '11318.png,chado ralph rucci\\n', '9414.png,jil sander\\n', '6733.png,tommy hilfiger\\n', '4124.png,derek lam\\n', '3893.png,versace\\n', '9366.png,missoni\\n', '11442.png,versace\\n', '7363.png,dolce & gabbana\\n', '8050.png,miu miu\\n', '9428.png,etro\\n', '7405.png,chado ralph rucci\\n', '8736.png,carolina herrera\\n', '3663.png,emporio armani\\n', '3677.png,emporio armani\\n', '11330.png,marc jacobs\\n', '14248.png,armani prive\\n', '13527.png,yves saint laurent\\n', '8044.png,carolina herrera\\n', '5560.png,missoni\\n', '3111.png,jason wu\\n', '10990.png,chado ralph rucci\\n', '279.png,bottega veneta\\n', '6082.png,alexander wang\\n', '3850.png,marc jacobs\\n', '6096.png,dries van noten\\n', '13296.png,jil sander\\n', '1909.png,vera wang\\n', '11481.png,giorgio armani\\n', '4871.png,dries van noten\\n', '12822.png,derek lam\\n', '4865.png,missoni\\n', '906.png,armani prive\\n', '9616.png,derek lam\\n', '10204.png,chado ralph rucci\\n', '2025.png,fendi\\n', '9170.png,missoni\\n', '6257.png,miu miu\\n', '9164.png,tommy hilfiger\\n', '14710.png,dolce & gabbana\\n', '1538.png,tommy hilfiger\\n', '7149.png,carolina herrera\\n', '4440.png,marni\\n', '10576.png,missoni\\n', '2757.png,vera wang\\n', '10210.png,chado ralph rucci\\n', '912.png,armani prive\\n', '5010.png,dries van noten\\n', '10238.png,chado ralph rucci\\n', '11126.png,armani prive\\n', '3461.png,etro\\n', '7607.png,armani prive\\n', '1276.png,marc by marc jacobs\\n', '13057.png,carolina herrera\\n', '4468.png,armani prive\\n', '11898.png,emporio armani\\n', '5776.png,jil sander\\n', '3313.png,emporio armani\\n', '5762.png,jil sander\\n', '7175.png,jil sander\\n', '7613.png,alexander wang\\n', '5004.png,jason wu\\n', '11132.png,tommy hilfiger\\n', '6280.png,vera wang\\n', '11873.png,yves saint laurent\\n', '5945.png,jil sander\\n', '709.png,giorgio armani\\n', '13902.png,marni\\n', '2794.png,jason wu\\n', '721.png,alexander wang\\n', '11683.png,emporio armani\\n', '8291.png,marni\\n', '10589.png,chado ralph rucci\\n', '11697.png,yves saint laurent\\n', '735.png,giorgio armani\\n', '7808.png,marc jacobs\\n', '7997.png,bottega veneta\\n', '4246.png,dries van noten\\n', '3529.png,jil sander\\n', '5158.png,dolce & gabbana\\n', '9004.png,alexander wang\\n', '12215.png,chado ralph rucci\\n', '9010.png,christian dior\\n', '4534.png,miu miu\\n', '10364.png,chado ralph rucci\\n', '3515.png,jil sander\\n', '696.png,giorgio armani\\n', '1302.png,jil sander\\n', '13645.png,giambattista valli\\n', '9986.png,versace\\n', '8326.png,alexander wang\\n', '1464.png,marc by marc jacobs\\n', '5602.png,miu miu\\n', '3273.png,etro\\n', '5616.png,missoni\\n', '2179.png,ralph lauren\\n', '3267.png,jil sander\\n', '1470.png,chado ralph rucci\\n', '682.png,giorgio armani\\n', '6492.png,bottega veneta\\n', '11907.png,giorgio armani\\n', '2186.png,alexander wang\\n', '7798.png,etro\\n', '7940.png,marc jacobs\\n', '2838.png,alexander wang\\n', '655.png,giorgio armani\\n', '2810.png,armani prive\\n', '11091.png,missoni\\n', '5819.png,miu miu\\n', '2804.png,chado ralph rucci\\n', '6135.png,alexander wang\\n', '8118.png,chado ralph rucci\\n', '4722.png,jil sander\\n', '10614.png,emporio armani\\n', '10172.png,chado ralph rucci\\n', '14300.png,miu miu\\n', '10166.png,chado ralph rucci\\n', '4050.png,yves saint laurent\\n', '3059.png,jason wu\\n', '10600.png,vera wang\\n', '5428.png,carolina herrera\\n', '14466.png,miu miu\\n', '6121.png,carolina herrera\\n', '13309.png,ralph lauren\\n', '10628.png,vera wang\\n', '1666.png,tommy hilfiger\\n', '7217.png,marc by marc jacobs\\n', '8124.png,ralph lauren\\n', '7571.png,missoni\\n', '12981.png,dries van noten\\n', '1114.png,armani prive\\n', '12995.png,missoni\\n', '13335.png,marni\\n', '8130.png,miu miu\\n', '3065.png,jil sander\\n', '5414.png,missoni\\n', '4939.png,dries van noten\\n', '2390.png,yves saint laurent\\n', '6690.png,carolina herrera\\n', '6848.png,chado ralph rucci\\n', '1855.png,chado ralph rucci\\n', '2384.png,emporio armani\\n', '13484.png,yves saint laurent\\n', '8859.png,jason wu\\n', '12942.png,carolina herrera\\n', '8681.png,jason wu\\n', '13490.png,yves saint laurent\\n', '443.png,bottega veneta\\n', '8695.png,carolina herrera\\n', '325.png,missoni\\n', '8683.png,miu miu\\n', '455.png,bottega veneta\\n', '10831.png,vera wang\\n', '333.png,bottega veneta\\n', '327.png,bottega veneta\\n', '4913.png,dries van noten\\n', '8697.png,jason wu\\n', '12954.png,carolina herrera\\n', '9589.png,dolce & gabbana\\n', '13492.png,vera wang\\n', '441.png,bottega veneta\\n', '4085.png,jil sander\\n', '1843.png,marc jacobs\\n', '2392.png,dries van noten\\n', '2386.png,dries van noten\\n', '6686.png,bottega veneta\\n', '8873.png,miu miu\\n', '5364.png,missoni\\n', '12983.png,carolina herrera\\n', '13445.png,chado ralph rucci\\n', '496.png,gucci\\n', '8898.png,jason wu\\n', '8126.png,miu miu\\n', '13323.png,marc by marc jacobs\\n', '11534.png,missoni\\n', '11520.png,marc jacobs\\n', '8132.png,marc by marc jacobs\\n', '12029.png,emporio armani\\n', '13337.png,ralph lauren\\n', '14458.png,alexander wang\\n', '12997.png,carolina herrera\\n', '482.png,bottega veneta\\n', '3701.png,giorgio armani\\n', '11246.png,carolina herrera\\n', '10158.png,chado ralph rucci\\n', '9562.png,derek lam\\n', '4046.png,jil sander\\n', '10170.png,chado ralph rucci\\n', '10616.png,vera wang\\n', '2351.png,yves saint laurent\\n', '4720.png,chado ralph rucci\\n', '1880.png,bottega veneta\\n', '7229.png,marc jacobs\\n', '1658.png,missoni\\n', '14470.png,giorgio armani\\n', '1894.png,fendi\\n', '10602.png,yves saint laurent\\n', '4734.png,carolina herrera\\n', '10164.png,chado ralph rucci\\n', '9576.png,derek lam\\n', '8668.png,emporio armani\\n', '11093.png,missoni\\n', '8481.png,miu miu\\n', '643.png,alexander wang\\n', '8495.png,armani prive\\n', '11087.png,missoni\\n', '2806.png,miu miu\\n', '11939.png,missoni\\n', '125.png,dries van noten\\n', '2190.png,emporio armani\\n', '1499.png,miu miu\\n', '7956.png,marc jacobs\\n', '13860.png,bottega veneta\\n', '4287.png,emporio armani\\n', '13874.png,marc jacobs\\n', '5833.png,etro\\n', '1466.png,jil sander\\n', '13121.png,dolce & gabbana\\n', '7017.png,marc by marc jacobs\\n', '9984.png,jil sander\\n', '1300.png,chado ralph rucci\\n', '14128.png,gucci\\n', '11050.png,armani prive\\n', '5172.png,carolina herrera\\n', '3503.png,chado ralph rucci\\n', '858.png,giorgio armani\\n', '8456.png,miu miu\\n', '9990.png,alexander wang\\n', '18.png,missoni\\n', '1472.png,miu miu\\n', '8330.png,chado ralph rucci\\n', '6335.png,christian dior\\n', '8318.png,miu miu\\n', '4522.png,armani prive\\n', '2153.png,alexander wang\\n', '10414.png,vera wang\\n', '870.png,chado ralph rucci\\n', '12565.png,jason wu\\n', '14114.png,etro\\n', '9760.png,etro\\n', '7759.png,versace\\n', '864.png,alexander wang\\n', '9774.png,miu miu\\n', '2621.png,miu miu\\n', '10366.png,chado ralph rucci\\n', '4536.png,alexander mcqueen\\n', '3259.png,etro\\n', '13109.png,dries van noten\\n', '12217.png,giorgio armani\\n', '8293.png,tommy hilfiger\\n', '13928.png,miu miu\\n', '723.png,giorgio armani\\n', '737.png,giorgio armani\\n', '13082.png,carolina herrera\\n', '9199.png,missoni\\n', '8287.png,miu miu\\n', '4495.png,alexander wang\\n', '2782.png,alexander wang\\n', '13900.png,carolina herrera\\n', '7836.png,etro\\n', '6296.png,missoni\\n', '5947.png,jil sander\\n', '5774.png,jil sander\\n', '11642.png,etro\\n', '3305.png,marc jacobs\\n', '8250.png,yves saint laurent\\n', '1512.png,marc jacobs\\n', '13055.png,carolina herrera\\n', '9628.png,derek lam\\n', '13733.png,marc by marc jacobs\\n', '8536.png,givenchy\\n', '7605.png,alexander wang\\n', '11124.png,marni\\n', '4318.png,alexander mcqueen\\n', '14048.png,bottega veneta\\n', '12439.png,alexander wang\\n', '5760.png,jil sander\\n', '9172.png,bottega veneta\\n', '5748.png,jil sander\\n', '10560.png,vera wang\\n', '2999.png,bottega veneta\\n', '11118.png,lanvin\\n', '6527.png,missoni\\n', '14060.png,gucci\\n', '904.png,jason wu\\n', '6533.png,vera wang\\n', '14074.png,carolina herrera\\n', '9600.png,derek lam\\n', '910.png,marc by marc jacobs\\n', '5984.png,missoni\\n', '12363.png,missoni\\n', '4873.png,emporio armani\\n', '13294.png,carolina herrera\\n', '8085.png,miu miu\\n', '253.png,miu miu\\n', '10789.png,emporio armani\\n', '12820.png,carolina herrera\\n', '1089.png,armani prive\\n', '8913.png,marc by marc jacobs\\n', '12808.png,missoni\\n', '1937.png,vera wang\\n', '5589.png,marc by marc jacobs\\n', '10979.png,yves saint laurent\\n', '3852.png,jason wu\\n', '10038.png,marc by marc jacobs\\n', '11326.png,marc jacobs\\n', '8734.png,versace\\n', '1076.png,armani prive\\n', '13257.png,derek lam\\n', '284.png,marni\\n', '7361.png,bottega veneta\\n', '2219.png,carolina herrera\\n', '5562.png,missoni\\n', '13243.png,ralph lauren\\n', '8046.png,miu miu\\n', '7413.png,bottega veneta\\n', '1062.png,armani prive\\n', '3675.png,emporio armani\\n', '11332.png,dries van noten\\n', '14262.png,armani prive\\n', '4654.png,yves saint laurent\\n', '2225.png,alexander wang\\n', '6043.png,missoni\\n', '12175.png,carolina herrera\\n', '6057.png,marc by marc jacobs\\n', '4640.png,bottega veneta\\n', '3891.png,etro\\n', '2557.png,chado ralph rucci\\n', '13519.png,yves saint laurent\\n', '10777.png,miu miu\\n', '4899.png,missoni\\n', '7348.png,alexander wang\\n', '1739.png,missoni\\n', '9365.png,dolce & gabbana\\n', '3890.png,armani prive\\n', '4133.png,chado ralph rucci\\n', '3884.png,jil sander\\n', '12612.png,chado ralph rucci\\n', '6042.png,dries van noten\\n', '2224.png,gucci\\n', '7374.png,emporio armani\\n', '9359.png,missoni\\n', '5563.png,vera wang\\n', '11333.png,dries van noten\\n', '3674.png,bottega veneta\\n', '13524.png,yves saint laurent\\n', '6718.png,carolina herrera\\n', '3660.png,vera wang\\n', '5577.png,missoni\\n', '10987.png,miu miu\\n', '8053.png,bottega veneta\\n', '7360.png,marc by marc jacobs\\n', '13256.png,bottega veneta\\n', '14539.png,jil sander\\n', '4682.png,yves saint laurent\\n', '2595.png,missoni\\n', '4696.png,carolina herrera\\n', '10788.png,vera wang\\n', '252.png,givenchy\\n', '6917.png,jil sander\\n', '12821.png,carolina herrera\\n', '534.png,jil sander\\n', '520.png,bottega veneta\\n', '246.png,givenchy\\n', '13295.png,ralph lauren\\n', '10944.png,carolina herrera\\n', '4872.png,dries van noten\\n', '2754.png,marc jacobs\\n', '4325.png,dolce & gabbana\\n', '911.png,armani prive\\n', '9601.png,miu miu\\n', '5985.png,giorgio armani\\n', '2032.png,derek lam\\n', '4457.png,missoni\\n', '2026.png,derek lam\\n', '5749.png,jil sander\\n', '12410.png,miu miu\\n', '14061.png,gucci\\n', '9615.png,derek lam\\n', '10207.png,dolce & gabbana\\n', '3476.png,jil sander\\n', '11131.png,armani prive\\n', '11657.png,carolina herrera\\n', '10549.png,emporio armani\\n', '5761.png,jil sander\\n', '1507.png,miu miu\\n', '7176.png,marc jacobs\\n', '1513.png,missoni\\n', '11643.png,etro\\n', '5775.png,jil sander\\n', '939.png,armani prive\\n', '8537.png,bottega veneta\\n', '1275.png,marc by marc jacobs\\n', '3489.png,jason wu\\n', '2797.png,miu miu\\n', '11870.png,missoni\\n', '6283.png,etro\\n', '4494.png,yves saint laurent\\n', '5952.png,jil sander\\n', '2783.png,miu miu\\n', '7823.png,carolina herrera\\n', '13915.png,derek lam\\n', '8286.png,miu miu\\n', '13083.png,marc jacobs\\n', '11858.png,giorgio armani\\n', '11680.png,bottega veneta\\n', '12389.png,miu miu\\n', '2967.png,giambattista valli\\n', '10367.png,chado ralph rucci\\n', '9775.png,chado ralph rucci\\n', '25.png,marc jacobs\\n', '6446.png,jason wu\\n', '7758.png,vera wang\\n', '5629.png,missoni\\n', '10401.png,vera wang\\n', '3258.png,alexander wang\\n', '8319.png,miu miu\\n', '12202.png,lanvin\\n', '9761.png,derek lam\\n', '9749.png,derek lam\\n', '13652.png,yves saint laurent\\n', '1315.png,marc by marc jacobs\\n', '681.png,giorgio armani\\n', '859.png,giorgio armani\\n', '5173.png,armani prive\\n', '8331.png,miu miu\\n', '1473.png,marc by marc jacobs\\n', '7016.png,bottega veneta\\n', '13120.png,ralph lauren\\n', '10429.png,yves saint laurent\\n', '3270.png,emporio armani\\n', '3516.png,jason wu\\n', '2608.png,etro\\n', '14129.png,dries van noten\\n', '695.png,giorgio armani\\n', '1301.png,jil sander\\n', '7770.png,marc jacobs\\n', '7943.png,dries van noten\\n', '5832.png,jil sander\\n', '11904.png,etro\\n', '1498.png,missoni\\n', '11910.png,dries van noten\\n', '2191.png,yves saint laurent\\n', '6491.png,vera wang\\n', '7957.png,jil sander\\n', '2807.png,marc by marc jacobs\\n', '11086.png,marc by marc jacobs\\n', '9952.png,miu miu\\n', '11938.png,vera wang\\n', '130.png,marni\\n', '13685.png,yves saint laurent\\n', '2813.png,carolina herrera\\n', '2344.png,alexander wang\\n', '10603.png,missoni\\n', '6122.png,miu miu\\n', '10165.png,marc jacobs\\n', '3728.png,emporio armani\\n', '4047.png,jil sander\\n', '9563.png,derek lam\\n', '6136.png,missoni\\n', '7228.png,vera wang\\n', '1881.png,chado ralph rucci\\n', '10617.png,vera wang\\n', '13336.png,ralph lauren\\n', '8133.png,miu miu\\n', '5417.png,missoni\\n', '5371.png,missoni\\n', '10159.png,chado ralph rucci\\n', '1117.png,armani prive\\n', '13450.png,yves saint laurent\\n', '8641.png,derek lam\\n', '7572.png,yves saint laurent\\n', '8899.png,dries van noten\\n', '13444.png,yves saint laurent\\n', '12982.png,giorgio armani\\n', '11253.png,marc jacobs\\n', '3714.png,emporio armani\\n', '1665.png,marc by marc jacobs\\n', '8127.png,alexander wang\\n', '7214.png,carolina herrera\\n', '12969.png,carolina herrera\\n', '6687.png,miu miu\\n', '4084.png,chado ralph rucci\\n', '4912.png,dries van noten\\n', '440.png,bottega veneta\\n', '9588.png,yves saint laurent\\n', '12955.png,carolina herrera\\n', '8696.png,dolce & gabbana\\n', '454.png,bottega veneta\\n', '332.png,bottega veneta\\n', '4906.png,jason wu\\n', '1825.png,derek lam\\n', '6186.png,alexander wang\\n', '4791.png,chado ralph rucci\\n', '4949.png,dries van noten\\n', '3798.png,missoni\\n', '6838.png,carolina herrera\\n', '8815.png,carolina herrera\\n', '433.png,chado ralph rucci\\n', '12926.png,jason wu\\n', '355.png,bottega veneta\\n', '12098.png,etro\\n', '10857.png,missoni\\n', '10843.png,chado ralph rucci\\n', '9289.png,giorgio armani\\n', '4020.png,chado ralph rucci\\n', '3997.png,yves saint laurent\\n', '2451.png,vera wang\\n', '10116.png,chado ralph rucci\\n', '12701.png,emporio armani\\n', '12067.png,marni\\n', '6151.png,carolina herrera\\n', '13379.png,alexander wang\\n', '10670.png,givenchy\\n', '2323.png,missoni\\n', '6145.png,yves saint laurent\\n', '9510.png,chado ralph rucci\\n', '4034.png,chado ralph rucci\\n', '3983.png,versace\\n', '13423.png,yves saint laurent\\n', '9538.png,derek lam\\n', '8626.png,jason wu\\n', '11234.png,missoni\\n', '7267.png,miu miu\\n', '6179.png,carolina herrera\\n', '5470.png,yves saint laurent\\n', '11546.png,marc jacobs\\n', '3001.png,chado ralph rucci\\n', '11220.png,chado ralph rucci\\n', '2479.png,miu miu\\n', '13437.png,yves saint laurent\\n', '6390.png,etro\\n', '11963.png,nina ricci\\n', '4593.png,marc by marc jacobs\\n', '8381.png,jil sander\\n', '13184.png,yves saint laurent\\n', '157.png,dolce & gabbana\\n', '631.png,marc by marc jacobs\\n', '7918.png,jil sander\\n', '8395.png,alexander wang\\n', '11787.png,derek lam\\n', '5869.png,jil sander\\n', '4.png,ralph lauren\\n', '10472.png,vera wang\\n', '2135.png,dries van noten\\n', '14614.png,dolce & gabbana\\n', '9706.png,derek lam\\n', '14172.png,gucci\\n', '42.png,jil sander\\n', '13609.png,yves saint laurent\\n', '6347.png,alexander wang\\n', '5896.png,bottega veneta\\n', '11778.png,yves saint laurent\\n', '2121.png,alexander wang\\n', '5100.png,missoni\\n', '3571.png,emporio armani\\n', '1366.png,marc by marc jacobs\\n', '13635.png,gucci\\n', '1372.png,carolina herrera\\n', '5672.png,jil sander\\n', '7065.png,marc by marc jacobs\\n', '5935.png,jil sander\\n', '2082.png,vera wang\\n', '9869.png,dolce & gabbana\\n', '779.png,etro\\n', '7850.png,marc jacobs\\n', '6596.png,carolina herrera\\n', '2096.png,yves saint laurent\\n', '11817.png,marc by marc jacobs\\n', '5909.png,missoni\\n', '13796.png,carolina herrera\\n', '2914.png,alexander wang\\n', '13782.png,vera wang\\n', '751.png,giorgio armani\\n', '10506.png,missoni\\n', '6227.png,chado ralph rucci\\n', '13769.png,marc jacobs\\n', '14006.png,vera wang\\n', '3439.png,bottega veneta\\n', '2727.png,tommy hilfiger\\n', '10274.png,chado ralph rucci\\n', '9666.png,derek lam\\n', '14012.png,yves saint laurent\\n', '6555.png,dries van noten\\n', '976.png,armani prive\\n', '9100.png,gucci\\n', '14774.png,carolina herrera\\n', '10512.png,vera wang\\n', '7105.png,jil sander\\n', '1574.png,chado ralph rucci\\n', '5712.png,jil sander\\n', '3363.png,missoni\\n', '11142.png,missoni\\n', '5074.png,marni\\n', '786.png,dolce & gabbana\\n', '13741.png,giorgio armani\\n', '792.png,giorgio armani\\n', '6569.png,marni\\n', '8544.png,miu miu\\n', '11156.png,miu miu\\n', '2069.png,vera wang\\n', '5706.png,jil sander\\n', '7111.png,jason wu\\n', '14748.png,dolce & gabbana\\n', '6780.png,chado ralph rucci\\n', '4197.png,carolina herrera\\n', '2280.png,alexander wang\\n', '1951.png,nina ricci\\n', '1789.png,derek lam\\n', '1945.png,vera wang\\n', '209.png,fendi\\n', '4183.png,chado ralph rucci\\n', '3834.png,emporio armani\\n', '547.png,bottega veneta\\n', '8949.png,missoni\\n', '6964.png,jil sander\\n', '8791.png,bottega veneta\\n', '1979.png,fendi\\n', '14589.png,carolina herrera\\n', '553.png,marc jacobs\\n', '13580.png,jil sander\\n', '10089.png,givenchy\\n', '12675.png,chado ralph rucci\\n', '14204.png,gucci\\n', '9470.png,chado ralph rucci\\n', '6025.png,givenchy\\n', '8008.png,jil sander\\n', '3149.png,jason wu\\n', '10710.png,missoni\\n', '6031.png,alexander wang\\n', '8752.png,derek lam\\n', '7461.png,missoni\\n', '14238.png,chado ralph rucci\\n', '584.png,bottega veneta\\n', '13557.png,missoni\\n', '2519.png,derek lam\\n', '4168.png,yves saint laurent\\n', '3161.png,jason wu\\n', '5510.png,vera wang\\n', '13231.png,ralph lauren\\n', '5504.png,marni\\n', '11354.png,marc by marc jacobs\\n', '3613.png,emporio armani\\n', '8746.png,alexander mcqueen\\n', '7475.png,missoni\\n', '9458.png,miu miu\\n', '590.png,bottega veneta\\n', '13543.png,yves saint laurent\\n', '5505.png,carolina herrera\\n', '3174.png,jason wu\\n', '7312.png,emporio armani\\n', '13224.png,ralph lauren\\n', '591.png,bottega veneta\\n', '1005.png,armani prive\\n', '12884.png,carolina herrera\\n', '11355.png,vera wang\\n', '3606.png,marc jacobs\\n', '2518.png,alexander wang\\n', '13556.png,yves saint laurent\\n', '585.png,jil sander\\n', '14239.png,jason wu\\n', '8753.png,chado ralph rucci\\n', '7306.png,bottega veneta\\n', '13230.png,giorgio armani\\n', '3160.png,derek lam\\n', '11427.png,chado ralph rucci\\n', '1987.png,marc jacobs\\n', '9303.png,giorgio armani\\n', '6030.png,carolina herrera\\n', '3148.png,miu miu\\n', '11369.png,yves saint laurent\\n', '4141.png,miu miu\\n', '10077.png,yves saint laurent\\n', '2530.png,jason wu\\n', '1039.png,giorgio armani\\n', '14211.png,vera wang\\n', '9465.png,missoni\\n', '6756.png,carolina herrera\\n', '9471.png,marc by marc jacobs\\n', '14205.png,vera wang\\n', '10063.png,missoni\\n', '8009.png,marc jacobs\\n', '9317.png,miu miu\\n', '14563.png,dries van noten\\n', '6024.png,missoni\\n', '1978.png,yves saint laurent\\n', '10936.png,jason wu\\n', '4800.png,derek lam\\n', '8784.png,etro\\n', '6971.png,jil sander\\n', '2295.png,yves saint laurent\\n', '3835.png,vera wang\\n', '3821.png,emporio armani\\n', '8974.png,miu miu\\n', '1788.png,vera wang\\n', '2281.png,marni\\n', '5061.png,jason wu\\n', '11157.png,yves saint laurent\\n', '9883.png,derek lam\\n', '13740.png,marc jacobs\\n', '8223.png,chado ralph rucci\\n', '7110.png,jil sander\\n', '4419.png,marc by marc jacobs\\n', '5707.png,jil sander\\n', '2068.png,etro\\n', '5713.png,jil sander\\n', '9129.png,missoni\\n', '8237.png,miu miu\\n', '11143.png,christian dior\\n', '12462.png,tommy hilfiger\\n', '14013.png,carolina herrera\\n', '9667.png,miu miu\\n', '4425.png,miu miu\\n', '1549.png,jil sander\\n', '6226.png,miu miu\\n', '14761.png,dolce & gabbana\\n', '7138.png,missoni\\n', '2040.png,ralph lauren\\n', '10507.png,etro\\n', '10261.png,bottega veneta\\n', '963.png,armani prive\\n', '6540.png,marc jacobs\\n', '9673.png,alexander wang\\n', '14007.png,tommy hilfiger\\n', '750.png,giorgio armani\\n', '13783.png,emporio armani\\n', '988.png,armani prive\\n', '13797.png,alexander wang\\n', '12489.png,alexander wang\\n', '4380.png,gucci\\n', '6597.png,etro\\n', '13967.png,miu miu\\n', '11816.png,jil sander\\n', '3389.png,bottega veneta\\n', '11802.png,emporio armani\\n', '7845.png,marc jacobs\\n', '778.png,giorgio armani\\n', '9868.png,marni\\n', '1373.png,derek lam\\n', '13634.png,etro\\n', '7702.png,jason wu\\n', '8357.png,emporio armani\\n', '9049.png,missoni\\n', '2108.png,alexander wang\\n', '11989.png,miu miu\\n', '4579.png,miu miu\\n', '11751.png,yves saint laurent\\n', '13146.png,chado ralph rucci\\n', '1367.png,marc by marc jacobs\\n', '13620.png,ralph lauren\\n', '3570.png,emporio armani\\n', '6420.png,miu miu\\n', '14167.png,gucci\\n', '3558.png,emporio armani\\n', '4237.png,yves saint laurent\\n', '5129.png,jil sander\\n', '2646.png,missoni\\n', '5897.png,jil sander\\n', '12270.png,dries van noten\\n', '7058.png,derek lam\\n', '12264.png,chado ralph rucci\\n', '6352.png,derek lam\\n', '14615.png,carolina herrera\\n', '2134.png,armani prive\\n', '4545.png,miu miu\\n', '4223.png,gucci\\n', '57.png,armani prive\\n', '9707.png,derek lam\\n', '8419.png,jil sander\\n', '817.png,giorgio armani\\n', '9934.png,jil sander\\n', '624.png,giorgio armani\\n', '7919.png,marc jacobs\\n', '5868.png,dolce & gabbana\\n', '10498.png,emporio armani\\n', '8394.png,chado ralph rucci\\n', '14832.png,alexander wang\\n', '630.png,giorgio armani\\n', '2685.png,etro\\n', '618.png,giorgio armani\\n', '11962.png,missoni\\n', '5698.png,jil sander\\n', '6391.png,marni\\n', '3000.png,jason wu\\n', '10881.png,miu miu\\n', '5471.png,carolina herrera\\n', '383.png,bottega veneta\\n', '7266.png,missoni\\n', '8155.png,miu miu\\n', '7500.png,vera wang\\n', '13436.png,yves saint laurent\\n', '1171.png,dries van noten\\n', '11221.png,alexander wang\\n', '8627.png,giorgio armani\\n', '1165.png,armani prive\\n', '13344.png,alexander wang\\n', '7272.png,chado ralph rucci\\n', '3014.png,jil sander\\n', '11553.png,chado ralph rucci\\n', '14403.png,giorgio armani\\n', '8169.png,bottega veneta\\n', '2444.png,derek lam\\n', '4035.png,jil sander\\n', '6622.png,dolce & gabbana\\n', '12700.png,jil sander\\n', '14371.png,marc by marc jacobs\\n', '9505.png,derek lam\\n', '6636.png,missoni\\n', '10117.png,armani prive\\n', '11209.png,alexander wang\\n', '4747.png,marni\\n', '12066.png,vera wang\\n', '9288.png,chado ralph rucci\\n', '11584.png,missoni\\n', '12933.png,carolina herrera\\n', '426.png,bottega veneta\\n', '10856.png,dolce & gabbana\\n', '8182.png,marc by marc jacobs\\n', '1818.png,dries van noten\\n', '12099.png,vera wang\\n', '13387.png,yves saint laurent\\n', '354.png,bottega veneta\\n', '4948.png,marc jacobs\\n', '4790.png,jason wu\\n', '6187.png,marni\\n', '2487.png,derek lam\\n', '3941.png,yves saint laurent\\n', '2493.png,carolina herrera\\n', '1824.png,marc jacobs\\n', '6193.png,marni\\n', '4784.png,dries van noten\\n', '6185.png,jil sander\\n', '4792.png,chado ralph rucci\\n', '5498.png,carolina herrera\\n', '1826.png,carolina herrera\\n', '12919.png,giorgio armani\\n', '3957.png,armani prive\\n', '6807.png,marc jacobs\\n', '11586.png,miu miu\\n', '4976.png,dries van noten\\n', '10698.png,alexander wang\\n', '13391.png,yves saint laurent\\n', '342.png,jason wu\\n', '13385.png,yves saint laurent\\n', '8180.png,miu miu\\n', '11592.png,yves saint laurent\\n', '430.png,bottega veneta\\n', '12925.png,armani prive\\n', '14367.png,carolina herrera\\n', '3980.png,marni\\n', '10101.png,chado ralph rucci\\n', '10667.png,chado ralph rucci\\n', '4989.png,dries van noten\\n', '2320.png,derek lam\\n', '11579.png,missoni\\n', '6152.png,derek lam\\n', '10673.png,vera wang\\n', '3994.png,yves saint laurent\\n', '10115.png,chado ralph rucci\\n', '2452.png,derek lam\\n', '8619.png,chado ralph rucci\\n', '13434.png,alexander wang\\n', '1173.png,yves saint laurent\\n', '8157.png,jil sander\\n', '13352.png,ralph lauren\\n', '3002.png,chado ralph rucci\\n', '2308.png,alexander wang\\n', '10897.png,gucci\\n', '12058.png,marc by marc jacobs\\n', '7270.png,emporio armani\\n', '395.png,alexander wang\\n', '13420.png,yves saint laurent\\n', '1167.png,armani prive\\n', '7516.png,givenchy\\n', '8625.png,chado ralph rucci\\n', '3770.png,emporio armani\\n', '11237.png,miu miu\\n', '5856.png,jil sander\\n', '82.png,marni\\n', '2693.png,yves saint laurent\\n', '96.png,yves saint laurent\\n', '6393.png,marc jacobs\\n', '5842.png,jil sander\\n', '13193.png,ralph lauren\\n', '2863.png,dries van noten\\n', '9936.png,dries van noten\\n', '632.png,chado ralph rucci\\n', '2877.png,giambattista valli\\n', '11948.png,miu miu\\n', '8382.png,givenchy\\n', '6344.png,bottega veneta\\n', '2122.png,jason wu\\n', '10465.png,vera wang\\n', '10303.png,vera wang\\n', '4235.png,alexander mcqueen\\n', '801.png,giorgio armani\\n', '6422.png,derek lam\\n', '41.png,marc jacobs\\n', '2650.png,miu miu\\n', '10317.png,chado ralph rucci\\n', '11009.png,missoni\\n', '7.png,carolina herrera\\n', '4547.png,marc jacobs\\n', '5659.png,chado ralph rucci\\n', '12266.png,lanvin\\n', '11747.png,etro\\n', '7066.png,bottega veneta\\n', '8433.png,jil sander\\n', '14159.png,etro\\n', '13636.png,yves saint laurent\\n', '2678.png,giambattista valli\\n', '829.png,chado ralph rucci\\n', '1365.png,etro\\n', '11753.png,armani prive\\n', '11814.png,miu miu\\n', '13965.png,marc jacobs\\n', '7853.png,jason wu\\n', '6595.png,chado ralph rucci\\n', '4382.png,jil sander\\n', '13971.png,miu miu\\n', '5936.png,jil sander\\n', '11800.png,vera wang\\n', '10288.png,chado ralph rucci\\n', '13959.png,carolina herrera\\n', '9842.png,miu miu\\n', '13781.png,derek lam\\n', '9856.png,marc jacobs\\n', '746.png,chado ralph rucci\\n', '13018.png,carolina herrera\\n', '2056.png,derek lam\\n', '4427.png,derek lam\\n', '3348.png,etro\\n', '11169.png,missoni\\n', '2730.png,tommy hilfiger\\n', '14011.png,jason wu\\n', '7648.png,marc jacobs\\n', '975.png,armani prive\\n', '12460.png,alexander wang\\n', '6542.png,jil sander\\n', '9671.png,jason wu\\n', '961.png,armani prive\\n', '14763.png,dolce & gabbana\\n', '3374.png,etro\\n', '7112.png,jil sander\\n', '1563.png,missoni\\n', '791.png,etro\\n', '1205.png,marc by marc jacobs\\n', '13742.png,etro\\n', '9659.png,derek lam\\n', '949.png,armani prive\\n', '11141.png,alexander wang\\n', '3406.png,derek lam\\n', '14039.png,gucci\\n', '8553.png,armani prive\\n', '12448.png,alexander wang\\n', '8235.png,miu miu\\n', '6218.png,missoni\\n', '10539.png,vera wang\\n', '11627.png,chado ralph rucci\\n', '3189.png,vera wang\\n', '2283.png,alexander wang\\n', '3823.png,emporio armani\\n', '13583.png,jil sander\\n', '550.png,bottega veneta\\n', '6973.png,jil sander\\n', '10934.png,giorgio armani\\n', '236.png,miu miu\\n', '544.png,bottega veneta\\n', '6967.png,jil sander\\n', '8779.png,bottega veneta\\n', '14213.png,vera wang\\n', '6754.png,carolina herrera\\n', '4625.png,miu miu\\n', '10713.png,carolina herrera\\n', '2254.png,alexander wang\\n', '1749.png,chado ralph rucci\\n', '14561.png,etro\\n', '7338.png,miu miu\\n', '4631.png,giorgio armani\\n', '2240.png,jason wu\\n', '2526.png,vera wang\\n', '10049.png,jil sander\\n', '5261.png,yves saint laurent\\n', '7476.png,marc jacobs\\n', '13540.png,etro\\n', '14549.png,vera wang\\n', '12138.png,vera wang\\n', '4619.png,givenchy\\n', '5507.png,missoni\\n', '3162.png,jason wu\\n', '13232.png,missoni\\n', '1775.png,tommy hilfiger\\n', '8037.png,yves saint laurent\\n', '8751.png,dolce & gabbana\\n', '587.png,jason wu\\n', '5512.png,vera wang\\n', '3163.png,jason wu\\n', '3605.png,emporio armani\\n', '1012.png,alexander wang\\n', '13555.png,yves saint laurent\\n', '12893.png,carolina herrera\\n', '8750.png,carolina herrera\\n', '12887.png,carolina herrera\\n', '7477.png,yves saint laurent\\n', '10048.png,marc jacobs\\n', '4618.png,jil sander\\n', '12139.png,chado ralph rucci\\n', '13227.png,marc jacobs\\n', '10706.png,vera wang\\n', '1990.png,marc by marc jacobs\\n', '9314.png,etro\\n', '14560.png,dolce & gabbana\\n', '14206.png,gucci\\n', '3639.png,vera wang\\n', '4156.png,versace\\n', '10060.png,marc jacobs\\n', '2533.png,jason wu\\n', '8778.png,alexander mcqueen\\n', '12105.png,emporio armani\\n', '6033.png,bottega veneta\\n', '545.png,derek lam\\n', '13596.png,yves saint laurent\\n', '12850.png,missoni\\n', '11381.png,christian dior\\n', '11395.png,marc by marc jacobs\\n', '8787.png,giorgio armani\\n', '6972.png,jil sander\\n', '12844.png,carolina herrera\\n', '1953.png,marni\\n', '3822.png,emporio armani\\n', '6796.png,emporio armani\\n', '3188.png,jason wu\\n', '2296.png,alexander wang\\n', '1210.png,marc by marc jacobs\\n', '2719.png,missoni\\n', '4368.png,giambattista valli\\n', '3407.png,etro\\n', '11140.png,yves saint laurent\\n', '10538.png,jason wu\\n', '13031.png,carolina herrera\\n', '1576.png,vera wang\\n', '13025.png,carolina herrera\\n', '8220.png,miu miu\\n', '3375.png,jil sander\\n', '5704.png,jil sander\\n', '948.png,armani prive\\n', '1204.png,jil sander\\n', '2725.png,gucci\\n', '9670.png,derek lam\\n', '9116.png,christian dior\\n', '6225.png,jil sander\\n', '4432.png,marc by marc jacobs\\n', '10504.png,yves saint laurent\\n', '2043.png,bottega veneta\\n', '3349.png,giorgio armani\\n', '5738.png,jil sander\\n', '2057.png,vera wang\\n', '14776.png,dolce & gabbana\\n', '9102.png,christian dior\\n', '6231.png,derek lam\\n', '12307.png,miu miu\\n', '9664.png,derek lam\\n', '1238.png,marc by marc jacobs\\n', '2902.png,bottega veneta\\n', '11183.png,alexander wang\\n', '13794.png,miu miu\\n', '747.png,giorgio armani\\n', '9857.png,jil sander\\n', '11829.png,marc jacobs\\n', '13780.png,missoni\\n', '8585.png,marc jacobs\\n', '11197.png,alexander wang\\n', '5937.png,jil sander\\n', '2094.png,alexander wang\\n', '5923.png,jil sander\\n', '7715.png,alexander wang\\n', '828.png,giorgio armani\\n', '3215.png,jason wu\\n', '8340.png,jil sander\\n', '7067.png,jil sander\\n', '3201.png,jason wu\\n', '11746.png,bottega veneta\\n', '4208.png,derek lam\\n', '2679.png,yves saint laurent\\n', '13637.png,yves saint laurent\\n', '2889.png,marc jacobs\\n', '10316.png,carolina herrera\\n', '814.png,marc by marc jacobs\\n', '7729.png,alexander wang\\n', '12501.png,jason wu\\n', '14616.png,dolce & gabbana\\n', '9062.png,derek lam\\n', '13179.png,ralph lauren\\n', '10470.png,emporio armani\\n', '8368.png,armani prive\\n', '9076.png,missoni\\n', '14602.png,alexander wang\\n', '6345.png,carolina herrera\\n', '9710.png,derek lam\\n', '12515.png,missoni\\n', '4234.png,derek lam\\n', '2876.png,giambattista valli\\n', '13838.png,marc by marc jacobs\\n', '633.png,giorgio armani\\n', '13186.png,etro\\n', '8383.png,givenchy\\n', '13192.png,carolina herrera\\n', '141.png,bottega veneta\\n', '9937.png,alexander mcqueen\\n', '627.png,giorgio armani\\n', '2862.png,giambattista valli\\n', '2692.png,giambattista valli\\n', '4585.png,missoni\\n', '5843.png,jil sander\\n', '6392.png,derek lam\\n', '14819.png,giorgio armani\\n', '4591.png,marc jacobs\\n', '5857.png,missoni\\n', '3598.png,emporio armani\\n', '8142.png,miu miu\\n', '7271.png,jason wu\\n', '8624.png,giorgio armani\\n', '7517.png,marc jacobs\\n', '13421.png,chado ralph rucci\\n', '8630.png,jason wu\\n', '7503.png,jil sander\\n', '1172.png,armani prive\\n', '13435.png,yves saint laurent\\n', '5314.png,giorgio armani\\n', '11222.png,alexander wang\\n', '3765.png,emporio armani\\n', '11544.png,emporio armani\\n', '5472.png,missoni\\n', '13353.png,missoni\\n', '7265.png,marni\\n', '2335.png,alexander wang\\n', '10672.png,etro\\n', '12065.png,etro\\n', '8618.png,alexander wang\\n', '14372.png,alexander wang\\n', '9506.png,derek lam\\n', '2453.png,miu miu\\n', '10114.png,chado ralph rucci\\n', '10100.png,missoni\\n', '3981.png,versace\\n', '12717.png,derek lam\\n', '6621.png,alexander wang\\n', '9512.png,derek lam\\n', '1628.png,alexander wang\\n', '2321.png,marc jacobs\\n', '4988.png,derek lam\\n', '4963.png,dries van noten\\n', '357.png,bottega veneta\\n', '13384.png,marc by marc jacobs\\n', '431.png,bottega veneta\\n', '12930.png,marc jacobs\\n', '425.png,bottega veneta\\n', '6806.png,tommy hilfiger\\n', '343.png,bottega veneta\\n', '13390.png,yves saint laurent\\n', '10841.png,dries van noten\\n', '6190.png,armani prive\\n', '4787.png,dries van noten\\n', '1199.png,marc jacobs\\n', '8803.png,alexander mcqueen\\n', '8817.png,dolce & gabbana\\n', '2484.png,vera wang\\n', '1833.png,carolina herrera\\n', '6184.png,lanvin\\n', '10845.png,etro\\n', '13394.png,alexander wang\\n', '421.png,bottega veneta\\n', '6816.png,carolina herrera\\n', '435.png,miu miu\\n', '353.png,bottega veneta\\n', '8185.png,miu miu\\n', '10689.png,vera wang\\n', '6180.png,armani prive\\n', '10879.png,chado ralph rucci\\n', '5489.png,yves saint laurent\\n', '12908.png,missoni\\n', '3952.png,armani prive\\n', '1823.png,bottega veneta\\n', '8152.png,miu miu\\n', '12049.png,vera wang\\n', '14438.png,christian dior\\n', '11226.png,missoni\\n', '10138.png,chado ralph rucci\\n', '1176.png,armani prive\\n', '8634.png,emporio armani\\n', '11232.png,missoni\\n', '3013.png,jason wu\\n', '13343.png,jil sander\\n', '390.png,bottega veneta\\n', '9270.png,missoni\\n', '6143.png,chado ralph rucci\\n', '14362.png,marc by marc jacobs\\n', '6625.png,marc jacobs\\n', '4032.png,versace\\n', '10104.png,marc by marc jacobs\\n', '3985.png,versace\\n', '3749.png,emporio armani\\n', '10110.png,chado ralph rucci\\n', '3991.png,alexander wang\\n', '9502.png,vera wang\\n', '14376.png,marc by marc jacobs\\n', '12707.png,derek lam\\n', '7249.png,missoni\\n', '14410.png,dolce & gabbana\\n', '6157.png,marni\\n', '4998.png,dries van noten\\n', '11568.png,miu miu\\n', '4740.png,miu miu\\n', '623.png,giorgio armani\\n', '13828.png,yves saint laurent\\n', '12288.png,jil sander\\n', '8393.png,alexander wang\\n', '145.png,marc jacobs\\n', '11959.png,etro\\n', '14835.png,emporio armani\\n', '8387.png,givenchy\\n', '13182.png,giambattista valli\\n', '2872.png,versace\\n', '11965.png,etro\\n', '5853.png,jil sander\\n', '6396.png,vera wang\\n', '4581.png,missoni\\n', '5847.png,jil sander\\n', '3588.png,emporio armani\\n', '7936.png,alexander wang\\n', '11024.png,alexander wang\\n', '186.png,carolina herrera\\n', '8350.png,jil sander\\n', '3211.png,bottega veneta\\n', '10448.png,etro\\n', '5660.png,jil sander\\n', '11030.png,missoni\\n', '4218.png,jil sander\\n', '8422.png,jil sander\\n', '7711.png,missoni\\n', '1360.png,chado ralph rucci\\n', '13627.png,yves saint laurent\\n', '14148.png,dries van noten\\n', '11018.png,marni\\n', '2899.png,chado ralph rucci\\n', '804.png,giorgio armani\\n', '6427.png,marc by marc jacobs\\n', '9714.png,derek lam\\n', '12277.png,miu miu\\n', '5890.png,jil sander\\n', '2127.png,alexander wang\\n', '10460.png,chado ralph rucci\\n', '2.png,miu miu\\n', '5884.png,jil sander\\n', '2133.png,etro\\n', '9066.png,alexander wang\\n', '8378.png,alexander wang\\n', '12505.png,chado ralph rucci\\n', '11193.png,alexander wang\\n', '757.png,giorgio armani\\n', '11839.png,alexander wang\\n', '9853.png,chado ralph rucci\\n', '10299.png,miu miu\\n', '6590.png,bottega veneta\\n', '11811.png,marni\\n', '2090.png,alexander wang\\n', '6584.png,dolce & gabbana\\n', '794.png,giorgio armani\\n', '1200.png,jil sander\\n', '9884.png,carolina herrera\\n', '3417.png,jil sander\\n', '5700.png,jil sander\\n', '10528.png,vera wang\\n', '3371.png,vera wang\\n', '6209.png,marni\\n', '8230.png,miu miu\\n', '11622.png,etro\\n', '3365.png,jil sander\\n', '11144.png,missoni\\n', '9648.png,derek lam\\n', '780.png,giorgio armani\\n', '958.png,armani prive\\n', '2735.png,giambattista valli\\n', '10272.png,jil sander\\n', '6553.png,dolce & gabbana\\n', '9660.png,derek lam\\n', '7895.png,jil sander\\n', '12465.png,jil sander\\n', '970.png,armani prive\\n', '8218.png,miu miu\\n', '2053.png,vera wang\\n', '10514.png,etro\\n', '5728.png,jil sander\\n', '10500.png,etro\\n', '4436.png,etro\\n', '12317.png,alexander wang\\n', '14766.png,miu miu\\n', '964.png,armani prive\\n', '11178.png,marc jacobs\\n', '2721.png,giambattista valli\\n', '4807.png,marc by marc jacobs\\n', '233.png,marc by marc jacobs\\n', '555.png,bottega veneta\\n', '12840.png,missoni\\n', '8783.png,alexander mcqueen\\n', '12698.png,derek lam\\n', '11385.png,miu miu\\n', '13592.png,yves saint laurent\\n', '541.png,bottega veneta\\n', '12854.png,carolina herrera\\n', '9489.png,miu miu\\n', '4813.png,jason wu\\n', '2292.png,jason wu\\n', '6792.png,yves saint laurent\\n', '12868.png,derek lam\\n', '4191.png,vera wang\\n', '3826.png,jason wu\\n', '2286.png,etro\\n', '9338.png,derek lam\\n', '8026.png,marc jacobs\\n', '5264.png,givenchy\\n', '8740.png,jason wu\\n', '1002.png,armani prive\\n', '596.png,chado ralph rucci\\n', '12883.png,carolina herrera\\n', '8754.png,carolina herrera\\n', '13551.png,chado ralph rucci\\n', '1016.png,armani prive\\n', '6779.png,jason wu\\n', '5270.png,jason wu\\n', '4608.png,marni\\n', '2279.png,alexander wang\\n', '13237.png,ralph lauren\\n', '1770.png,missoni\\n', '8032.png,carolina herrera\\n', '9304.png,yves saint laurent\\n', '1758.png,tommy hilfiger\\n', '1980.png,chado ralph rucci\\n', '6989.png,alexander wang\\n', '13579.png,alexander wang\\n', '14216.png,gucci\\n', '10070.png,chado ralph rucci\\n', '4146.png,vera wang\\n', '4152.png,lanvin\\n', '8768.png,carolina herrera\\n', '14564.png,dolce & gabbana\\n', '6023.png,givenchy\\n', '10702.png,carolina herrera\\n', '2245.png,alexander wang\\n', '14203.png,gucci\\n', '12672.png,derek lam\\n', '4153.png,gucci\\n', '2244.png,carolina herrera\\n', '4635.png,marc jacobs\\n', '12114.png,miu miu\\n', '7328.png,bottega veneta\\n', '1759.png,miu miu\\n', '2250.png,alexander wang\\n', '11409.png,yves saint laurent\\n', '4147.png,jil sander\\n', '3628.png,emporio armani\\n', '13578.png,yves saint laurent\\n', '5271.png,missoni\\n', '13550.png,yves saint laurent\\n', '7466.png,marni\\n', '5517.png,carolina herrera\\n', '2278.png,vera wang\\n', '3172.png,bottega veneta\\n', '7314.png,bottega veneta\\n', '1765.png,missoni\\n', '13222.png,carolina herrera\\n', '13544.png,yves saint laurent\\n', '3614.png,emporio armani\\n', '7499.png,yves saint laurent\\n', '6787.png,yves saint laurent\\n', '3199.png,emporio armani\\n', '2287.png,alexander wang\\n', '6793.png,dolce & gabbana\\n', '8966.png,missoni\\n', '8796.png,marc jacobs\\n', '12855.png,chado ralph rucci\\n', '4812.png,emporio armani\\n', '12699.png,chado ralph rucci\\n', '13587.png,missoni\\n', '6220.png,marni\\n', '13008.png,carolina herrera\\n', '12316.png,alexander wang\\n', '3358.png,etro\\n', '10267.png,chado ralph rucci\\n', '2720.png,armani prive\\n', '12470.png,carolina herrera\\n', '1229.png,marc by marc jacobs\\n', '9675.png,giambattista valli\\n', '971.png,armani prive\\n', '7894.png,etro\\n', '9661.png,derek lam\\n', '6552.png,chado ralph rucci\\n', '10273.png,ralph lauren\\n', '4423.png,jason wu\\n', '2052.png,giorgio armani\\n', '6234.png,jil sander\\n', '3364.png,etro\\n', '5715.png,jil sander\\n', '13034.png,carolina herrera\\n', '1573.png,marc jacobs\\n', '7102.png,jil sander\\n', '7664.png,jil sander\\n', '13752.png,miu miu\\n', '781.png,carolina herrera\\n', '11151.png,emporio armani\\n', '12458.png,balmain\\n', '13746.png,jil sander\\n', '1201.png,carolina herrera\\n', '1567.png,givenchy\\n', '8225.png,jil sander\\n', '5701.png,jil sander\\n', '2085.png,alexander wang\\n', '7843.png,marc jacobs\\n', '13975.png,marni\\n', '6591.png,emporio armani\\n', '2091.png,alexander wang\\n', '2907.png,derek lam\\n', '13791.png,marc jacobs\\n', '756.png,bottega veneta\\n', '8580.png,bottega veneta\\n', '9846.png,missoni\\n', '11192.png,missoni\\n', '10475.png,vera wang\\n', '10313.png,chado ralph rucci\\n', '9701.png,derek lam\\n', '12510.png,jason wu\\n', '805.png,derek lam\\n', '2898.png,chado ralph rucci\\n', '11019.png,etro\\n', '5649.png,missoni\\n', '10461.png,yves saint laurent\\n', '2126.png,vera wang\\n', '5891.png,jil sander\\n', '13168.png,ralph lauren\\n', '9073.png,missoni\\n', '6340.png,missoni\\n', '5661.png,versace\\n', '10449.png,alexander wang\\n', '11757.png,marc jacobs\\n', '7076.png,jil sander\\n', '6368.png,vera wang\\n', '14149.png,gucci\\n', '13626.png,yves saint laurent\\n', '1361.png,miu miu\\n', '12538.png,dolce & gabbana\\n', '8423.png,givenchy\\n', '4219.png,jil sander\\n', '2668.png,emporio armani\\n', '8437.png,etro\\n', '1413.png,marc by marc jacobs\\n', '7089.png,jil sander\\n', '14808.png,dries van noten\\n', '92.png,vera wang\\n', '5852.png,bottega veneta\\n', '13829.png,marc jacobs\\n', '622.png,giorgio armani\\n', '2867.png,giambattista valli\\n', '11958.png,vera wang\\n', '144.png,marni\\n', '14820.png,dolce & gabbana\\n', '12706.png,dries van noten\\n', '13418.png,yves saint laurent\\n', '9503.png,derek lam\\n', '4027.png,christian dior\\n', '2330.png,alexander wang\\n', '10677.png,missoni\\n', '6156.png,givenchy\\n', '1639.png,miu miu\\n', '9271.png,jil sander\\n', '14405.png,alexander wang\\n', '10105.png,marc by marc jacobs\\n', '4033.png,versace\\n', '8609.png,marc by marc jacobs\\n', '6624.png,carolina herrera\\n', '9517.png,derek lam\\n', '14363.png,marc by marc jacobs\\n', '5305.png,yves saint laurent\\n', '11233.png,derek lam\\n', '13342.png,ralph lauren\\n', '8147.png,miu miu\\n', '3012.png,jason wu\\n', '3006.png,jason wu\\n', '2318.png,alexander wang\\n', '385.png,bottega veneta\\n', '13356.png,alexander mcqueen\\n', '12048.png,miu miu\\n', '8153.png,ralph lauren\\n', '13430.png,yves saint laurent\\n', '5311.png,dries van noten\\n', '3760.png,emporio armani\\n', '3953.png,versace\\n', '2495.png,tommy hilfiger\\n', '8806.png,miu miu\\n', '6195.png,marc by marc jacobs\\n', '5488.png,versace\\n', '4796.png,dries van noten\\n', '6181.png,marni\\n', '12909.png,carolina herrera\\n', '434.png,bottega veneta\\n', '6817.png,carolina herrera\\n', '4966.png,dries van noten\\n', '11596.png,emporio armani\\n', '8184.png,alexander wang\\n', '346.png,bottega veneta\\n', '13395.png,yves saint laurent\\n', '420.png,yves saint laurent\\n', '12935.png,carolina herrera\\n', '13383.png,yves saint laurent\\n', '9298.png,gucci\\n', '12923.png,carolina herrera\\n', '8838.png,alexander mcqueen\\n', '11580.png,miu miu\\n', '10846.png,marc jacobs\\n', '13397.png,yves saint laurent\\n', '1808.png,yves saint laurent\\n', '12089.png,vera wang\\n', '4958.png,dries van noten\\n', '7289.png,carolina herrera\\n', '1820.png,giambattista valli\\n', '3951.png,yves saint laurent\\n', '4794.png,dries van noten\\n', '11557.png,marc by marc jacobs\\n', '3010.png,jason wu\\n', '10891.png,dries van noten\\n', '8145.png,tommy hilfiger\\n', '14349.png,giorgio armani\\n', '1161.png,armani prive\\n', '9529.png,derek lam\\n', '13432.png,yves saint laurent\\n', '8151.png,miu miu\\n', '5475.png,jason wu\\n', '11543.png,marc jacobs\\n', '3004.png,jason wu\\n', '12062.png,missoni\\n', '6154.png,alexander wang\\n', '9267.png,chado ralph rucci\\n', '2332.png,alexander wang\\n', '10675.png,emporio armani\\n', '4025.png,yves saint laurent\\n', '10113.png,chado ralph rucci\\n', '9501.png,missoni\\n', '14375.png,marc by marc jacobs\\n', '10107.png,chado ralph rucci\\n', '2326.png,alexander wang\\n', '12076.png,etro\\n', '6140.png,alexander wang\\n', '9273.png,tommy hilfiger\\n', '7909.png,marc jacobs\\n', '634.png,giorgio armani\\n', '5878.png,jil sander\\n', '13195.png,ralph lauren\\n', '2865.png,vera wang\\n', '7935.png,marc jacobs\\n', '90.png,chado ralph rucci\\n', '5844.png,jil sander\\n', '5688.png,jil sander\\n', '13817.png,alexander wang\\n', '2681.png,giambattista valli\\n', '11033.png,armani prive\\n', '7712.png,miu miu\\n', '8347.png,alexander wang\\n', '5663.png,jil sander\\n', '11741.png,etro\\n', '2118.png,yves saint laurent\\n', '11999.png,etro\\n', '5677.png,jil sander\\n', '1411.png,armani prive\\n', '7060.png,missoni\\n', '7706.png,yves saint laurent\\n', '8435.png,givenchy\\n', '11027.png,marc jacobs\\n', '813.png,giorgio armani\\n', '12506.png,derek lam\\n', '9703.png,alexander wang\\n', '10311.png,alexander mcqueen\\n', '3548.png,marc jacobs\\n', '1.png,lanvin\\n', '5887.png,jil sander\\n', '10477.png,chado ralph rucci\\n', '9065.png,marc by marc jacobs\\n', '12260.png,missoni\\n', '9071.png,missoni\\n', '10463.png,vera wang\\n', '10305.png,chado ralph rucci\\n', '4233.png,etro\\n', '8409.png,jason wu\\n', '807.png,giorgio armani\\n', '9717.png,derek lam\\n', '998.png,alexander wang\\n', '8596.png,givenchy\\n', '740.png,giorgio armani\\n', '2905.png,giambattista valli\\n', '11184.png,armani prive\\n', '7841.png,marc jacobs\\n', '5930.png,jil sander\\n', '3399.png,etro\\n', '11806.png,marc by marc jacobs\\n', '11812.png,chado ralph rucci\\n', '9878.png,dolce & gabbana\\n', '768.png,giorgio armani\\n', '11147.png,marni\\n', '13750.png,miu miu\\n', '783.png,giorgio armani\\n', '8555.png,bottega veneta\\n', '8233.png,carolina herrera\\n', '12328.png,chado ralph rucci\\n', '13036.png,carolina herrera\\n', '5717.png,dries van noten\\n', '11621.png,miu miu\\n', '11635.png,alexander wang\\n', '7114.png,jil sander\\n', '8227.png,miu miu\\n', '13022.png,carolina herrera\\n', '1565.png,armani prive\\n', '8541.png,alexander wang\\n', '14003.png,bottega veneta\\n', '7882.png,marc jacobs\\n', '12472.png,emporio armani\\n', '967.png,yves saint laurent\\n', '2722.png,miu miu\\n', '10503.png,vera wang\\n', '2044.png,bottega veneta\\n', '12314.png,alexander wang\\n', '6222.png,armani prive\\n', '10517.png,etro\\n', '11609.png,etro\\n', '4421.png,miu miu\\n', '9663.png,derek lam\\n', '6550.png,carolina herrera\\n', '1968.png,derek lam\\n', '14598.png,vera wang\\n', '4810.png,jason wu\\n', '11386.png,chado ralph rucci\\n', '3819.png,jason wu\\n', '10098.png,marc jacobs\\n', '542.png,bottega veneta\\n', '8794.png,jason wu\\n', '13585.png,yves saint laurent\\n', '12843.png,carolina herrera\\n', '11392.png,miu miu\\n', '4804.png,miu miu\\n', '230.png,jil sander\\n', '218.png,marc by marc jacobs\\n', '8970.png,christian dior\\n', '6791.png,miu miu\\n', '6949.png,jil sander\\n', '3164.png,jason wu\\n', '1773.png,tommy hilfiger\\n', '581.png,dries van noten\\n', '13552.png,yves saint laurent\\n', '12894.png,carolina herrera\\n', '5273.png,ralph lauren\\n', '11345.png,jil sander\\n', '2508.png,yves saint laurent\\n', '3616.png,emporio armani\\n', '4179.png,etro\\n', '8743.png,jason wu\\n', '7470.png,armani prive\\n', '595.png,bottega veneta\\n', '13546.png,yves saint laurent\\n', '1767.png,dries van noten\\n', '13220.png,ralph lauren\\n', '8025.png,derek lam\\n', '10729.png,vera wang\\n', '5501.png,missoni\\n', '14567.png,dries van noten\\n', '4637.png,miu miu\\n', '2246.png,vera wang\\n', '10067.png,missoni\\n', '11379.png,marc jacobs\\n', '4151.png,chado ralph rucci\\n', '12670.png,emporio armani\\n', '14201.png,etro\\n', '2534.png,jason wu\\n', '10715.png,derek lam\\n', '14573.png,dolce & gabbana\\n', '4144.png,versace\\n', '2535.png,marc jacobs\\n', '14214.png,etro\\n', '12665.png,chado ralph rucci\\n', '6035.png,missoni\\n', '2253.png,missoni\\n', '4622.png,missoni\\n', '3159.png,derek lam\\n', '4636.png,marc jacobs\\n', '1996.png,alexander wang\\n', '13209.png,missoni\\n', '6747.png,jason wu\\n', '11378.png,chado ralph rucci\\n', '10066.png,missoni\\n', '14228.png,gucci\\n', '13547.png,miu miu\\n', '12659.png,derek lam\\n', '7471.png,marc jacobs\\n', '5500.png,bottega veneta\\n', '11436.png,yves saint laurent\\n', '1766.png,tommy hilfiger\\n', '7303.png,marc by marc jacobs\\n', '13235.png,alexander wang\\n', '5514.png,etro\\n', '3165.png,chado ralph rucci\\n', '11422.png,alexander wang\\n', '12895.png,carolina herrera\\n', '9448.png,tommy hilfiger\\n', '13553.png,yves saint laurent\\n', '6948.png,alexander wang\\n', '3830.png,vera wang\\n', '2290.png,alexander wang\\n', '1941.png,derek lam\\n', '3824.png,emporio armani\\n', '6974.png,dolce & gabbana\\n', '12842.png,carolina herrera\\n', '8959.png,derek lam\\n', '10927.png,vera wang\\n', '14599.png,dolce & gabbana\\n', '12856.png,emporio armani\\n', '13590.png,yves saint laurent\\n', '11387.png,jason wu\\n', '11608.png,missoni\\n', '6237.png,alexander wang\\n', '14770.png,miu miu\\n', '9104.png,carolina herrera\\n', '7129.png,bottega veneta\\n', '12467.png,chado ralph rucci\\n', '7897.png,jil sander\\n', '9662.png,derek lam\\n', '10270.png,derek lam\\n', '3429.png,etro\\n', '6223.png,miu miu\\n', '4434.png,jil sander\\n', '2045.png,fendi\\n', '9138.png,marc by marc jacobs\\n', '7115.png,jil sander\\n', '5702.png,bottega veneta\\n', '5064.png,marni\\n', '11152.png,chado ralph rucci\\n', '9886.png,giorgio armani\\n', '13745.png,chado ralph rucci\\n', '13989.png,gucci\\n', '9892.png,miu miu\\n', '10258.png,chado ralph rucci\\n', '11146.png,marni\\n', '5716.png,jil sander\\n', '14758.png,dolce & gabbana\\n', '13037.png,carolina herrera\\n', '12329.png,chado ralph rucci\\n', '8232.png,miu miu\\n', '6592.png,carolina herrera\\n', '13962.png,carolina herrera\\n', '3398.png,etro\\n', '5931.png,jil sander\\n', '5919.png,jil sander\\n', '13786.png,jil sander\\n', '7868.png,marc jacobs\\n', '11191.png,alexander wang\\n', '2910.png,yves saint laurent\\n', '999.png,armani prive\\n', '2125.png,alexander wang\\n', '5892.png,marc by marc jacobs\\n', '9070.png,christian dior\\n', '9716.png,derek lam\\n', '10310.png,jason wu\\n', '52.png,missoni\\n', '812.png,armani prive\\n', '7049.png,jil sander\\n', '2131.png,alexander wang\\n', '5886.png,jil sander\\n', '7061.png,lanvin\\n', '12249.png,dolce & gabbana\\n', '5676.png,jil sander\\n', '3207.png,jason wu\\n', '4568.png,dolce & gabbana\\n', '11026.png,alexander wang\\n', '3561.png,emporio armani\\n', '5110.png,marc by marc jacobs\\n', '13631.png,yves saint laurent\\n', '1362.png,jil sander\\n', '11032.png,dolce & gabbana\\n', '5662.png,jil sander\\n', '8346.png,jason wu\\n', '4597.png,miu miu\\n', '11967.png,miu miu\\n', '85.png,missoni\\n', '1389.png,marc by marc jacobs\\n', '609.png,giorgio armani\\n', '2694.png,giambattista valli\\n', '14823.png,dolce & gabbana\\n', '621.png,giorgio armani\\n', '2864.png,chado ralph rucci\\n', '2870.png,miu miu\\n', '14189.png,chado ralph rucci\\n', '635.png,giorgio armani\\n', '8385.png,jil sander\\n', '11797.png,etro\\n', '5879.png,jil sander\\n', '10489.png,vera wang\\n', '10106.png,chado ralph rucci\\n', '2441.png,jason wu\\n', '11218.png,vera wang\\n', '4030.png,armani prive\\n', '9514.png,miu miu\\n', '6141.png,vera wang\\n', '3039.png,jason wu\\n', '10660.png,dries van noten\\n', '5448.png,carolina herrera\\n', '10674.png,vera wang\\n', '12063.png,vera wang\\n', '8178.png,miu miu\\n', '6633.png,jason wu\\n', '3993.png,miu miu\\n', '10112.png,derek lam\\n', '2455.png,jason wu\\n', '4024.png,miu miu\\n', '3763.png,vera wang\\n', '1606.png,alexander wang\\n', '7277.png,dries van noten\\n', '3011.png,chado ralph rucci\\n', '10648.png,vera wang\\n', '3777.png,emporio armani\\n', '7511.png,gucci\\n', '8622.png,marc jacobs\\n', '13427.png,yves saint laurent\\n', '3944.png,versace\\n', '4795.png,miu miu\\n', '4959.png,dries van noten\\n', '4781.png,derek lam\\n', '3950.png,dolce & gabbana\\n', '2496.png,marc by marc jacobs\\n', '8805.png,marni\\n', '423.png,bottega veneta\\n', '12936.png,carolina herrera\\n', '13396.png,jil sander\\n', '345.png,bottega veneta\\n', '4965.png,dries van noten\\n', '11595.png,versace\\n', '8187.png,miu miu\\n', '8839.png,chado ralph rucci\\n', '6814.png,chado ralph rucci\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('./data/result_3TTA.csv') as f4:\n",
    "    lines4 = f4.readlines()\n",
    "    print('csv_len: ',len(lines4))\n",
    "    print(lines4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
